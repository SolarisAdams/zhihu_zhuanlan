{
    "title": "python小哥哥", 
    "description": "", 
    "followers": [
        "https://www.zhihu.com/people/marcus-95-55", 
        "https://www.zhihu.com/people/xiao-yu-31-1-15", 
        "https://www.zhihu.com/people/luozhongbin", 
        "https://www.zhihu.com/people/jeff-30-97", 
        "https://www.zhihu.com/people/zhou-kai-xin-58", 
        "https://www.zhihu.com/people/hahahahahacn", 
        "https://www.zhihu.com/people/yu-shi-long-86", 
        "https://www.zhihu.com/people/li-li-7-80-42", 
        "https://www.zhihu.com/people/xing-fu-de-chang-feng", 
        "https://www.zhihu.com/people/wang-lin-29-77", 
        "https://www.zhihu.com/people/lu-qi-da", 
        "https://www.zhihu.com/people/sample-73", 
        "https://www.zhihu.com/people/xiao-zhi-112", 
        "https://www.zhihu.com/people/dxk-65", 
        "https://www.zhihu.com/people/yikun-yi", 
        "https://www.zhihu.com/people/xiao-jie-1895", 
        "https://www.zhihu.com/people/seek2find", 
        "https://www.zhihu.com/people/kathyluzheng", 
        "https://www.zhihu.com/people/chenxueming", 
        "https://www.zhihu.com/people/wang-yi-chang-66", 
        "https://www.zhihu.com/people/yang-guo-dong-66", 
        "https://www.zhihu.com/people/wsyy357", 
        "https://www.zhihu.com/people/tian-gao-yun-dan-54-94-78", 
        "https://www.zhihu.com/people/jia-liao-bo-he-de-chocolate", 
        "https://www.zhihu.com/people/xue-jiao-hu", 
        "https://www.zhihu.com/people/wangshanxu", 
        "https://www.zhihu.com/people/zhi-hu-9-15-9", 
        "https://www.zhihu.com/people/la-la-la-8-58-55", 
        "https://www.zhihu.com/people/chen-dian-xin-fei", 
        "https://www.zhihu.com/people/sha-bao-75-16", 
        "https://www.zhihu.com/people/kingpin-70", 
        "https://www.zhihu.com/people/tian-chang-78-35", 
        "https://www.zhihu.com/people/dong-li-cai-ju-36", 
        "https://www.zhihu.com/people/shui-hen-99-13", 
        "https://www.zhihu.com/people/kellen-84-15", 
        "https://www.zhihu.com/people/tao-tao-38-56-37", 
        "https://www.zhihu.com/people/xu-wen-tan-16", 
        "https://www.zhihu.com/people/e-xing-mou", 
        "https://www.zhihu.com/people/zyhshh", 
        "https://www.zhihu.com/people/pythonji-zhu-xiao-wu-gui", 
        "https://www.zhihu.com/people/sang-zha-18", 
        "https://www.zhihu.com/people/chen-ze-qiang-99", 
        "https://www.zhihu.com/people/song-ping-99-29", 
        "https://www.zhihu.com/people/aebn", 
        "https://www.zhihu.com/people/tang-cu-gan-dou-fu", 
        "https://www.zhihu.com/people/xiao-ya-xiao-gong-zhu-40", 
        "https://www.zhihu.com/people/yi00yu", 
        "https://www.zhihu.com/people/tian-zhi-dao-huan-shi-duo-shao", 
        "https://www.zhihu.com/people/wang-yi-95-84-88", 
        "https://www.zhihu.com/people/gu-xiao-hua-96", 
        "https://www.zhihu.com/people/he-hua-cheng-81", 
        "https://www.zhihu.com/people/doublej-2", 
        "https://www.zhihu.com/people/mo-qi-miao-46", 
        "https://www.zhihu.com/people/nickyu-88", 
        "https://www.zhihu.com/people/dr-gavin", 
        "https://www.zhihu.com/people/hu-yu-tian-48", 
        "https://www.zhihu.com/people/hui-yi-li-dai-xu-64-30", 
        "https://www.zhihu.com/people/lin-jian-ming-19", 
        "https://www.zhihu.com/people/fu-shi-nuo-2", 
        "https://www.zhihu.com/people/ti-mo-dui-chang-1234", 
        "https://www.zhihu.com/people/deng-dai-ge-duo-94-82", 
        "https://www.zhihu.com/people/dong-yue-1991", 
        "https://www.zhihu.com/people/huangwolong", 
        "https://www.zhihu.com/people/no-one-57-56", 
        "https://www.zhihu.com/people/wang-si-zhan-33", 
        "https://www.zhihu.com/people/csrh", 
        "https://www.zhihu.com/people/zhang-zi-cheng-81", 
        "https://www.zhihu.com/people/key-88-47", 
        "https://www.zhihu.com/people/jesta", 
        "https://www.zhihu.com/people/lijina982", 
        "https://www.zhihu.com/people/dreamintime", 
        "https://www.zhihu.com/people/eric-80-5", 
        "https://www.zhihu.com/people/lang-er-ting-feng", 
        "https://www.zhihu.com/people/mMatix", 
        "https://www.zhihu.com/people/wang-ming-66-71", 
        "https://www.zhihu.com/people/tang-wei-tuo-90", 
        "https://www.zhihu.com/people/rou-long-wan", 
        "https://www.zhihu.com/people/gyoona", 
        "https://www.zhihu.com/people/wolf-tim", 
        "https://www.zhihu.com/people/speaker", 
        "https://www.zhihu.com/people/su-xiao-run", 
        "https://www.zhihu.com/people/thelittlestars", 
        "https://www.zhihu.com/people/wang-lin-jie", 
        "https://www.zhihu.com/people/charlex-18", 
        "https://www.zhihu.com/people/yi-zhi-a-mu-mu-16", 
        "https://www.zhihu.com/people/yin-wei-23-33", 
        "https://www.zhihu.com/people/feng-yu-42-59", 
        "https://www.zhihu.com/people/he-he-91-41-72", 
        "https://www.zhihu.com/people/lu-yao-ming-50", 
        "https://www.zhihu.com/people/zhangsan-97-39", 
        "https://www.zhihu.com/people/qu-shui-liu-shang-39-11", 
        "https://www.zhihu.com/people/ga-ba-zi", 
        "https://www.zhihu.com/people/ai-de-tai-chi-83", 
        "https://www.zhihu.com/people/zhao-yun-hao-40-29", 
        "https://www.zhihu.com/people/wo-ye-bu-zhi-dao-jiao-sha-hao", 
        "https://www.zhihu.com/people/lu-xiang-yu-65", 
        "https://www.zhihu.com/people/meng-xiao-long-10", 
        "https://www.zhihu.com/people/hu-run-76", 
        "https://www.zhihu.com/people/xiao-jing-bo-53", 
        "https://www.zhihu.com/people/zhan-zai-feng-kou-de-zhu-88", 
        "https://www.zhihu.com/people/lvhongli-57", 
        "https://www.zhihu.com/people/freakly", 
        "https://www.zhihu.com/people/dian-jing-62", 
        "https://www.zhihu.com/people/xiao-bai-60-55-28", 
        "https://www.zhihu.com/people/zhang-wei-51-38", 
        "https://www.zhihu.com/people/wong-hsiao-yen", 
        "https://www.zhihu.com/people/cai-zhi-xing-9", 
        "https://www.zhihu.com/people/hou-ge-shi-di", 
        "https://www.zhihu.com/people/johnny-63-54", 
        "https://www.zhihu.com/people/zheng-zhao-64-27"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/70391535", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 1, 
            "title": "学弟抱怨说期末选修网课太多，于是我教他用python如何实现自动刷课，简直不要太爽！", 
            "content": "<p><b>声明：只是用来学习，请不要使用非法用途，责任自负</b></p><p><b>工具</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>使用python 3.6版本，安装如下库：</p><p>安装win32api</p><p>pip3 install pywin32</p><p>安装PIL</p><p>pip install Pillow</p><p>安装pyautogui</p><p>pip install pyautogui</p><p>安装numpy</p><p>pip install numpy</p><p>安装cv2</p><p>pip install opencv-python</p><p>安装matplotlib</p><p>pip install matplotlib</p><p class=\"ztext-empty-paragraph\"><br/></p><p>使用SPY查看相关窗口标题, 类名。此标题唯一, 故可以以此来查找相关窗口。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>得到窗口句柄</b></p><div class=\"highlight\"><pre><code class=\"language-text\">    window_title = &#39;课件学习 - Google Chrome&#39;\n    screen_width = win32api.GetSystemMetrics(0)\n    screen_height = win32api.GetSystemMetrics(1) \n    hwnd = win32gui.FindWindow(win32con.NULL,window_title) \n    if hwnd == 0 :\n        error_exit(&#39;%s not found&#39; % window_title)\n        exit()\n    else:\n        print(&#39;hwnd = %x&#39;%(hwnd))\n\n    window_left,window_top,window_right,window_bottom = win32gui.GetWindowRect(hwnd)</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>主循环</b></p><p>原理：主要通信截图屏幕的图片，然后通过模板图像与之比较，如果出现我们需要的场景，那么得到对应的位置坐标，然后自动调用点击功能，从而实现自动化操作。那么这里主要使用opencv的两个算法，一个是图像相似度打分算法，另一个是图像搜索算法。</p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\"> while True:\n        grab_image = snapshot.grab_screen(deal_left,deal_top,deal_right,deal_bottom)\n        #grab_image.show()\n        grab_image.save(r&#39;.\\tmp_output\\full_screen.png&#39;)\n\n        #big pic size = 1936x1056\n        full_screen_w = 1936\n        full_screen_h = 1056\n        \n        pixel_core_x = 877.0\n        pixel_core_y = 25.0\n        \n        deal_left = window_left #window_left + kejian_x / full_screen_w * window_width - 100\n        deal_top = window_top + pixel_core_y / full_screen_h * window_height - 20\n        deal_right = window_left + window_width#window_left + kejian_x / full_screen_w * window_width + 150    \n        deal_bottom = window_top + pixel_core_y / full_screen_h * window_height + 20\n        grab_image = snapshot.grab_screen(deal_left,deal_top,deal_right,deal_bottom)\n\n        search_pic = r&#39;.\\tmp_output\\search_kejianxuexi.png&#39; \n        grab_image.save(search_pic)\n\n\n        #find kejian_tem\n        template_pic = r&#39;.\\template\\kejian_tem.png&#39;\n        num, w, h, pos_list = match.lookup_pos(template_pic, search_pic)\n        left = 0\n        top = 0\n        find_kejian_flag = 0\n        no_voice_flag = 0\n        if num == 1:\n            left = pos_list[0][0]\n            top = pos_list[0][1]\n            find_kejian_flag = 1\n        else:\n            print(&#39;==========warning search_kejianxuexi = &#39; + str(num))\n            find_kejian_flag = 0\n        if find_kejian_flag:\n            img_rgb = cv2.imread(search_pic)\n            img_rgb = img_rgb[top:top + h, left:left + w + 80, :]  # h, w, c\n            compare_pic = r&#39;.\\tmp_output\\kejianxuexi_compare.png&#39;\n            cv2.imwrite(compare_pic, img_rgb)\n            \n            temp_voice = r&#39;.\\template\\kejianhua_tem_voice.png&#39;\n            temp_no_voice = r&#39;.\\template\\kejianhua_tem_no_voice.png&#39;\n            no_voice_flag = match.score_pic(compare_pic, temp_voice, temp_no_voice)\n\n        if no_voice_flag:\n            print(&#39;===============find no_voice_flag&#39;)\n          \n            find_question_flag = find_question()\n            if find_question_flag:\n                #second\n                time.sleep(5)\n                find_daan()\n                time.sleep(5)\n                find_quding()\n      \n            find_chongbo_flag = find_chong_bo()\n\n            if find_question_flag and find_chongbo_flag:\n                print(&#39;========&gt;find_chongbo_flag and  find_chongbo_flag&#39;)\n                exit()\n                \n            if find_chongbo_flag:\n                weikaishi()         \n            \n        else:\n            print(&#39;===============every thing is ok&#39;)\n\n        time.sleep(2) \n\n        #exit(0)</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>图像相似度打分算法</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>那么如何判断一张被PS过的图片是否与另一张图片本质上相同呢？比较简单、易用的解决方案是采用感知哈希算法（Perceptual Hash Algorithm)。</p><p>感知哈希算法是一类算法的总称，包括aHash、pHash、dHash。顾名思义，感知哈希不是以严格的方式计算Hash值，而是以更加相对的方式计算哈希值，因为“相似”与否，就是一种相对的判定。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>aHash：平均值哈希。速度比较快，但是常常不太精确。</p><p>pHash：感知哈希。精确度比较高，但是速度方面较差一些。</p><p>dHash：差异值哈希。Amazing！精确度较高，且速度也非常快。因此我就选择了dHash作为我图片判重的def</p><div class=\"highlight\"><pre><code class=\"language-text\">pHash(imgfile):\n    &#34;&#34;&#34;get image pHash value&#34;&#34;&#34;\n    #加载并调整图片为32x32灰度图片\n    img=cv2.imread(imgfile, 0) \n    img=cv2.resize(img,(64,64),interpolation=cv2.INTER_CUBIC)\n\n    #创建二维列表\n    h, w = img.shape[:2]\n    vis0 = np.zeros((h,w), np.float32)\n    vis0[:h,:w] = img       #填充数据\n\n    #二维Dct变换\n    vis1 = cv2.dct(cv2.dct(vis0))\n    #cv.SaveImage(&#39;a.jpg&#39;,cv.fromarray(vis0)) #保存图片\n    vis1.resize(32,32)\n\n    #把二维list变成一维list\n    img_list=(vis1.tolist())\n\n    print(&#39;----------&#39;)\n    sum(img_list)\n\n    #计算均值\n    avg = sum(img_list)/(len(img_list)*1.0)\n    print(&#39;----------&#39;)\n    avg_list = [&#39;0&#39; if i&lt;avg else &#39;1&#39; for i in img_list]\n\n    #得到哈希值\n    return &#39;&#39;.join([&#39;%x&#39; % int(&#39;&#39;.join(avg_list[x:x+4]),2) for x in range(0,32*32,4)])\n\ndef hammingDist(s1, s2):\n    assert len(s1) == len(s2)\n    return sum([ch1 != ch2 for ch1, ch2 in zip(s1, s2)])\n\ndef aHash(img):\n   #缩放为8*8\n   img=cv2.resize(img,(8,8),interpolation=cv2.INTER_CUBIC)\n   #转换为灰度图\n   gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n   #s为像素和初值为0，hash_str为hash值初值为&#39;&#39;\n   s=0\n   hash_str=&#39;&#39;\n   #遍历累加求像素和\n   for i in range(8):\n       for j in range(8):\n           s=s+gray[i,j]\n   #求平均灰度\n   avg=s/64\n   #灰度大于平均值为1相反为0生成图片的hash值\n   for i in range(8):\n       for j in range(8):\n           if gray[i,j]&gt;avg:\n               hash_str=hash_str+&#39;1&#39;\n           else:\n               hash_str=hash_str+&#39;0&#39;\n   return hash_str\n\ndef cmpHash(hash1,hash2):\n    n=0\n    #hash长度不同则返回-1代表传参出错\n    if len(hash1)!=len(hash2):\n        return -1\n    #遍历判断\n    for i in range(len(hash1)):\n    #不相等则n计数+1，n最终为相似度\n        if hash1[i]!=hash2[i]:\n            n=n+1\n    return 1 - n / 64\n\n\ndef score_pic(compare_pic, temp_voice, temp_no_voice):\n\n    #HASH1=pHash(compare_pic)\n    #HASH2=pHash(temp_voice)\n    #out_score = 1 - hammingDist(HASH1,HASH2)*1. / (32*32/4)\n\n    img1 = cv2.imread(compare_pic)\n    img2 = cv2.imread(temp_voice)\n    img3 = cv2.imread(temp_no_voice)\n    #time1 = time.time()\n    hash1 = aHash(img1)\n    hash2 = aHash(img2)\n    voice_score = cmpHash(hash1, hash2)\n    \n    hash1 = aHash(img1)\n    hash3 = aHash(img3)\n    no_voice_score = cmpHash(hash1, hash3)\n    no_voice_flag = 0\n\n\n    #print(str(voice_score) + &#39;=&gt;&#39; + str(no_voice_score))\n    if no_voice_score &gt;= voice_score:\n        no_voice_flag = 1\n    else:\n        no_voice_flag = 0\n\n    return no_voice_flag</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>图像搜索算法</b></p><p>使用res = cv2.matchTemplate(img_gray,template,cv2.TM_CCOEFF_NORMED)</p><div class=\"highlight\"><pre><code class=\"language-text\">def lookup_pos(template_pic, search_pic):\n    img_rgb = cv2.imread(search_pic)\n    img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)\n    img = img_gray\n    #print(img.shape)\n    template = cv2.imread(template_pic,0)\n    w, h = template.shape[::-1]\n\n    res = cv2.matchTemplate(img,template,cv2.TM_CCOEFF_NORMED)\n    threshold = 0.95\n    loc = np.where( res &gt;= threshold)\n    num = 0\n    left = 0\n    top = 0\n\n    pos_list = []\n    for pt in zip(*loc[::-1]):\n        cv2.rectangle(img_rgb, pt, (pt[0] + w, pt[1] + h), (0,0,255), 2)\n        left = pt[0]\n        top = pt[1]\n        pos_list.append(pt)\n        num = num + 1\n    \n    res = res*256\n    cv2.imwrite(r&#39;.\\tmp_output\\out.png&#39;, img_rgb)\n    cv2.imwrite(r&#39;.\\tmp_output\\res.png&#39;, res)\n\n\n    return num, w, h, pos_list</code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/70292799", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "利用Python爬取获取博客园文章定时发送到邮箱", 
            "content": "<h2><b>写在前面</b></h2><p>关于获取文章自动发送到邮箱，这类需求其实可以写好几个网站，弄完博客园，弄CSDN，弄掘金，弄其他的，网站多的是呢~哈哈</p><p>先从博客园开始，基本需求，获取python板块下面的新文章，间隔60分钟发送一次，时间太短估摸着没有多少新博客产出~</p><p>抓取的页面就是这个</p><div class=\"highlight\"><pre><code class=\"language-text\">https://www.cnblogs.com/cate/python</code></pre></div><h2><b>需求整理</b></h2><ol><li>获取指定页面的所有文章，记录文章相关信息，并且记录最后一篇文章的时间</li><li>将文章发送到指定邮箱，更新最后一篇文章的时间</li></ol><h2><b>实际编码环节</b></h2><h3>查看一下需要导入的模块</h3><h3>模块清单</h3><div class=\"highlight\"><pre><code class=\"language-text\">import requests\nimport time\nimport re\nimport smtplib\nfrom email.mime.text import MIMEText\nfrom email.utils import formataddr\nfrom email.header import Header\nfrom email.mime.application import MIMEApplication\nfrom email.mime.multipart import MIMEMultipart</code></pre></div><h3>初始化基本数据</h3><div class=\"highlight\"><pre><code class=\"language-text\"># 初始化数据\n    def __init__(self):\n        self.start_url = &#34;https://www.cnblogs.com/cate/python&#34;\n        self.headers = {\n            &#34;user-agent&#34;: &#34;Mozilla/..... Safari/537.36&#34;,\n            &#34;referer&#34;: &#34;https://www.cnblogs.com/cate/python/&#34;\n        }\n        self.pattern = r&#39;&lt;div class=&#34;post_item_body&#34;&gt;[\\s\\S.]*?&lt;h3&gt;&lt;a class=&#34;titlelnk&#34; href=&#34;(.*?)&#34; target=&#34;_blank&#34;&gt;(.*?)&lt;/a&gt;&lt;/h3&gt;[\\s\\S.]*?&lt;div class=&#34;post_item_foot&#34;&gt;[\\s\\S.]*?&lt;a href=&#34;.*?&#34; class=&#34;lightblue&#34;&gt;(.*?)&lt;/a&gt;([\\s\\S.]*?)&lt;span class=&#34;article_comment&#34;&gt;&#39;\n        self.last_blog_time = 0\n        self.need_send_articles = []</code></pre></div><p>参数说明</p><ul><li>self.start_url 数据爬取地址</li><li>self.headers 头文件</li><li>self.pattern 正则表达式，用来匹配我们需要的数据内容的，你可以使用BS4,LXML,PyQuery等内容实现</li><li>self.last_blog_time 最后一篇博客的更新时间</li><li>self.need_send_articles 需要发送的博客地址</li></ul><h3>解析博客网页内容</h3><p>涉及代码较多，我将关键点编写相应的注释</p><div class=\"highlight\"><pre><code class=\"language-text\"># 解析网页内容\n    def get_articles(self):\n        try:\n            # 正常的数据获取\n            res = requests.get(self.start_url,headers=self.headers,timeout=3)\n        except Exception as e:\n            print(&#34;error %s&#34;% e)\n            time.sleep(3)\n            return self.get_articles()  # 重新发起请求\n\n        html = res.text\n        # 这个地方的正则表达式是考验你正则功底的地方了\n        all = re.findall(self.pattern,html)\n        # 判断，如果没有新文章\n        last_time = self.change_time(all[0][3].strip().replace(&#34;发布于 &#34;, &#34;&#34;))\n\n        if last_time &lt;= self.last_blog_time:\n            print(&#34;没有新文章更新&#34;)\n            return\n\n        for item in all:\n            public_time = item[3]\n            if public_time:\n                # 格式化时间\n                public_time = self.change_time(public_time.strip().replace(&#34;发布于 &#34;,&#34;&#34;))\n\n                if(public_time &gt; self.last_blog_time):\n                    self.need_send_articles.append({\n                        &#34;url&#34;:item[0],\n                        &#34;title&#34;:item[1],\n                        &#34;author&#34;:item[2],\n                        &#34;time&#34;:public_time\n                    })\n\n        # 文章获取完毕，更新时间\n        self.last_blog_time = last_time\n        ##### 测试输出\n        print(self.need_send_articles)\n        print(&#34;现在文章的最后时间为&#34;,self.last_blog_time)\n        ##### 测试输出</code></pre></div><h3>时间字符串转换成时间戳</h3><p>采用时间戳可以直接比较大小，非常方便</p><div class=\"highlight\"><pre><code class=\"language-text\">def change_time(self,need_change_time):\n        &#39;&#39;&#39;\n        # 时间的转换\n        :param need_change_time:\n        :return:返回时间戳\n        &#39;&#39;&#39;\n        time_array = time.strptime(need_change_time, &#34;%Y-%m-%d %H:%M&#34;)\n        time_stamp = int(time.mktime(time_array))\n        return time_stamp</code></pre></div><h3>邮件发送环节</h3><p>本篇博客采用的是QQ邮箱发送<br/>关于QQ邮箱发送的一些参考文章，我给大家列一下，方便你查阅</p><div class=\"highlight\"><pre><code class=\"language-text\">参考文章\n# https://blog.csdn.net/qiye005/article/details/80789666\n# https://blog.csdn.net/Momorrine/article/details/79881251\n# https://www.cnblogs.com/lovealways/p/6701662.html\n# https://www.cnblogs.com/yufeihlf/p/5726619.html</code></pre></div><p>因为我采用的是QQ邮箱，所以有的地方设定起来比较麻烦，发短信还花了2毛钱，建议你采用其它的邮箱，设置是一样的哦~~</p><h3>发送邮件send_email函数</h3><p>你看一下上面的文章之后，就可以对邮件发送进行相应的编写了，非常简单</p><blockquote><i>QQ邮箱是SSL认证的邮箱系统，因此用QQ邮箱发送邮件，需要创建一个SMTP_SSL对象，而不是SMTP对象</i></blockquote><div class=\"highlight\"><pre><code class=\"language-text\"># 发送邮件\n    def send_email(self,articles):\n        smtp = smtplib.SMTP_SSL()  # 这个地方注意\n        smtp.connect(&#34;smtp.qq.com&#34;,465)\n        smtp.login(&#34;860866679@qq.com&#34;, &#34;授权码&#34;)\n\n\n        sender = &#39;860866679@qq.com&#39;\n        receivers = [&#39;找个自己的其他邮箱@163.com&#39;]  # 接收邮件，可设置为你的QQ邮箱或者其他邮箱\n\n        # 完善发件人收件人，主题信息\n        message = MIMEMultipart()\n        message[&#39;From&#39;] = formataddr([&#34;博客采集器&#34;, sender])\n        message[&#39;To&#39;] = formataddr([&#34;hi,baby&#34;, &#39;&#39;.join(receivers)])\n        subject = &#39;你有新采集到的文章清单&#39;\n        message[&#39;Subject&#39;] = Header(subject, &#39;utf-8&#39;)\n        # 正文部分\n        html = &#34;&#34;\n        for item in articles:\n            html+=(&#34;&lt;p&gt;&lt;a href=&#39;{url}&#39;&gt;{title}&lt;/a&gt;--文章作者{author}--发布时间{time}&lt;/p&gt;&#34;.format(title=item[&#34;title&#34;],url=item[&#34;url&#34;],author=item[&#34;author&#34;],time=item[&#34;time&#34;]))\n\n        textmessage = MIMEText(&#39;&lt;p&gt;新采集到的文章清单&lt;/p&gt;&#39; +html,\n                               &#39;html&#39;, &#39;utf-8&#39;)\n        message.attach(textmessage)\n\n        # 发送邮件操作\n        smtp.sendmail(sender, receivers, message.as_string())\n        smtp.quit()</code></pre></div><h3>邮箱收到邮件</h3><p>当收到邮件的那一刻，你就可以感受到happy了~</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-98c4e3ab1e8f76affd8294f8db91eac3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"976\" data-rawheight=\"520\" class=\"origin_image zh-lightbox-thumb\" width=\"976\" data-original=\"https://pic4.zhimg.com/v2-98c4e3ab1e8f76affd8294f8db91eac3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;976&#39; height=&#39;520&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"976\" data-rawheight=\"520\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"976\" data-original=\"https://pic4.zhimg.com/v2-98c4e3ab1e8f76affd8294f8db91eac3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-98c4e3ab1e8f76affd8294f8db91eac3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h3>部署到服务器</h3><p>最后一个步骤，如果想要持续的获取，那么找一个服务器，然后部署就行啦，有兴趣的博友，继续研究下去吧~</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5bd0d3a3766fb0885bbd4e45de99f960_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"285\" data-rawheight=\"208\" class=\"content_image\" width=\"285\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;285&#39; height=&#39;208&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"285\" data-rawheight=\"208\" class=\"content_image lazy\" width=\"285\" data-actualsrc=\"https://pic1.zhimg.com/v2-5bd0d3a3766fb0885bbd4e45de99f960_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/70290764", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 3, 
            "title": "利用python爬虫通过m3u8文件下载ts视频", 
            "content": "<h2><b>什么是m3u8文件</b></h2><p>M3U8文件是指UTF-8编码格式的<code>M3U文件</code>。<br/><code>M3U文件</code>是记录了一个<code>索引纯文本文件</code>，<br/>打开它时播放软件并不是播放它，而是根据它的索引找到对应的音视频文件的网络地址进行在线播放。</p><p>原视频数据分割为很多个TS流，每个TS流的地址记录在m3u8文件列表中</p><p>比如我这里有一个m3u8文件，文件内容如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">#EXTM3U\n#EXT-X-VERSION:3\n#EXT-X-MEDIA-SEQUENCE:0\n#EXT-X-ALLOW-CACHE:YES\n#EXT-X-TARGETDURATION:15\n#EXTINF:6.916667,\nout000.ts\n#EXTINF:10.416667,\nout001.ts\n#EXTINF:10.416667,\nout002.ts\n#EXTINF:1.375000,\nout003.ts\n#EXTINF:1.541667,\nout004.ts\n#EXTINF:7.666667,\nout005.ts\n#EXTINF:10.416667,\n</code></pre></div><h2><b>ts 文件一般怎么处理</b></h2><ul><li>只有m3u8文件，需要下载ts文件</li><li>有ts文件，但因为被加密无法播放，需要解码</li><li>ts文件能正常播放，但太多而小，需要合并</li></ul><p>本篇文章处理第1和第2条内容，加密部分跳过。</p><p>上面我提供的ts文件中并没有加密，也就是没有关键字<code>key</code> ，下载ts文件之后直接合并即可</p><h2><b>ts文件路径获取</b></h2><p>由于上面的m3u8文件中所有的ts文件都是相对地址，所以需要依据<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/hihell/article/details/87256020\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">上篇博客</a>中获取到的链接</p><div class=\"highlight\"><pre><code class=\"language-text\">{&#39;url&#39;: &#39;https://videos5.jsyunbf.com/2019/02/07/iQX7y3p1dleAhIv7/playlist.m3u8&#39;, &#39;ext&#39;: &#39;dplay&#39;, &#39;msg&#39;: &#39;ok&#39;, &#39;playertype&#39;: None}</code></pre></div><p>其中前面的部分是ts的播放地址的前缀地址</p><div class=\"highlight\"><pre><code class=\"language-text\"># https://videos5.jsyunbf.com/2019/02/07/iQX7y3p1dleAhIv7/out005.ts\nimport datetime\nimport requests\n# m3u8是本地的文件路径\ndef get_ts_urls(m3u8_path,base_url):  \n    urls = []\n    with open(m3u8_path,&#34;r&#34;) as file:\n        lines = file.readlines()\n        for line in lines:\n            if line.endswith(&#34;.ts\\n&#34;):\n                urls.append(base_url+line.strip(&#34;\\n&#34;))\n\n    return urls</code></pre></div><h2><b>ts文件下载</b></h2><p>所有的路径读取完毕之后，需要对ts文件进行下载，文件的下载办法很多。</p><div class=\"highlight\"><pre><code class=\"language-text\">def download(ts_urls,download_path):\n    for i in range(len(ts_urls)):\n        ts_url = ts_urls[i]\n        file_name = ts_url.split(&#34;/&#34;)[-1]\n        print(&#34;开始下载 %s&#34; %file_name)\n        start = datetime.datetime.now().replace(microsecond=0)\n        try:\n            response = requests.get(ts_url,stream=True,verify=False)\n        except Exception as e:\n            print(&#34;异常请求：%s&#34;%e.args)\n            return\n\n        ts_path = download_path+&#34;/{0}.ts&#34;.format(i)\n        with open(ts_path,&#34;wb+&#34;) as file:\n            for chunk in response.iter_content(chunk_size=1024):\n                if chunk:\n                    file.write(chunk)\n\n        end = datetime.datetime.now().replace(microsecond=0)\n        print(&#34;耗时：%s&#34;%(end-start))</code></pre></div><p>下载过程显示，表示下载成功，剩下的就是拼网速的时候了。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f07b9284ba9fe37fe34bc92f5d0bcbc1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"699\" data-rawheight=\"142\" class=\"origin_image zh-lightbox-thumb\" width=\"699\" data-original=\"https://pic2.zhimg.com/v2-f07b9284ba9fe37fe34bc92f5d0bcbc1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;699&#39; height=&#39;142&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"699\" data-rawheight=\"142\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"699\" data-original=\"https://pic2.zhimg.com/v2-f07b9284ba9fe37fe34bc92f5d0bcbc1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f07b9284ba9fe37fe34bc92f5d0bcbc1_b.jpg\"/></figure><p><br/>下载完毕，是一大堆ts文件，记住，只要一个可以看，就可以合并了<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3699a1e6ebb287cf7fdbafa6b79b1d42_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"909\" data-rawheight=\"286\" class=\"origin_image zh-lightbox-thumb\" width=\"909\" data-original=\"https://pic3.zhimg.com/v2-3699a1e6ebb287cf7fdbafa6b79b1d42_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;909&#39; height=&#39;286&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"909\" data-rawheight=\"286\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"909\" data-original=\"https://pic3.zhimg.com/v2-3699a1e6ebb287cf7fdbafa6b79b1d42_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-3699a1e6ebb287cf7fdbafa6b79b1d42_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>合并ts文件</b></h2><p>使用<code>copy命令</code> 如果不清楚，就去百度即可</p><div class=\"highlight\"><pre><code class=\"language-text\">copy/b D:\\newpython\\doutu\\sao\\ts_files\\*.ts d:\\fnew.ts</code></pre></div><h3>代码合并</h3><div class=\"highlight\"><pre><code class=\"language-text\">import os\nfrom os import path\ndef file_walker(path):\n    file_list = []\n    for root, dirs, files in os.walk(path): # 生成器\n        for fn in files:\n            p = str(root+&#39;/&#39;+fn)\n            file_list.append(p)\n\n    print(file_list)\n    return file_list\n\ndef combine(ts_path, combine_path, file_name):\n    file_list = file_walker(ts_path)\n    file_path = combine_path + file_name + &#39;.ts&#39;\n    with open(file_path, &#39;wb+&#39;) as fw:\n        for i in range(len(file_list)):\n\n            fw.write(open(file_list[i], &#39;rb&#39;).read())\n\nif __name__ == &#39;__main__&#39;:\n    #urls = get_ts_urls(&#34;playlist.m3u8&#34;,&#34;https://videos5.jsyunbf.com/2019/02/07/iQX7y3p1dleAhIv7/&#34;)\n    #download(urls,&#34;./tsfiles&#34;)\n    combine(&#34;./ts_files&#34;,&#34;d:/ts&#34;,&#34;haha&#34;)</code></pre></div><p>最终合并之后，形成一个ts文件，当然你还可以用软件把视频转换成mp4格式</p><p>也可以利用<b>FFMPEG</b>可以直接实现m3u8 转MP4</p><p>愉快的下载下来看VIP视频吧<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-80a79fb3f816ea671363cf8035b4a389_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"238\" data-rawheight=\"215\" class=\"content_image\" width=\"238\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;238&#39; height=&#39;215&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"238\" data-rawheight=\"215\" class=\"content_image lazy\" width=\"238\" data-actualsrc=\"https://pic2.zhimg.com/v2-80a79fb3f816ea671363cf8035b4a389_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>备注部分</b></h2><p>m3u8文件中的 m3u8标签与属性说明</p><div class=\"highlight\"><pre><code class=\"language-text\">#EXTM3U\n每个M3U文件第一行必须是这个tag，请标示作用\n\n#EXT-X-VERSION:3 \n该属性可以没有\n\n#EXT-X-MEDIA-SEQUENCE:140651513 \n每一个media URI在PlayList中只有唯一的序号，相邻之间序号+1, \n一个media URI并不是必须要包含的，如果没有，默认为0\n\n #EXT-X-TARGETDURATION\n指定最大的媒体段时间长（秒）。所以#EXTINF中指定的时间长度必须小于或是等于这\n个最大值。这个tag在整个PlayList文件中只能出现一 次（在嵌套的情况下，一般有\n真正ts url的m3u8才会出现该tag）\n\n#EXT-X-PLAYLIST-TYPE\n提供关于PlayList的可变性的信息，这个对整个PlayList文件有效，是可选的，格式\n如下：#EXT-X-PLAYLIST-TYPE:：如果是VOD，则服务器不能改变PlayList 文件；\n如果是EVENT，则服务器不能改变或是删除PlayList文件中的任何部分，但是可以向该\n文件中增加新的一行内容。\n\n#EXTINF\nduration指定每个媒体段(ts)的持续时间（秒），仅对其后面的URI有效，title是\n下载资源的url\n\n#EXT-X-KEY\n表示怎么对media segments进行解码。其作用范围是下次该tag出现前的所有media \nURI，属性为NONE 或者 AES-128。NONE表示 URI以及IV（Initialization \nVector）属性必须不存在， AES-128(Advanced EncryptionStandard)表示URI\n必须存在，IV可以不存在。\n\n#EXT-X-PROGRAM-DATE-TIME\n将一个绝对时间或是日期和一个媒体段中的第一个sample相关联，只对下一个meida \nURI有效，格式如#EXT-X-PROGRAM-DATE-TIME:\nFor example: #EXT-X-PROGRAM-DATETIME:2010-02-19T14:54:23.031+08:00\n\n#EXT-X-ALLOW-CACHE\n是否允许做cache，这个可以在PlayList文件中任意地方出现，并且最多出现一次，作\n用效果是所有的媒体段。格式如下：#EXT-X-ALLOW-CACHE:\n\n#EXT-X-ENDLIST\n表示PlayList的末尾了，它可以在PlayList中任意位置出现，但是只能出现一个，格\n式如下：#EXT-X-ENDLIST</code></pre></div><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": [
                {
                    "userName": "田畅", 
                    "userLink": "https://www.zhihu.com/people/89c94928ab40b6a4df738b50e2fbfbf3", 
                    "content": "<p>代码挺好的,要是能有多线程线程,加速下载就好了</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/69822893", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "Python  Fiddler+夜神模拟器+雷电模拟器配置手机APP爬虫部分", 
            "content": "<p>我将逐步讲解一下手机APP的爬虫，关于这部分，我们尽量简化博客内容，在这部分中可能涉及到一些逆向，破解的内容，这部分尽量跳过，毕竟它涉及的东西有点复杂，并且偏离了爬虫体系太远，有兴趣的博友，可以一起研究下。</p><p>之前看到知乎有人对手机App爬虫归类，基本符合规则，接下来的10篇博客可能集中在80%的App上，所以还是比较简单的</p><ol><li>50%的app，通过抓包软件就可以分析出抓取参数并抓取到信息。</li><li>30%的app，可能需要适当的反编译，分析出加密算法并抓取到信息。</li><li>10%的app，可能加固，需要脱壳，然后反编译，分析出加密算法并抓取到信息</li><li>10%的app，通过各式各样的签名，证书，设备绑定等方法，隐藏加密算法。</li></ol><p>首先配置第一轮的环境，配置好了，下一篇博客，就采用Fiddler+夜神模拟器[雷电模拟器]等实现儿歌多多APP的数据抓取工作</p><h2><b>抓包</b></h2><p>抓包是爬虫里面经常用到的一个词，完整的应该叫做抓取<code>数据请求响应包</code> ，而Fiddler这款工具就是干这个的，当然还有其他工具，后面的博客中咱也会提及到</p><p>你可以百度关键字：<b>Fiddler抓取手机APP</b> 相关的博客一大票~，哪篇博客基本都可以叫你入门，所以你可以看看各家的，今天我这篇博客主要写的是Fiddler配合模拟器实现抓包操作。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5109cdbeed551268f703c6f790ce44a5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"764\" data-rawheight=\"772\" class=\"origin_image zh-lightbox-thumb\" width=\"764\" data-original=\"https://pic2.zhimg.com/v2-5109cdbeed551268f703c6f790ce44a5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;764&#39; height=&#39;772&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"764\" data-rawheight=\"772\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"764\" data-original=\"https://pic2.zhimg.com/v2-5109cdbeed551268f703c6f790ce44a5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5109cdbeed551268f703c6f790ce44a5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>百度Fiddler软件，找到官网，下载按照流程安装即可，当然你可以自己去找一个汉化版，这个看你使用百度的水平了，我使用的是4.0以上版本，建议你也使用这个吧，当然版本越高越好。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-cb168fa13a08cdbc21d5254069f1eb75_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1657\" data-rawheight=\"295\" class=\"origin_image zh-lightbox-thumb\" width=\"1657\" data-original=\"https://pic2.zhimg.com/v2-cb168fa13a08cdbc21d5254069f1eb75_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1657&#39; height=&#39;295&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1657\" data-rawheight=\"295\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1657\" data-original=\"https://pic2.zhimg.com/v2-cb168fa13a08cdbc21d5254069f1eb75_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-cb168fa13a08cdbc21d5254069f1eb75_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>做一些简单的配置</p><h3>普通https抓包设置</h3><p>打开Fiddler ------&gt; Options .然后打开的对话框中，选择HTTPS tab页，如图所示：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4fadfd9a687a01eda7c3ae830cca6707_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"678\" data-rawheight=\"460\" class=\"origin_image zh-lightbox-thumb\" width=\"678\" data-original=\"https://pic4.zhimg.com/v2-4fadfd9a687a01eda7c3ae830cca6707_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;678&#39; height=&#39;460&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"678\" data-rawheight=\"460\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"678\" data-original=\"https://pic4.zhimg.com/v2-4fadfd9a687a01eda7c3ae830cca6707_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4fadfd9a687a01eda7c3ae830cca6707_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h3>说明 (配置完后记得要重启Fiddler)</h3><ol><li>选中&#34;Decrpt HTTPS traffic&#34;, Fiddler就可以截获HTTPS请求</li><li>Ignore server certificate errors忽略证书错误</li></ol><p><b>第一次会提示是否信任fiddler证书及安全提醒，选择yes，之后也可以在系统的证书管理中进行管理。</b></p><h3>配置Fiddler允许远程连接</h3><p>切换tab选项卡到 <code>Connections</code> 配置端口</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-524ec582c6811440a7f8e2d62df5d15c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"678\" data-rawheight=\"460\" class=\"origin_image zh-lightbox-thumb\" width=\"678\" data-original=\"https://pic1.zhimg.com/v2-524ec582c6811440a7f8e2d62df5d15c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;678&#39; height=&#39;460&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"678\" data-rawheight=\"460\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"678\" data-original=\"https://pic1.zhimg.com/v2-524ec582c6811440a7f8e2d62df5d15c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-524ec582c6811440a7f8e2d62df5d15c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ol><li>选中&#34;Allow remote computers to connect&#34;. 是允许别的机器把HTTP/HTTPS请求发送到Fiddler上来</li></ol><blockquote><i>等会设置手机代理时需要。设置好后重启fiddler保证设置生效。</i></blockquote><p><b>到现在为止，其实Fiddler已经可以抓取你电脑上浏览器访问的数据了，如果不行，重启Fiddler和浏览器即可</b></p><h2><b>记录本机的IP</b></h2><p>打开cmd窗口，不会的自行百度</p><p>在里面输入 ipconfig 获取你的ip4地址 ，这个地址一定要记住，后面配置模拟器的时候是需要用的~</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-33951b7d633d1704c2ed8678835217a0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"671\" data-rawheight=\"128\" class=\"origin_image zh-lightbox-thumb\" width=\"671\" data-original=\"https://pic1.zhimg.com/v2-33951b7d633d1704c2ed8678835217a0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;671&#39; height=&#39;128&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"671\" data-rawheight=\"128\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"671\" data-original=\"https://pic1.zhimg.com/v2-33951b7d633d1704c2ed8678835217a0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-33951b7d633d1704c2ed8678835217a0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><blockquote><i>ip 192.168.137.1</i></blockquote><h2><b>下载模拟器</b></h2><p>你可能更多的时候是使用模拟器来打游戏，作为程序猿，模拟器上安装APK是非常好用的。<br/>自行下载安装夜神模拟器，雷电模拟器或者其他的都可以</p><p>安装完毕，长成这个样子<br/>下载APK文件，直接拖拽到下面视图，就可以直接安装</p><p>找到设置，点击Wlan，长按鼠标左键，出现如下界面</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3cdf81e6dea2cc368eb2c2aea21046aa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"662\" data-rawheight=\"502\" class=\"origin_image zh-lightbox-thumb\" width=\"662\" data-original=\"https://pic3.zhimg.com/v2-3cdf81e6dea2cc368eb2c2aea21046aa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;662&#39; height=&#39;502&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"662\" data-rawheight=\"502\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"662\" data-original=\"https://pic3.zhimg.com/v2-3cdf81e6dea2cc368eb2c2aea21046aa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-3cdf81e6dea2cc368eb2c2aea21046aa_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>点击修改网络</p><p>代理模式选择手动，输入刚才的IP和上述提到的8888端口</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c3f9c778d6364457dec56651be20ccc2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"749\" data-rawheight=\"367\" class=\"origin_image zh-lightbox-thumb\" width=\"749\" data-original=\"https://pic3.zhimg.com/v2-c3f9c778d6364457dec56651be20ccc2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;749&#39; height=&#39;367&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"749\" data-rawheight=\"367\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"749\" data-original=\"https://pic3.zhimg.com/v2-c3f9c778d6364457dec56651be20ccc2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c3f9c778d6364457dec56651be20ccc2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>使用模拟器默认浏览器打开 <a href=\"https://link.zhihu.com/?target=http%3A//192.168.137.1%3A8888/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">http://192.168.137.1:8888</a>， 点&#34;FiddlerRoot certificate&#34; 然后安装证书，如图:</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4416fa1ed6b61aa47fc25936aec46808_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"795\" data-rawheight=\"146\" class=\"origin_image zh-lightbox-thumb\" width=\"795\" data-original=\"https://pic1.zhimg.com/v2-4416fa1ed6b61aa47fc25936aec46808_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;795&#39; height=&#39;146&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"795\" data-rawheight=\"146\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"795\" data-original=\"https://pic1.zhimg.com/v2-4416fa1ed6b61aa47fc25936aec46808_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-4416fa1ed6b61aa47fc25936aec46808_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>安装完毕之后，打开模拟器上的浏览器，输入<a href=\"https://link.zhihu.com/?target=http%3A//www.baidu.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">baidu.com</span><span class=\"invisible\"></span></a></p><p>在Fiddler中抓取到如下链接,代表环境配置已经完毕<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-827f50aefcbda3b6fa7ffb7d18566031_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"843\" data-rawheight=\"256\" class=\"origin_image zh-lightbox-thumb\" width=\"843\" data-original=\"https://pic2.zhimg.com/v2-827f50aefcbda3b6fa7ffb7d18566031_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;843&#39; height=&#39;256&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"843\" data-rawheight=\"256\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"843\" data-original=\"https://pic2.zhimg.com/v2-827f50aefcbda3b6fa7ffb7d18566031_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-827f50aefcbda3b6fa7ffb7d18566031_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>温馨提示下，在抓包过程中，你的fiddler不能关闭哦。关闭了之后你手机网络就不能用了。</p><p>停止网络监控的话去掉wifi的代理设置即可</p><h2><b>设置过滤</b></h2><p>1.手机上设置代理后，这时候fiddler上抓到的是pc和app所有的请求，如果pc上打开网址，会很多，这时候就需要开启过滤功能了。</p><p>2.打开fiddler&gt;Tools&gt;Fiddler Options&gt;HTTPS&gt;...from remote clients only,勾选这个选项就可以了<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-63d301d0fb60faf6e1238534142dd7a8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"675\" data-rawheight=\"457\" class=\"origin_image zh-lightbox-thumb\" width=\"675\" data-original=\"https://pic1.zhimg.com/v2-63d301d0fb60faf6e1238534142dd7a8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;675&#39; height=&#39;457&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"675\" data-rawheight=\"457\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"675\" data-original=\"https://pic1.zhimg.com/v2-63d301d0fb60faf6e1238534142dd7a8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-63d301d0fb60faf6e1238534142dd7a8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>...from all processes :抓所有的请求</li><li>...from browsers only ：只抓浏览器的请求</li><li>...from non-browsers only :只抓非浏览器的请求</li><li>...from remote clients only:只抓远程客户端请求</li></ul><h2><b>Fiddler 抓包简介</b></h2><p>Fiddler想要抓到数据包，要确保Capture Traffic是开启，在File –&gt; Capture Traffic。开启后再左下角会有显示，当然也可以直接点击左下角的图标来关闭/开启抓包功能。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d019fdc90be819ab8df7006c43dbb3f3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"499\" data-rawheight=\"324\" class=\"origin_image zh-lightbox-thumb\" width=\"499\" data-original=\"https://pic4.zhimg.com/v2-d019fdc90be819ab8df7006c43dbb3f3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;499&#39; height=&#39;324&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"499\" data-rawheight=\"324\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"499\" data-original=\"https://pic4.zhimg.com/v2-d019fdc90be819ab8df7006c43dbb3f3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d019fdc90be819ab8df7006c43dbb3f3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>需要能简单看懂Fiddler上的内容</b></h2><p>更详细的使用，可以参考一篇写的很不错的文章 <a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/yyhh/p/5140852.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">参考博客</a></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f2417718ab5a49e4f6bc40fe7efc8014_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1109\" data-rawheight=\"1192\" class=\"origin_image zh-lightbox-thumb\" width=\"1109\" data-original=\"https://pic1.zhimg.com/v2-f2417718ab5a49e4f6bc40fe7efc8014_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1109&#39; height=&#39;1192&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1109\" data-rawheight=\"1192\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1109\" data-original=\"https://pic1.zhimg.com/v2-f2417718ab5a49e4f6bc40fe7efc8014_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f2417718ab5a49e4f6bc40fe7efc8014_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上述横着的红色框<br/>|名称 |含义 |<br/>|------|------|<br/>|#|抓取HTTP Request的顺序，从1开始，以此递增|<br/>|Result|HTTP状态码|<br/>|Protocol|请求使用的协议，如HTTP/HTTPS/FTP等|<br/>|Host|请求地址的主机名|<br/>|URL|请求资源的位置|<br/>|Body|该请求的大小|<br/>|Caching|请求的缓存过期时间或者缓存控制值|<br/>|Content-Type|请求响应的类型|<br/>|Process|发送此请求的进程：进程ID|<br/>|Comments|允许用户为此回话添加备注|<br/>|Custom|允许用户设置自定义值|</p><p>下面竖着的红色框<br/>|图标|含义|<br/>|-----|-----|<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c1dafb3c8d4dcaaab7b52ba99d75f30a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"527\" data-rawheight=\"571\" class=\"origin_image zh-lightbox-thumb\" width=\"527\" data-original=\"https://pic3.zhimg.com/v2-c1dafb3c8d4dcaaab7b52ba99d75f30a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;527&#39; height=&#39;571&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"527\" data-rawheight=\"571\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"527\" data-original=\"https://pic3.zhimg.com/v2-c1dafb3c8d4dcaaab7b52ba99d75f30a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c1dafb3c8d4dcaaab7b52ba99d75f30a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>好了，文章到此结束了~~大家Fiddler用起来吧</b></h2><p>下一篇，将写一下如何爬取儿歌多多APP</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-dbe3af2b0a81252444532dd4ab71601f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"240\" data-rawheight=\"203\" class=\"content_image\" width=\"240\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;240&#39; height=&#39;203&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"240\" data-rawheight=\"203\" class=\"content_image lazy\" width=\"240\" data-actualsrc=\"https://pic4.zhimg.com/v2-dbe3af2b0a81252444532dd4ab71601f_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/69816866", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "利用python爬虫框架scrapy抓取某市科技计划项目成果库数据", 
            "content": "<p>今天本来没有打算抓取这个网站的，无意中看到某个微信群有人问了一嘴这个网站，想看一下有什么特别复杂的地方，一顿操作下来，发现这个网站除了卡慢，经常自己宕机以外，好像还真没有什么特殊的....</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-db35eba3d5cbbb72bc6c1f1a7c7a54ec_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"350\" data-rawheight=\"320\" class=\"content_image\" width=\"350\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;350&#39; height=&#39;320&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"350\" data-rawheight=\"320\" class=\"content_image lazy\" width=\"350\" data-actualsrc=\"https://pic1.zhimg.com/v2-db35eba3d5cbbb72bc6c1f1a7c7a54ec_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h3>爬取网址 <a href=\"https://link.zhihu.com/?target=http%3A//cgk.kxjs.tj.gov.cn/navigation.do\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">cgk.kxjs.tj.gov.cn/navi</span><span class=\"invisible\">gation.do</span><span class=\"ellipsis\"></span></a></h3><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6f39ca6237e9e1f319ecab243a63b513_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1008\" data-rawheight=\"280\" class=\"origin_image zh-lightbox-thumb\" width=\"1008\" data-original=\"https://pic4.zhimg.com/v2-6f39ca6237e9e1f319ecab243a63b513_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1008&#39; height=&#39;280&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1008\" data-rawheight=\"280\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1008\" data-original=\"https://pic4.zhimg.com/v2-6f39ca6237e9e1f319ecab243a63b513_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6f39ca6237e9e1f319ecab243a63b513_b.jpg\"/></figure><p><br/>有很明显的分页表示</p><p>列表如下</p><div class=\"highlight\"><pre><code class=\"language-text\">Request URL: http://cgk.kxjs.tj.gov.cn/navigation.do\nRequest Method: POST</code></pre></div><p>参数说明,里面两个比较重要的 <code>pageNum</code> 页码，<code>numPerPage</code> 每页显示的数据</p><div class=\"highlight\"><pre><code class=\"language-text\">trades: \nfields: \nenterprise_type: \narchive_year: \nhsql: \nsearchKey: \npageNum: 2\nnumPerPage: 25\ndate_low: \ndate_high:</code></pre></div><h3>拼接地址</h3><p>由于是POST请求，所以需要引入<code>FormRequest</code> 类。重写<code>start_requests</code>方法，注意</p><div class=\"highlight\"><pre><code class=\"language-text\">yield FormRequest(url=self.start_url,callback=self.parse,formdata=data,dont_filter=True)</code></pre></div><p>中<code>dont_filter=True</code> 不过滤重复请求。</p><div class=\"highlight\"><pre><code class=\"language-text\">import scrapy\nfrom scrapy import Request,FormRequest,Selector\nimport time\n\nclass TjSpider(scrapy.Spider):\n    name = &#39;Tj&#39;\n    allowed_domains = [&#39;cgk.kxjs.tj.gov.cn&#39;]\n    start_url = &#34;http://cgk.kxjs.tj.gov.cn/navigation.do&#34;\n\n    def start_requests(self):\n        #yield scrapy.Request(url=&#34;http://cgk.kxjs.tj.gov.cn/detail.do?id=1&#34;, callback=self.parse_detail)\n        for i in range(1,73): #73\n            data = {\n                &#34;trades&#34;:&#34;&#34;,\n                &#34;fields&#34;:&#34;&#34;,\n                &#34;enterprise_type&#34;:&#34;&#34;,\n                &#34;archive_year&#34;:&#34;&#34;,\n                &#34;hsql&#34;:&#34;&#34;,\n                &#34;searchKey&#34;:&#34;&#34;,\n                &#34;pageNum&#34;: str(i),\n                &#34;numPerPage&#34;: &#34;25&#34;,\n                &#34;date_low&#34;:&#34;&#34;,\n                &#34;date_high&#34;:&#34;&#34;,\n            }\n            print(&#34;正在爬取{i}&#34;.format(i=i))\n            yield FormRequest(url=self.start_url,callback=self.parse,formdata=data,dont_filter=True)\n            time.sleep(10)</code></pre></div><h3>数据解析</h3><p>这个步骤分为2步，第一步解析列表页，获取详情页面的链接，第二步获取具体的字段，在匹配字段的时候采用字典动态更新，用来生成mongodb的字典格式。</p><div class=\"highlight\"><pre><code class=\"language-text\">def parse(self, response):\n\n        links = response.css(&#39;#Result tr td:nth-child(1)&gt;a::attr(href)&#39;).extract()\n        date = response.css(&#39;#Result tr td:nth-child(2)::text&#39;).extract()\n\n        for item in range(len(links)):\n            # yield {\n            #     &#34;link&#34;:links[item],\n            #     &#34;date&#34;:date[item]\n            # }\n            yield scrapy.Request(url=response.urljoin(links[0]),callback=self.parse_detail,meta={&#34;date&#34;:date[item]})\n\n    # python学习交流群 1004391443\n    def parse_detail(self,response):\n\n        trs = Selector(response).xpath(&#34;//table[@class=&#39;tab_lx003&#39;][2]/tbody/tr&#34;)\n        item = {}\n        item.update({&#34;date&#34;:response.meta[&#34;date&#34;]})\n        for tr_item in trs:\n            item.update({tr_item.xpath(&#34;td[1]/text()&#34;).extract_first():tr_item.xpath(&#34;td[2]/text()&#34;).extract_first()})\n        yield item\n        time.sleep(3)</code></pre></div><h3>科技计划项目成果数据入库</h3><p>入库操作非常简单了，走一遍之前的博客就可以，这个网站爬取的过程中没有太多问题，就是总是宕机掉，采用代理IP也没有解决，应该只是访问速度慢的原因，建议多爬取一下。</p><p>最后，发现详情页，最后的<code>id=数字</code>是连续性的，可以直接迭代</p><p><a href=\"https://link.zhihu.com/?target=http%3A//cgk.kxjs.tj.gov.cn/detail.do%3Fid%3D60\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">cgk.kxjs.tj.gov.cn/deta</span><span class=\"invisible\">il.do?id=60</span><span class=\"ellipsis\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ec773f433888d82793f3705915745d1a_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"180\" data-rawheight=\"173\" data-thumbnail=\"https://pic3.zhimg.com/v2-ec773f433888d82793f3705915745d1a_b.jpg\" class=\"content_image\" width=\"180\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;180&#39; height=&#39;173&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"180\" data-rawheight=\"173\" data-thumbnail=\"https://pic3.zhimg.com/v2-ec773f433888d82793f3705915745d1a_b.jpg\" class=\"content_image lazy\" width=\"180\" data-actualsrc=\"https://pic3.zhimg.com/v2-ec773f433888d82793f3705915745d1a_b.gif\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>对付这种小数据的网站，其实采用Selenium也未尝不可啊~~</p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "python爬虫", 
                    "tagLink": "https://api.zhihu.com/topics/20086364"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/66978273", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "Python爬虫神器scrapy框架爬取博客园Python相关40W博客！", 
            "content": "<p>经常看博客的同志知道，博客园每个栏目下面有200页，多了的数据他就不显示了，最多显示<code>4000篇博客</code>如何尽可能多的得到博客数据，是这篇文章研究的一点点核心内容，能√get到多少就看你的了~</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8a141513ea8f79412ae75529abb03650_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"554\" data-rawheight=\"109\" class=\"origin_image zh-lightbox-thumb\" width=\"554\" data-original=\"https://pic1.zhimg.com/v2-8a141513ea8f79412ae75529abb03650_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;554&#39; height=&#39;109&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"554\" data-rawheight=\"109\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"554\" data-original=\"https://pic1.zhimg.com/v2-8a141513ea8f79412ae75529abb03650_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8a141513ea8f79412ae75529abb03650_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>单纯的从每个栏目去爬取是不显示的，转换一下思路，看到搜索页面，有时间~，有时间！<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a1bafe7afb776db7f5e521277abb15c7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"380\" data-rawheight=\"434\" class=\"content_image\" width=\"380\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;380&#39; height=&#39;434&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"380\" data-rawheight=\"434\" class=\"content_image lazy\" width=\"380\" data-actualsrc=\"https://pic4.zhimg.com/v2-a1bafe7afb776db7f5e521277abb15c7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>注意看URL链接</p><div class=\"highlight\"><pre><code class=\"language-text\">https://zzk.cnblogs.com/s/blogpost?Keywords=python&amp;datetimerange=Customer&amp;from=2019-01-01&amp;to=2019-01-01  </code></pre></div><p>这个链接得到之后，其实用一个比较简单的思路就可以获取到所有python相关的文章了，迭代时间。<br/>下面编写核心代码，比较重要的几个点，我单独提炼出来。</p><ol><li>页面搜索的时候因为加了验证，所以你必须要获取到你本地的cookie，这个你很容易得到</li><li>字典生成器的语法是时候去复习一下了</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">import scrapy\nfrom scrapy import Request,Selector\nimport time\nimport datetime\n\nclass BlogsSpider(scrapy.Spider):\n    name = &#39;Blogs&#39;\n    allowed_domains = [&#39;zzk.cnblogs.com&#39;]\n    start_urls = [&#39;http://zzk.cnblogs.com/&#39;]\n    from_time = &#34;2010-01-01&#34;\n    end_time = &#34;2010-01-01&#34;\n    keywords = &#34;python&#34;\n    page =1\n    url = &#34;https://zzk.cnblogs.com/s/blogpost?Keywords={keywords}&amp;datetimerange=Customer&amp;from={from_time}&amp;to={end_time}&amp;pageindex={page}&#34;\n    custom_settings = {\n        &#34;DEFAULT_REQUEST_HEADERS&#34;:{\n            &#34;HOST&#34;:&#34;zzk.cnblogs.com&#34;,\n            &#34;TE&#34;:&#34;Trailers&#34;,\n            &#34;referer&#34;: &#34;https://zzk.cnblogs.com/s/blogpost?w=python&#34;,\n            &#34;upgrade-insecure-requests&#34;: &#34;1&#34;,\n            &#34;user-agent&#34;: &#34;Mozilla/5.0 Gecko/20100101 Firefox/64.0&#34;\n\n        }\n    }\n\n\n    def start_requests(self):\n        cookie_str = &#34;想办法自己获取到&#34;\n        self.cookies = {item.split(&#34;=&#34;)[0]: item.split(&#34;=&#34;)[1] for item in cookie_str.split(&#34;; &#34;)}\n        yield Request(self.url.format(keywords=self.keywords,from_time=self.from_time,end_time=self.end_time,page=self.page),cookies=self.cookies,callback=self.parse)</code></pre></div><p>页面爬取完毕之后，需要进行解析，获取翻页页码，同时将时间+1天，下面的代码重点看时间叠加部分的操作。</p><div class=\"highlight\"><pre><code class=\"language-text\">def parse(self, response):\n        print(&#34;正在爬取&#34;,response.url)\n        count = int(response.css(&#39;#CountOfResults::text&#39;).extract_first()) # 获取是否有数据\n        if count&gt;0:\n            for page in range(1,int(count/10)+2):\n                # 抓取详细数据\n                yield Request(self.url.format(keywords=self.keywords,from_time=self.from_time,end_time=self.end_time,page=page),cookies=self.cookies,callback=self.parse_detail,dont_filter=True)\n\n        time.sleep(2)\n        # 跳转下一个日期\n        d = datetime.datetime.strptime(self.from_time, &#39;%Y-%m-%d&#39;)\n        delta = datetime.timedelta(days=1)\n        d = d + delta\n        self.from_time = d.strftime(&#39;%Y-%m-%d&#39;)\n        self.end_time =self.from_time\n        yield Request(\n            self.url.format(keywords=self.keywords, from_time=self.from_time, end_time=self.end_time, page=self.page),\n            cookies=self.cookies, callback=self.parse, dont_filter=True)</code></pre></div><h2><b>页面解析入库</b></h2><p>本部分操作逻辑没有复杂点，只需要按照流程编写即可，运行代码，跑起来，在mongodb等待一些时间</p><div class=\"highlight\"><pre><code class=\"language-text\">db.getCollection(&#39;dict&#39;).count({})</code></pre></div><p>返回</p><div class=\"highlight\"><pre><code class=\"language-text\">372352条数据\ndef parse_detail(self,response):\n        items = response.xpath(&#39;//div[@class=&#34;searchItem&#34;]&#39;)\n        for item in items:\n            title = item.xpath(&#39;h3[@class=&#34;searchItemTitle&#34;]/a//text()&#39;).extract()\n            title = &#34;&#34;.join(title)\n\n            author = item.xpath(&#34;.//span[@class=&#39;searchItemInfo-userName&#39;]/a/text()&#34;).extract_first()\n            public_date = item.xpath(&#34;.//span[@class=&#39;searchItemInfo-publishDate&#39;]/text()&#34;).extract_first()\n            pv = item.xpath(&#34;.//span[@class=&#39;searchItemInfo-views&#39;]/text()&#34;).extract_first()\n            if pv:\n                pv = pv[3:-1]\n            url = item.xpath(&#34;.//span[@class=&#39;searchURL&#39;]/text()&#34;).extract_first()\n            #print(title,author,public_date,pv)\n            yield {\n                &#34;title&#34;:title,\n                &#34;author&#34;:author,\n                &#34;public_date&#34;:public_date,\n                &#34;pv&#34;:pv,\n                &#34;url&#34;:url\n            }</code></pre></div><h2><b>数据入库</b></h2><p>一顿操作猛如虎，数据就到手了~后面可以做一些简单的数据分析，那篇博客再见啦@</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d5d1ed063d1df014017ade082b1dd14d_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"324\" data-thumbnail=\"https://pic2.zhimg.com/v2-d5d1ed063d1df014017ade082b1dd14d_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"600\" data-original=\"https://pic2.zhimg.com/v2-d5d1ed063d1df014017ade082b1dd14d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;600&#39; height=&#39;324&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"324\" data-thumbnail=\"https://pic2.zhimg.com/v2-d5d1ed063d1df014017ade082b1dd14d_b.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"600\" data-original=\"https://pic2.zhimg.com/v2-d5d1ed063d1df014017ade082b1dd14d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d5d1ed063d1df014017ade082b1dd14d_b.gif\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "python爬虫", 
                    "tagLink": "https://api.zhihu.com/topics/20086364"
                }, 
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "scrapy", 
                    "tagLink": "https://api.zhihu.com/topics/19950086"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/66222007", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "python scrapy框架爬取天津市科技计划项目成果库数据", 
            "content": "<p>今天本来没有打算抓取这个网站的，无意中看到某个微信群有人问了一嘴这个网站，想看一下有什么特别复杂的地方，一顿操作下来，发现这个网站除了卡慢，经常自己宕机以外，好像还真没有什么特殊的....</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-db35eba3d5cbbb72bc6c1f1a7c7a54ec_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"350\" data-rawheight=\"320\" class=\"content_image\" width=\"350\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;350&#39; height=&#39;320&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"350\" data-rawheight=\"320\" class=\"content_image lazy\" width=\"350\" data-actualsrc=\"https://pic1.zhimg.com/v2-db35eba3d5cbbb72bc6c1f1a7c7a54ec_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h3>爬取网址 <a href=\"https://link.zhihu.com/?target=http%3A//cgk.kxjs.tj.gov.cn/navigation.do\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">cgk.kxjs.tj.gov.cn/navi</span><span class=\"invisible\">gation.do</span><span class=\"ellipsis\"></span></a></h3><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6f39ca6237e9e1f319ecab243a63b513_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1008\" data-rawheight=\"280\" class=\"origin_image zh-lightbox-thumb\" width=\"1008\" data-original=\"https://pic4.zhimg.com/v2-6f39ca6237e9e1f319ecab243a63b513_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1008&#39; height=&#39;280&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1008\" data-rawheight=\"280\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1008\" data-original=\"https://pic4.zhimg.com/v2-6f39ca6237e9e1f319ecab243a63b513_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6f39ca6237e9e1f319ecab243a63b513_b.jpg\"/></figure><p><br/>有很明显的分页表示</p><p>列表如下</p><div class=\"highlight\"><pre><code class=\"language-text\">Request URL: http://cgk.kxjs.tj.gov.cn/navigation.do\nRequest Method: POST</code></pre></div><p>参数说明,里面两个比较重要的 <code>pageNum</code> 页码，<code>numPerPage</code> 每页显示的数据</p><div class=\"highlight\"><pre><code class=\"language-text\">trades: \nfields: \nenterprise_type: \narchive_year: \nhsql: \nsearchKey: \npageNum: 2\nnumPerPage: 25\ndate_low: \ndate_high:</code></pre></div><h3>拼接地址</h3><p>由于是POST请求，所以需要引入<code>FormRequest</code> 类。重写<code>start_requests</code>方法，注意</p><div class=\"highlight\"><pre><code class=\"language-text\">yield FormRequest(url=self.start_url,callback=self.parse,formdata=data,dont_filter=True)</code></pre></div><p>中<code>dont_filter=True</code> 不过滤重复请求。</p><div class=\"highlight\"><pre><code class=\"language-text\">import scrapy\nfrom scrapy import Request,FormRequest,Selector\nimport time\n\nclass TjSpider(scrapy.Spider):\n    name = &#39;Tj&#39;\n    allowed_domains = [&#39;cgk.kxjs.tj.gov.cn&#39;]\n    start_url = &#34;http://cgk.kxjs.tj.gov.cn/navigation.do&#34;\n\n    def start_requests(self):\n        #yield scrapy.Request(url=&#34;http://cgk.kxjs.tj.gov.cn/detail.do?id=1&#34;, callback=self.parse_detail)\n        for i in range(1,73): #73\n            data = {\n                &#34;trades&#34;:&#34;&#34;,\n                &#34;fields&#34;:&#34;&#34;,\n                &#34;enterprise_type&#34;:&#34;&#34;,\n                &#34;archive_year&#34;:&#34;&#34;,\n                &#34;hsql&#34;:&#34;&#34;,\n                &#34;searchKey&#34;:&#34;&#34;,\n                &#34;pageNum&#34;: str(i),\n                &#34;numPerPage&#34;: &#34;25&#34;,\n                &#34;date_low&#34;:&#34;&#34;,\n                &#34;date_high&#34;:&#34;&#34;,\n            }\n            print(&#34;正在爬取{i}&#34;.format(i=i))\n            yield FormRequest(url=self.start_url,callback=self.parse,formdata=data,dont_filter=True)\n            time.sleep(10)</code></pre></div><h3>数据解析</h3><p>这个步骤分为2步，第一步解析列表页，获取详情页面的链接，第二步获取具体的字段，在匹配字段的时候采用字典动态更新，用来生成mongodb的字典格式。</p><div class=\"highlight\"><pre><code class=\"language-text\">def parse(self, response):\n\n        links = response.css(&#39;#Result tr td:nth-child(1)&gt;a::attr(href)&#39;).extract()\n        date = response.css(&#39;#Result tr td:nth-child(2)::text&#39;).extract()\n\n        for item in range(len(links)):\n            # yield {\n            #     &#34;link&#34;:links[item],\n            #     &#34;date&#34;:date[item]\n            # }\n            yield scrapy.Request(url=response.urljoin(links[0]),callback=self.parse_detail,meta={&#34;date&#34;:date[item]})\n\n\n    def parse_detail(self,response):\n\n        trs = Selector(response).xpath(&#34;//table[@class=&#39;tab_lx003&#39;][2]/tbody/tr&#34;)\n        item = {}\n        item.update({&#34;date&#34;:response.meta[&#34;date&#34;]})\n        for tr_item in trs:\n            item.update({tr_item.xpath(&#34;td[1]/text()&#34;).extract_first():tr_item.xpath(&#34;td[2]/text()&#34;).extract_first()})\n        yield item\n        time.sleep(3)</code></pre></div><h3>科技计划项目成果数据入库</h3><p>入库操作非常简单了，走一遍之前的博客就可以，这个网站爬取的过程中没有太多问题，就是总是宕机掉，采用代理IP也没有解决，应该只是访问速度慢的原因，建议多爬取一下。</p><p>最后，发现详情页，最后的<code>id=数字</code>是连续性的，可以直接迭代，小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~</p><p><a href=\"https://link.zhihu.com/?target=http%3A//cgk.kxjs.tj.gov.cn/detail.do%3Fid%3D60\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">cgk.kxjs.tj.gov.cn/deta</span><span class=\"invisible\">il.do?id=60</span><span class=\"ellipsis\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ec773f433888d82793f3705915745d1a_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"180\" data-rawheight=\"173\" data-thumbnail=\"https://pic3.zhimg.com/v2-ec773f433888d82793f3705915745d1a_b.jpg\" class=\"content_image\" width=\"180\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;180&#39; height=&#39;173&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"180\" data-rawheight=\"173\" data-thumbnail=\"https://pic3.zhimg.com/v2-ec773f433888d82793f3705915745d1a_b.jpg\" class=\"content_image lazy\" width=\"180\" data-actualsrc=\"https://pic3.zhimg.com/v2-ec773f433888d82793f3705915745d1a_b.gif\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>对付这种小数据的网站，其实采用Selenium也未尝不可啊~~</p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "python爬虫", 
                    "tagLink": "https://api.zhihu.com/topics/20086364"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/66221161", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 1, 
            "title": "利用python 爬虫scrapy框架爬取教育部高考名单数据！一键获取自己的信息！", 
            "content": "<h2><b>爬前叨叨</b></h2><p>今天要爬取一下正规大学名单，这些名单是教育部公布具有招生资格的高校名单，除了这些学校以外，其他招生的单位，其所招学生的学籍、发放的毕业证书国家均不予承认，也就是俗称的<b>野鸡大学</b>！</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-faace275bc579e34aa535cbbac5dc937_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"275\" data-rawheight=\"237\" class=\"content_image\" width=\"275\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;275&#39; height=&#39;237&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"275\" data-rawheight=\"237\" class=\"content_image lazy\" width=\"275\" data-actualsrc=\"https://pic4.zhimg.com/v2-faace275bc579e34aa535cbbac5dc937_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>网址是 <code>https://daxue.eol.cn/mingdan.shtml</code> 爬取完毕之后，我们进行一些基本的数据分析，套路如此类似，哈哈</p><p>这个小项目采用的是<code>scrapy</code>，关键代码</p><div class=\"highlight\"><pre><code class=\"language-text\">import scrapy\nfrom scrapy import Request,Selector\n\nclass SchoolSpider(scrapy.Spider):\n    name = &#39;School&#39;\n    allowed_domains = [&#39;daxue.eol.cn&#39;]\n    start_urls = [&#39;https://daxue.eol.cn/mingdan.shtml&#39;]\n\n    def parse(self, response):\n        select = Selector(response)\n        links = select.css(&#34;.province&gt;a&#34;)\n        \n        for item in links:\n            name = item.css(&#34;::text&#34;).extract_first()\n            link = item.css(&#34;::attr(href)&#34;).extract_first()\n\n            if name in [&#34;河南&#34;,&#34;山东&#34;]:\n                yield Request(link,callback=self.parse_he_shan,meta={&#34;name&#34; : name})\n            else:\n                yield Request(link,callback=self.parse_school,meta={&#34;name&#34; : name})</code></pre></div><p>注意到几个问题，第一个所有的页面都可以通过第一步抓取到<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4c70e05a14fb725ab1bd6832b57caa8c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1180\" data-rawheight=\"234\" class=\"origin_image zh-lightbox-thumb\" width=\"1180\" data-original=\"https://pic1.zhimg.com/v2-4c70e05a14fb725ab1bd6832b57caa8c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1180&#39; height=&#39;234&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1180\" data-rawheight=\"234\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1180\" data-original=\"https://pic1.zhimg.com/v2-4c70e05a14fb725ab1bd6832b57caa8c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-4c70e05a14fb725ab1bd6832b57caa8c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>但是里面出现了两个特殊页面，也就是山东和河南</p><p><b>北京等学校</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5b65ac773bdfae9ed6a4dc44735ff75b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1241\" data-rawheight=\"518\" class=\"origin_image zh-lightbox-thumb\" width=\"1241\" data-original=\"https://pic4.zhimg.com/v2-5b65ac773bdfae9ed6a4dc44735ff75b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1241&#39; height=&#39;518&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1241\" data-rawheight=\"518\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1241\" data-original=\"https://pic4.zhimg.com/v2-5b65ac773bdfae9ed6a4dc44735ff75b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5b65ac773bdfae9ed6a4dc44735ff75b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>河南等学校</b><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ca7faba30b70d78d9795ecc004fb81db_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1183\" data-rawheight=\"592\" class=\"origin_image zh-lightbox-thumb\" width=\"1183\" data-original=\"https://pic4.zhimg.com/v2-ca7faba30b70d78d9795ecc004fb81db_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1183&#39; height=&#39;592&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1183\" data-rawheight=\"592\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1183\" data-original=\"https://pic4.zhimg.com/v2-ca7faba30b70d78d9795ecc004fb81db_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ca7faba30b70d78d9795ecc004fb81db_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>对于两种不同的排版，我们采用2个方法处理，细节的地方看代码就可以啦！<br/>尤其是下面对字符串的处理，你要仔细的查阅~</p><div class=\"highlight\"><pre><code class=\"language-text\"># 专门为河南和山东编写的提取方法\n    def parse_he_shan(self,response):\n        name = response.meta[&#34;name&#34;]\n        data = response.css(&#34;.table-x tr&#34;)\n        for item in data:\n            school_name = item.css(&#34;td:not(.tmax)::text&#34;).extract()\n\n            if len(school_name)&gt;0:\n                for s in school_name:\n                    if len(s.strip())&gt;0:\n                        if len(s.split(&#34;.&#34;))==1:\n                            last_name = s.split(&#34;.&#34;)[0]\n                        else:\n                            last_name = s.split(&#34;.&#34;)[1]  # 最终获取到的名字\n                        yield {\n                            &#34;city_name&#34;: name,\n                            &#34;school_name&#34;: last_name,\n                            &#34;code&#34;: &#34;&#34;,\n                            &#34;department&#34;: &#34;&#34;,\n                            &#34;location&#34;: &#34;&#34;,\n                            &#34;subject&#34;: &#34;&#34;,\n                            &#34;private&#34;: &#34;&#34;\n                        }\n\n    # 通用学校提取\n    def parse_school(self,response):\n        name = response.meta[&#34;name&#34;]\n\n        schools = response.css(&#34;.table-x tr&#34;)[2:]\n\n        for item in schools:\n\n            school_name = item.css(&#34;td:nth-child(2)::text&#34;).extract_first()\n            code =  item.css(&#34;td:nth-child(3)::text&#34;).extract_first()\n            department = item.css(&#34;td:nth-child(4)::text&#34;).extract_first()\n            location = item.css(&#34;td:nth-child(5)::text&#34;).extract_first()\n            subject = item.css(&#34;td:nth-child(6)::text&#34;).extract_first()\n            private = item.css(&#34;td:nth-child(7)::text&#34;).extract_first()\n            yield {\n                &#34;city_name&#34;:name,\n                &#34;school_name&#34;:school_name,\n                &#34;code&#34;:code,\n                &#34;department&#34;:department,\n                &#34;location&#34;:location,\n                &#34;subject&#34;:subject,\n                &#34;private&#34;:private\n            }</code></pre></div><p>运行代码，跑起来，一会数据到手。O(∩_∩)O哈哈~</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f5ac63a305191a0896cf6949f0e9eb4c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"708\" data-rawheight=\"331\" class=\"origin_image zh-lightbox-thumb\" width=\"708\" data-original=\"https://pic1.zhimg.com/v2-f5ac63a305191a0896cf6949f0e9eb4c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;708&#39; height=&#39;331&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"708\" data-rawheight=\"331\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"708\" data-original=\"https://pic1.zhimg.com/v2-f5ac63a305191a0896cf6949f0e9eb4c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f5ac63a305191a0896cf6949f0e9eb4c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>查看专科学校和本科学校数量差别</b></h2><p>因为河南和山东数据的缺失，需要踢出这两个省份</p><div class=\"highlight\"><pre><code class=\"language-text\">import pymongo\nimport numpy as np\nimport pandas as pd\nfrom pandas import Series,DataFrame\nimport matplotlib.pyplot as plt\n\nclient = pymongo.MongoClient(&#34;localhost&#34;,27017)\nschools = client[&#34;school&#34;]\ncollection = schools[&#34;schools&#34;]\n\ndf = DataFrame(list(collection.find()))\n\ndf = df[df[&#34;code&#34;]!=&#34;&#34;]\n# 汇总本科和专业\ndf.groupby([&#34;subject&#34;]).size()</code></pre></div><p>结果显示，数量基本平衡</p><div class=\"highlight\"><pre><code class=\"language-text\">subject\n专科    1240\n本科    1121\ndtype: int64</code></pre></div><h2><b>查看各省排名</b></h2><div class=\"highlight\"><pre><code class=\"language-text\">rank = df.groupby(by=&#34;city_name&#34;).size()\nrank = rank.sort_values(ascending=False)\n\n# 设置中文字体和负号正常显示\nplt.rcParams[&#39;font.sans-serif&#39;] = [&#39;SimHei&#39;]\nplt.rcParams[&#39;axes.unicode_minus&#39;] = False\n\nplt.figure(figsize=(12,8),dpi=80)\nplt.subplot(1,1,1)\n\n\nx = np.arange(len(rank.index))\ny = rank.values\nrect = plt.bar(left=x,height=y,width=0.618,label=&#34;学校数目&#34;,align=&#34;center&#34;,color=&#34;#03a9f4&#34;,edgecolor=&#34;#03a9f4&#34;,)\n\nplt.xticks(x,rank.index,rotation=45,fontsize=9)\nplt.yticks(np.arange(0,180,10))\n\n\nplt.xlabel(&#34;城市&#34;)\nplt.ylabel(&#34;大学数量&#34;)\n\nplt.legend(loc = &#34;upper right&#34;)\n\n## 编辑文本\n\nfor r in rect:\n    height = r.get_height() # 获取高度\n    \n    plt.text(r.get_x()+r.get_width()/2,height+1,str(height),size=6,ha=&#34;center&#34;,va=&#34;bottom&#34;)\n\nplt.show()</code></pre></div><blockquote><i>好好研究这部分代码，咱已经开始慢慢的在爬虫中添加数据分析的内容了，我会尽量把一些常见的参数写的清晰一些</i></blockquote><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cb97cf441635650ad3e3d4674721a1c4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1023\" data-rawheight=\"689\" class=\"origin_image zh-lightbox-thumb\" width=\"1023\" data-original=\"https://pic1.zhimg.com/v2-cb97cf441635650ad3e3d4674721a1c4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1023&#39; height=&#39;689&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1023\" data-rawheight=\"689\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1023\" data-original=\"https://pic1.zhimg.com/v2-cb97cf441635650ad3e3d4674721a1c4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-cb97cf441635650ad3e3d4674721a1c4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>江苏和广东大学真多~小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-baef32080e09084fe568199b05813856_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"120\" data-rawheight=\"120\" class=\"content_image\" width=\"120\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;120&#39; height=&#39;120&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"120\" data-rawheight=\"120\" class=\"content_image lazy\" width=\"120\" data-actualsrc=\"https://pic3.zhimg.com/v2-baef32080e09084fe568199b05813856_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "python爬虫", 
                    "tagLink": "https://api.zhihu.com/topics/20086364"
                }
            ], 
            "comments": [
                {
                    "userName": "张幺羽", 
                    "userLink": "https://www.zhihu.com/people/60f3831f43b0f6c81548841392697f22", 
                    "content": "亲 这个输出怎么写鸭", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/65630728", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "Python爬虫入门教程第三十二讲： scrapy爬取云沃客项目外包网数据！", 
            "content": "<h2><b>爬前叨叨</b></h2><p>​闲暇写一个外包网站的爬虫，万一你从这个外包网站弄点外快呢，呵呵哒</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-26a1cd091130ba5707613245ae050c96_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"440\" data-rawheight=\"405\" class=\"origin_image zh-lightbox-thumb\" width=\"440\" data-original=\"https://pic3.zhimg.com/v2-26a1cd091130ba5707613245ae050c96_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;440&#39; height=&#39;405&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"440\" data-rawheight=\"405\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"440\" data-original=\"https://pic3.zhimg.com/v2-26a1cd091130ba5707613245ae050c96_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-26a1cd091130ba5707613245ae050c96_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>数据分析</b></h2><p>官方网址为 <code>https://www.clouderwork.com/</code></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c3432ee82cdb1410b250025dc9cfec17_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"478\" data-rawheight=\"218\" class=\"origin_image zh-lightbox-thumb\" width=\"478\" data-original=\"https://pic4.zhimg.com/v2-c3432ee82cdb1410b250025dc9cfec17_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;478&#39; height=&#39;218&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"478\" data-rawheight=\"218\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"478\" data-original=\"https://pic4.zhimg.com/v2-c3432ee82cdb1410b250025dc9cfec17_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c3432ee82cdb1410b250025dc9cfec17_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>进入全部项目列表页面，很容易分辨出来项目的分页方式</p><p>get异步请求</p><div class=\"highlight\"><pre><code class=\"language-text\">Request URL:https://www.clouderwork.com/api/v2/jobs/search?ts=1546395904852&amp;keyword=&amp;budget_range=&amp;work_status=&amp;pagesize=20&amp;pagenum=3&amp;sort=1&amp;scope=\nRequest Method:GET\nStatus Code:200 OK</code></pre></div><p>参数如下</p><div class=\"highlight\"><pre><code class=\"language-text\">ts:1546395904852  # 时间戳\n    keyword:   # 搜索关键字，查找全部，使用空即可\n    budget_range:   # 暂时无用\n    work_status:\n    pagesize:20   # 每页数据量\n    pagenum:3   # 页码\n    sort:1   # 排序规则\n    scope:</code></pre></div><p>下面就是拼接请求了，确定一下 <code>request</code> 相关参数</p><div class=\"highlight\"><pre><code class=\"language-text\">Accept:application/json, text/javascript, */*; q=0.01\nAccept-Encoding:gzip, deflate, br\nAccept-Language:zh-CN,zh;q=0.9\nConnection:keep-alive\nCookie:\nHost:www.clouderwork.com\nReferer:https://www.clouderwork.com/jobs?keyword=\nUser-Agent:Mozilla/5.0 你自己的UA QQBrowser/10.3.3006.400\nX-Requested-With:XMLHttpRequest</code></pre></div><p>爬虫采用<code>scrapy</code><br/>这个网站没有反爬措施，所以直接上就可以了</p><div class=\"highlight\"><pre><code class=\"language-text\"># -*- coding: utf-8 -*-\nimport scrapy\nfrom scrapy import Request\nimport time\nimport json\n\nclass CloudeworkSpider(scrapy.Spider):\n    name = &#39;cloudework&#39;\n    allowed_domains = [&#39;www.clouderwork.com&#39;]\n    start_url = &#39;https://www.clouderwork.com/api/v2/jobs/search?ts={times}&amp;keyword=&amp;budget_range=&amp;work_status=&amp;pagesize={pagesize}&amp;pagenum={pagenum}&amp;sort=1&amp;scope=&#39;\n\n    def start_requests(self):\n        for page in range(1,353):\n            yield Request(self.start_url.format(times=time.time(),pagesize=20,pagenum=page))\n\n    def parse(self, response):\n        json_data = json.loads(response.text)\n        for item in  json_data[&#34;jobs&#34;]:\n            yield item</code></pre></div><p>数据存储到 <code>mongodb</code>中，合计爬取到 <b>7000+</b> 数据</p><h2><b>数据分析</b></h2><h3>从mongdo读取数据</h3><div class=\"highlight\"><pre><code class=\"language-text\">import pymongo\nimport pandas as pd\nfrom pandas import Series,DataFrame\nimport matplotlib.pyplot as plt\n\nplt.rcParams[&#39;font.sans-serif&#39;]=[&#39;SimHei&#39;] #用来正常显示中文标签\nplt.rcParams[&#39;axes.unicode_minus&#39;]=False #用来正常显示负号\n# 连接数据库\nclient = pymongo.MongoClient(&#34;localhost&#34;,27017)\ncloud = client[&#34;cloud&#34;]\ncollection = cloud[&#34;cloudework&#34;]\n\n# 加载数据\ndata = DataFrame(list(collection.find()))</code></pre></div><p>结果显示为 <code>[7032 rows x 35 columns]</code></p><h3>查看数据基本情况</h3><p>直接使用<code>data.shape</code> 可以查看一下数据的基本情况</p><h3>查看一下工期的分布</h3><div class=\"highlight\"><pre><code class=\"language-text\">periods = data.groupby([&#34;period&#34;]).size()\n\nx = periods.index \ny = periods.values \nplt.figure()\nplt.scatter(x,y, color=&#34;#03a9f4&#34;, alpha = 0.5) # 绘制图表\nplt.xlim((0, 360))\nplt.ylim((0, 2000))\nplt.xlabel(&#34;工期&#34;)\nplt.ylabel(&#34;项目数&#34;)\nplt.show()</code></pre></div><p>可以看到数据散点集中在0~50天</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-132e7e9f9a05202df855ae5171684b4c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"583\" data-rawheight=\"377\" class=\"origin_image zh-lightbox-thumb\" width=\"583\" data-original=\"https://pic1.zhimg.com/v2-132e7e9f9a05202df855ae5171684b4c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;583&#39; height=&#39;377&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"583\" data-rawheight=\"377\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"583\" data-original=\"https://pic1.zhimg.com/v2-132e7e9f9a05202df855ae5171684b4c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-132e7e9f9a05202df855ae5171684b4c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>过滤一下40天以内的数据</p><div class=\"highlight\"><pre><code class=\"language-text\">periods = data.groupby([&#34;period&#34;]).size().reset_index(name=&#34;count&#34;)\n\ndf = periods[periods[&#34;period&#34;]&lt;=40]\n\nx = df[&#34;period&#34;]\ny = df[&#34;count&#34;]\n\nplt.figure()\nplt.scatter(x,y,label=&#39;项目数折线&#39;,color=&#34;#ff44cc&#34;)\nplt.title(&#34;工期对应项目数&#34;)\nplt.xlim((0, 360))\nplt.ylim((0, 500))\nplt.show()</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c1b4ba9e97a9ea25f128cb761b42b0ff_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"559\" data-rawheight=\"366\" class=\"origin_image zh-lightbox-thumb\" width=\"559\" data-original=\"https://pic4.zhimg.com/v2-c1b4ba9e97a9ea25f128cb761b42b0ff_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;559&#39; height=&#39;366&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"559\" data-rawheight=\"366\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"559\" data-original=\"https://pic4.zhimg.com/v2-c1b4ba9e97a9ea25f128cb761b42b0ff_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c1b4ba9e97a9ea25f128cb761b42b0ff_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>发现竟然有1天工期的任务，可以瞅瞅都是什么任务</p><div class=\"highlight\"><pre><code class=\"language-text\">periods = data.groupby([&#34;period&#34;]).size()\ndata[data[&#34;period&#34;]==1][[&#34;name&#34;,&#34;period&#34;]]</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b70cbdd2574f171e385ccc3c3036e9f7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"555\" data-rawheight=\"212\" class=\"origin_image zh-lightbox-thumb\" width=\"555\" data-original=\"https://pic4.zhimg.com/v2-b70cbdd2574f171e385ccc3c3036e9f7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;555&#39; height=&#39;212&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"555\" data-rawheight=\"212\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"555\" data-original=\"https://pic4.zhimg.com/v2-b70cbdd2574f171e385ccc3c3036e9f7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b70cbdd2574f171e385ccc3c3036e9f7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>果然比较简单唉~~不过也没有多少钱，有个急活，1000￥</p><h3>查看阅览量Top10</h3><div class=\"highlight\"><pre><code class=\"language-text\">views = data[&#34;views_count&#34;]\ntop10 = views.sort_values(ascending=False)[:10]\n\ntop10 = data[data.views_count.isin(top10.values)][[&#34;name&#34;,&#34;views_count&#34;,&#34;period&#34;,&#34;summary&#34;]]\ntop10</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-86fb53c6020d8f63e28830ef89d67bd7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1001\" data-rawheight=\"412\" class=\"origin_image zh-lightbox-thumb\" width=\"1001\" data-original=\"https://pic4.zhimg.com/v2-86fb53c6020d8f63e28830ef89d67bd7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1001&#39; height=&#39;412&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1001\" data-rawheight=\"412\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1001\" data-original=\"https://pic4.zhimg.com/v2-86fb53c6020d8f63e28830ef89d67bd7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-86fb53c6020d8f63e28830ef89d67bd7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h3>查阅一下开发模式</h3><p>看一下什么类型的项目比较多？？？数据上反应，Web网站和APP最多了，所以这方面的技能的大神么，可以冲一波了<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-327416f8cca2adcd9a06f53b561c09c3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"751\" data-rawheight=\"586\" class=\"origin_image zh-lightbox-thumb\" width=\"751\" data-original=\"https://pic4.zhimg.com/v2-327416f8cca2adcd9a06f53b561c09c3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;751&#39; height=&#39;586&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"751\" data-rawheight=\"586\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"751\" data-original=\"https://pic4.zhimg.com/v2-327416f8cca2adcd9a06f53b561c09c3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-327416f8cca2adcd9a06f53b561c09c3_b.jpg\"/></figure><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "python爬虫", 
                    "tagLink": "https://api.zhihu.com/topics/20086364"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/65630050", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 1, 
            "title": "Python爬虫入门教程第三十一讲： scrapy爬取酷安网全站应用", 
            "content": "<h2><b>爬前叨叨</b></h2><p>2018年就要结束了，还有4天，就要开始写2019年的教程了，没啥感动的，一年就这么过去了，今天要爬取一个网站叫做<b>酷安</b>，是一个应用商店，大家可以尝试从手机APP爬取，不过爬取APP的博客，我打算在50篇博客之后在写，所以现在就放一放啦~~~</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-635beab9b24038ffd730ae67902a04f7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"356\" data-rawheight=\"267\" class=\"content_image\" width=\"356\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;356&#39; height=&#39;267&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"356\" data-rawheight=\"267\" class=\"content_image lazy\" width=\"356\" data-actualsrc=\"https://pic4.zhimg.com/v2-635beab9b24038ffd730ae67902a04f7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>酷安网站打开首页之后是一个广告页面，点击头部的应用即可</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d63d54904a443c9c32b3443d789b3dc4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"766\" data-rawheight=\"271\" class=\"origin_image zh-lightbox-thumb\" width=\"766\" data-original=\"https://pic1.zhimg.com/v2-d63d54904a443c9c32b3443d789b3dc4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;766&#39; height=&#39;271&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"766\" data-rawheight=\"271\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"766\" data-original=\"https://pic1.zhimg.com/v2-d63d54904a443c9c32b3443d789b3dc4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d63d54904a443c9c32b3443d789b3dc4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>页面分析</b></h2><p>分页地址找到，这样就可以构建全部页面信息<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-571a6db4e2ae0bcc6eac903b7db013a8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"746\" data-rawheight=\"184\" class=\"origin_image zh-lightbox-thumb\" width=\"746\" data-original=\"https://pic1.zhimg.com/v2-571a6db4e2ae0bcc6eac903b7db013a8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;746&#39; height=&#39;184&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"746\" data-rawheight=\"184\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"746\" data-original=\"https://pic1.zhimg.com/v2-571a6db4e2ae0bcc6eac903b7db013a8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-571a6db4e2ae0bcc6eac903b7db013a8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们想要保存的数据找到，用来后续的数据分析<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7608bc5f2052d5c9875591f6d82abad0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"768\" data-rawheight=\"218\" class=\"origin_image zh-lightbox-thumb\" width=\"768\" data-original=\"https://pic1.zhimg.com/v2-7608bc5f2052d5c9875591f6d82abad0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;768&#39; height=&#39;218&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"768\" data-rawheight=\"218\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"768\" data-original=\"https://pic1.zhimg.com/v2-7608bc5f2052d5c9875591f6d82abad0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7608bc5f2052d5c9875591f6d82abad0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1cb209d06cc4c5073efb5ed441f3a4f7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"540\" data-rawheight=\"365\" class=\"origin_image zh-lightbox-thumb\" width=\"540\" data-original=\"https://pic4.zhimg.com/v2-1cb209d06cc4c5073efb5ed441f3a4f7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;540&#39; height=&#39;365&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"540\" data-rawheight=\"365\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"540\" data-original=\"https://pic4.zhimg.com/v2-1cb209d06cc4c5073efb5ed441f3a4f7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1cb209d06cc4c5073efb5ed441f3a4f7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上述信息都是我们需要的信息，接下来，只需要爬取即可，本篇文章使用的还是<code>scrapy</code>，所有的代码都会在文章中出现，阅读全文之后，你就拥有完整的代码啦</p><div class=\"highlight\"><pre><code class=\"language-text\">import scrapy\n\nfrom apps.items import AppsItem  # 导入item类\nimport re  # 导入正则表达式类\n\nclass AppsSpider(scrapy.Spider):\n    name = &#39;Apps&#39;\n    allowed_domains = [&#39;www.coolapk.com&#39;]\n    start_urls = [&#39;https://www.coolapk.com/apk?p=1&#39;]\n    custom_settings = {\n        &#34;DEFAULT_REQUEST_HEADERS&#34; :{\n            &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,\n            &#39;Accept-Language&#39;: &#39;en&#39;,\n            &#39;User-Agent&#39;:&#39;Mozilla/5.0 你的UA&#39;\n\n        }\n    }</code></pre></div><h3>代码讲解</h3><blockquote><i>custom_settings 第一次出现，目的是为了修改默认<code>setting.py</code></i> <i>文件中的配置</i></blockquote><div class=\"highlight\"><pre><code class=\"language-text\">def parse(self, response):\n        list_items = response.css(&#34;.app_left_list&gt;a&#34;)\n        for item in list_items:\n            url = item.css(&#34;::attr(&#39;href&#39;)&#34;).extract_first()\n\n            url = response.urljoin(url)\n\n            yield scrapy.Request(url,callback=self.parse_url)\n\n        next_page = response.css(&#39;.pagination li:nth-child(8) a::attr(href)&#39;).extract_first()\n        url = response.urljoin(next_page)\n        yield scrapy.Request(url, callback=self.parse)</code></pre></div><h3>代码讲解</h3><ol><li><i>response.css 可以解析网页，具体的语法，你可以参照上述代码，重点阅读 ::attr(&#39;href&#39;) 和 ::text</i></li><li><i>response.urljoin 用来合并URL</i></li><li><i>next_page 表示翻页</i></li></ol><p class=\"ztext-empty-paragraph\"><br/></p><p><code>parse_url函数</code>用来解析内页，本函数内容又出现了3个辅助函数，分别是<code>self.getinfo(response)</code>,<code>self.gettags(response)</code>，<code>self.getappinfo(response)</code> 还有<code>response.css().re</code>支持正则表达式匹配，可以匹配文字内部内容</p><div class=\"highlight\"><pre><code class=\"language-text\">def parse_url(self,response):\n        item = AppsItem()\n\n        item[&#34;title&#34;] = response.css(&#34;.detail_app_title::text&#34;).extract_first()\n        info = self.getinfo(response)\n\n        item[&#39;volume&#39;] = info[0]\n        item[&#39;downloads&#39;] = info[1]\n        item[&#39;follow&#39;] = info[2]\n        item[&#39;comment&#39;] = info[3]\n\n        item[&#34;tags&#34;] = self.gettags(response)\n        item[&#39;rank_num&#39;] = response.css(&#39;.rank_num::text&#39;).extract_first()\n        item[&#39;rank_num_users&#39;] = response.css(&#39;.apk_rank_p1::text&#39;).re(&#34;共(.*?)个评分&#34;)[0]\n        item[&#34;update_time&#34;],item[&#34;rom&#34;],item[&#34;developer&#34;] = self.getappinfo(response)\n\n        yield item</code></pre></div><p>三个辅助方法如下</p><div class=\"highlight\"><pre><code class=\"language-text\">def getinfo(self,response):\n\n        info = response.css(&#34;.apk_topba_message::text&#34;).re(&#34;\\s+(.*?)\\s+/\\s+(.*?)下载\\s+/\\s+(.*?)人关注\\s+/\\s+(.*?)个评论.*?&#34;)\n        return info\n\n    def gettags(self,response):\n        tags = response.css(&#34;.apk_left_span2&#34;)\n        tags = [item.css(&#39;::text&#39;).extract_first() for item in tags]\n\n        return tags\n\n    def getappinfo(self,response):\n        #app_info = response.css(&#34;.apk_left_title_info::text&#34;).re(&#34;[\\s\\S]+更新时间：(.*?)&#34;)\n        body_text = response.body_as_unicode()\n\n        update = re.findall(r&#34;更新时间：(.*)?[&lt;]&#34;,body_text)[0]\n        rom =  re.findall(r&#34;支持ROM：(.*)?[&lt;]&#34;,body_text)[0]\n        developer = re.findall(r&#34;开发者名称：(.*)?[&lt;]&#34;, body_text)[0]\n        return update,rom,developer</code></pre></div><h2><b>保存数据</b></h2><p>数据传输的item在这个地方就不提供给你了，需要从我的代码中去推断一下即可，哈哈</p><div class=\"highlight\"><pre><code class=\"language-text\">import pymongo\n\nclass AppsPipeline(object):\n\n    def __init__(self,mongo_url,mongo_db):\n        self.mongo_url = mongo_url\n        self.mongo_db = mongo_db\n\n    @classmethod\n    def from_crawler(cls,crawler):\n        return cls(\n            mongo_url=crawler.settings.get(&#34;MONGO_URL&#34;),\n            mongo_db=crawler.settings.get(&#34;MONGO_DB&#34;)\n        )\n\n    def open_spider(self,spider):\n        try:\n            self.client = pymongo.MongoClient(self.mongo_url)\n            self.db = self.client[self.mongo_db]\n            \n        except Exception as e:\n            print(e)\n\n    def process_item(self, item, spider):\n        name = item.__class__.__name__\n\n        self.db[name].insert(dict(item))\n        return item\n\n    def close_spider(self,spider):\n        self.client.close()</code></pre></div><h3>代码解读</h3><ol><li><i>open_spider 开启爬虫时，打开Mongodb</i></li><li><i>process_item 存储每一条数据</i></li><li><i>close_spider 关闭爬虫</i></li><li><i>重点查看本方法 from_crawler 是一个类方法，在初始化的时候，从<b>setting.py</b>中读取配置</i></li></ol><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">SPIDER_MODULES = [&#39;apps.spiders&#39;]\nNEWSPIDER_MODULE = &#39;apps.spiders&#39;\nMONGO_URL = &#39;127.0.0.1&#39;\nMONGO_DB = &#39;KuAn&#39;</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5bbb02bf89d5c36f3b661ea29b81a0a2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"312\" data-rawheight=\"277\" class=\"content_image\" width=\"312\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;312&#39; height=&#39;277&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"312\" data-rawheight=\"277\" class=\"content_image lazy\" width=\"312\" data-actualsrc=\"https://pic3.zhimg.com/v2-5bbb02bf89d5c36f3b661ea29b81a0a2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>得到数据</b></h2><p>调整一下爬取速度和并发数</p><div class=\"highlight\"><pre><code class=\"language-text\">DOWNLOAD_DELAY = 3\n# The download delay setting will honor only one of:\nCONCURRENT_REQUESTS_PER_DOMAIN = 8</code></pre></div><p>代码走起，经过一系列的努力，得到数据啦！！！<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3a3de6940c1bc2b1f5e191e07fe3c1d7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1192\" data-rawheight=\"474\" class=\"origin_image zh-lightbox-thumb\" width=\"1192\" data-original=\"https://pic4.zhimg.com/v2-3a3de6940c1bc2b1f5e191e07fe3c1d7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1192&#39; height=&#39;474&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1192\" data-rawheight=\"474\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1192\" data-original=\"https://pic4.zhimg.com/v2-3a3de6940c1bc2b1f5e191e07fe3c1d7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3a3de6940c1bc2b1f5e191e07fe3c1d7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>抽空写个酷安的数据分析，有需要源码的，自己从头到尾的跟着写一遍就O98K了~</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-441b3cb8345ce1bb14ff66c8bc6b201e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"240\" data-rawheight=\"240\" class=\"content_image\" width=\"240\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;240&#39; height=&#39;240&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"240\" data-rawheight=\"240\" class=\"content_image lazy\" width=\"240\" data-actualsrc=\"https://pic3.zhimg.com/v2-441b3cb8345ce1bb14ff66c8bc6b201e_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "python爬虫", 
                    "tagLink": "https://api.zhihu.com/topics/20086364"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/65489115", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "Python爬虫入门教程第三十讲： 知乎网全站用户爬虫 scrapy", 
            "content": "<h2><b>爬前叨叨</b></h2><p>全站爬虫有时候做起来其实比较容易，因为规则相对容易建立起来，只需要做好反爬就可以了，今天咱们爬取知乎。继续使用<code>scrapy</code>当然对于这个小需求来说，使用scrapy确实用了牛刀，不过毕竟本博客这个系列到这个阶段需要不断使用<code>scrapy</code>进行过度，so，我写了一会就写完了。</p><p>你第一步找一个爬取种子，算作爬虫入口</p><p><code>https://www.zhihu.com/people/zhang-jia-wei/following</code></p><p>我们需要的信息如下，所有的框图都是我们需要的信息。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b810aa53573ea30d08e6013b88bd6b82_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1119\" data-rawheight=\"593\" class=\"origin_image zh-lightbox-thumb\" width=\"1119\" data-original=\"https://pic3.zhimg.com/v2-b810aa53573ea30d08e6013b88bd6b82_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1119&#39; height=&#39;593&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1119\" data-rawheight=\"593\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1119\" data-original=\"https://pic3.zhimg.com/v2-b810aa53573ea30d08e6013b88bd6b82_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b810aa53573ea30d08e6013b88bd6b82_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>获取用户关注名单</b></h2><p>通过如下代码获取网页返回数据，会发现数据是由HTML+JSON拼接而成，增加了很多解析成本</p><div class=\"highlight\"><pre><code class=\"language-text\">class ZhihuSpider(scrapy.Spider):\n    name = &#39;Zhihu&#39;\n    allowed_domains = [&#39;www.zhihu.com&#39;]\n    start_urls = [&#39;https://www.zhihu.com/people/zhang-jia-wei/following&#39;]\n\n    def parse(self, response):\n        all_data = response.body_as_unicode()\n        print(all_data)</code></pre></div><p>首先配置一下基本的环境，比如间隔秒数，爬取的UA，是否存储cookies,启用随机UA的中间件<code>DOWNLOADER_MIDDLEWARES</code></p><p><code>middlewares.py</code> 文件</p><div class=\"highlight\"><pre><code class=\"language-text\">from zhihu.settings import USER_AGENT_LIST # 导入中间件\nimport random\n\nclass RandomUserAgentMiddleware(object):\n    def process_request(self, request, spider):\n        rand_use  = random.choice(USER_AGENT_LIST)\n        if rand_use:\n            request.headers.setdefault(&#39;User-Agent&#39;, rand_use)</code></pre></div><p><code>setting.py</code> 文件</p><div class=\"highlight\"><pre><code class=\"language-text\">BOT_NAME = &#39;zhihu&#39;\n\nSPIDER_MODULES = [&#39;zhihu.spiders&#39;]\nNEWSPIDER_MODULE = &#39;zhihu.spiders&#39;\nUSER_AGENT_LIST=[  # 可以写多个，测试用，写了一个\n    &#34;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36&#34;\n]\n# Obey robots.txt rules\nROBOTSTXT_OBEY = False\n# See also autothrottle settings and docs\nDOWNLOAD_DELAY = 2\n# Disable cookies (enabled by default)\nCOOKIES_ENABLED = False\n# Override the default request headers:\nDEFAULT_REQUEST_HEADERS = {\n  &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,\n  &#39;Accept-Language&#39;: &#39;en&#39;,\n}\n# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html\nDOWNLOADER_MIDDLEWARES = {\n    &#39;zhihu.middlewares.RandomUserAgentMiddleware&#39;: 400,\n}\n# Configure item pipelines\n# See https://doc.scrapy.org/en/latest/topics/item-pipeline.html\nITEM_PIPELINES = {\n   &#39;zhihu.pipelines.ZhihuPipeline&#39;: 300,\n}</code></pre></div><p>主要爬取函数,内容说明</p><ol><li>start_requests 用来处理首次爬取请求，作为程序入口</li><li>下面的代码主要处理了2种情况，一种是HTML部分，一种是JSON部分</li><li>JSON部分使用re模块进行匹配，在通过json模块格式化</li><li><code>extract_first()</code> 获取xpath匹配数组的第一项</li><li><code>dont_filter=False</code> scrapy URL去重</li></ol><div class=\"highlight\"><pre><code class=\"language-text\"># 起始位置\n    def start_requests(self):\n        for url in self.start_urls:\n            yield scrapy.Request(url.format(&#34;zhang-jia-wei&#34;), callback=self.parse)\n\n    def parse(self, response):\n\n        print(&#34;正在获取 {} 信息&#34;.format(response.url))\n        all_data = response.body_as_unicode()\n\n        select = Selector(response)\n\n        # 所有知乎用户都具备的信息\n        username = select.xpath(&#34;//span[@class=&#39;ProfileHeader-name&#39;]/text()&#34;).extract_first()       # 获取用户昵称\n        sex = select.xpath(&#34;//div[@class=&#39;ProfileHeader-iconWrapper&#39;]/svg/@class&#34;).extract()\n        if len(sex) &gt; 0:\n            sex = 1 if str(sex[0]).find(&#34;male&#34;) else 0\n        else:\n            sex = -1\n        answers = select.xpath(&#34;//li[@aria-controls=&#39;Profile-answers&#39;]/a/span/text()&#34;).extract_first()\n        asks = select.xpath(&#34;//li[@aria-controls=&#39;Profile-asks&#39;]/a/span/text()&#34;).extract_first()\n        posts = select.xpath(&#34;//li[@aria-controls=&#39;Profile-posts&#39;]/a/span/text()&#34;).extract_first()\n        columns = select.xpath(&#34;//li[@aria-controls=&#39;Profile-columns&#39;]/a/span/text()&#34;).extract_first()\n        pins = select.xpath(&#34;//li[@aria-controls=&#39;Profile-pins&#39;]/a/span/text()&#34;).extract_first()\n        # 用户有可能设置了隐私，必须登录之后看到，或者记录cookie！\n        follwers = select.xpath(&#34;//strong[@class=&#39;NumberBoard-itemValue&#39;]/@title&#34;).extract()\n\n\n\n        item = ZhihuItem()\n        item[&#34;username&#34;] = username\n        item[&#34;sex&#34;] = sex\n        item[&#34;answers&#34;] = answers\n        item[&#34;asks&#34;] = asks\n        item[&#34;posts&#34;] = posts\n        item[&#34;columns&#34;] = columns\n        item[&#34;pins&#34;] = pins\n        item[&#34;follwering&#34;] = follwers[0] if len(follwers) &gt; 0 else 0\n        item[&#34;follwers&#34;] = follwers[1] if len(follwers) &gt; 0 else 0\n\n        yield item\n\n\n\n        # 获取第一页关注者列表\n        pattern = re.compile(&#39;&lt;script id=\\&#34;js-initialData\\&#34; type=\\&#34;text/json\\&#34;&gt;(.*?)&lt;\\/script&gt;&#39;)\n        json_data = pattern.search(all_data).group(1)\n        if json_data:\n            users = json.loads(json_data)[&#34;initialState&#34;][&#34;entities&#34;][&#34;users&#34;]\n        for user in users:\n            yield scrapy.Request(self.start_urls[0].format(user),callback=self.parse, dont_filter=False)</code></pre></div><p>在获取数据的时候，我绕开了一部分数据，这部分数据可以通过正则表达式去匹配。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d7809327a6f0df690cb3d1087a94d6d6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"467\" data-rawheight=\"472\" class=\"origin_image zh-lightbox-thumb\" width=\"467\" data-original=\"https://pic3.zhimg.com/v2-d7809327a6f0df690cb3d1087a94d6d6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;467&#39; height=&#39;472&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"467\" data-rawheight=\"472\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"467\" data-original=\"https://pic3.zhimg.com/v2-d7809327a6f0df690cb3d1087a94d6d6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d7809327a6f0df690cb3d1087a94d6d6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>数据存储，采用的依旧是<code>mongodb</code>　　小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~ </p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cbedf18c48bd030469442f7a404098d8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"423\" data-rawheight=\"115\" class=\"origin_image zh-lightbox-thumb\" width=\"423\" data-original=\"https://pic1.zhimg.com/v2-cbedf18c48bd030469442f7a404098d8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;423&#39; height=&#39;115&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"423\" data-rawheight=\"115\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"423\" data-original=\"https://pic1.zhimg.com/v2-cbedf18c48bd030469442f7a404098d8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-cbedf18c48bd030469442f7a404098d8_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "python爬虫", 
                    "tagLink": "https://api.zhihu.com/topics/20086364"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/65486019", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "Python爬虫入门教程第二十九讲：掘金网全站用户爬虫 scrapy", 
            "content": "<h2><b>爬前叨叨</b></h2><p>已经编写了33篇爬虫文章了，如果你按着一个个的实现，你的爬虫技术已经入门，从今天开始慢慢的就要写一些有分析价值的数据了，今天我选了一个《掘金网》，我们去爬取一下他的<b>全站用户</b>数据。</p><h3>爬取思路</h3><p>获取全站用户，理论来说从1个用户作为切入点就可以，我们需要爬取用户的关注列表，从关注列表不断的叠加下去。</p><p>随便打开一个用户的个人中心</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7e2be34160787a38e60c388c91bc6a36_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1012\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb\" width=\"1012\" data-original=\"https://pic3.zhimg.com/v2-7e2be34160787a38e60c388c91bc6a36_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1012&#39; height=&#39;391&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1012\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1012\" data-original=\"https://pic3.zhimg.com/v2-7e2be34160787a38e60c388c91bc6a36_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7e2be34160787a38e60c388c91bc6a36_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>绿色圆圈里面的都是我们想要采集到的信息。这个用户关注0人？那么你还需要继续找一个入口，这个用户一定要关注了别人。选择关注列表，是为了让数据有价值，因为关注者里面可能大量的小号或者不活跃的账号，价值不大。</p><p>我选了这样一个入口页面，它关注了3个人，你也可以选择多一些的，这个没有太大影响！<br/><code>https://juejin.im/user/55fa7cd460b2e36621f07dde/following</code><br/>我们要通过这个页面，去抓取用户的ID<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d620f0a10ef6494626a89536986e9150_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1538\" data-rawheight=\"479\" class=\"origin_image zh-lightbox-thumb\" width=\"1538\" data-original=\"https://pic1.zhimg.com/v2-d620f0a10ef6494626a89536986e9150_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1538&#39; height=&#39;479&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1538\" data-rawheight=\"479\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1538\" data-original=\"https://pic1.zhimg.com/v2-d620f0a10ef6494626a89536986e9150_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d620f0a10ef6494626a89536986e9150_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>得到ID之后，你才可以拼接出来下面的链接</p><div class=\"highlight\"><pre><code class=\"language-text\">https://juejin.im/user/用户ID/following</code></pre></div><h2><b>爬虫编写</b></h2><p>分析好了之后，就可以创建一个<code>scrapy</code>项目了</p><p><code>items.py</code> 文件，用来限定我们需要的所有数据，注意到下面有个<code>_id = scrapy.Field()</code> 这个先预留好，是为了<code>mongdb</code>准备的，其他的字段解释请参照注释即可。</p><div class=\"highlight\"><pre><code class=\"language-text\">class JuejinItem(scrapy.Item):\n   \n    _id = scrapy.Field()\n    username = scrapy.Field()\n    job = scrapy.Field()\n    company =scrapy.Field()\n    intro = scrapy.Field()\n    # 专栏\n    columns = scrapy.Field()\n    # 沸点\n    boiling = scrapy.Field()\n    # 分享\n    shares = scrapy.Field()\n    # 赞\n    praises = scrapy.Field()\n    #\n    books = scrapy.Field()\n    # 关注了\n    follow = scrapy.Field()\n    # 关注者\n    followers = scrapy.Field()\n    goods = scrapy.Field()\n    editer = scrapy.Field()\n    reads = scrapy.Field()\n    collections = scrapy.Field()\n    tags = scrapy.Field()</code></pre></div><p>编写爬虫主入口文件 <code>JuejinspiderSpider.py</code></p><div class=\"highlight\"><pre><code class=\"language-text\">import scrapy\nfrom scrapy.selector import Selector\nfrom Juejin.items import JuejinItem\n\nclass JuejinspiderSpider(scrapy.Spider):\n    name = &#39;JuejinSpider&#39;\n    allowed_domains = [&#39;juejin.im&#39;]\n    # 起始URL    5c0f372b5188255301746103\n    start_urls = [&#39;https://juejin.im/user/55fa7cd460b2e36621f07dde/following&#39;]</code></pre></div><p><code>def parse</code> 函数，逻辑不复杂，处理两个业务即可</p><ol><li>返回item</li><li>返回关注列表的Request</li></ol><p>item的获取，我们需要使用xpath匹配即可，为了简化代码量，我编写了一个提取方法，叫做<code>get_default</code>函数。</p><div class=\"highlight\"><pre><code class=\"language-text\">def get_default(self,exts):\n        if len(exts)&gt;0:\n            ret = exts[0]\n        else:\n            ret = 0\n        return ret\n\n    def parse(self, response):\n        #base_data = response.body_as_unicode()\n        select = Selector(response)\n        item = JuejinItem()\n        # 这个地方获取一下数据\n        item[&#34;username&#34;] = select.xpath(&#34;//h1[@class=&#39;username&#39;]/text()&#34;).extract()[0]\n        position = select.xpath(&#34;//div[@class=&#39;position&#39;]/span/span/text()&#34;).extract()\n        if position:\n            job = position[0]\n            if len(position)&gt;1:\n                company = position[1]\n            else:\n                company = &#34;&#34;\n        else:\n            job = company = &#34;&#34;\n        item[&#34;job&#34;] = job\n        item[&#34;company&#34;] = company\n        item[&#34;intro&#34;] = self.get_default(select.xpath(&#34;//div[@class=&#39;intro&#39;]/span/text()&#34;).extract())\n        # 专栏\n        item[&#34;columns&#34;] = self.get_default(select.xpath(&#34;//div[@class=&#39;header-content&#39;]/a[2]/div[2]/text()&#34;).extract())\n        # 沸点\n        item[&#34;boiling&#34;] = self.get_default(select.xpath(&#34;//div[@class=&#39;header-content&#39;]/a[3]/div[2]/text()&#34;).extract())\n        # 分享\n        item[&#34;shares&#34;] = self.get_default(select.xpath(&#34;//div[@class=&#39;header-content&#39;]/a[4]/div[2]/text()&#34;).extract())\n        # 赞\n        item[&#34;praises&#34;] = self.get_default(select.xpath(&#34;//div[@class=&#39;header-content&#39;]/a[5]/div[2]/text()&#34;).extract())\n        #\n        item[&#34;books&#34;] = self.get_default(select.xpath(&#34;//div[@class=&#39;header-content&#39;]/a[6]/div[2]/text()&#34;).extract())\n\n        # 关注了\n        item[&#34;follow&#34;] = self.get_default(select.xpath(&#34;//div[@class=&#39;follow-block block shadow&#39;]/a[1]/div[2]/text()&#34;).extract())\n        # 关注者\n        item[&#34;followers&#34;] = self.get_default(select.xpath(&#34;//div[@class=&#39;follow-block block shadow&#39;]/a[2]/div[2]/text()&#34;).extract())\n\n\n        right = select.xpath(&#34;//div[@class=&#39;stat-block block shadow&#39;]/div[2]/div&#34;).extract()\n        if len(right) == 3:\n            item[&#34;editer&#34;] = self.get_default(select.xpath(&#34;//div[@class=&#39;stat-block block shadow&#39;]/div[2]/div[1]/span/text()&#34;).extract())\n            item[&#34;goods&#34;] = self.get_default(select.xpath(&#34;//div[@class=&#39;stat-block block shadow&#39;]/div[2]/div[2]/span/span/text()&#34;).extract())\n            item[&#34;reads&#34;] = self.get_default(select.xpath(&#34;//div[@class=&#39;stat-block block shadow&#39;]/div[2]/div[3]/span/span/text()&#34;).extract())\n\n        else:\n            item[&#34;editer&#34;] = &#34;&#34;\n            item[&#34;goods&#34;] = self.get_default(select.xpath(&#34;//div[@class=&#39;stat-block block shadow&#39;]/div[2]/div[1]/span/span/text()&#34;).extract())\n            item[&#34;reads&#34;] = self.get_default(select.xpath(&#34;//div[@class=&#39;stat-block block shadow&#39;]/div[2]/div[2]/span/span/text()&#34;).extract())\n\n\n        item[&#34;collections&#34;] = self.get_default(select.xpath(&#34;//div[@class=&#39;more-block block&#39;]/a[1]/div[2]/text()&#34;).extract())\n        item[&#34;tags&#34;] = self.get_default(select.xpath(&#34;//div[@class=&#39;more-block block&#39;]/a[2]/div[2]/text()&#34;).extract())\n        yield item  # 返回item</code></pre></div><p>上述代码，已经成功返回了item，打开<code>setting.py</code>文件中的<code>pipelines</code>设置，测试一下是否可以存储数据,顺便在<br/><code>DEFAULT_REQUEST_HEADERS</code> 配置一下request的请求参数。</p><p><code>setting.py</code></p><div class=\"highlight\"><pre><code class=\"language-text\">DEFAULT_REQUEST_HEADERS = {\n    &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,\n    &#39;Accept-Language&#39;: &#39;en&#39;,\n    &#34;Host&#34;: &#34;juejin.im&#34;,\n    &#34;Referer&#34;: &#34;https://juejin.im/timeline?sort=weeklyHottest&#34;,\n    &#34;Upgrade-Insecure-Requests&#34;: &#34;1&#34;,\n    &#34;User-Agent&#34;: &#34;Mozilla/5.0 浏览器UA&#34;\n}\n\nITEM_PIPELINES = {\n   &#39;Juejin.pipelines.JuejinPipeline&#39;: 20,\n}</code></pre></div><p>本爬虫数据存储到<code>mongodb</code>里面，所以需要你在<code>pipelines.py</code>文件编写存储代码。</p><div class=\"highlight\"><pre><code class=\"language-text\">import time\nimport pymongo\n\nDATABASE_IP = &#39;127.0.0.1&#39;\nDATABASE_PORT = 27017\nDATABASE_NAME = &#39;sun&#39;\nclient = pymongo.MongoClient(DATABASE_IP,DATABASE_PORT)\ndb = client.sun\ndb.authenticate(&#34;dba&#34;, &#34;dba&#34;)\ncollection = db.jujin  # 准备插入数据\n\n\nclass JuejinPipeline(object):\n\n    def process_item(self, item, spider):\n        try:\n            collection.insert(item)\n        except Exception as e:\n            print(e.args)</code></pre></div><p>运行代码之后，如果没有报错，完善最后一步即可，在Spider里面将爬虫的循环操作完成</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"n\">list_li</span> <span class=\"o\">=</span> <span class=\"n\">select</span><span class=\"o\">.</span><span class=\"n\">xpath</span><span class=\"p\">(</span><span class=\"s2\">&#34;//ul[@class=&#39;tag-list&#39;]/li&#34;</span><span class=\"p\">)</span>  <span class=\"c1\"># 获取所有的关注</span>\n      <span class=\"k\">for</span> <span class=\"n\">li</span> <span class=\"ow\">in</span> <span class=\"n\">list_li</span><span class=\"p\">:</span>\n           <span class=\"n\">a_link</span> <span class=\"o\">=</span> <span class=\"n\">li</span><span class=\"o\">.</span><span class=\"n\">xpath</span><span class=\"p\">(</span><span class=\"s2\">&#34;.//meta[@itemprop=&#39;url&#39;]/@content&#34;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">extract</span><span class=\"p\">()[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"c1\"># 获取URL</span>\n             <span class=\"c1\"># 返回拼接好的数据请求</span>\n           <span class=\"k\">yield</span> <span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">Request</span><span class=\"p\">(</span><span class=\"n\">a_link</span><span class=\"o\">+</span><span class=\"s2\">&#34;/following&#34;</span><span class=\"p\">,</span><span class=\"n\">callback</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">parse</span><span class=\"p\">)</span></code></pre></div><p>所有的代码都已经写完啦<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fd1fcf12eca9becff37816a0ac44667e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"302\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https://pic3.zhimg.com/v2-fd1fcf12eca9becff37816a0ac44667e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1280&#39; height=&#39;302&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"302\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1280\" data-original=\"https://pic3.zhimg.com/v2-fd1fcf12eca9becff37816a0ac44667e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fd1fcf12eca9becff37816a0ac44667e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>全站用户爬虫编写完毕，厉害吧。小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~</p><p>扩展方向</p><ol><li>爬虫每次只爬取关注列表的第一页，也可以循环下去，这个不麻烦</li><li>在<code>setting.py</code>中开启多线程操作</li><li>添加redis速度更快，后面会陆续的写几篇分布式爬虫，提高爬取速度</li><li>思路可以扩展，N多网站的用户爬虫，咱后面也写几个</li></ol><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5a0ec3cfea167b61ebe075f85c562ab0_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"120\" data-rawheight=\"120\" class=\"content_image\" width=\"120\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;120&#39; height=&#39;120&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"120\" data-rawheight=\"120\" class=\"content_image lazy\" width=\"120\" data-actualsrc=\"https://pic1.zhimg.com/v2-5a0ec3cfea167b61ebe075f85c562ab0_b.png\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "python爬虫", 
                    "tagLink": "https://api.zhihu.com/topics/20086364"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/65437690", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 1, 
            "title": "Python小技巧：QPython，一个在手机上运行Python的神器", 
            "content": "<p>之前安利过一款手机上运行Python的神器Termux，不过Termux的使用比较重，它实际是一款linux系统模拟器，安装好Termux后还要再安装python，并且是全命令行操作，一些读者使用起来有障碍。今天安利一款更友好的QPython。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-73325e2a353f818bc808fa4ef29ec7d0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"414\" class=\"origin_image zh-lightbox-thumb\" width=\"550\" data-original=\"https://pic1.zhimg.com/v2-73325e2a353f818bc808fa4ef29ec7d0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;550&#39; height=&#39;414&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"414\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"550\" data-original=\"https://pic1.zhimg.com/v2-73325e2a353f818bc808fa4ef29ec7d0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-73325e2a353f818bc808fa4ef29ec7d0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>Qpython是一个Python引擎，只能运行在安卓系统上，相比Termux，它可以全图形界面操作，非常友好。</p><p>内置了一个Python编辑器，可以直接在手机上写Python代码，支持缩进，语法高亮等特性。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-dbbba625b4cf4fdc041a7a4d1526ead4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"1059\" class=\"origin_image zh-lightbox-thumb\" width=\"550\" data-original=\"https://pic1.zhimg.com/v2-dbbba625b4cf4fdc041a7a4d1526ead4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;550&#39; height=&#39;1059&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"1059\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"550\" data-original=\"https://pic1.zhimg.com/v2-dbbba625b4cf4fdc041a7a4d1526ead4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-dbbba625b4cf4fdc041a7a4d1526ead4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>也内置了一个ftp，可以很方便的拷贝电脑上的py文件到手机上运行。</p><p>由于内置了SL4A，可以很方便的调用安卓操作系统的一些API做些有趣的事情，比如可以通过SL4A获取手机地理位置，打开蓝牙，发送手机短信，打开手机摄像头等等。</p><p>吹水了一阵，下面具体介绍下 <b>安装和使用的细节步骤</b> 。</p><p>QPython有两个版本一个是QPython支持python2.7版本，一个是QPython 3支持Python3版本，不过我测试QPython目前貌似也支持python3。这里我介绍的是QPython3，它对安卓特性的支持更多一些。</p><p>官方的APK安装包在github上 网页链接</p><p>下载最新的qpython3-app-release.apk安装即可。</p><p>不过国内的手机安卓应用市场上也有，可以直接搜索qpython，看发布者是一家北京的公司，不清楚这家公司跟QPython是什么关系。</p><p>安装成功， <b>打开QPython APP后就是这个样子</b> 。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fb9d4b8f83dd4f8528d8a253e68bbe46_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"1066\" class=\"origin_image zh-lightbox-thumb\" width=\"550\" data-original=\"https://pic3.zhimg.com/v2-fb9d4b8f83dd4f8528d8a253e68bbe46_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;550&#39; height=&#39;1066&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"1066\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"550\" data-original=\"https://pic3.zhimg.com/v2-fb9d4b8f83dd4f8528d8a253e68bbe46_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fb9d4b8f83dd4f8528d8a253e68bbe46_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>可以看见有六个图标模块。</p><p>1.终端：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-530279295673e9bd893e8683f8772ac6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"1048\" class=\"origin_image zh-lightbox-thumb\" width=\"550\" data-original=\"https://pic3.zhimg.com/v2-530279295673e9bd893e8683f8772ac6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;550&#39; height=&#39;1048&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"1048\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"550\" data-original=\"https://pic3.zhimg.com/v2-530279295673e9bd893e8683f8772ac6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-530279295673e9bd893e8683f8772ac6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这是编程IDE的标配，可以执行一些代码片段，不过写手机上输入代码还是挺麻烦的。</p><p>2.编辑器：</p><p>上面已经有图示了，可以新建或者修改py文件，然后点击最下面的三角形执行文件，注意文件要保存为.py后缀才行。</p><p>3.程序：</p><p>里面提供一些程序示例，供你参考，比如如何打开蓝牙，如何打开摄像头，如何语音合成（语音转文字等）。</p><p>4.QPYPI：</p><p>一个缩减版的pypi，Python第三方库可以通过QPYPI来安装。</p><p>5.课程：</p><p>6.社区：</p><p>课程和社区要进入官方社区，因为要访问facebook ，所以是打不开的。</p><p>如何把电脑py文件传到手机上？</p><p>QPython内置了一个ftp，点击开启ftp服务后，只要你电脑上有安装ftp客户端，就可以直接把文件传到手机上，存放都目录不用修改。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-bfcc6ede8e24f3d62ed5c01c2a6c68b2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"437\" class=\"origin_image zh-lightbox-thumb\" width=\"550\" data-original=\"https://pic3.zhimg.com/v2-bfcc6ede8e24f3d62ed5c01c2a6c68b2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;550&#39; height=&#39;437&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"437\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"550\" data-original=\"https://pic3.zhimg.com/v2-bfcc6ede8e24f3d62ed5c01c2a6c68b2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-bfcc6ede8e24f3d62ed5c01c2a6c68b2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e52428ec863b4e73f7d87f7a9297ba84_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"1056\" class=\"origin_image zh-lightbox-thumb\" width=\"550\" data-original=\"https://pic1.zhimg.com/v2-e52428ec863b4e73f7d87f7a9297ba84_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;550&#39; height=&#39;1056&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"1056\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"550\" data-original=\"https://pic1.zhimg.com/v2-e52428ec863b4e73f7d87f7a9297ba84_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e52428ec863b4e73f7d87f7a9297ba84_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>当然你也可以通过你熟悉的方式来在电脑和手机间传输文件。</p><p>文件或文件夹存放在/qpython/project3/位置即可。</p><p>另外QPython中有个有趣的传代码方式：扫描二维码传代码。把电脑上的python代码生成为二维码，用手机上的qpython app扫描二维码，代码会直接在手机上生成。QPython3中没有这个功能。</p><p>PS:一个二维码最大能容纳1850个字母，所以传大的py文件是传不了的。</p><p>QPython可以干哪些事情？</p><p>1).Web开发，支持django和flask Web开发，当然你需要安装django和flsak，也可以写python爬虫，但是运行效率非常低下。</p><p>2).游戏开发，使用pygame开发手机游戏。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1fb14668fc705031fd46c1ca73c57c71_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"300\" data-rawheight=\"236\" data-thumbnail=\"https://pic2.zhimg.com/v2-1fb14668fc705031fd46c1ca73c57c71_b.jpg\" class=\"content_image\" width=\"300\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;300&#39; height=&#39;236&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"300\" data-rawheight=\"236\" data-thumbnail=\"https://pic2.zhimg.com/v2-1fb14668fc705031fd46c1ca73c57c71_b.jpg\" class=\"content_image lazy\" width=\"300\" data-actualsrc=\"https://pic2.zhimg.com/v2-1fb14668fc705031fd46c1ca73c57c71_b.gif\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>3).app开发，可以开发手机APP。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b9c49b59c37768b963505c2e605b42af_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"413\" class=\"origin_image zh-lightbox-thumb\" width=\"550\" data-original=\"https://pic4.zhimg.com/v2-b9c49b59c37768b963505c2e605b42af_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;550&#39; height=&#39;413&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"413\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"550\" data-original=\"https://pic4.zhimg.com/v2-b9c49b59c37768b963505c2e605b42af_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b9c49b59c37768b963505c2e605b42af_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上，可以把你手机上的闹钟，日历等app都换成是你自己，而且全都用python开发的。</p><p>用python开发app，要安装使用kivy来支持，因为安卓上只能运行java程序，所以配置kivy也是一件麻烦事，可以单独拿篇文章来说了，喜欢捣鼓的可以自行研究。</p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/65437392", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "如何在7分钟内快速完整地浏览Python3中的列表!", 
            "content": "<p>Python列表与数组不同。在处理数组时，我们讨论了一组同类数据元素。对于python中的列表，情况并非如此。Python List可以存储异构的元素集合。此功能将帮助开发人员和程序员以更灵活的方式处理列表。python中的List是最强大的内置数据结构之一。</p><p>python中的列表还可以存储整数，浮点值，字符串，布尔值和复杂值。</p><p>如何在python中创建一个List</p><p>我们可以用两种方式在python中创建一个list</p><ol><li>通过声明一个带有空方括号的变量 i.e []</li><li>通过使用list()。</li></ol><p>例</p><p>Python学习交流群：1004391443，这里是python学习者聚集地，有大牛答疑，有资源共享！小编也准备了一份python学习资料，有想学习python编程的，或是转行，或是大学生，还有工作中想提升自己能力的，正在学习的小伙伴欢迎加入学习。</p><div class=\"highlight\"><pre><code class=\"language-text\"># Here first I&#39;m creating a my todo list which is used to store my to-do activities.\nmyTODOList = []\n# The above line will create a list object for me\n# I&#39;m creating a another list which will store my general information.\nmyGeneralInfo = list()\n# The above line will also create a list object for me\n# Getting the types of list objects\nprint(type(myTODOList))\nprint(type(myGeneralInfo))\n</code></pre></div><p>输出</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e76b21bb3345fd50b09f4a5ff267115b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"542\" data-rawheight=\"237\" class=\"origin_image zh-lightbox-thumb\" width=\"542\" data-original=\"https://pic4.zhimg.com/v2-e76b21bb3345fd50b09f4a5ff267115b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;542&#39; height=&#39;237&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"542\" data-rawheight=\"237\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"542\" data-original=\"https://pic4.zhimg.com/v2-e76b21bb3345fd50b09f4a5ff267115b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e76b21bb3345fd50b09f4a5ff267115b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>您可以使用最常用的方法创建新的列表对象。现在我们将继续讨论如何在列表中添加新元素以及更多内容。</p><p>如何将数据添加到列表？</p><p>首先，我想介绍一下Mutability的概念。可变性意味着改变其行为的能力。Python列表本质上是可变的。我们可以在列表中添加或删除元素。与其他内置数据结构相比，这是吸引程序员使用列表的最大优势之一。</p><p>我们可以通过两种方式向列表添加元素：</p><ol><li>通过使用append（）</li><li>通过使用insert（）</li></ol><p>通过使用append（）</p><p>借助append方法，我们可以一次添加一个元素。此方法将帮助我们仅在列表的末尾添加元素。</p><p>append函数的语法是 -</p><p>listName.append（项目/元件）</p><div class=\"highlight\"><pre><code class=\"language-text\"># Adding Elements to the lists\nmyTODOList.append(&#39;Wake up Early Morning&#39;)\nmyTODOList.append(&#39;Go to Gym&#39;)\nmyTODOList.append(&#39;Play Some Games&#39;)\nmyTODOList.append(&#39;Get ready to go the college&#39;)\nmyTODOList.append(&#39;Go to library&#39;)\n# Printing the entire list elements\nprint(myTODOList)\n</code></pre></div><p>输出</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-93aa587d84fd441acaa97902a30496d9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb\" width=\"550\" data-original=\"https://pic2.zhimg.com/v2-93aa587d84fd441acaa97902a30496d9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;550&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"550\" data-original=\"https://pic2.zhimg.com/v2-93aa587d84fd441acaa97902a30496d9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-93aa587d84fd441acaa97902a30496d9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>通过使用insert（）</p><p>此插入方法用于在给定列表中的指定位置添加元素。</p><p>insert函数的语法是 -</p><p>listName.insert（position，item / element）</p><p>Insert()使用两个参数 - position和list item。该位置是元素需要保留在列表中的位置。这些位置通常称为索引。通常，python中的列表索引从0开始。（即第一个元素索引为0，第二个元素为1，第三个元素索引为2，依此类推）。由此，我们可以得出结论：</p><p>n个元素的列表最多具有n-1的索引号，即具有5个元素的列表将具有最大索引值4。</p><p>例</p><div class=\"highlight\"><pre><code class=\"language-text\"># Adding Elements to our list with the help of insert()\nmyGeneralInfo.insert(0, &#39;Paid the Library Fee&#39;)\nmyGeneralInfo.insert(1, 12000)\nmyGeneralInfo.insert(2, True)\nmyGeneralInfo.insert(3, 14+12j)\nmyGeneralInfo.insert(4, 3.141521)\n# Printing the myGeneralInfo list information\nprint(myGeneralInfo)\n</code></pre></div><p>输出</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7eb02d14f8ddc96ed056ed0df5c1273e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"491\" data-rawheight=\"174\" class=\"origin_image zh-lightbox-thumb\" width=\"491\" data-original=\"https://pic3.zhimg.com/v2-7eb02d14f8ddc96ed056ed0df5c1273e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;491&#39; height=&#39;174&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"491\" data-rawheight=\"174\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"491\" data-original=\"https://pic3.zhimg.com/v2-7eb02d14f8ddc96ed056ed0df5c1273e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7eb02d14f8ddc96ed056ed0df5c1273e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如何访问列表元素</p><p>我们可以使用以下两种方式访问元素列表：</p><ol><li>通过使用索引运算符。</li><li>通过使用切片运算符</li></ol><p>通过使用索引运算符</p><p>我们可以在索引运算符的帮助下直接访问列表元素。</p><p>例</p><div class=\"highlight\"><pre><code class=\"language-text\"># Acessing the certain values from the list\nprint(myTODOList[1])\nprint(myTODOList[3])\nprint(myTODOList[4])\n</code></pre></div><p>输出</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7890f7b5b58cbde1ffadfe4ce3e9ee38_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"464\" data-rawheight=\"131\" class=\"origin_image zh-lightbox-thumb\" width=\"464\" data-original=\"https://pic1.zhimg.com/v2-7890f7b5b58cbde1ffadfe4ce3e9ee38_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;464&#39; height=&#39;131&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"464\" data-rawheight=\"131\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"464\" data-original=\"https://pic1.zhimg.com/v2-7890f7b5b58cbde1ffadfe4ce3e9ee38_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7890f7b5b58cbde1ffadfe4ce3e9ee38_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>通过使用切片运算符</p><p>切片运算符是有效访问列表元素的最常用运算符之一。slice运算符的语法是：</p><p>listName [start：stop：step]</p><p>start - 它表示切片必须开始的索引。默认值为0。</p><p>stop - 它表示切片必须结束的索引。默认值是列表的最大允许索引，即列表的长度。</p><p>step - 增加值。默认值为1。</p><p>例</p><div class=\"highlight\"><pre><code class=\"language-text\"># Getting the information using slice operator\nprint(myTODOList[0:3]) # we don&#39;t need to specify the step value.\nprint(myTODOList[2:4:1])\nprint(myTODOList[0:4:2])\n</code></pre></div><p>输出</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b638691390723a08af128f4147a880de_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"517\" data-rawheight=\"110\" class=\"origin_image zh-lightbox-thumb\" width=\"517\" data-original=\"https://pic3.zhimg.com/v2-b638691390723a08af128f4147a880de_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;517&#39; height=&#39;110&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"517\" data-rawheight=\"110\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"517\" data-original=\"https://pic3.zhimg.com/v2-b638691390723a08af128f4147a880de_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b638691390723a08af128f4147a880de_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>Python列表是可迭代的对象。对于python中的任何可迭代对象，我们可以编写for循环来打印出所有数据。</p><p>例</p><div class=\"highlight\"><pre><code class=\"language-text\"># Iterating over the list\nfor item in myGeneralInfo:\n print(item)\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8f6be3800da266df6591c61dea294913_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"442\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb\" width=\"442\" data-original=\"https://pic4.zhimg.com/v2-8f6be3800da266df6591c61dea294913_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;442&#39; height=&#39;168&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"442\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"442\" data-original=\"https://pic4.zhimg.com/v2-8f6be3800da266df6591c61dea294913_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8f6be3800da266df6591c61dea294913_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如何从列表中删除元素</p><p>我们可以通过以下两种方式删除列表元素：</p><ol><li>通过使用remove()</li><li>通过使用pop()</li></ol><p>通过使用remove()</p><p>remove()用于删除指定给它的元素。remove()的语法是：</p><p>listName.remove（项目/元件）</p><div class=\"highlight\"><pre><code class=\"language-text\"># Deleting the element from the list\nmyGeneralInfo.remove(12000)\nmyGeneralInfo.remove(&#39;Paid the Library Fee&#39;)\n# printing the result after deleting the elements\nprint(myGeneralInfo)\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-615fc88f7550609d9c7b30e7ea30d29c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"490\" data-rawheight=\"110\" class=\"origin_image zh-lightbox-thumb\" width=\"490\" data-original=\"https://pic1.zhimg.com/v2-615fc88f7550609d9c7b30e7ea30d29c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;490&#39; height=&#39;110&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"490\" data-rawheight=\"110\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"490\" data-original=\"https://pic1.zhimg.com/v2-615fc88f7550609d9c7b30e7ea30d29c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-615fc88f7550609d9c7b30e7ea30d29c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>通过使用pop()</p><p>它是一个迭代器方法，用于一次删除单个（或）多个元素。它从背面删除元素。pop()方法的语法是：</p><p>listName.pop()</p><div class=\"highlight\"><pre><code class=\"language-text\"># printing the list items before deleting\nprint(&#39;My TODO List Elements: &#39;,myTODOList)\nprint(&#39;My General list Elements: &#39;,myGeneralInfo)\n# Deleting the list elements using pop()\nmyTODOList.pop()\nmyTODOList.pop()\n# Deleting the list elements completely\nfor item in range(len(myGeneralInfo)):\n myGeneralInfo.pop()\n# printing the results\nprint(&#39;My TODO List Elements: &#39;,myTODOList)\nprint(&#39;My General list Elements: &#39;,myGeneralInfo)\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ffc243d7a2799647314e2c815b2b97ef_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"540\" data-rawheight=\"154\" class=\"origin_image zh-lightbox-thumb\" width=\"540\" data-original=\"https://pic4.zhimg.com/v2-ffc243d7a2799647314e2c815b2b97ef_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;540&#39; height=&#39;154&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"540\" data-rawheight=\"154\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"540\" data-original=\"https://pic4.zhimg.com/v2-ffc243d7a2799647314e2c815b2b97ef_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ffc243d7a2799647314e2c815b2b97ef_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><i>在上面的程序中，我们在for循环中使用了len</i> () <i>。len</i> () <i>用于给出列表的长度，即列表中存在的元素的数量。</i></p><p>列表对象上的各种属性和函数</p><p>python dir()函数用于提供与之关联的内置属性和方法集。</p><p>例</p><div class=\"highlight\"><pre><code class=\"language-text\"># Printing all the attributes and functions on the list object\nprint(dir(myTODOList))\n</code></pre></div><p>输出</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ad1ef2a117bc7059cc11958a6b1d8b32_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"48\" class=\"origin_image zh-lightbox-thumb\" width=\"550\" data-original=\"https://pic3.zhimg.com/v2-ad1ef2a117bc7059cc11958a6b1d8b32_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;550&#39; height=&#39;48&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"48\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"550\" data-original=\"https://pic3.zhimg.com/v2-ad1ef2a117bc7059cc11958a6b1d8b32_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ad1ef2a117bc7059cc11958a6b1d8b32_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>各种列表方法及其用途：</p><p>1. <b>append</b> () <b>-</b> 它会在列表末尾添加一个元素。</p><p>2. <b>clear</b> () <b>-</b> 用于从列表中删除所有项目。</p><p>3. <b>copy</b> () <b>-</b> 用于返回列表的另一个副本。</p><p>4. <b>count</b> () <b>-</b> 用于返回作为参数传递的项数的计数。</p><p>5. <b>extend</b> () <b>-</b> 它将列表的所有元素添加到另一个列表中。</p><p>6. <b>index</b> () <b>-</b> 用于返回第一个匹配项的索引。</p><p>7. <b>insert</b> () <b>-</b> 用于在定义的索引处插入项目。</p><p>8. <b>pop</b> () <b>-</b> 用于删除和返回给定索引处的元素。</p><p>9. <b>remove</b> () <b>-</b> 用于从列表中删除项目。</p><p>10. <b>reverse</b> () <b>-</b> 用于反转列表中项目的顺序。</p><p>11. <b>sort</b> () <b>-</b> 用于按升序对列表中的项目进行排序。</p><p>何时使用列表数据结构？</p><p>如果要存储多个数据对象，则必须保留插入顺序。如果您还想存储重复值，那么此数据结构将更有助于执行此类操作。</p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/65435944", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "Python爬虫入门教程第二十八讲： 《海王》评论数据抓取 scrapy", 
            "content": "<h2><b>1. 海王评论数据爬取前分析</b></h2><p>海王上映了，然后口碑炸了，对咱来说，多了一个可爬可分析的电影，美哉~<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1b7486c6f8fe821da985aa68643d68ba_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"910\" data-rawheight=\"354\" class=\"origin_image zh-lightbox-thumb\" width=\"910\" data-original=\"https://pic3.zhimg.com/v2-1b7486c6f8fe821da985aa68643d68ba_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;910&#39; height=&#39;354&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"910\" data-rawheight=\"354\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"910\" data-original=\"https://pic3.zhimg.com/v2-1b7486c6f8fe821da985aa68643d68ba_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1b7486c6f8fe821da985aa68643d68ba_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>摘录一个评论</p><blockquote><i>零点场刚看完，温导的电影一直很不错，无论是速7，电锯惊魂还是招魂都很棒。打斗和音效方面没话说非常棒，特别震撼。总之，DC扳回一分（￣▽￣）。比正义联盟好的不止一点半点（我个人感觉）。还有艾梅伯希尔德是真的漂亮，温导选的人都很棒。</i><br/><i>真的第一次看到这么牛逼的电影 转场特效都吊炸天</i></blockquote><h2><b>2. 海王案例开始爬取数据</b></h2><p>数据爬取的依旧是猫眼的评论，这部分内容咱们用把牛刀，<code>scrapy</code>爬取，一般情况下，用一下<code>requests</code>就好了</p><p>抓取地址</p><div class=\"highlight\"><pre><code class=\"language-text\">http://m.maoyan.com/mmdb/comments/movie/249342.json?_v_=yes&amp;offset=15&amp;startTime=2018-12-11%2009%3A58%3A43</code></pre></div><p>关键参数</p><div class=\"highlight\"><pre><code class=\"language-text\">url:http://m.maoyan.com/mmdb/comments/movie/249342.json\noffset:15\nstartTime:起始时间</code></pre></div><p>scrapy 爬取猫眼代码特别简单，我分开几个py文件即可。</p><p><code>Haiwang.py</code></p><div class=\"highlight\"><pre><code class=\"language-text\">import scrapy\nimport json\nfrom haiwang.items import HaiwangItem\n\nclass HaiwangSpider(scrapy.Spider):\n    name = &#39;Haiwang&#39;\n    allowed_domains = [&#39;m.maoyan.com&#39;]\n    start_urls = [&#39;http://m.maoyan.com/mmdb/comments/movie/249342.json?_v_=yes&amp;offset=0&amp;startTime=0&#39;]\n\n    def parse(self, response):\n        print(response.url)\n        body_data = response.body_as_unicode()\n\n        js_data = json.loads(body_data)\n        item = HaiwangItem()\n        for info in js_data[&#34;cmts&#34;]:\n\n            item[&#34;nickName&#34;] = info[&#34;nickName&#34;]\n            item[&#34;cityName&#34;] = info[&#34;cityName&#34;] if &#34;cityName&#34; in info else &#34;&#34;\n            item[&#34;content&#34;] = info[&#34;content&#34;]\n            item[&#34;score&#34;] = info[&#34;score&#34;]\n            item[&#34;startTime&#34;] = info[&#34;startTime&#34;]\n            item[&#34;approve&#34;] = info[&#34;approve&#34;]\n            item[&#34;reply&#34;] = info[&#34;reply&#34;]\n            item[&#34;avatarurl&#34;] = info[&#34;avatarurl&#34;]\n\n            yield item\n\n        yield scrapy.Request(&#34;http://m.maoyan.com/mmdb/comments/movie/249342.json?_v_=yes&amp;offset=0&amp;startTime={}&#34;.format(item[&#34;startTime&#34;]),callback=self.parse)</code></pre></div><p><code>setting.py</code></p><p>设置需要配置headers</p><div class=\"highlight\"><pre><code class=\"language-text\">DEFAULT_REQUEST_HEADERS = {\n    &#34;Referer&#34;:&#34;http://m.maoyan.com/movie/249342/comments?_v_=yes&#34;,\n    &#34;User-Agent&#34;:&#34;Mozilla/5.0 Chrome/63.0.3239.26 Mobile Safari/537.36&#34;,\n    &#34;X-Requested-With&#34;:&#34;superagent&#34;\n}</code></pre></div><p>需要配置一些抓取条件</p><div class=\"highlight\"><pre><code class=\"language-text\"># Obey robots.txt rules\nROBOTSTXT_OBEY = False\n# See also autothrottle settings and docs\nDOWNLOAD_DELAY = 1\n# Disable cookies (enabled by default)\nCOOKIES_ENABLED = False</code></pre></div><p>开启管道</p><div class=\"highlight\"><pre><code class=\"language-text\"># Configure item pipelines\n# See https://doc.scrapy.org/en/latest/topics/item-pipeline.html\nITEM_PIPELINES = {\n   &#39;haiwang.pipelines.HaiwangPipeline&#39;: 300,\n}</code></pre></div><p><code>items.py</code><br/>获取你想要的数据</p><div class=\"highlight\"><pre><code class=\"language-text\">import scrapy\n\n\nclass HaiwangItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    nickName = scrapy.Field()\n    cityName = scrapy.Field()\n    content = scrapy.Field()\n    score = scrapy.Field()\n    startTime = scrapy.Field()\n    approve = scrapy.Field()\n    reply =scrapy.Field()\n    avatarurl = scrapy.Field()</code></pre></div><p><code>pipelines.py</code><br/>保存数据，数据存储到<code>csv</code>文件中</p><div class=\"highlight\"><pre><code class=\"language-text\">import os\nimport csv\n\n\nclass HaiwangPipeline(object):\n    def __init__(self):\n        store_file = os.path.dirname(__file__) + &#39;/spiders/haiwang.csv&#39;\n        self.file = open(store_file, &#34;a+&#34;, newline=&#34;&#34;, encoding=&#34;utf-8&#34;)\n        self.writer = csv.writer(self.file)\n\n    def process_item(self, item, spider):\n        try:\n            self.writer.writerow((\n                item[&#34;nickName&#34;],\n                item[&#34;cityName&#34;],\n                item[&#34;content&#34;],\n                item[&#34;approve&#34;],\n                item[&#34;reply&#34;],\n                item[&#34;startTime&#34;],\n                item[&#34;avatarurl&#34;],\n                item[&#34;score&#34;]\n            ))\n\n        except Exception as e:\n            print(e.args)\n\n        def close_spider(self, spider):\n            self.file.close()</code></pre></div><p><code>begin.py</code><br/>编写运行脚本</p><div class=\"highlight\"><pre><code class=\"language-text\">from scrapy import cmdline\ncmdline.execute((&#34;scrapy crawl Haiwang&#34;).split())</code></pre></div><p>走起，搞定，等着数据来到，就可以了，小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e8d52c97b71c76df911ec597338e29a7_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"100\" data-rawheight=\"100\" data-thumbnail=\"https://pic4.zhimg.com/v2-e8d52c97b71c76df911ec597338e29a7_b.jpg\" class=\"content_image\" width=\"100\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;100&#39; height=&#39;100&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"100\" data-rawheight=\"100\" data-thumbnail=\"https://pic4.zhimg.com/v2-e8d52c97b71c76df911ec597338e29a7_b.jpg\" class=\"content_image lazy\" width=\"100\" data-actualsrc=\"https://pic4.zhimg.com/v2-e8d52c97b71c76df911ec597338e29a7_b.gif\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "python爬虫", 
                    "tagLink": "https://api.zhihu.com/topics/20086364"
                }, 
                {
                    "tag": "scrapy", 
                    "tagLink": "https://api.zhihu.com/topics/19950086"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/65435327", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "Python爬虫入门教程第二十七讲： B站博人传评论数据抓取 scrapy", 
            "content": "<h2><b>1. B站博人传评论数据爬取简介</b></h2><p>今天想了半天不知道抓啥，去B站看跳舞的小姐姐，忽然看到了评论，那就抓取一下B站的评论数据，视频动画那么多，也不知道抓取哪个，选了一个博人传跟火影相关的，抓取看看。网址： <code>https://www.bilibili.com/bangumi/media/md5978/?from=search&amp;seid=16013388136765436883#short</code><br/>在这个网页看到了18560条短评，数据量也不大，抓取看看，使用的还是scrapy。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d25cc5a9f51454d3348cedaaff3a361d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"640\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-d25cc5a9f51454d3348cedaaff3a361d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;640&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"640\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-d25cc5a9f51454d3348cedaaff3a361d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d25cc5a9f51454d3348cedaaff3a361d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0579b5160672ce00572718a45b00e754_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1291\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb\" width=\"1291\" data-original=\"https://pic1.zhimg.com/v2-0579b5160672ce00572718a45b00e754_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1291&#39; height=&#39;430&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1291\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1291\" data-original=\"https://pic1.zhimg.com/v2-0579b5160672ce00572718a45b00e754_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0579b5160672ce00572718a45b00e754_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>2. B站博人传评论数据案例---获取链接</b></h2><p>从开发者工具中你能轻易的得到如下链接，有链接之后就好办了，如何创建项目就不在啰嗦了，我们直接进入主题。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f6034b7e74bd1e771d39e01d81742b35_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"524\" data-rawheight=\"167\" class=\"origin_image zh-lightbox-thumb\" width=\"524\" data-original=\"https://pic2.zhimg.com/v2-f6034b7e74bd1e771d39e01d81742b35_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;524&#39; height=&#39;167&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"524\" data-rawheight=\"167\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"524\" data-original=\"https://pic2.zhimg.com/v2-f6034b7e74bd1e771d39e01d81742b35_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f6034b7e74bd1e771d39e01d81742b35_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我在代码中的<code>parse</code>函数中，设定了两个<code>yield</code>一个用来返回<code>items</code> 一个用来返回<code>requests</code>。<br/>然后实现一个新的功能，每次访问切换<code>UA</code>，这个点我们需要使用到中间件技术。</p><div class=\"highlight\"><pre><code class=\"language-text\">class BorenSpider(scrapy.Spider):\n    BASE_URL = &#34;https://bangumi.bilibili.com/review/web_api/short/list?media_id=5978&amp;folded=0&amp;page_size=20&amp;sort=0&amp;cursor={}&#34;\n    name = &#39;Boren&#39;\n    allowed_domains = [&#39;bangumi.bilibili.com&#39;]\n\n    start_urls = [BASE_URL.format(&#34;76742479839522&#34;)]\n\n    def parse(self, response):\n        print(response.url)\n        resdata = json.loads(response.body_as_unicode())\n\n        if resdata[&#34;code&#34;] == 0:\n            # 获取最后一个数据\n            if len(resdata[&#34;result&#34;][&#34;list&#34;]) &gt; 0:\n                data = resdata[&#34;result&#34;][&#34;list&#34;]\n                cursor = data[-1][&#34;cursor&#34;]\n                for one in data:\n                    item = BorenzhuanItem()\n\n                    item[&#34;author&#34;]  = one[&#34;author&#34;][&#34;uname&#34;]\n                    item[&#34;content&#34;] = one[&#34;content&#34;]\n                    item[&#34;ctime&#34;] = one[&#34;ctime&#34;]\n                    item[&#34;disliked&#34;] = one[&#34;disliked&#34;]\n                    item[&#34;liked&#34;] = one[&#34;liked&#34;]\n                    item[&#34;likes&#34;] = one[&#34;likes&#34;]\n                    item[&#34;user_season&#34;] = one[&#34;user_season&#34;][&#34;last_ep_index&#34;] if &#34;user_season&#34; in one else &#34;&#34;\n                    item[&#34;score&#34;] = one[&#34;user_rating&#34;][&#34;score&#34;]\n                    yield item\n\n            yield scrapy.Request(self.BASE_URL.format(cursor),callback=self.parse)</code></pre></div><h2><b>3. B站博人传评论数据案例---实现随机UA</b></h2><p>第一步， 在settings文件中添加一些UserAgent,我从互联网找了一些</p><div class=\"highlight\"><pre><code class=\"language-text\">USER_AGENT_LIST=[\n    &#34;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1&#34;,\n    &#34;Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11&#34;,\n    &#34;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6&#34;,\n    &#34;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6&#34;,\n    &#34;Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1&#34;,\n    &#34;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5&#34;,\n    &#34;Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5&#34;,\n    &#34;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3&#34;,\n    &#34;Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3&#34;,\n    &#34;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SE 2.X MetaSr 1.0; SE 2.X MetaSr 1.0; .NET CLR 2.0.50727; SE 2.X MetaSr 1.0)&#34;,\n    &#34;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3&#34;,\n    &#34;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3&#34;,\n    &#34;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; 360SE)&#34;,\n    &#34;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3&#34;,\n    &#34;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3&#34;,\n    &#34;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3&#34;,\n    &#34;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24&#34;,\n    &#34;Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24&#34;\n]</code></pre></div><p>第二步，在settings文件中设置 <code>“DOWNLOADER_MIDDLEWARES”</code></p><div class=\"highlight\"><pre><code class=\"language-text\"># Enable or disable downloader middlewares\n# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html\nDOWNLOADER_MIDDLEWARES = {\n   #&#39;borenzhuan.middlewares.BorenzhuanDownloaderMiddleware&#39;: 543,\n    &#39;borenzhuan.middlewares.RandomUserAgentMiddleware&#39;: 400,\n}</code></pre></div><p>第三步，在 <code>middlewares.py</code> 文件中导入 settings模块中的 USER_AGENT_LIST 方法</p><div class=\"highlight\"><pre><code class=\"language-text\">from borenzhuan.settings import USER_AGENT_LIST # 导入中间件\nimport random\n\nclass RandomUserAgentMiddleware(object):\n    def process_request(self, request, spider):\n        rand_use  = random.choice(USER_AGENT_LIST)\n        if rand_use:\n            request.headers.setdefault(&#39;User-Agent&#39;, rand_use)</code></pre></div><p>好了，随机的UA已经实现，你可以在<code>parse</code>函数中编写如下代码进行测试</p><div class=\"highlight\"><pre><code class=\"language-text\">print(response.request.headers)</code></pre></div><h2><b>4. B站博人传评论数据----完善item</b></h2><p>这个操作相对简单，这些数据就是我们要保存的数据了。！</p><div class=\"highlight\"><pre><code class=\"language-text\">author = scrapy.Field()\n    content = scrapy.Field()\n    ctime = scrapy.Field()\n    disliked = scrapy.Field()\n    liked = scrapy.Field()\n    likes = scrapy.Field()\n    score = scrapy.Field()\n    user_season = scrapy.Field()</code></pre></div><h2><b>5. B站博人传评论数据案例---提高爬取速度</b></h2><p>在settings.py中设置如下参数：</p><div class=\"highlight\"><pre><code class=\"language-text\"># Configure maximum concurrent requests performed by Scrapy (default: 16)\nCONCURRENT_REQUESTS = 32\n# Configure a delay for requests for the same website (default: 0)\n# See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\nDOWNLOAD_DELAY = 1\n# The download delay setting will honor only one of:\nCONCURRENT_REQUESTS_PER_DOMAIN = 16\nCONCURRENT_REQUESTS_PER_IP = 16\n# Disable cookies (enabled by default)\nCOOKIES_ENABLED = False</code></pre></div><h3>解释说明</h3><h3>一、降低下载延迟</h3><p>DOWNLOAD_DELAY = 0</p><p>将下载延迟设为0，这时需要相应的防ban措施，一般使用user agent轮转，构建user agent池，轮流选择其中之一来作为user agent。</p><h3>二、多线程</h3><p>CONCURRENT_REQUESTS = 32<br/>CONCURRENT_REQUESTS_PER_DOMAIN = 16<br/>CONCURRENT_REQUESTS_PER_IP = 16</p><p>scrapy网络请求是基于Twisted，而Twisted默认支持多线程，而且scrapy默认也是通过多线程请求的，并且支持多核CPU的并发，我们通过一些设置提高scrapy的并发数可以提高爬取速度。</p><h3>三、禁用cookies</h3><p>COOKIES_ENABLED = False</p><h2><b>6. B站博人传评论数据案例---保存数据</b></h2><p>最后在<code>pipelines.py</code> 文件中，编写保存代码即可</p><div class=\"highlight\"><pre><code class=\"language-text\">import os\nimport csv\n\nclass BorenzhuanPipeline(object):\n\n\n    def __init__(self):\n        store_file = os.path.dirname(__file__)+&#39;/spiders/bore.csv&#39;\n        self.file = open(store_file,&#34;a+&#34;,newline=&#34;&#34;,encoding=&#34;utf-8&#34;)\n        self.writer = csv.writer(self.file)\n\n    def process_item(self, item, spider):\n        try:\n\n            self.writer.writerow((\n                item[&#34;author&#34;],\n                item[&#34;content&#34;],\n                item[&#34;ctime&#34;],\n                item[&#34;disliked&#34;],\n                item[&#34;liked&#34;],\n                item[&#34;likes&#34;],\n                item[&#34;score&#34;],\n                item[&#34;user_season&#34;]\n            ))\n\n        except Exception as e:\n            print(e.args)\n\n        def close_spider(self, spider):\n            self.file.close()</code></pre></div><p>运行代码之后，发现过了一会报错了<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-200c560cd16c43ec5ecd94a49f592e4e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1253\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb\" width=\"1253\" data-original=\"https://pic3.zhimg.com/v2-200c560cd16c43ec5ecd94a49f592e4e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1253&#39; height=&#39;430&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1253\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1253\" data-original=\"https://pic3.zhimg.com/v2-200c560cd16c43ec5ecd94a49f592e4e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-200c560cd16c43ec5ecd94a49f592e4e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>去看了一眼，原来是数据爬取完毕~！！！小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-02f1cc5ccacfdff37c79bfa5f6245408_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"94\" data-rawheight=\"108\" data-thumbnail=\"https://pic1.zhimg.com/v2-02f1cc5ccacfdff37c79bfa5f6245408_b.jpg\" class=\"content_image\" width=\"94\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;94&#39; height=&#39;108&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"94\" data-rawheight=\"108\" data-thumbnail=\"https://pic1.zhimg.com/v2-02f1cc5ccacfdff37c79bfa5f6245408_b.jpg\" class=\"content_image lazy\" width=\"94\" data-actualsrc=\"https://pic1.zhimg.com/v2-02f1cc5ccacfdff37c79bfa5f6245408_b.gif\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "python爬虫", 
                    "tagLink": "https://api.zhihu.com/topics/20086364"
                }, 
                {
                    "tag": "scrapy", 
                    "tagLink": "https://api.zhihu.com/topics/19950086"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/65434852", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 4, 
            "title": "Python爬虫入门教程第二十六讲： 36氪(36kr)数据抓取 scrapy", 
            "content": "<h2><b>1. 36氪(36kr)数据----写在前面</b></h2><p>今天抓取一个新闻媒体，36kr的文章内容，也是为后面的数据分析做相应的准备的，预计在12月底，爬虫大概写到50篇案例的时刻，将会迎来一个新的内容，系统的数据分析博文，记得关注哦~</p><p>36kr 让一部分人先看到未来，而你今天要做的事情确实要抓取它的过去。</p><p>网址 <a href=\"https://link.zhihu.com/?target=https%3A//36kr.com/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">36kr.com/</span><span class=\"invisible\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b4d49c81d16b358615d9e03771c8e329_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1155\" data-rawheight=\"477\" class=\"origin_image zh-lightbox-thumb\" width=\"1155\" data-original=\"https://pic2.zhimg.com/v2-b4d49c81d16b358615d9e03771c8e329_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1155&#39; height=&#39;477&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1155\" data-rawheight=\"477\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1155\" data-original=\"https://pic2.zhimg.com/v2-b4d49c81d16b358615d9e03771c8e329_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b4d49c81d16b358615d9e03771c8e329_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>2. 36氪(36kr)数据----数据分析</b></h2><p>36kr的页面是一个瀑布流的效果，当你不断的下拉页面的时候，数据从后台追加过来，基于此，基本可以判断它是ajax异步的数据，只需要打开开发者工具，就能快速的定位到想要的数据，我们尝试一下！</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-05115bc823f127b923acdae7796bb190_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1515\" data-rawheight=\"504\" class=\"origin_image zh-lightbox-thumb\" width=\"1515\" data-original=\"https://pic1.zhimg.com/v2-05115bc823f127b923acdae7796bb190_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1515&#39; height=&#39;504&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1515\" data-rawheight=\"504\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1515\" data-original=\"https://pic1.zhimg.com/v2-05115bc823f127b923acdae7796bb190_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-05115bc823f127b923acdae7796bb190_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>捕获链接如下</p><div class=\"highlight\"><pre><code class=\"language-text\">https://36kr.com/api/search-column/mainsite?per_page=20&amp;page=1&amp;_=1543840108547\nhttps://36kr.com/api/search-column/mainsite?per_page=20&amp;page=2&amp;_=1543840108547\nhttps://36kr.com/api/search-column/mainsite?per_page=20&amp;page=3&amp;_=1543840108547\nhttps://36kr.com/api/search-column/mainsite?per_page=20&amp;page=4&amp;_=1543840108547</code></pre></div><p>在多次尝试之后，发现per_page最大可以扩展到300，但是当大于100的数据，返回的数据并不是很理想，所以，我们拟定为100即可，page就是页码，这个不断循环叠加即可。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-077af6277cec553c88075055d967c1a9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"645\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb\" width=\"645\" data-original=\"https://pic2.zhimg.com/v2-077af6277cec553c88075055d967c1a9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;645&#39; height=&#39;480&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"645\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"645\" data-original=\"https://pic2.zhimg.com/v2-077af6277cec553c88075055d967c1a9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-077af6277cec553c88075055d967c1a9_b.jpg\"/></figure><p><br/>上面的参数还有一个更加重要的值，叫做<code>total_count</code> 总共有多少文章数目。有这个参数，我们就能快速的拼接出来，想要的页码了。</p><h2><b>3. 36氪(36kr)数据----创建scrapy项目</b></h2><div class=\"highlight\"><pre><code class=\"language-text\">scrapy startproject kr36</code></pre></div><h2><b>4. 36氪(36kr)数据----创建爬虫入口页面</b></h2><div class=\"highlight\"><pre><code class=\"language-text\">scrapy genspider Kr36 &#34;www.gaokaopai.com&#34;</code></pre></div><h2><b>5. 36氪(36kr)数据----编写url生成器</b></h2><p>页面起始地址<code>start_urls</code>为第一页数据，之后会调用<code>parse</code>函数，在函数内容，我们去获取<code>total_count</code>这个参数<br/>这个地方，需要注意 <code>yield</code> 返回数据为<code>Request()</code> 关于他的详细说明，请参照<br/><a href=\"https://link.zhihu.com/?target=https%3A//scrapy-chs.readthedocs.io/zh_CN/0.24/topics/request-response.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">scrapy-chs.readthedocs.io</span><span class=\"invisible\">/zh_CN/0.24/topics/request-response.html</span><span class=\"ellipsis\"></span></a></p><p>所有参数清单，参数名字起得好，基本都能代表所有的意思了。比较重要的是<code>url</code>和<code>callback</code></p><div class=\"highlight\"><pre><code class=\"language-text\">class scrapy.http.Request(url[, callback, method=&#39;GET&#39;, headers, body, cookies, meta, encoding=&#39;utf-8&#39;, priority=0, dont_filter=False, errback])\nclass Kr36Spider(scrapy.Spider):\n    name = &#39;Kr36&#39;\n    allowed_domains = [&#39;36kr.com&#39;]\n\n    start_urls = [&#39;https://36kr.com/api/search-column/mainsite?per_page=100&amp;page=1&amp;_=&#39;]\n    def parse(self, response):\n        data = json.loads(response.body_as_unicode())\n        totle = int(data[&#34;data&#34;][&#34;total_count&#34;])\n        #totle = 201\n\n        for page in range(2,int(totle/100)+2):\n            print(&#34;正在爬取{}页&#34;.format(page),end=&#34;&#34;)\n            yield Request(&#34;https://36kr.com/api/search-column/mainsite?per_page=100&amp;page={}&amp;_=&#34;.format(str(page)), callback=self.parse_item)</code></pre></div><h2><b>6. 36氪(36kr)数据----解析数据</b></h2><p>在解析数据过程中，发现有时候数据有缺失的情况发生，所以需要判断一下 <code>app_views_count</code> ， <code>mobile_views_count</code> ， <code>views_count</code> ， <code>favourite_num</code> 是否出现在字典中。</p><p>注意下面代码中的Kr36Item类，这个需要提前创建一下</p><p>Kr36Item</p><div class=\"highlight\"><pre><code class=\"language-text\">class Kr36Item(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    app_views_count = scrapy.Field() # APP观看数量\n    mobile_views_count = scrapy.Field() # 移动端观看数量\n    views_count = scrapy.Field() # PC观看数量\n    column_name = scrapy.Field() # 类别\n    favourite_num = scrapy.Field() # 收藏数量\n    title = scrapy.Field() # 标题\n    published_at = scrapy.Field() # 发布时间\n    is_free = scrapy.Field() # 是否免费\n    username = scrapy.Field()\ndef parse_item(self,response):\n\n        data = json.loads(response.body_as_unicode())\n        item = Kr36Item()\n        for one_item in data[&#34;data&#34;][&#34;items&#34;]:\n            print(one_item)\n            item[&#34;app_views_count&#34;] = one_item[&#34;app_views_count&#34;] if &#34;app_views_count&#34; in one_item else 0# APP观看数量\n            item[&#34;mobile_views_count&#34;] = one_item[&#34;mobile_views_count&#34;]  if &#34;mobile_views_count&#34; in one_item else 0 # 移动端观看数量\n            item[&#34;views_count&#34;] = one_item[&#34;views_count&#34;]  if &#34;views_count&#34; in one_item else 0  # PC观看数量\n            item[&#34;column_name&#34;] = one_item[&#34;column_name&#34;]  # 类别\n            item[&#34;favourite_num&#34;] = one_item[&#34;favourite_num&#34;]  if &#34;favourite_num&#34; in one_item else 0  # 收藏数量\n            item[&#34;title&#34;] = one_item[&#34;title&#34;] # 标题\n            item[&#34;published_at&#34;] = one_item[&#34;published_at&#34;]  # 发布时间\n            item[&#34;is_free&#34;] = one_item[&#34;is_free&#34;] if &#34;is_free&#34; in one_item else 0# 是否免费\n            item[&#34;username&#34;] = json.loads(one_item[&#34;user_info&#34;])[&#34;name&#34;]\n            yield item</code></pre></div><p>最后打开<code>settings.py</code>中的<code>pipelines</code>编写数据持久化代码</p><div class=\"highlight\"><pre><code class=\"language-text\">ITEM_PIPELINES = {\n   &#39;kr36.pipelines.Kr36Pipeline&#39;: 300,\n}\nimport os\nimport csv\n\nclass Kr36Pipeline(object):\n    def __init__(self):\n        store_file = os.path.dirname(__file__)+&#39;/spiders/36kr.csv&#39;\n        self.file = open(store_file,&#34;a+&#34;,newline=&#34;&#34;,encoding=&#34;utf_8_sig&#34;)\n        self.writer = csv.writer(self.file)\n    def process_item(self, item, spider):\n        try:\n            self.writer.writerow((\n                item[&#34;title&#34;],\n                item[&#34;app_views_count&#34;],\n                item[&#34;mobile_views_count&#34;],\n                item[&#34;views_count&#34;],\n                item[&#34;column_name&#34;],\n                item[&#34;favourite_num&#34;],\n                item[&#34;published_at&#34;],\n                item[&#34;is_free&#34;],\n                item[&#34;username&#34;]\n            ))\n            print(&#34;数据存储完毕&#34;)\n        except Exception as e:\n            print(e.args)\n\n    def close_spider(self,spider):\n        self.file.close()</code></pre></div><h2><b>7. 36氪(36kr)数据----获取数据</b></h2><p>运行上述代码，没有做过多的处理，也没有调整并发速度，也没有做反爬措施。跑了一下，大概获取到了<code>69936</code>条数据，和预估的差了300多条，问题不大，原因没细查，哈哈哈哈,小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~</p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "python爬虫", 
                    "tagLink": "https://api.zhihu.com/topics/20086364"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/65328508", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 7, 
            "title": "Python爬虫入门教程第二十五讲：高考派大学数据抓取 scrapy", 
            "content": "<h2><b>1. 高考派大学数据----写在前面</b></h2><p>终于写到了<code>scrapy</code>爬虫框架了，这个框架可以说是python爬虫框架里面出镜率最高的一个了，我们接下来重点研究一下它的使用规则。</p><p>安装过程自己百度一下，就能找到3种以上的安装手法，哪一个都可以安装上<br/>可以参考 <code>https://scrapy-chs.readthedocs.io/zh_CN/0.24/intro/install.html</code> 官方说明进行安装。</p><h2><b>2. 高考派大学数据----创建scrapy项目</b></h2><p>通用使用下面的命令，创建即可</p><blockquote><i>scrapy startproject mySpider</i></blockquote><p>完成之后，你的项目的目录结构为</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8178a16e966519167bac959edfc2d7ad_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"371\" data-rawheight=\"323\" class=\"content_image\" width=\"371\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;371&#39; height=&#39;323&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"371\" data-rawheight=\"323\" class=\"content_image lazy\" width=\"371\" data-actualsrc=\"https://pic2.zhimg.com/v2-8178a16e966519167bac959edfc2d7ad_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>每个文件对应的意思为</p><ul><li>scrapy.cfg 项目的配置文件</li><li>mySpider/ 根目录</li><li>mySpider/items.py 项目的目标文件，规范数据格式，用来定义解析对象对应的属性或字段。</li><li>mySpider/pipelines.py 项目的管道文件，负责处理被spider提取出来的item。典型的处理有清理、 验证及持久化(例如存取到数据库）</li><li>mySpider/settings.py 项目的设置文件</li><li>mySpider/spiders/ 爬虫主目录</li><li><i>middlewares.py Spider中间件是在引擎及Spider之间的特定钩子(specific hook)，处理spider的输入(response)和输出(items及requests)。 其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。</i> 本篇文章没有涉及</li></ul><h3>高考派大学数据----创建Scrapy爬虫</h3><p>通过命令行进入到 mySpider/spiders/ 目录，然后执行如下命令</p><blockquote><i>scrapy genspider GaoKao &#34;<a href=\"https://link.zhihu.com/?target=http%3A//www.gaokaopai.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">gaokaopai.com</span><span class=\"invisible\"></span></a>&#34;</i></blockquote><p>打开mySpider/spiders/ 目录里面的 GaoKao，默认增加了 下列代码</p><div class=\"highlight\"><pre><code class=\"language-text\">import scrapy\n \nclass GaoKaoSpider(scrapy.Spider):\n    name = &#34;GaoKao&#34;\n    allowed_domains = [&#34;www.gaokaopai.com&#34;]\n    start_urls = [&#39;http://www.gaokaopai.com/&#39;]\n \n    def parse(self, response):\n        pass</code></pre></div><p>默认生成的代码，包含一个<code>GaoKaoSpider</code>的类，并且这个类是用<code>scrapy.Spider</code>继承来的<br/>而且默认实现了三个属性和一个方法</p><p>name = &#34;&#34; 这个是爬虫的名字，必须唯一，在不同的爬虫需要定义不同的名字<br/>allowed_domains = [] 域名范围，限制爬虫爬取当前域名下的网页<br/>start_urls =[] 爬取的URL元组/列表。爬虫从这里开始爬取数据，第一次爬取的页面就是从这里开始，其他的URL将会从这些起始的URL爬取的结果中生成<br/>parse(self,response) 解析网页的方法，每个初始URL完成下载后将调用，调用的时候传入每一个初始URL返回的Response对象作为唯一参数，主要作用<code>1、负责解析返回的网页数据，response.body</code> <code>2、生成下一页的URL请求</code></p><h3>高考派大学数据----第一个案例</h3><p>我们要爬取的是<code>高考派大学数据</code> 数据为 <code>http://www.gaokaopai.com/rank-index.html</code><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2623420b4828828fcadf688268226fef_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1033\" data-rawheight=\"508\" class=\"origin_image zh-lightbox-thumb\" width=\"1033\" data-original=\"https://pic4.zhimg.com/v2-2623420b4828828fcadf688268226fef_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1033&#39; height=&#39;508&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1033\" data-rawheight=\"508\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1033\" data-original=\"https://pic4.zhimg.com/v2-2623420b4828828fcadf688268226fef_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-2623420b4828828fcadf688268226fef_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>页面下部有一个加载更多，点击抓取链接</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-722d1186dd227c89f57574f61b5f5c34_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1270\" data-rawheight=\"541\" class=\"origin_image zh-lightbox-thumb\" width=\"1270\" data-original=\"https://pic1.zhimg.com/v2-722d1186dd227c89f57574f61b5f5c34_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1270&#39; height=&#39;541&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1270\" data-rawheight=\"541\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1270\" data-original=\"https://pic1.zhimg.com/v2-722d1186dd227c89f57574f61b5f5c34_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-722d1186dd227c89f57574f61b5f5c34_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>尴尬的事情发生了，竟然是一个POST请求，本打算实现一个GET的，这回代码量有点大了~</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a7425f2404a104eed9285ecc81601778_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"160\" data-rawheight=\"160\" class=\"content_image\" width=\"160\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;160&#39; height=&#39;160&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"160\" data-rawheight=\"160\" class=\"content_image lazy\" width=\"160\" data-actualsrc=\"https://pic1.zhimg.com/v2-a7425f2404a104eed9285ecc81601778_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>scrapy 模式是GET请求的，如果我们需要修改成POST，那么需要重写Spider类的start_requests(self) 方法，并且不再调用start_urls里面的url了，所以，咱对代码进行一些修改。重写代码之后，注意下面这段代码</p><div class=\"highlight\"><pre><code class=\"language-text\">request = FormRequest(self.start_url,headers=self.headers,formdata=form_data,callback=self.parse)</code></pre></div><p>FormRequest 需要引入模块 <code>from scrapy import FormRequest</code><br/>self.start_url 写上post请求的地址即可<br/>formdata用来提交表单数据<br/>callback调用网页解析参数<br/>最后的 yield request 表示这个函数是一个生成器</p><div class=\"highlight\"><pre><code class=\"language-text\">import scrapy\nfrom scrapy import FormRequest\nimport json\n\nfrom items import MyspiderItem\nclass GaokaoSpider(scrapy.Spider):\n    name = &#39;GaoKao&#39;\n    allowed_domains = [&#39;gaokaopai.com&#39;]\n    start_url = &#39;http://www.gaokaopai.com/rank-index.html&#39;\n\n    def __init__(self):\n        self.headers = {\n            &#34;User-Agent&#34;:&#34;自己找个UA&#34;,\n            &#34;X-Requested-With&#34;:&#34;XMLHttpRequest&#34;\n        }\n\n    # 需要重写start_requests() 方法\n    def start_requests(self):\n        for page in range(0,7):\n            form_data = {\n                &#34;otype&#34;: &#34;4&#34;,\n                &#34;city&#34;:&#34;&#34;,\n                &#34;start&#34;:str(25*page),\n                &#34;amount&#34;: &#34;25&#34;\n            }\n\n            request = FormRequest(self.start_url,headers=self.headers,formdata=form_data,callback=self.parse)\n            yield request\n\n    def parse(self, response):\n        print(response.body)\n        print(response.url)\n        print(response.body_as_unicode())</code></pre></div><p>我们在 <code>def parse(self, response):</code> 函数里面，输出一下网页内容，这个地方，需要用到1个知识点是</p><p>获取网页内容 <code>response.body</code> <code>response.body_as_unicode()</code></p><ul><li>response.url获取抓取的rul</li><li>response.body获取网页内容字节类型</li><li>response.body_as_unicode()获取网站内容字符串类型</li></ul><p><b>我们接下来就可以运行一下爬虫程序了</b></p><p>在项目根目录创建一个<code>begin.py</code> 文件，里面写入如下代码<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-56d5d7228bf6aad9d5ccaa65a92a5554_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"276\" data-rawheight=\"314\" class=\"content_image\" width=\"276\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;276&#39; height=&#39;314&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"276\" data-rawheight=\"314\" class=\"content_image lazy\" width=\"276\" data-actualsrc=\"https://pic1.zhimg.com/v2-56d5d7228bf6aad9d5ccaa65a92a5554_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">from scrapy import cmdline\ncmdline.execute((&#34;scrapy crawl GaoKao&#34;).split())</code></pre></div><p>运行该文件，记住在scrapy中的其他py文件中，运行是不会显示相应的结果的，每次测试的时候，都需要运行begin.py 当然，你可起一个其他的名字。</p><p>如果你不这么干的，那么你只能 采用下面的操作，就是比较麻烦。</p><div class=\"highlight\"><pre><code class=\"language-text\">cd到爬虫目录里执行scrapy crawl GaoKao--nolog命令\n说明：scrapy crawl GaoKao(GaoKao表示爬虫名称) --nolog(--nolog表示不显示日志)</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e5980032daf4d66d06ee60b792b4ebb1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"269\" data-rawheight=\"214\" class=\"content_image\" width=\"269\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;269&#39; height=&#39;214&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"269\" data-rawheight=\"214\" class=\"content_image lazy\" width=\"269\" data-actualsrc=\"https://pic2.zhimg.com/v2-e5980032daf4d66d06ee60b792b4ebb1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>运行起来，就在控制台打印数据了，测试方便，可以把上述代码中那个数字7，修改成2，有心人能看到我这个小文字</p><p>pycharm在运行过程中，会在控制台打印很多红色的字，没事，那不是BUG</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-86a430759e1e712e0d9ab5ab1a39b7aa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"680\" data-rawheight=\"301\" class=\"origin_image zh-lightbox-thumb\" width=\"680\" data-original=\"https://pic3.zhimg.com/v2-86a430759e1e712e0d9ab5ab1a39b7aa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;680&#39; height=&#39;301&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"680\" data-rawheight=\"301\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"680\" data-original=\"https://pic3.zhimg.com/v2-86a430759e1e712e0d9ab5ab1a39b7aa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-86a430759e1e712e0d9ab5ab1a39b7aa_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>一定要在红色的字中间找到黑色的字，黑色的字才是你打印出来的数据，如下，得到这样的内容，就成功一大半了。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f6152e2310ec52e86a276ccc6c3ca696_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"943\" data-rawheight=\"202\" class=\"origin_image zh-lightbox-thumb\" width=\"943\" data-original=\"https://pic3.zhimg.com/v2-f6152e2310ec52e86a276ccc6c3ca696_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;943&#39; height=&#39;202&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"943\" data-rawheight=\"202\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"943\" data-original=\"https://pic3.zhimg.com/v2-f6152e2310ec52e86a276ccc6c3ca696_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f6152e2310ec52e86a276ccc6c3ca696_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>但是这个地方有个小坑，就是，你会发现返回的数据不一致，这个我测试了一下，是因为第一页的数据返回的不是JSON格式的，而是普通的网页，那么我们需要针对性处理一下，这个先不用管，我们把<code>items.py</code> 进行完善</p><div class=\"highlight\"><pre><code class=\"language-text\">import scrapy\nclass MyspiderItem(scrapy.Item):\n    # 学校名称\n    uni_name = scrapy.Field()\n    uni_id = scrapy.Field()\n    city_code = scrapy.Field()\n    uni_type = scrapy.Field()\n    slogo = scrapy.Field()\n    # 录取难度\n    safehard = scrapy.Field()\n    # 院校所在地\n    rank = scrapy.Field()</code></pre></div><p>然后在刚才的GaokaoSpider类中，继续完善parse函数，通过判断 <code>response.headers[&#34;Content-Type&#34;]</code> 去确定本页面是HTML格式，还是JSON格式。</p><div class=\"highlight\"><pre><code class=\"language-text\">if(content_type.find(&#34;text/html&#34;)&gt;0):\n            # print(response.body_as_unicode())\n            trs = response.xpath(&#34;//table[@id=&#39;results&#39;]//tr&#34;)[1:]\n            for item in trs:\n                school = MyspiderItem()\n                rank = item.xpath(&#34;td[1]/span/text()&#34;).extract()[0]\n                uni_name = item.xpath(&#34;td[2]/a/text()&#34;).extract()[0]\n                safehard  = item.xpath(&#34;td[3]/text()&#34;).extract()[0]\n                city_code = item.xpath(&#34;td[4]/text()&#34;).extract()[0]\n                uni_type = item.xpath(&#34;td[6]/text()&#34;).extract()[0]\n\n                school[&#34;uni_name&#34;] = uni_name\n                school[&#34;uni_id&#34;] = &#34;&#34;\n                school[&#34;city_code&#34;] = city_code\n                school[&#34;uni_type&#34;] = uni_type\n                school[&#34;slogo&#34;] = &#34;&#34;\n                school[&#34;rank&#34;] = rank\n                school[&#34;safehard&#34;] = safehard\n                yield school\n\n\n        else:\n            data = json.loads(response.body_as_unicode())\n            data = data[&#34;data&#34;][&#34;ranks&#34;] # 获取数据\n            \n            for item in data:\n                school = MyspiderItem()\n                school[&#34;uni_name&#34;] = item[&#34;uni_name&#34;]\n                school[&#34;uni_id&#34;] = item[&#34;uni_id&#34;]\n                school[&#34;city_code&#34;] = item[&#34;city_code&#34;]\n                school[&#34;uni_type&#34;] = item[&#34;uni_type&#34;]\n                school[&#34;slogo&#34;] = item[&#34;slogo&#34;]\n                school[&#34;rank&#34;] = item[&#34;rank&#34;]\n                school[&#34;safehard&#34;] = item[&#34;safehard&#34;]\n                # 将获取的数据交给pipelines，pipelines在settings.py中定义\n                yield school</code></pre></div><p><b>parse() 方法的执行机制</b></p><ol><li>使用yield返回数据，不要使用return。这样子parse就会被当做一个生成器。scarpy将parse生成的数据，逐一返回</li><li>如果返回值是request则加入爬取队列，如果是item类型，则交给pipeline出来，其他类型报错</li></ol><p>到这里，如果想要数据准备的进入到 pipeline 中，你需要在<code>setting.py</code>中将配置开启</p><div class=\"highlight\"><pre><code class=\"language-text\"># See https://doc.scrapy.org/en/latest/topics/item-pipeline.html\n    ITEM_PIPELINES = {\n       &#39;mySpider.pipelines.MyspiderPipeline&#39;: 300,\n    }</code></pre></div><p>同时编写 <code>pipeline.py</code> 文件</p><div class=\"highlight\"><pre><code class=\"language-text\">import os\nimport csv\n\nclass MyspiderPipeline(object):\n\n    def __init__(self):\n        # csv 文件\n        store_file = os.path.dirname(__file__)+&#34;/spiders/school1.csv&#34;\n        self.file = open(store_file,&#34;a+&#34;,newline=&#39;&#39;,encoding=&#34;utf-8&#34;)\n        self.writer = csv.writer(self.file)\n\n    def process_item(self, item, spider):\n        try:\n     \n            self.writer.writerow((\n                item[&#34;uni_name&#34;],\n                item[&#34;uni_id&#34;],\n                item[&#34;city_code&#34;],\n                item[&#34;uni_type&#34;],\n                item[&#34;slogo&#34;],\n                item[&#34;rank&#34;],\n                item[&#34;safehard&#34;]\n            ))\n\n        except Exception as e:\n            print(e.args)\n\n\n    def close_spider(self,spider):\n        self.file.close()</code></pre></div><p>好了，代码全部编写完毕，还是比较简单的吧，把上面的数字在修改成7，为啥是7，因为只能获取到前面150条数据，小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fdf3bb5c58b0ef49c403792f7e6433f2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"783\" data-rawheight=\"325\" class=\"origin_image zh-lightbox-thumb\" width=\"783\" data-original=\"https://pic3.zhimg.com/v2-fdf3bb5c58b0ef49c403792f7e6433f2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;783&#39; height=&#39;325&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"783\" data-rawheight=\"325\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"783\" data-original=\"https://pic3.zhimg.com/v2-fdf3bb5c58b0ef49c403792f7e6433f2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fdf3bb5c58b0ef49c403792f7e6433f2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-398963e84de0e05cf29fd0f2954ec597_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"198\" data-rawheight=\"171\" class=\"content_image\" width=\"198\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;198&#39; height=&#39;171&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"198\" data-rawheight=\"171\" class=\"content_image lazy\" width=\"198\" data-actualsrc=\"https://pic4.zhimg.com/v2-398963e84de0e05cf29fd0f2954ec597_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "python爬虫", 
                    "tagLink": "https://api.zhihu.com/topics/20086364"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/65326504", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 2, 
            "title": "Python爬虫入门教程第二十四讲：手机APP数据抓取 pyspider", 
            "content": "<h2><b>1. 手机APP数据----写在前面</b></h2><p>继续练习pyspider的使用，最近搜索了一些这个框架的一些使用技巧，发现文档竟然挺难理解的，不过使用起来暂时没有障碍，估摸着，要在写个5篇左右关于这个框架的教程。今天教程中增加了图片的处理，你可以重点学习一下。</p><h2><b>2. 手机APP数据----页面分析</b></h2><p>咱要爬取的网站是 <code>http://www.liqucn.com/rj/new/</code> 这个网站我看了一下，有大概20000页，每页数据是9个，数据量大概在180000左右，可以抓取下来，后面做数据分析使用，也可以练习优化数据库。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-dc0829b783ade6974bed9627604e5ea9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1195\" data-rawheight=\"309\" class=\"origin_image zh-lightbox-thumb\" width=\"1195\" data-original=\"https://pic2.zhimg.com/v2-dc0829b783ade6974bed9627604e5ea9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1195&#39; height=&#39;309&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1195\" data-rawheight=\"309\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1195\" data-original=\"https://pic2.zhimg.com/v2-dc0829b783ade6974bed9627604e5ea9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-dc0829b783ade6974bed9627604e5ea9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>网站基本没有反爬措施，上去爬就可以，略微控制一下并发，毕竟不要给别人服务器太大的压力。</p><p>页面经过分析之后，可以看到它是基于URL进行的分页，这就简单了，我们先通过首页获取总页码，然后批量生成所有页码即可</p><div class=\"highlight\"><pre><code class=\"language-text\">http://www.liqucn.com/rj/new/?page=1\nhttp://www.liqucn.com/rj/new/?page=2\nhttp://www.liqucn.com/rj/new/?page=3\nhttp://www.liqucn.com/rj/new/?page=4</code></pre></div><p>获取总页码的代码</p><div class=\"highlight\"><pre><code class=\"language-text\">class Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(&#39;http://www.liqucn.com/rj/new/?page=1&#39;, callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        # 获取最后一页的页码\n        totle = int(response.doc(&#34;.current&#34;).text())\n        for page in range(1,totle+1):\n            self.crawl(&#39;http://www.liqucn.com/rj/new/?page={}&#39;.format(page), callback=self.detail_page)</code></pre></div><p>然后copy一段官方中文翻译，过来，时刻提醒自己</p><div class=\"highlight\"><pre><code class=\"language-text\">代码简单分析：\n\ndef on_start(self) 方法是入口代码。当在web控制台点击run按钮时会执行此方法。\n\nself.crawl(url, callback=self.index_page)这个方法是调用API生成一个新的爬取任务，\n            这个任务被添加到待抓取队列。\ndef index_page(self, response) 这个方法获取一个Response对象。 \n            response.doc是pyquery对象的一个扩展方法。pyquery是一个类似于jQuery的对象选择器。\n\ndef detail_page(self, response)返回一个结果集对象。\n            这个结果默认会被添加到resultdb数据库（如果启动时没有指定数据库默认调用sqlite数据库）。你也可以重写\n            on_result(self,result)方法来指定保存位置。\n\n更多知识：\n@every(minutes=24*60, seconds=0) 这个设置是告诉scheduler（调度器）on_start方法每天执行一次。\n@config(age=10 * 24 * 60 * 60) 这个设置告诉scheduler（调度器）这个request（请求）过期时间是10天，\n    10天内再遇到这个请求直接忽略。这个参数也可以在self.crawl(url, age=10*24*60*60) 和 crawl_config中设置。\n@config(priority=2) 这个是优先级设置。数字越大越先执行。</code></pre></div><p>分页数据已经添加到待爬取队列中去了，下面开始分析爬取到的数据，这个在<code>detail_page</code>函数实现</p><div class=\"highlight\"><pre><code class=\"language-text\">    @config(priority=2)\n    def detail_page(self, response):\n        docs = response.doc(&#34;.tip_blist li&#34;).items()\n        dicts = []\n        for item in docs:\n            title = item(&#34;.tip_list&gt;span&gt;a&#34;).text()\n            pubdate = item(&#34;.tip_list&gt;i:eq(0)&#34;).text()\n            info = item(&#34;.tip_list&gt;i:eq(1)&#34;).text()\n            # 手机类型\n            category = info.split(&#34;：&#34;)[1]\n            size = info.split(&#34;/&#34;)\n            if len(size) == 2:\n                size = size[1]\n            else:\n                size = &#34;0MB&#34;\n            app_type = item(&#34;p&#34;).text()\n            mobile_type = item(&#34;h3&gt;a&#34;).text()\n            # 保存数据\n            \n            # 建立图片下载渠道\n            \n            img_url = item(&#34;.tip_list&gt;a&gt;img&#34;).attr(&#34;src&#34;)\n            # 获取文件名字\n            filename = img_url[img_url.rindex(&#34;/&#34;)+1:]\n            # 添加软件logo图片下载地址\n            self.crawl(img_url,callback=self.save_img,save={&#34;filename&#34;:filename},validate_cert=False)\n            dicts.append({\n                &#34;title&#34;:title,\n                &#34;pubdate&#34;:pubdate,\n                &#34;category&#34;:category,\n                &#34;size&#34;:size,\n                &#34;app_type&#34;:app_type,\n                &#34;mobile_type&#34;:mobile_type\n                \n                })\n        return dicts</code></pre></div><p>数据已经集中返回，我们重写<code>on_result</code>来保存数据到<code>mongodb</code>中，在编写以前，先把链接<code>mongodb</code>的相关内容编写完毕</p><div class=\"highlight\"><pre><code class=\"language-text\">import os\n\nimport pymongo\nimport pandas as pd\nimport numpy as np\nimport time\nimport json\n\nDATABASE_IP = &#39;127.0.0.1&#39;\nDATABASE_PORT = 27017\nDATABASE_NAME = &#39;sun&#39;\nclient = pymongo.MongoClient(DATABASE_IP,DATABASE_PORT)\ndb = client.sun\ndb.authenticate(&#34;dba&#34;, &#34;dba&#34;)\ncollection = db.liqu  # 准备插入数据</code></pre></div><p>数据存储</p><div class=\"highlight\"><pre><code class=\"language-text\">def on_result(self,result):\n        if result:\n            self.save_to_mongo(result)            \n \n    def save_to_mongo(self,result):\n        df = pd.DataFrame(result)\n        #print(df)\n        content = json.loads(df.T.to_json()).values()\n        if collection.insert_many(content):\n            print(&#39;存储到 mongondb 成功&#39;)</code></pre></div><p>获取到的数据，如下表所示。到此为止，咱已经完成大部分的工作了，最后把图片下载完善一下，就收工啦！</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9fd4d10e483650b86375a4e2b3e0e5e7_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"240\" data-rawheight=\"240\" data-thumbnail=\"https://pic4.zhimg.com/v2-9fd4d10e483650b86375a4e2b3e0e5e7_b.jpg\" class=\"content_image\" width=\"240\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;240&#39; height=&#39;240&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"240\" data-rawheight=\"240\" data-thumbnail=\"https://pic4.zhimg.com/v2-9fd4d10e483650b86375a4e2b3e0e5e7_b.jpg\" class=\"content_image lazy\" width=\"240\" data-actualsrc=\"https://pic4.zhimg.com/v2-9fd4d10e483650b86375a4e2b3e0e5e7_b.gif\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d51c681b9696e55ae2e49ec55833438a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"851\" data-rawheight=\"266\" class=\"origin_image zh-lightbox-thumb\" width=\"851\" data-original=\"https://pic3.zhimg.com/v2-d51c681b9696e55ae2e49ec55833438a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;851&#39; height=&#39;266&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"851\" data-rawheight=\"266\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"851\" data-original=\"https://pic3.zhimg.com/v2-d51c681b9696e55ae2e49ec55833438a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d51c681b9696e55ae2e49ec55833438a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>3. 手机APP数据----图片存储</b></h2><p>图片下载，其实就是保存网络图片到一个地址即可</p><div class=\"highlight\"><pre><code class=\"language-text\">def save_img(self,response):\n        content = response.content\n        file_name = response.save[&#34;filename&#34;]\n        #创建文件夹（如果不存在）\n        if not os.path.exists(DIR_PATH):                         \n            os.makedirs(DIR_PATH) \n            \n        file_path = DIR_PATH + &#34;/&#34; + file_name\n        \n        with open(file_path,&#34;wb&#34; ) as f:\n            f.write(content)</code></pre></div><p>到此为止，任务完成，保存之后，调整爬虫的抓取速度，点击run，数据跑起来~~~~小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-fcf3fbfe277b99dde29e2fe51e1fe62b_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"240\" data-rawheight=\"213\" data-thumbnail=\"https://pic4.zhimg.com/v2-fcf3fbfe277b99dde29e2fe51e1fe62b_b.jpg\" class=\"content_image\" width=\"240\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;240&#39; height=&#39;213&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"240\" data-rawheight=\"213\" data-thumbnail=\"https://pic4.zhimg.com/v2-fcf3fbfe277b99dde29e2fe51e1fe62b_b.jpg\" class=\"content_image lazy\" width=\"240\" data-actualsrc=\"https://pic4.zhimg.com/v2-fcf3fbfe277b99dde29e2fe51e1fe62b_b.gif\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "python爬虫", 
                    "tagLink": "https://api.zhihu.com/topics/20086364"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/65323900", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "Python爬虫入门教程第二十三讲：微医挂号网专家团队数据抓取pyspider", 
            "content": "<h2><b>1. 微医挂号网专家团队数据----写在前面</b></h2><p>今天尝试使用一个新的爬虫库进行数据的爬取，这个库叫做<code>pyspider</code>，国人开发的，当然支持一下。</p><p>github地址： <a href=\"https://link.zhihu.com/?target=https%3A//github.com/binux/pyspider\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/binux/pyspid</span><span class=\"invisible\">er</span><span class=\"ellipsis\"></span></a><br/>官方文档地址：<a href=\"https://link.zhihu.com/?target=http%3A//docs.pyspider.org/en/latest/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">docs.pyspider.org/en/la</span><span class=\"invisible\">test/</span><span class=\"ellipsis\"></span></a></p><p>安装起来是非常简单的</p><div class=\"highlight\"><pre><code class=\"language-text\">pip install pyspider</code></pre></div><p>安装之后，启动 在<code>CMD控制台</code>里面敲入命令</p><div class=\"highlight\"><pre><code class=\"language-text\">pyspider</code></pre></div><p>出现如下界面，代表运行成功，一般情况下，你的电脑如果没有安装 <code>phantomjs</code> 他会先给你安装一下。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5b391818de3bc812b4a2047a40619ac3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1025\" data-rawheight=\"243\" class=\"origin_image zh-lightbox-thumb\" width=\"1025\" data-original=\"https://pic4.zhimg.com/v2-5b391818de3bc812b4a2047a40619ac3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1025&#39; height=&#39;243&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1025\" data-rawheight=\"243\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1025\" data-original=\"https://pic4.zhimg.com/v2-5b391818de3bc812b4a2047a40619ac3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5b391818de3bc812b4a2047a40619ac3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>接下来打开浏览器，访问地址输入 <code>127.0.0.1:5000</code>, 应该显示如下界面，就可以愉快的进行编码了~</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ffec924bd2aa7a52e7225aee0af35392_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"673\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb\" width=\"673\" data-original=\"https://pic3.zhimg.com/v2-ffec924bd2aa7a52e7225aee0af35392_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;673&#39; height=&#39;430&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"673\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"673\" data-original=\"https://pic3.zhimg.com/v2-ffec924bd2aa7a52e7225aee0af35392_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ffec924bd2aa7a52e7225aee0af35392_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>3步创建一个项目</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e00f2372e0d8de3c7393b7b299a33438_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1605\" data-rawheight=\"697\" class=\"origin_image zh-lightbox-thumb\" width=\"1605\" data-original=\"https://pic1.zhimg.com/v2-e00f2372e0d8de3c7393b7b299a33438_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1605&#39; height=&#39;697&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1605\" data-rawheight=\"697\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1605\" data-original=\"https://pic1.zhimg.com/v2-e00f2372e0d8de3c7393b7b299a33438_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e00f2372e0d8de3c7393b7b299a33438_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>2. 微医挂号网专家团队数据----库基本使用入门</b></h2><p>这款工具的详细使用，给你提供一个非常好的博文，写的很完善了，我就不在赘述了。咱们直接进入到编码的部分。</p><p><code>https://blog.csdn.net/weixin_37947156/article/details/76495144</code></p><h2><b>3. 微医挂号网专家团队数据----爬虫源码</b></h2><p>我们要爬取的目标站点是<code>微医挂号网专家团队数据</code> 网页地址<code>https://www.guahao.com/eteam/index</code></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ed0fe14c33a3fd0c36b8fc33c0701fa3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"516\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic4.zhimg.com/v2-ed0fe14c33a3fd0c36b8fc33c0701fa3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;516&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"516\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic4.zhimg.com/v2-ed0fe14c33a3fd0c36b8fc33c0701fa3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ed0fe14c33a3fd0c36b8fc33c0701fa3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>分析AJAX链接地址，寻找爬取规律</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b130ce2429a0784d0796262a7e6cb3ad_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"863\" data-rawheight=\"272\" class=\"origin_image zh-lightbox-thumb\" width=\"863\" data-original=\"https://pic2.zhimg.com/v2-b130ce2429a0784d0796262a7e6cb3ad_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;863&#39; height=&#39;272&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"863\" data-rawheight=\"272\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"863\" data-original=\"https://pic2.zhimg.com/v2-b130ce2429a0784d0796262a7e6cb3ad_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b130ce2429a0784d0796262a7e6cb3ad_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>经过分析之后获取到的链接为 <code>https://www.guahao.com/json/white/search/eteams?q=&amp;dept=&amp;page=2&amp;cid=&amp;pid=&amp;_=1542794523454</code></p><p>其中<code>page</code>参数最重要，表示页码，实际测试中发现，当代码翻页到 84页的时候，数据竟然开始重复了，应该是网站本身系统的问题，这个没有办法。</p><h3>爬虫流程</h3><ol><li>获取总页数</li><li>循环爬取每页的数据</li></ol><h3>爬取总页数</h3><p>在入口函数<code>on_start</code>的位置去爬取第一页数据，爬取成功之后调用<code>index_page</code>函数</p><div class=\"highlight\"><pre><code class=\"language-text\">from pyspider.libs.base_handler import *\nimport pandas as pd\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl(&#39;https://www.guahao.com/json/white/search/eteams?page=1&#39;, callback=self.index_page,validate_cert=False)</code></pre></div><p><code>index_page</code>函数用来获取页码总数，并且将所有待爬取的地址存放到self.crawl中，这个地方因为数据重复的原因，最终硬编码为84页数据了</p><div class=\"highlight\"><pre><code class=\"language-text\">    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        doctors = response.json\n        if doctors:\n            if doctors[&#34;data&#34;]:\n                page_count = doctors[&#34;data&#34;][&#34;pageCount&#34;]\n                #for page in range(1,page_count+1):\n                for page in range(1,85):\n                    self.crawl(&#39;https://www.guahao.com/json/white/search/eteams?page={}&#39;.format(page),callback=self.detail_page,validate_cert=False)</code></pre></div><p>最后一步，解析数据，数据爬取完毕，存放到 csv 文件里面</p><div class=\"highlight\"><pre><code class=\"language-text\">    @config(priority=2)\n    def detail_page(self, response):\n        doctors = response.json\n        data = doctors[&#34;data&#34;][&#34;list&#34;]\n        return data\n    \n    def on_result(self,result):\n        if result:\n            print(&#34;正在存储数据....&#34;)\n            data = pd.DataFrame(result)\n            data.to_csv(&#34;专家数据.csv&#34;, mode=&#39;a&#39;, header=False, encoding=&#39;utf_8_sig&#39;)</code></pre></div><p>完成的代码预览<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3fb443e5c37af24220d8b1ab05bd979e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"794\" data-rawheight=\"549\" class=\"origin_image zh-lightbox-thumb\" width=\"794\" data-original=\"https://pic3.zhimg.com/v2-3fb443e5c37af24220d8b1ab05bd979e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;794&#39; height=&#39;549&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"794\" data-rawheight=\"549\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"794\" data-original=\"https://pic3.zhimg.com/v2-3fb443e5c37af24220d8b1ab05bd979e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-3fb443e5c37af24220d8b1ab05bd979e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>回到主页面，此时看到任务列表显示了我们刚刚创建的任务，设置 status 为 running，然后点击 Run 按钮执行</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-89045f388948cc072b3619e9c6df4caa_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1899\" data-rawheight=\"218\" class=\"origin_image zh-lightbox-thumb\" width=\"1899\" data-original=\"https://pic3.zhimg.com/v2-89045f388948cc072b3619e9c6df4caa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1899&#39; height=&#39;218&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1899\" data-rawheight=\"218\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1899\" data-original=\"https://pic3.zhimg.com/v2-89045f388948cc072b3619e9c6df4caa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-89045f388948cc072b3619e9c6df4caa_b.png\"/></figure><p><br/>执行完成后，点击 Results 按钮，进入到爬取结果的页面</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-272bcba4a53440f2a97dd4dbae3e3e1d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"882\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb\" width=\"882\" data-original=\"https://pic2.zhimg.com/v2-272bcba4a53440f2a97dd4dbae3e3e1d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;882&#39; height=&#39;148&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"882\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"882\" data-original=\"https://pic2.zhimg.com/v2-272bcba4a53440f2a97dd4dbae3e3e1d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-272bcba4a53440f2a97dd4dbae3e3e1d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>等着就可以了</p><h2><b>4. 微医挂号网专家团队数据----最后几步</b></h2><ol><li>Web UI 控制台上的 rate/burst 参数来调节速度，rate 是 每秒抓取的数量，burst 是并发的数量</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-54e351f9c1ede4658508abf9e2f5b2a3_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1488\" data-rawheight=\"144\" class=\"origin_image zh-lightbox-thumb\" width=\"1488\" data-original=\"https://pic4.zhimg.com/v2-54e351f9c1ede4658508abf9e2f5b2a3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1488&#39; height=&#39;144&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1488\" data-rawheight=\"144\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1488\" data-original=\"https://pic4.zhimg.com/v2-54e351f9c1ede4658508abf9e2f5b2a3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-54e351f9c1ede4658508abf9e2f5b2a3_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ol><li>pyspider 爬取完毕之后，你在点击run是不会在运行的。解决办法如下，停止 pyspider，找到下图的几个文件<br/>project.db 和 result.db 两个文件不要删除，删除其他文件即可。</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-714ffd06457d9d7d9df204e4a68d8592_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"674\" data-rawheight=\"149\" class=\"origin_image zh-lightbox-thumb\" width=\"674\" data-original=\"https://pic3.zhimg.com/v2-714ffd06457d9d7d9df204e4a68d8592_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;674&#39; height=&#39;149&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"674\" data-rawheight=\"149\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"674\" data-original=\"https://pic3.zhimg.com/v2-714ffd06457d9d7d9df204e4a68d8592_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-714ffd06457d9d7d9df204e4a68d8592_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>写完啦~ 得到了 ·1000·多个专家团队。小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-70b192313fc1411e9041ce572e465da3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"261\" data-rawheight=\"264\" class=\"content_image\" width=\"261\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;261&#39; height=&#39;264&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"261\" data-rawheight=\"264\" class=\"content_image lazy\" width=\"261\" data-actualsrc=\"https://pic4.zhimg.com/v2-70b192313fc1411e9041ce572e465da3_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "python爬虫", 
                    "tagLink": "https://api.zhihu.com/topics/20086364"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/65322960", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "Python爬虫入门教程第二十二讲：知乎文章图片爬取器之二", 
            "content": "<h2><b>1. 知乎文章图片爬取器之二博客背景</b></h2><p>昨天写了知乎文章图片爬取器的一部分代码，针对知乎问题的答案json进行了数据抓取，博客中出现了部分写死的内容，今天把那部分信息调整完毕，并且将图片下载完善到代码中去。</p><p>首先，需要获取任意知乎的问题，只需要你输入问题的ID，就可以获取相关的页面信息，比如最重要的合计有多少人回答问题。<br/>问题ID为如下标红数字<br/><a href=\"https://www.zhihu.com/question/\" class=\"internal\"><span class=\"invisible\">https://www.</span><span class=\"visible\">zhihu.com/question/</span><span class=\"invisible\"></span></a>29024583</p><p>编写代码，下面的代码用来检测用户输入的是否是正确的ID，并且通过拼接URL去获取该问题下面合计有多少答案。</p><div class=\"highlight\"><pre><code class=\"language-text\">import requests\nimport re\nimport pymongo\nimport time\nDATABASE_IP = &#39;127.0.0.1&#39;\nDATABASE_PORT = 27017\nDATABASE_NAME = &#39;sun&#39;\nclient = pymongo.MongoClient(DATABASE_IP,DATABASE_PORT)\ndb = client.sun\ndb.authenticate(&#34;dba&#34;, &#34;dba&#34;)\ncollection = db.zhihuone  # 准备插入数据\n\nBASE_URL = &#34;https://www.zhihu.com/question/{}&#34;\ndef get_totle_answers(article_id):\n    headers = {\n        &#34;user-agent&#34;: &#34;需要自己补全 Mozilla/5.0 (Windows NT 10.0; WOW64)&#34;\n    }\n\n    with requests.Session() as s:\n        with s.get(BASE_URL.format(article_id),headers=headers,timeout=3) as rep:\n            html = rep.text\n            pattern =re.compile( &#39;&lt;meta itemProp=&#34;answerCount&#34; content=&#34;(\\d*?)&#34;/&gt;&#39;)\n            s = pattern.search(html)\n            print(&#34;查找到{}条数据&#34;.format(s.groups()[0]))\n            return s.groups()[0]\n\nif __name__ == &#39;__main__&#39;:\n\n    # 用死循环判断用户输入的是否是数字\n    article_id = &#34;&#34;\n    while not article_id.isdigit():\n        article_id = input(&#34;请输入文章ID：&#34;)\n\n    totle = get_totle_answers(article_id)\n    if int(totle)&gt;0:\n        zhi = ZhihuOne(article_id,totle)\n        zhi.run()\n    else:\n        print(&#34;没有任何数据！&#34;)</code></pre></div><p>完善图片下载部分，图片下载地址在查阅过程中发现，存在json字段的<code>content</code>中，我们采用简单的正则表达式将他匹配出来。细节如下图展示</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-310ee40926525feaf995cc2e6bff2323_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1857\" data-rawheight=\"305\" class=\"origin_image zh-lightbox-thumb\" width=\"1857\" data-original=\"https://pic4.zhimg.com/v2-310ee40926525feaf995cc2e6bff2323_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1857&#39; height=&#39;305&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1857\" data-rawheight=\"305\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1857\" data-original=\"https://pic4.zhimg.com/v2-310ee40926525feaf995cc2e6bff2323_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-310ee40926525feaf995cc2e6bff2323_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>编写代码吧，下面的代码注释请仔细阅读，中间有一个小BUG，需要手动把pic3修改为pic2这个地方目前原因不明确，可能是我本地网络的原因，还有请在项目根目录先创建一个<code>imgs</code>的文件夹，用来存储图片</p><div class=\"highlight\"><pre><code class=\"language-text\">def download_img(self,data):\n        ## 下载图片\n        for item in data[&#34;data&#34;]:\n            content = item[&#34;content&#34;]\n            pattern = re.compile(&#39;&lt;noscript&gt;(.*?)&lt;/noscript&gt;&#39;)\n            imgs = pattern.findall(content)\n            if len(imgs) &gt; 0:\n                for img in imgs:\n                    match = re.search(&#39;&lt;img src=&#34;(.*?)&#34;&#39;, img)\n                    download = match.groups()[0]\n                    download = download.replace(&#34;pic3&#34;, &#34;pic2&#34;)  # 小BUG,pic3的下载不到\n\n                    print(&#34;正在下载{}&#34;.format(download), end=&#34;&#34;)\n                    try:\n                        with requests.Session() as s:\n                            with s.get(download) as img_down:\n                                # 获取文件名称\n                                file = download[download.rindex(&#34;/&#34;) + 1:]\n\n                                content = img_down.content\n                                with open(&#34;imgs/{}&#34;.format(file), &#34;wb+&#34;) as f:  # 这个地方进行了硬编码\n                                    f.write(content)\n\n                                print(&#34;图片下载完成&#34;, end=&#34;\\n&#34;)\n\n                    except Exception as e:\n                        print(e.args)\n\n\n\n            else:\n                pass</code></pre></div><p>运行结果为</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-807a19094c50763c0073836521861ab2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"725\" data-rawheight=\"481\" class=\"origin_image zh-lightbox-thumb\" width=\"725\" data-original=\"https://pic3.zhimg.com/v2-807a19094c50763c0073836521861ab2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;725&#39; height=&#39;481&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"725\" data-rawheight=\"481\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"725\" data-original=\"https://pic3.zhimg.com/v2-807a19094c50763c0073836521861ab2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-807a19094c50763c0073836521861ab2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>然后在玩知乎的过程中，发现了好多好问题，小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-818066c89fc25f95564b9fe414c869e8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"493\" data-rawheight=\"499\" class=\"origin_image zh-lightbox-thumb\" width=\"493\" data-original=\"https://pic1.zhimg.com/v2-818066c89fc25f95564b9fe414c869e8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;493&#39; height=&#39;499&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"493\" data-rawheight=\"499\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"493\" data-original=\"https://pic1.zhimg.com/v2-818066c89fc25f95564b9fe414c869e8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-818066c89fc25f95564b9fe414c869e8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1710e82d32f5d1eb22d8b0067c31b5f1_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"432\" data-rawheight=\"645\" data-thumbnail=\"https://pic2.zhimg.com/v2-1710e82d32f5d1eb22d8b0067c31b5f1_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"432\" data-original=\"https://pic2.zhimg.com/v2-1710e82d32f5d1eb22d8b0067c31b5f1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;432&#39; height=&#39;645&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"432\" data-rawheight=\"645\" data-thumbnail=\"https://pic2.zhimg.com/v2-1710e82d32f5d1eb22d8b0067c31b5f1_b.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"432\" data-original=\"https://pic2.zhimg.com/v2-1710e82d32f5d1eb22d8b0067c31b5f1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1710e82d32f5d1eb22d8b0067c31b5f1_b.gif\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "python爬虫", 
                    "tagLink": "https://api.zhihu.com/topics/20086364"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/65238845", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "Python爬虫入门教程第二十一讲：知乎文章图片爬取器之一", 
            "content": "<h2><b>1. 知乎文章图片写在前面</b></h2><p>今天开始尝试爬取一下知乎，看一下这个网站都有什么好玩的内容可以爬取到，可能断断续续会写几篇文章，今天首先爬取最简单的，单一文章的所有回答，爬取这个没有什么难度。</p><p>找到我们要爬取的页面，我随便选了一个</p><p><code>https://www.zhihu.com/question/292393947</code></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-aab266042f6d35faec76de7f3c8087dd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"706\" data-rawheight=\"222\" class=\"origin_image zh-lightbox-thumb\" width=\"706\" data-original=\"https://pic2.zhimg.com/v2-aab266042f6d35faec76de7f3c8087dd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;706&#39; height=&#39;222&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"706\" data-rawheight=\"222\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"706\" data-original=\"https://pic2.zhimg.com/v2-aab266042f6d35faec76de7f3c8087dd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-aab266042f6d35faec76de7f3c8087dd_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><code>1084</code>个回答，数据量可以说非常小了，就爬取它吧。</p><h2><b>2. 知乎文章图片选取操作库和爬取地址</b></h2><p>爬取使用<code>requests</code> 存储使用 mongodb 就可以了</p><p>爬取地址经过分析之后，找到了一个可以返回json的数据接口</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6a5234f2cfa8ca393ea40a2ac8461229_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1322\" data-rawheight=\"603\" class=\"origin_image zh-lightbox-thumb\" width=\"1322\" data-original=\"https://pic2.zhimg.com/v2-6a5234f2cfa8ca393ea40a2ac8461229_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1322&#39; height=&#39;603&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1322\" data-rawheight=\"603\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1322\" data-original=\"https://pic2.zhimg.com/v2-6a5234f2cfa8ca393ea40a2ac8461229_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6a5234f2cfa8ca393ea40a2ac8461229_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>提取链接，看一下各参数的意思，方便我们程序模拟</p><div class=\"highlight\"><pre><code class=\"language-text\">https://www.zhihu.com/api/v4/questions/292393947/answers?include=data%5B%2A%5D.is_normal%2Cadmin_closed_comment%2Creward_info%2Cis_collapsed%2Cannotation_action%2Cannotation_detail%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelevant_info%2Cquestion%2Cexcerpt%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%3Bdata%5B%2A%5D.mark_infos%5B%2A%5D.url%3Bdata%5B%2A%5D.author.follower_count%2Cbadge%5B%2A%5D.topics&amp;limit=5&amp;offset=10&amp;sort_by=default</code></pre></div><p>上面的连接进行了URL编码，去找个解码工具解析一下，编程下面的URL就比较好解释了，<code>answers</code>后面跟了一堆的参数，应该是返回的关键字，找到<code>limit</code>每页显示的数据量，<code>offset</code>偏移量，我们下拉滚动条，发现这个在不断的叠加+5，<code>sort_by</code> 就是排序。</p><div class=\"highlight\"><pre><code class=\"language-text\">https://www.zhihu.com/api/v4/questions/292393947/answers?include=data[*].is_normal,admin_closed_comment,reward_info,is_collapsed,annotation_action,annotation_detail,collapse_reason,is_sticky,collapsed_by,suggest_edit,comment_count,can_comment,content,editable_content,voteup_count,reshipment_settings,comment_permission,created_time,updated_time,review_info,relevant_info,question,excerpt,relationship.is_authorized,is_author,voting,is_thanked,is_nothelp;data[*].mark_infos[*].url;data[*].author.follower_count,badge[*].topics&amp;limit=5&amp;offset=10&amp;sort_by=default</code></pre></div><p>做好上面的工作，接下来就是爬取了，我简化了一下爬取的地址，只保留了一些关键的信息</p><div class=\"highlight\"><pre><code class=\"language-text\">https://www.zhihu.com/api/v4/questions/292393947/answers?include=comment_count,content,voteup_count,reshipment_settings,is_author,voting,is_thanked,is_nothelp;data[*].mark_infos[*].url;data[*].author.follower_count,badge[*].topics&amp;limit=5&amp;offset=0&amp;sort_by=default</code></pre></div><h2><b>3. 知乎文章图片编写代码</b></h2><p>分析完毕之后，发现代码非常简单了</p><div class=\"highlight\"><pre><code class=\"language-text\">import requests\nfrom fake_useragent import UserAgent\n\n############## 数据存储\nimport pymongo\nimport time\nDATABASE_IP = &#39;127.0.0.1&#39;\nDATABASE_PORT = 27017\nDATABASE_NAME = &#39;sun&#39;\nclient = pymongo.MongoClient(DATABASE_IP,DATABASE_PORT)\ndb = client.sun\ndb.authenticate(&#34;dba&#34;, &#34;dba&#34;)\ncollection = db.zhihuone  # 准备插入数据\n\n##################################\n\nclass ZhihuOne(object):\n\n    def __init__(self,totle):\n\n        self._offset = 0\n        self._totle = totle\n        #self._ua = UserAgent()\n\n\n    def run(self):\n\n        print(&#34;正在抓取 {} 数据&#34;.format(self._offset))\n        headers = {\n            &#34;upgrade-insecure-requests&#34;:&#34;1&#34;,\n            &#34;user-agent&#34;: &#34;Mozilla/5.0 (Windows NT 10.0; WOW64)&#34;\n        }\n        with requests.Session() as s:\n            try:\n                with s.get(&#34;https://www.zhihu.com/api/v4/questions/292393947/answers?include=comment_count,content,voteup_count,reshipment_settings,is_author,voting,is_thanked,is_nothelp;data[*].mark_infos[*].url;data[*].author.follower_count,badge[*].topics&amp;limit=5&amp;offset={}&amp;sort_by=default&#34;.format(self._offset),headers=headers,timeout=3) as rep:\n                    data =  rep.json()\n                    if data:\n                        collection.insert_many(data[&#34;data&#34;])\n            except Exception as e:\n                print(e.args)\n\n            finally:\n             \n                if self._offset &lt;= self._totle:\n                    self._offset  = self._offset + 5 # 每次+5\n                    print(&#34;防止被办，休息3s&#34;)\n                    time.sleep(3)\n                    self.run()\n                else:\n                    print(&#34;所有数据获取完毕&#34;)\n\n\n\n\nif __name__ == &#39;__main__&#39;:\n    # 偏移量是0,5,10   i=1  (i-1)*5\n    zhi = ZhihuOne(1084)  \n    zhi.run()</code></pre></div><p>上面主程序入口中，我写了个<code>1084</code> ，这个偷懒，就硬编码了，数据当然也可以通过爬取获取，没有任何问题，小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~</p><h2><b>4. 知乎文章图片写在后面</b></h2><p>本篇文章是知乎文章爬取器之一，接下来完善的功能</p><ol><li>爬取地址用户可以输入</li><li>自动答案总数</li><li>文章中图片自动下载</li><li>等功能</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a3eaf388b855d24a6a9c9662b8cc9587_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"150\" data-rawheight=\"150\" class=\"content_image\" width=\"150\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;150&#39; height=&#39;150&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"150\" data-rawheight=\"150\" class=\"content_image lazy\" width=\"150\" data-actualsrc=\"https://pic4.zhimg.com/v2-a3eaf388b855d24a6a9c9662b8cc9587_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "python爬虫", 
                    "tagLink": "https://api.zhihu.com/topics/20086364"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/65236970", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "Python爬虫入门教程第二十讲： 微医挂号网医生数据抓取", 
            "content": "<h2><b>1. 写在前面</b></h2><p>今天要抓取的一个网站叫做<code>微医</code>网站，地址为 <code>https://www.guahao.com</code> ，我们将通过python3爬虫抓取这个网址，然后数据存储到CSV里面，为后面的一些分析类的教程做准备。本篇文章主要使用的库为<code>pyppeteer</code> 和 <code>pyquery</code></p><p>首先找到 医生列表页</p><div class=\"highlight\"><pre><code class=\"language-text\">https://www.guahao.com/expert/all/全国/all/不限/p5</code></pre></div><p>这个页面显示有 <b>75952</b> 条数据 ，实际测试中，翻页到第38页，数据就加载不出来了，目测后台程序猿没有把数据返回，不过为了学习，我们忍了。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-068ae4b7e1b4272828c811a74c506621_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"964\" data-rawheight=\"277\" class=\"origin_image zh-lightbox-thumb\" width=\"964\" data-original=\"https://pic2.zhimg.com/v2-068ae4b7e1b4272828c811a74c506621_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;964&#39; height=&#39;277&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"964\" data-rawheight=\"277\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"964\" data-original=\"https://pic2.zhimg.com/v2-068ae4b7e1b4272828c811a74c506621_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-068ae4b7e1b4272828c811a74c506621_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>2. 页面URL</b></h2><div class=\"highlight\"><pre><code class=\"language-text\">https://www.guahao.com/expert/all/全国/all/不限/p1\nhttps://www.guahao.com/expert/all/全国/all/不限/p2\n...\nhttps://www.guahao.com/expert/all/全国/all/不限/p38</code></pre></div><p>数据总过38页，量不是很大，咱只需要随便选择一个库抓取就行，这篇博客，我找了一个冷门的库<br/><code>pyppeteer</code> 在使用过程中，发现资料好少，很尴尬。而且官方的文档写的也不好，有兴趣的可以自行去看看。关于这个库的安装也在下面的网址中。</p><p><a href=\"https://link.zhihu.com/?target=https%3A//miyakogi.github.io/pyppeteer/index.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">miyakogi.github.io/pypp</span><span class=\"invisible\">eteer/index.html</span><span class=\"ellipsis\"></span></a></p><p>最简单的使用方法，在官方文档中也简单的写了一下，如下，可以把一个网页直接保存为一张图片。</p><div class=\"highlight\"><pre><code class=\"language-text\">import asyncio\nfrom pyppeteer import launch\n\nasync def main():\n    browser = await launch()  # 运行一个无头的浏览器\n    page = await browser.newPage()  # 打开一个选项卡\n    await page.goto(&#39;http://www.baidu.com&#39;)  # 加载一个页面\n    await page.screenshot({&#39;path&#39;: &#39;baidu.png&#39;})  # 把网页生成截图\n    await browser.close()\n\nasyncio.get_event_loop().run_until_complete(main())  # 异步</code></pre></div><p>我整理了下面的一些参考代码，你可以 做一些参考。</p><div class=\"highlight\"><pre><code class=\"language-text\">browser = await launch(headless=False)  # 可以打开浏览器\nawait page.click(&#39;#login_user&#39;)  # 点击一个按钮\nawait page.type(&#39;#login_user&#39;, &#39;admin&#39;)  # 输入内容\n\nawait page.click(&#39;#password&#39;)  \nawait page.type(&#39;#password&#39;, &#39;123456&#39;)\n\nawait page.click(&#39;#login-submit&#39;)\n\nawait page.waitForNavigation()  \n\n# 设置浏览器窗口大小\nawait page.setViewport({\n    &#39;width&#39;: 1350,\n    &#39;height&#39;: 850\n})\n\ncontent = await page.content()  # 获取网页内容\ncookies = await page.cookies()  # 获取网页cookies</code></pre></div><h2><b>3. 爬取页面</b></h2><p>运行下面的代码，你就可以看到控制台不断的打印网页的源码，只要获取到源码，就可以进行后面的解析与保存数据了。如果出现控制不输出任何东西的情况，那么请把下面的<br/><code>await launch(headless=True) 修改为 await launch(headless=False)</code></p><div class=\"highlight\"><pre><code class=\"language-text\">import asyncio\nfrom pyppeteer import launch\n\n\nclass DoctorSpider(object):\n    async def main(self, num):\n        try:\n            browser = await launch(headless=True)\n            page = await browser.newPage()\n\n            print(f&#34;正在爬取第 {num} 页面&#34;)\n            await page.goto(&#34;https://www.guahao.com/expert/all/全国/all/不限/p{}&#34;.format(num))\n\n            content = await page.content()\n            print(content)\n\n        except Exception as e:\n            print(e.args)\n\n        finally:\n            num += 1\n            await browser.close()\n            await self.main(num)\n\n\n    def run(self):\n        loop = asyncio.get_event_loop()\n        asyncio.get_event_loop().run_until_complete(self.main(1))\n\n\nif __name__ == &#39;__main__&#39;:\n    doctor = DoctorSpider()\n    doctor.run()</code></pre></div><h2><b>4. 解析数据</b></h2><p>解析数据采用的是pyquery ，这个库在之前的博客中有过使用，直接应用到案例中即可。最终产生的数据通过<code>pandas</code>保存到CSV文件中。</p><div class=\"highlight\"><pre><code class=\"language-text\">import asyncio\n\nfrom pyppeteer import launch\nfrom pyquery import PyQuery as pq\nimport pandas as pd  # 保存csv文件\n\nclass DoctorSpider(object):\n\n    def __init__(self):\n        self._data = list()\n\n    async def main(self,num):\n\n        try:\n\n            browser = await launch(headless=True)\n            page = await browser.newPage()\n\n            print(f&#34;正在爬取第 {num} 页面&#34;)\n            await page.goto(&#34;https://www.guahao.com/expert/all/全国/all/不限/p{}&#34;.format(num))\n            content = await page.content()\n\n            self.parse_html(content)\n            print(&#34;正在存储数据....&#34;)\n\n            data = pd.DataFrame(self._data)\n            data.to_csv(&#34;微医数据.csv&#34;, encoding=&#39;utf_8_sig&#39;)\n        except Exception as e:\n            print(e.args)\n        finally:\n            num+=1\n\n            await browser.close()\n\n            await self.main(num)\n    def parse_html(self,content):\n\n        doc = pq(content)\n\n        items = doc(&#34;.g-doctor-item&#34;).items()\n        for item in items:\n            #doctor_name = item.find(&#34;.seo-anchor-text&#34;).text()\n            name_level = item.find(&#34;.g-doc-baseinfo&gt;dl&gt;dt&#34;).text() # 姓名和级别\n            department = item.find(&#34;.g-doc-baseinfo&gt;dl&gt;dd&gt;p:eq(0)&#34;).text() # 科室\n            address = item.find(&#34;.g-doc-baseinfo&gt;dl&gt;dd&gt;p:eq(1)&#34;).text()  # 医院地址\n            star = item.find(&#34;.star-count em&#34;).text()  # 评分\n            inquisition = item.find(&#34;.star-count i&#34;).text() # 问诊量\n            expert_team = item.find(&#34;.expert-team&#34;).text()  # 专家团队\n            service_price_img = item.find(&#34;.service-name:eq(0)&gt;.fee&#34;).text()\n            service_price_video = item.find(&#34;.service-name:eq(1)&gt;.fee&#34;).text()\n\n            one_data = {\n                &#34;name&#34;: name_level.split(&#34; &#34;)[0],\n                &#34;level&#34;: name_level.split(&#34; &#34;)[1],\n                &#34;department&#34;: department,\n                &#34;address&#34;: address,\n                &#34;star&#34;: star,\n                &#34;inquisition&#34;: inquisition,\n                &#34;expert_team&#34;: expert_team,\n                &#34;service_price_img&#34;: service_price_img,\n                &#34;service_price_video&#34;: service_price_video\n            }\n\n            self._data.append(one_data)\n\n\n\n    def run(self):\n        loop = asyncio.get_event_loop()\n\n        asyncio.get_event_loop().run_until_complete(self.main(1))\n\n\n\nif __name__ == &#39;__main__&#39;:\n\n    doctor = DoctorSpider()\n    doctor.run()</code></pre></div><p>总结一下，这个库不怎么好用，可能之前没有细细的研究过，感觉一般，你可以在多尝试一下，看一下是否可以把整体的效率提高上去。小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~</p><p>数据清单：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e5aeb6b180aae5106d51af7425fc9344_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"411\" class=\"origin_image zh-lightbox-thumb\" width=\"720\" data-original=\"https://pic1.zhimg.com/v2-e5aeb6b180aae5106d51af7425fc9344_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;720&#39; height=&#39;411&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"411\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"720\" data-original=\"https://pic1.zhimg.com/v2-e5aeb6b180aae5106d51af7425fc9344_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e5aeb6b180aae5106d51af7425fc9344_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "python爬虫", 
                    "tagLink": "https://api.zhihu.com/topics/20086364"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/65235896", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "Python爬虫入门教程第十九讲：链家租房数据抓取", 
            "content": "<h2><b>1. 写在前面</b></h2><p>作为一个活跃在京津冀地区的开发者，要闲着没事就看看<code>石家庄</code>这个国际化大都市的一些数据，这篇博客爬取了链家网的租房信息，爬取到的数据在后面的博客中可以作为一些数据分析的素材。<br/>我们需要爬取的网址为：<code>https://sjz.lianjia.com/zufang/</code></p><h2><b>2. 分析网址</b></h2><p>首先确定一下，哪些数据是我们需要的<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1f60820a90591472427c2832089a690e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1049\" data-rawheight=\"594\" class=\"origin_image zh-lightbox-thumb\" width=\"1049\" data-original=\"https://pic3.zhimg.com/v2-1f60820a90591472427c2832089a690e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1049&#39; height=&#39;594&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1049\" data-rawheight=\"594\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1049\" data-original=\"https://pic3.zhimg.com/v2-1f60820a90591472427c2832089a690e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1f60820a90591472427c2832089a690e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>可以看到，黄色框就是我们需要的数据。</p><p>接下来，确定一下翻页规律</p><div class=\"highlight\"><pre><code class=\"language-text\">https://sjz.lianjia.com/zufang/pg1/\nhttps://sjz.lianjia.com/zufang/pg2/\nhttps://sjz.lianjia.com/zufang/pg3/\nhttps://sjz.lianjia.com/zufang/pg4/\nhttps://sjz.lianjia.com/zufang/pg5/\n... \nhttps://sjz.lianjia.com/zufang/pg80/</code></pre></div><h2><b>3. 解析网页</b></h2><p>有了分页地址，就可以快速把链接拼接完毕，我们采用<code>lxml</code>模块解析网页源码，获取想要的数据。</p><p>本次编码使用了一个新的模块 <code>fake_useragent</code> ，这个模块，可以随机的去获取一个UA（user-agent），模块使用比较简单，可以去百度百度就很多教程。</p><p>本篇博客主要使用的是调用一个随机的UA</p><div class=\"highlight\"><pre><code class=\"language-text\">self._ua = UserAgent()\nself._headers = {&#34;User-Agent&#34;: self._ua.random}  # 调用一个随机的UA</code></pre></div><p>由于可以快速的把页码拼接出来，所以采用协程进行抓取，写入csv文件采用的<code>pandas</code>模块</p><div class=\"highlight\"><pre><code class=\"language-text\">from fake_useragent import UserAgent\nfrom lxml import etree\nimport asyncio\nimport aiohttp\nimport pandas as pd\n\nclass LianjiaSpider(object):\n\n    def __init__(self):\n        self._ua = UserAgent()\n        self._headers = {&#34;User-Agent&#34;: self._ua.random}\n        self._data = list()\n\n\n    async def get(self,url):\n        async with aiohttp.ClientSession() as session:\n            try:\n                async with session.get(url,headers=self._headers,timeout=3) as resp:\n                    if resp.status==200:\n                        result = await resp.text()\n                        return result\n            except Exception as e:\n                print(e.args)\n\n    async def parse_html(self):\n        for page in range(1,77):\n            url = &#34;https://sjz.lianjia.com/zufang/pg{}/&#34;.format(page)\n            print(&#34;正在爬取{}&#34;.format(url))\n            html = await self.get(url)   # 获取网页内容\n            html = etree.HTML(html)  # 解析网页\n            self.parse_page(html)   # 匹配我们想要的数据\n\n            print(&#34;正在存储数据....&#34;)\n            ######################### 数据写入\n            data = pd.DataFrame(self._data)\n            data.to_csv(&#34;链家网租房数据.csv&#34;, encoding=&#39;utf_8_sig&#39;)   # 写入文件\n            ######################### 数据写入\n\n\n\n    def run(self):\n        loop = asyncio.get_event_loop()\n        tasks = [asyncio.ensure_future(self.parse_html())]\n        loop.run_until_complete(asyncio.wait(tasks))\n\n\nif __name__ == &#39;__main__&#39;:\n    l = LianjiaSpider()\n    l.run()</code></pre></div><p>上述代码中缺少一个解析网页的函数，我们接下来把他补全</p><div class=\"highlight\"><pre><code class=\"language-text\">def parse_page(self,html):\n        info_panel = html.xpath(&#34;//div[@class=&#39;info-panel&#39;]&#34;)\n        for info in info_panel:\n            region = self.remove_space(info.xpath(&#34;.//span[@class=&#39;region&#39;]/text()&#34;))\n            zone = self.remove_space(info.xpath(&#34;.//span[@class=&#39;zone&#39;]/span/text()&#34;))\n            meters = self.remove_space(info.xpath(&#34;.//span[@class=&#39;meters&#39;]/text()&#34;))\n            where = self.remove_space(info.xpath(&#34;.//div[@class=&#39;where&#39;]/span[4]/text()&#34;))\n\n            con = info.xpath(&#34;.//div[@class=&#39;con&#39;]/text()&#34;)\n            floor = con[0]  # 楼层\n            type = con[1]   # 样式\n\n            agent = info.xpath(&#34;.//div[@class=&#39;con&#39;]/a/text()&#34;)[0]\n\n            has = info.xpath(&#34;.//div[@class=&#39;left agency&#39;]//text()&#34;)\n\n            price = info.xpath(&#34;.//div[@class=&#39;price&#39;]/span/text()&#34;)[0]\n            price_pre =  info.xpath(&#34;.//div[@class=&#39;price-pre&#39;]/text()&#34;)[0]\n            look_num = info.xpath(&#34;.//div[@class=&#39;square&#39;]//span[@class=&#39;num&#39;]/text()&#34;)[0]\n\n            one_data = {\n                &#34;region&#34;:region,\n                &#34;zone&#34;:zone,\n                &#34;meters&#34;:meters,\n                &#34;where&#34;:where,\n                &#34;louceng&#34;:floor,\n                &#34;type&#34;:type,\n                &#34;xiaoshou&#34;:agent,\n                &#34;has&#34;:has,\n                &#34;price&#34;:price,\n                &#34;price_pre&#34;:price_pre,\n                &#34;num&#34;:look_num\n            }\n            self._data.append(one_data)  # 添加数据，小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~</code></pre></div><p>不一会，数据就爬取的差不多了。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5951dd070e8147ebd36d765a61d67406_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1287\" data-rawheight=\"510\" class=\"origin_image zh-lightbox-thumb\" width=\"1287\" data-original=\"https://pic3.zhimg.com/v2-5951dd070e8147ebd36d765a61d67406_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1287&#39; height=&#39;510&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1287\" data-rawheight=\"510\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1287\" data-original=\"https://pic3.zhimg.com/v2-5951dd070e8147ebd36d765a61d67406_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5951dd070e8147ebd36d765a61d67406_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "python爬虫", 
                    "tagLink": "https://api.zhihu.com/topics/20086364"
                }
            ], 
            "comments": [
                {
                    "userName": "Excel知识管理", 
                    "userLink": "https://www.zhihu.com/people/20950330ce42c6123e7c1abd9f77514d", 
                    "content": "<p>老师好，能否分享一个完整代码？def parse_page(self,html):这个解析函数放在哪里呢？谢谢</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/65235200", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "Python爬虫入门教程第十八讲： 煎蛋网XXOO图片抓取", 
            "content": "<p>今天写一个爬虫爱好者特别喜欢的网站煎蛋网<code>http://jandan.net/ooxx</code>，这个网站其实还是有点意思的，网站很多人写了N多的教程了，各种方式的都有，当然网站本身在爬虫爱好者的不断进攻下，也在不断的完善，反爬措施也很多，今天我用<code>selenium</code>在揍他一波。</p><p>整体看上去，煎蛋网的妹子图质量还是可以的，不是很多，但是还蛮有味道的，这可能也是爬虫er，一批一批的奔赴上去的原因。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2bf46cb1982b2473725cac4efb3052af_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"610\" data-rawheight=\"586\" class=\"origin_image zh-lightbox-thumb\" width=\"610\" data-original=\"https://pic4.zhimg.com/v2-2bf46cb1982b2473725cac4efb3052af_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;610&#39; height=&#39;586&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"610\" data-rawheight=\"586\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"610\" data-original=\"https://pic4.zhimg.com/v2-2bf46cb1982b2473725cac4efb3052af_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-2bf46cb1982b2473725cac4efb3052af_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>1. 网站分析</b></h2><p>这个网站如果用 <code>selenium</code> 爬取，其实也没什么要分析的,模拟访问就行，导入必备的模块。</p><div class=\"highlight\"><pre><code class=\"language-text\">from selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom lxml import etree\nimport requests\n\nimport time</code></pre></div><p>我使用的是<code>PhantomJS</code> 去加载浏览器，关于这个<code>PhantomJS</code>，去互联网搜索一下吧，资料大把，会看的很爽的，总之呢，它可以模拟一个真实的浏览器做任何事情，得到你想要的数据。</p><div class=\"highlight\"><pre><code class=\"language-text\">browser = webdriver.PhantomJS()\nbrowser.set_window_size(1366, 768)  # 这个地方需要设置一下浏览器的尺寸\nwait = WebDriverWait(browser,10)\nbrowser.get(&#34;http://jandan.net/ooxx&#34;)</code></pre></div><h2><b>2. 分析数据</b></h2><p>程序获取到数据之后就可以对数据进行处理了，编写一个<b>get_content</b>函数，用来处理网页源码。</p><div class=\"highlight\"><pre><code class=\"language-text\">def get_content():\n\n    try:\n\n        wait.until(\n            EC.presence_of_element_located((By.XPATH,&#39;//*[@id=&#34;comments&#34;]/ol&#39;))\n        )\n        #\n        print(&#34;正在爬取{}&#34;.format(browser.current_url))\n        page_source = browser.page_source  # 获取网页源码\n        html = etree.HTML(page_source)  # 解析源码\n        imgs = html.xpath(&#34;//li[contains(@id,&#39;comment&#39;)]//img/@src&#34;)  # 匹配图片\n        download(imgs)\n\n    except Exception as e:\n        print(&#34;错误&#34;)\n        print(e)\n    finally:\n        browser.close()</code></pre></div><p>图片获取到之后，在上面的代码中，注意有一个地方调用了一个 <code>download</code>函数，这个函数就是用来下载图片的</p><div class=\"highlight\"><pre><code class=\"language-text\">def download(imgs):\n    path = &#34;./xxoo/{}&#34;  # 路径我写死了\n    for img in imgs:\n        try:\n            res = requests.get(img)\n            content = res.content\n        except Exception as e:\n            print(e)\n            continue\n\n        file_name = img.split(&#34;/&#34;)[-1] # 获取文件名\n\n        with open(path.format(file_name),&#34;wb&#34;) as f:\n            f.write(content)\n            print(file_name,&#34;成功下载文件&#34;)\n            time.sleep(0.3)\n\n    # 循环下载完毕，进行翻页操作 previous-comment-page\n    next = wait.until(\n        EC.presence_of_element_located((By.XPATH, &#39;//*[@id=&#34;comments&#34;]//a[@class=&#34;previous-comment-page&#34;]&#39;))\n    )\n    next.click()\n    return get_content()  # 继续调用上面的网页源码分析流程</code></pre></div><p>OK，运行一下。小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-dcd1e46f7bd99002faa9664d9e8228e8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1658\" data-rawheight=\"552\" class=\"origin_image zh-lightbox-thumb\" width=\"1658\" data-original=\"https://pic1.zhimg.com/v2-dcd1e46f7bd99002faa9664d9e8228e8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1658&#39; height=&#39;552&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1658\" data-rawheight=\"552\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1658\" data-original=\"https://pic1.zhimg.com/v2-dcd1e46f7bd99002faa9664d9e8228e8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-dcd1e46f7bd99002faa9664d9e8228e8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-451d850cdfa2c59f20df95a956ddc813_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"98\" data-rawheight=\"98\" data-thumbnail=\"https://pic4.zhimg.com/v2-451d850cdfa2c59f20df95a956ddc813_b.jpg\" class=\"content_image\" width=\"98\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;98&#39; height=&#39;98&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"98\" data-rawheight=\"98\" data-thumbnail=\"https://pic4.zhimg.com/v2-451d850cdfa2c59f20df95a956ddc813_b.jpg\" class=\"content_image lazy\" width=\"98\" data-actualsrc=\"https://pic4.zhimg.com/v2-451d850cdfa2c59f20df95a956ddc813_b.gif\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/65118643", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "Python爬虫入门教程第十七讲： CSD*博客抓取数据", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-54ce744f9c6d95501a326748a967307d_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"240\" data-rawheight=\"240\" data-thumbnail=\"https://pic2.zhimg.com/v2-54ce744f9c6d95501a326748a967307d_b.jpg\" class=\"content_image\" width=\"240\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;240&#39; height=&#39;240&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"240\" data-rawheight=\"240\" data-thumbnail=\"https://pic2.zhimg.com/v2-54ce744f9c6d95501a326748a967307d_b.jpg\" class=\"content_image lazy\" width=\"240\" data-actualsrc=\"https://pic2.zhimg.com/v2-54ce744f9c6d95501a326748a967307d_b.gif\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>其实这事情挺简单的，打开CSDN博客首页，他不是有个最新文章么，这个里面都是最新发布的文章。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-11ebe119100032afb6a307567b1f8a26_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"887\" data-rawheight=\"305\" class=\"origin_image zh-lightbox-thumb\" width=\"887\" data-original=\"https://pic3.zhimg.com/v2-11ebe119100032afb6a307567b1f8a26_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;887&#39; height=&#39;305&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"887\" data-rawheight=\"305\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"887\" data-original=\"https://pic3.zhimg.com/v2-11ebe119100032afb6a307567b1f8a26_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-11ebe119100032afb6a307567b1f8a26_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>打开F12抓取一下数据API，很容易就获取到了他的接口</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-cc00be19e5103af4dd6d425eebdc2753_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"672\" data-rawheight=\"318\" class=\"origin_image zh-lightbox-thumb\" width=\"672\" data-original=\"https://pic4.zhimg.com/v2-cc00be19e5103af4dd6d425eebdc2753_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;672&#39; height=&#39;318&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"672\" data-rawheight=\"318\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"672\" data-original=\"https://pic4.zhimg.com/v2-cc00be19e5103af4dd6d425eebdc2753_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-cc00be19e5103af4dd6d425eebdc2753_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>提取链接长成这个样子</p><p><code>https://blog.csdn.net/api/articles?type=more&amp;category=newarticles&amp;shown_offset=1540381234000000</code></p><p>发现博客最新文章是一个瀑布流页面，不断下拉，只有一个参数<code>shown_offset</code> 在变化，按照我多年的行医经验，这个参数是个时间戳，而且肯定是上一次数据最后一条的时间戳。</p><p>基于这个理论，看一下数据，咦，猜对了~~~~~</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5385de96435e6d9cc879028bc2e3c80f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"1080\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-5385de96435e6d9cc879028bc2e3c80f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;1080&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"1080\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-5385de96435e6d9cc879028bc2e3c80f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5385de96435e6d9cc879028bc2e3c80f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>博客返回的数据看一下，是否对味</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-387a7ffd02b6ecc50f9e142b2e505341_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"881\" data-rawheight=\"505\" class=\"origin_image zh-lightbox-thumb\" width=\"881\" data-original=\"https://pic2.zhimg.com/v2-387a7ffd02b6ecc50f9e142b2e505341_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;881&#39; height=&#39;505&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"881\" data-rawheight=\"505\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"881\" data-original=\"https://pic2.zhimg.com/v2-387a7ffd02b6ecc50f9e142b2e505341_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-387a7ffd02b6ecc50f9e142b2e505341_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>撸代码</b></h2><p>这个步骤就非常简单了，就是通过<code>requests</code>去抓取这个链接就好了</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">import</span> <span class=\"nn\">requests</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pymongo</span>\n<span class=\"kn\">import</span> <span class=\"nn\">time</span>\n\n<span class=\"n\">START_URL</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;https://www.csdn.net/api/articles?type=more&amp;category=newarticles&amp;shown_offset=</span><span class=\"si\">{}</span><span class=\"s2\">&#34;</span>\n<span class=\"n\">HEADERS</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&#34;Accept&#34;</span><span class=\"p\">:</span><span class=\"s2\">&#34;application/json&#34;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&#34;Host&#34;</span><span class=\"p\">:</span><span class=\"s2\">&#34;www.csdn.net&#34;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&#34;Referer&#34;</span><span class=\"p\">:</span><span class=\"s2\">&#34;https://www.csdn.net/nav/newarticles&#34;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&#34;User-Agent&#34;</span><span class=\"p\">:</span><span class=\"s2\">&#34;你自己的浏览器配置&#34;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&#34;X-Requested-With&#34;</span><span class=\"p\">:</span><span class=\"s2\">&#34;XMLHttpRequest&#34;</span>\n<span class=\"p\">}</span>\n<span class=\"k\">def</span> <span class=\"nf\">get_url</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">):</span>\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"n\">res</span> <span class=\"o\">=</span> <span class=\"n\">requests</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">,</span>\n                           <span class=\"n\">headers</span><span class=\"o\">=</span><span class=\"n\">HEADERS</span><span class=\"p\">,</span>\n                           <span class=\"n\">timeout</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">)</span>\n\n        <span class=\"n\">articles</span> <span class=\"o\">=</span> <span class=\"n\">res</span><span class=\"o\">.</span><span class=\"n\">json</span><span class=\"p\">()</span>\n        <span class=\"k\">if</span> <span class=\"n\">articles</span><span class=\"p\">[</span><span class=\"s2\">&#34;status&#34;</span><span class=\"p\">]:</span>\n            <span class=\"n\">need_data</span> <span class=\"o\">=</span> <span class=\"n\">articles</span><span class=\"p\">[</span><span class=\"s2\">&#34;articles&#34;</span><span class=\"p\">]</span>\n            <span class=\"k\">if</span> <span class=\"n\">need_data</span><span class=\"p\">:</span>\n                <span class=\"n\">collection</span><span class=\"o\">.</span><span class=\"n\">insert_many</span><span class=\"p\">(</span><span class=\"n\">need_data</span><span class=\"p\">)</span>  <span class=\"c1\"># 数据插入</span>\n                <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;成功插入</span><span class=\"si\">{}</span><span class=\"s2\">条数据&#34;</span><span class=\"o\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">need_data</span><span class=\"p\">)))</span>\n            <span class=\"n\">last_shown_offset</span> <span class=\"o\">=</span> <span class=\"n\">articles</span><span class=\"p\">[</span><span class=\"s2\">&#34;shown_offset&#34;</span><span class=\"p\">]</span>  <span class=\"c1\"># 获取最后一条数据的时间戳</span>\n            <span class=\"k\">if</span> <span class=\"n\">last_shown_offset</span><span class=\"p\">:</span>\n                <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">sleep</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n                <span class=\"n\">get_url</span><span class=\"p\">(</span><span class=\"n\">START_URL</span><span class=\"o\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">last_shown_offset</span><span class=\"p\">))</span>\n    <span class=\"k\">except</span> <span class=\"ne\">Exception</span> <span class=\"k\">as</span> <span class=\"n\">e</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">)</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;系统暂停60s，当前出问题的是</span><span class=\"si\">{}</span><span class=\"s2\">&#34;</span><span class=\"o\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">))</span>\n\n        <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">sleep</span><span class=\"p\">(</span><span class=\"mi\">60</span><span class=\"p\">)</span> <span class=\"c1\"># 出问题之后，停止60s，继续抓取</span>\n        <span class=\"n\">get_url</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span></code></pre></div><p>数据获取到了，当然要象征性的保存一下，mongo数据库的操作在上一篇文章，你可以去翻翻。小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-16ce2eceaeb5a4bec1df5e75753b2987_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1452\" data-rawheight=\"426\" class=\"origin_image zh-lightbox-thumb\" width=\"1452\" data-original=\"https://pic4.zhimg.com/v2-16ce2eceaeb5a4bec1df5e75753b2987_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1452&#39; height=&#39;426&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1452\" data-rawheight=\"426\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1452\" data-original=\"https://pic4.zhimg.com/v2-16ce2eceaeb5a4bec1df5e75753b2987_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-16ce2eceaeb5a4bec1df5e75753b2987_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e461460758eb0dfec60f8eb5ae626f7a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"440\" data-rawheight=\"440\" class=\"origin_image zh-lightbox-thumb\" width=\"440\" data-original=\"https://pic3.zhimg.com/v2-e461460758eb0dfec60f8eb5ae626f7a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;440&#39; height=&#39;440&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"440\" data-rawheight=\"440\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"440\" data-original=\"https://pic3.zhimg.com/v2-e461460758eb0dfec60f8eb5ae626f7a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e461460758eb0dfec60f8eb5ae626f7a_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/65109547", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "Python爬虫入门教程第十六讲： 500px摄影师社区抓取摄影师数据", 
            "content": "<p>今天要抓取的网站为 <code>https://500px.me/</code> ，这是一个摄影社区，在一个摄影社区里面本来应该爬取的是图片信息，可是我发现好像也没啥有意思的，忽然觉得爬取一下这个网站的摄影师更好玩一些，所以就有了这篇文章的由来。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7165d30d28f918153400ab0a7186117e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"300\" data-rawheight=\"300\" class=\"content_image\" width=\"300\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;300&#39; height=&#39;300&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"300\" data-rawheight=\"300\" class=\"content_image lazy\" width=\"300\" data-actualsrc=\"https://pic3.zhimg.com/v2-7165d30d28f918153400ab0a7186117e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>基于上面的目的，我找了了一个不错的页面 <code>https://500px.me/community/search/user</code><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7843c7d952c022ba99ca31841bfbd06f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1805\" data-rawheight=\"429\" class=\"origin_image zh-lightbox-thumb\" width=\"1805\" data-original=\"https://pic4.zhimg.com/v2-7843c7d952c022ba99ca31841bfbd06f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1805&#39; height=&#39;429&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1805\" data-rawheight=\"429\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1805\" data-original=\"https://pic4.zhimg.com/v2-7843c7d952c022ba99ca31841bfbd06f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7843c7d952c022ba99ca31841bfbd06f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>不过细细分析之后，发现这个页面并不能抓取到尽可能多的用户，因为下拉一段时间，就不能继续了，十分糟心，难道我止步于此了么，显然不可能的，一番的努力之后(大概废了1分钟吧)，我找到了突破口，任意打开一个用户的个人中心页，就是点击上述链接的任意用户头像，出现如下操作。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1fd514fff2a15a3a1f07454003afd279_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"496\" data-rawheight=\"517\" data-thumbnail=\"https://pic2.zhimg.com/v2-1fd514fff2a15a3a1f07454003afd279_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"496\" data-original=\"https://pic2.zhimg.com/v2-1fd514fff2a15a3a1f07454003afd279_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;496&#39; height=&#39;517&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"496\" data-rawheight=\"517\" data-thumbnail=\"https://pic2.zhimg.com/v2-1fd514fff2a15a3a1f07454003afd279_b.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"496\" data-original=\"https://pic2.zhimg.com/v2-1fd514fff2a15a3a1f07454003afd279_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1fd514fff2a15a3a1f07454003afd279_b.gif\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>用户个人中心页面，竟然有关注列表唉~~，nice啊，这个好趴啊，F12分析一下。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4830b39016295c58ae553664acf192e6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"576\" data-rawheight=\"407\" class=\"origin_image zh-lightbox-thumb\" width=\"576\" data-original=\"https://pic3.zhimg.com/v2-4830b39016295c58ae553664acf192e6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;576&#39; height=&#39;407&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"576\" data-rawheight=\"407\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"576\" data-original=\"https://pic3.zhimg.com/v2-4830b39016295c58ae553664acf192e6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4830b39016295c58ae553664acf192e6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>哒哒哒，数据得到了。<br/>URL是 <code>https://500px.me/community/res/relation/4f7fe110d4e0b8a1fae0632b2358c8898/follow?startTime=&amp;page=1&amp;size=10&amp;type=json</code></p><p>参数分别如下，实际测试发现size可以设置为<code>100</code></p><div class=\"highlight\"><pre><code class=\"language-text\">https://500px.me/community/res/relation/{用户ID}/follow?startTime=&amp;page={页码}&amp;size={每页数据}&amp;type=json</code></pre></div><p>那么我们只需要这么做就可以了</p><ol><li>获取关注总数</li><li>关注总数除以100，循环得到所有的关注者(这个地方为什么用关注，不用粉丝，是因为被关注的人更加有价值)<br/>明确我们的目标之后，就可以开始写代码了。</li></ol><h2><b>撸代码</b></h2><p>基本操作，获取网络请求，之后解析页面，取得关注总数。</p><p>用户的起始，我选择的id是<code>5769e51a04209a9b9b6a8c1e656ff9566</code>，你可以随机选择一个，只要他有关注名单，就可以。<br/>导入模块，这篇博客，用到了<code>redis</code>和<code>mongo</code>，所以相关的基础知识，我建议你提前准备一下，否则看起来吃力。</p><div class=\"highlight\"><pre><code class=\"language-text\">import requests\nimport threading\n\nfrom redis import StrictRedis\nimport pymongo\n\n#########mongo部分#########################\nDATABASE_IP = &#39;127.0.0.1&#39;\nDATABASE_PORT = 27017\nDATABASE_NAME = &#39;sun&#39;\nclient = pymongo.MongoClient(DATABASE_IP,DATABASE_PORT)\ndb = client.sun\ndb.authenticate(&#34;dba&#34;, &#34;dba&#34;)\ncollection = db.px500  # 准备插入数据\n\n#########mongo部分#########################\n\n#########redis部分#########################\nredis = StrictRedis(host=&#34;localhost&#34;,port=6379,db=1,decode_responses=True)\n#########redis部分#########################\n\n\n#########全局参数部分#########################\nSTART_URL = &#34;https://500px.me/community/v2/user/indexInfo?queriedUserId={}&#34; # 入口链接\nCOMMENT = &#34;https://500px.me/community/res/relation/{}/follow?startTime=&amp;page={}&amp;size=100&amp;type=json&#34;\nHEADERS = {\n    &#34;Accept&#34;:&#34;application/json&#34;,\n    &#34;User-Agent&#34;:&#34;你自己去找找可用的就行&#34;,\n    &#34;X-Requested-With&#34;:&#34;XMLHttpRequest&#34;\n}\n\nneed_crawlids = []  # 待爬取的userid\n\nlock = threading.Lock() # 线程锁\n#########全局参数部分#########################\ndef get_followee():\n    try:\n        res = requests.get(START_URL.format(&#34;5769e51a04209a9b9b6a8c1e656ff9566&#34;),\n        headers=HEADERS,timeout=3)\n        data = res.json()\n        if data:\n            totle = int(data[&#34;data&#34;][&#34;userFolloweeCount&#34;])  # 返回关注数\n            userid = data[&#34;data&#34;][&#34;id&#34;] # 返回用户ID\n            return {\n                &#34;userid&#34;:userid,\n                &#34;totle&#34;:totle\n            }  # 返回总数据\n    except Exception as e:\n        print(&#34;数据获取错误&#34;)\n        print(e)\nif __name__ == &#39;__main__&#39;:\n    start = get_followee()  # 获取入口\n    need_crawlids.append(start)</code></pre></div><p>上面代码中有一个非常重要的逻辑，就是为什么要先匹配<code>种子地址</code>的【关注数】和【用户ID】，这两个值是为了拼接下面的URL<br/><code>https://500px.me/community/res/relation/{}/follow?startTime=&amp;page={}&amp;size=100&amp;type=json</code><br/>经过分析，你已经知道，这个地方第一个参数是用户id,第二个参数是页码page，page需要通过关注总数除以100得到。不会算的，好好在纸上写写吧~</p><p>我们可以通过一个方法，获取到了种子用户的关注列表，以此继续爬取下去，完善生产者代码。关键代码都进行了注释标注。</p><p>思路如下：</p><ol><li>死循环不断获取<code>need_crawlids</code> 变量中的用户，然后获取该用户的关注者列表。</li><li>爬取到的信息，写入<code>redis</code>方便验证重复，快速存储。</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">class Product(threading.Thread):\n    def __init__(self):\n        threading.Thread.__init__(self)\n        self._headers = HEADERS\n\n    def get_follows(self,userid,totle):\n        try:\n            res = requests.get(COMMENT.format(userid,totle),headers=HEADERS,timeout=3)\n            data = res.json()\n\n            if data:\n                for item in data:\n                    yield {\n                        &#34;userid&#34;:item[&#34;id&#34;],\n                        &#34;totle&#34;:item[&#34;userFolloweeCount&#34;]\n                    }\n        except Exception as e:\n            print(&#34;错误信息&#34;)\n            print(e)\n            self.get_follows(userid,totle)  # 出错之后，重新调用\n\n    def run(self):\n\n        while 1:\n            global need_crawlids  # 调用全局等待爬取的内容\n\n            if lock.acquire():\n                if len(need_crawlids)==0:  # 如果为0，无法进入循环\n                    continue\n\n                data = need_crawlids[0]  # 取得第一个\n                del need_crawlids[0]  # 使用完删除\n\n                lock.release()\n\n            if data[&#34;totle&#34;] == 0:\n                continue\n\n            for page in range(1,data[&#34;totle&#34;]//100+2):\n                for i in self.get_follows(data[&#34;userid&#34;],page):\n                    if lock.acquire():\n                        need_crawlids.append(i)  # 新获取到的，追加到等待爬取的列表里面\n                        lock.release()\n                    self.save_redis(i)  # 存储到redis里面\n\n\n    def save_redis(self,data):\n        redis.setnx(data[&#34;userid&#34;],data[&#34;totle&#34;])\n        #print(data,&#34;插入成功&#34;)</code></pre></div><p>由于500px无反爬虫，所以运行起来速度也是飞快了，一会就爬取了大量的数据，目测大概<code>40000</code>多人，由于咱是写教程的，我停止了爬取。<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6849446770c5ed2feec1514c662347f6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"277\" data-rawheight=\"282\" class=\"content_image\" width=\"277\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;277&#39; height=&#39;282&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"277\" data-rawheight=\"282\" class=\"content_image lazy\" width=\"277\" data-actualsrc=\"https://pic3.zhimg.com/v2-6849446770c5ed2feec1514c662347f6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7aaacb21de4a97e50956ec44fa5450f5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"372\" data-rawheight=\"544\" class=\"content_image\" width=\"372\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;372&#39; height=&#39;544&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"372\" data-rawheight=\"544\" class=\"content_image lazy\" width=\"372\" data-actualsrc=\"https://pic2.zhimg.com/v2-7aaacb21de4a97e50956ec44fa5450f5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这些数据不能就在redis里面趴着，我们要用它获取用户的所有信息，那么先找到用户信息接口，其实在上面已经使用了一次<br/><code>https://500px.me/community/v2/user/indexInfo?queriedUserId={}</code> 后面的<code>queriedUserId</code>对应的是用户id，只需要从刚才的数据里面获取redis的<code>key</code>就可以了，开始编写消费者代码吧，我开启了5个线程抓取。小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~</p><div class=\"highlight\"><pre><code class=\"language-text\">class Consumer(threading.Thread):\n    def __init__(self):\n        threading.Thread.__init__(self)\n\n    def run(self):\n        while 1:\n            key = redis.randomkey() # 随机获取一个key\n            if key:\n                # 删除获取到的key\n                redis.delete(key)\n                self.get_info(key)\n\n\n\n    def get_info(self,key):\n        try:\n            res = requests.get(START_URL.format(key),headers=HEADERS,timeout=3)\n            data = res.json()\n            if data[&#39;status&#39;] == &#34;200&#34;:\n                collection.insert(data[&#34;data&#34;])  # 插入到mongodb中\n        except Exception as e:\n            print(e)\n            return\nif __name__ == &#39;__main__&#39;:\n    start = get_followee()  # 获取入口\n    need_crawlids.append(start)\n\n\n    p = Product()\n    p.start()\n\n    for i in range(1,5):\n        c = Consumer()\n        c.start()</code></pre></div><p>代码没有特别需要注意的，可以说非常简单了，关于<code>redis</code>使用也不多。</p><div class=\"highlight\"><pre><code class=\"language-text\">redis.randomkey() # 随机获取一个key\nredis.delete(key)  # 删除key</code></pre></div><p>(⊙o⊙)…经过几分钟的等待之后，大量的用户信息就来到了我的本地。<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-577df58623d05d3d792d34a5995799b9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"402\" data-rawheight=\"278\" class=\"content_image\" width=\"402\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;402&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"402\" data-rawheight=\"278\" class=\"content_image lazy\" width=\"402\" data-actualsrc=\"https://pic2.zhimg.com/v2-577df58623d05d3d792d34a5995799b9_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/65108633", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "Python爬虫入门教程第十五讲： 石家庄政民互动数据爬取", 
            "content": "<p>今天，咱抓取一个网站，这个网站呢，涉及的内容就是 <b>网友留言和回复</b>，特别简单，但是网站是<code>gov</code>的。网址为<br/><code>http://www.sjz.gov.cn/col/1490066682000/index.html</code><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-00cd6713865ef0573dc7fc07e2278ca4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1234\" data-rawheight=\"512\" class=\"origin_image zh-lightbox-thumb\" width=\"1234\" data-original=\"https://pic1.zhimg.com/v2-00cd6713865ef0573dc7fc07e2278ca4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1234&#39; height=&#39;512&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1234\" data-rawheight=\"512\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1234\" data-original=\"https://pic1.zhimg.com/v2-00cd6713865ef0573dc7fc07e2278ca4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-00cd6713865ef0573dc7fc07e2278ca4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><blockquote><i>首先声明，为了学习，绝无恶意抓取信息，不管你信不信，数据我没有长期存储，预计存储到重装操作系统就删除。</i></blockquote><h2><b>石家庄政民互动数据爬取-网页分析</b></h2><p>点击更多回复 ，可以查看到相应的数据。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5c6e61ecc205e3535379af50d08cce88_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1181\" data-rawheight=\"439\" class=\"origin_image zh-lightbox-thumb\" width=\"1181\" data-original=\"https://pic1.zhimg.com/v2-5c6e61ecc205e3535379af50d08cce88_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1181&#39; height=&#39;439&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1181\" data-rawheight=\"439\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1181\" data-original=\"https://pic1.zhimg.com/v2-5c6e61ecc205e3535379af50d08cce88_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-5c6e61ecc205e3535379af50d08cce88_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>数据量很大14万条,，数据爬完，还可以用来学习数据分析，真是nice</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b86f025c34c60727365642ee5be198ea_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"850\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb\" width=\"850\" data-original=\"https://pic3.zhimg.com/v2-b86f025c34c60727365642ee5be198ea_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;850&#39; height=&#39;192&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"850\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"850\" data-original=\"https://pic3.zhimg.com/v2-b86f025c34c60727365642ee5be198ea_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b86f025c34c60727365642ee5be198ea_b.jpg\"/></figure><p><br/>经过分析之后，找到了列表页面。<br/>数据的爬取这次我们采用的是 <code>selenium</code> ，解析页面采用<code>lxml</code>，数据存储采用<code>pymongo</code> ，关于<code>selenium</code> 你可以去搜索引擎搜索相关的教程，好多的，主要就是打开一个浏览器，然后模拟用户的操作，你可以去系统的学习一下。</p><h2><b>石家庄政民互动数据爬取-撸代码</b></h2><h2><b>导入必备模块</b></h2><div class=\"highlight\"><pre><code class=\"language-text\">from selenium import webdriver\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\nfrom lxml import etree\nimport pymongo\nimport time</code></pre></div><h2><b>石家庄政民互动数据爬取-打开浏览器，获取总页码</b></h2><p>这个操作最重要的步骤，你搜索之后就会知道，需要提前下载一个叫做 chromedriver.exe 的东东，然后把他配置好，自行解决去吧~</p><div class=\"highlight\"><pre><code class=\"language-text\"># 加载浏览器引擎，需要提前下载好 chromedriver.exe 。\nbrowser = webdriver.Chrome()\nwait = WebDriverWait(browser,10)\n\ndef get_totle_page():\n    try:\n        # 浏览器跳转\n        browser.get(&#34;http://www.sjz.gov.cn/zfxxinfolist.jsp?current=1&amp;wid=1&amp;cid=1259811582187&#34;)\n        # 等待元素加载到\n        totle_page = wait.until(\n            EC.presence_of_element_located((By.CSS_SELECTOR,&#39;input[type=&#34;hidden&#34;]:nth-child(4)&#39;))\n        )\n        # 获取属性\n        totle = totle_page.get_attribute(&#39;value&#39;)\n        # 获取首页数据，这个地方先不必须\n        ##############################\n        #get_content()\n        ##############################\n\n        return totle\n    except TimeoutError:\n        return get_totle_page()</code></pre></div><p>上面的代码在测试之后，你会得到如下结果<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-2aad0b9acfeda741d6f50b6d03a6c1a8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"205\" class=\"origin_image zh-lightbox-thumb\" width=\"600\" data-original=\"https://pic1.zhimg.com/v2-2aad0b9acfeda741d6f50b6d03a6c1a8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;600&#39; height=&#39;205&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"205\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"600\" data-original=\"https://pic1.zhimg.com/v2-2aad0b9acfeda741d6f50b6d03a6c1a8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-2aad0b9acfeda741d6f50b6d03a6c1a8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这时候，你已经得到<code>20565</code>这个总页码数目了，只需要进行一系列循环的操作即可，接下来有一个重要的函数，叫做<code>next_page</code> 这个函数里面，需要进行一个模拟用户行为的操作，输入一个页码，然后点击跳转。</p><div class=\"highlight\"><pre><code class=\"language-text\">def main():\n    totle = int(get_totle_page()) # 获取完整页码\n    for i in range(2,totle+1):\n        print(&#34;正在加载第{}页数据&#34;.format(i))\n        # 获取下一页\n        next_page(i)\n\nif __name__ == &#39;__main__&#39;:\n    print(main())</code></pre></div><h2><b>输入页码，点击跳转</b></h2><div class=\"highlight\"><pre><code class=\"language-text\">def next_page(page_num):\n    try:\n        input = wait.until(\n            EC.presence_of_element_located((By.CSS_SELECTOR,&#34;#pageto&#34;))\n        )\n        submit = wait.until(\n            EC.element_to_be_clickable((By.CSS_SELECTOR,&#34;#goPage&#34;))\n        )\n        input.clear() # 清空文本框\n        input.send_keys(page_num)  # 发送页码\n        submit.click()  # 点击跳转\n        #get_content(page_num)\n\n    except TimeoutException:\n        next_page(page_num)</code></pre></div><p>以上代码实现的效果动态演示为</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7a4b4543c9774b8cbdc4d64444605073_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"294\" data-rawheight=\"235\" data-thumbnail=\"https://pic4.zhimg.com/v2-7a4b4543c9774b8cbdc4d64444605073_b.jpg\" class=\"content_image\" width=\"294\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;294&#39; height=&#39;235&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"294\" data-rawheight=\"235\" data-thumbnail=\"https://pic4.zhimg.com/v2-7a4b4543c9774b8cbdc4d64444605073_b.jpg\" class=\"content_image lazy\" width=\"294\" data-actualsrc=\"https://pic4.zhimg.com/v2-7a4b4543c9774b8cbdc4d64444605073_b.gif\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>石家庄政民互动数据爬取-解析页面</b></h2><p>可以进行翻页之后，通过<code>browser.page_source</code> 获取网页源码，网页源码通过<code>lxml</code>进行解析。编写相应的方法为</p><div class=\"highlight\"><pre><code class=\"language-text\">def get_content(page_num=None):\n    try:\n        wait.until(\n            EC.presence_of_element_located((By.CSS_SELECTOR, &#34;table.tably&#34;))\n        )\n        html = browser.page_source   # 获取网页源码\n\n        tree = etree.HTML(html)  # 解析\n\n        tables = tree.xpath(&#34;//table[@class=&#39;tably&#39;]&#34;)\n\n        for table in tables:\n\n            name = table.xpath(&#34;tbody/tr[1]/td[1]/table/tbody/tr[1]/td&#34;)[0].text\n            public_time = table.xpath(&#34;tbody/tr[1]/td[1]/table/tbody/tr[2]/td&#34;)[0].text\n            to_people = table.xpath(&#34;tbody/tr[1]/td[1]/table/tbody/tr[3]/td&#34;)[0].text\n\n            content = table.xpath(&#34;tbody/tr[1]/td[2]/table/tbody/tr[1]/td&#34;)[0].text\n\n            repl_time  =  table.xpath(&#34;tbody/tr[2]/td[1]/table/tbody/tr[1]/td&#34;)[0].text\n            repl_depart = table.xpath(&#34;tbody/tr[2]/td[1]/table/tbody/tr[2]/td&#34;)[0].text\n\n            repl_content = table.xpath(&#34;tbody/tr[2]/td[2]/table/tbody/tr[1]/td&#34;)[0].text\n            # 清理数据\n            consult = {\n                &#34;name&#34;:name.replace(&#34;网友：&#34;,&#34;&#34;),\n                &#34;public_time&#34;:public_time.replace(&#34;时间：&#34;,&#34;&#34;),\n                &#34;to_people&#34;:to_people.replace(&#34;留言对象：&#34;,&#34;&#34;),\n                &#34;content&#34;:content,\n                &#34;repl_time&#34;:repl_time.replace(&#34;时间：&#34;,&#34;&#34;),\n                &#34;repl_depart&#34;:repl_depart.replace(&#34;回复部门：&#34;,&#34;&#34;),\n                &#34;repl_content&#34;:repl_content\n            }\n            # 数据存储到mongo里面\n            #save_mongo(consult)\n    except Exception:  # 这个地方需要特殊说明一下，小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~\n        print(&#34;异常错误X1&#34;)\n        print(&#34;浏览器休息一下&#34;)\n        time.sleep(60)\n        browser.get(&#34;http://www.sjz.gov.cn/zfxxinfolist.jsp?current={}&amp;wid=1&amp;cid=1259811582187&#34;.format(page_num))\n        get_content()</code></pre></div><p>在实际的爬取过程中发现，经过几百页之后，就会限制一下IP，所以当我们捕获页面信息出错，需要暂停一下，等待页面正常之后，在继续爬取数据。</p><h2><b>数据存储到mongodb里面</b></h2><p>爬取到的最终数据，我存储到了mongodb里面，这个就没有什么难度了，我们按照常规的套路编写即可。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-675bbe8f353a36755bc884eb4365da25_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1581\" data-rawheight=\"369\" class=\"origin_image zh-lightbox-thumb\" width=\"1581\" data-original=\"https://pic2.zhimg.com/v2-675bbe8f353a36755bc884eb4365da25_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1581&#39; height=&#39;369&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1581\" data-rawheight=\"369\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1581\" data-original=\"https://pic2.zhimg.com/v2-675bbe8f353a36755bc884eb4365da25_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-675bbe8f353a36755bc884eb4365da25_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>写在最后</b></h2><p>由于这次爬取的网站是<code>gov</code>的，所以建议不要用多线程，源码也不发送到github上去了，要不惹祸，如果有任何疑问，请评论。nice boy</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c00816df0e00b88174822bf48b4ec502_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"259\" data-rawheight=\"287\" class=\"content_image\" width=\"259\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;259&#39; height=&#39;287&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"259\" data-rawheight=\"287\" class=\"content_image lazy\" width=\"259\" data-actualsrc=\"https://pic3.zhimg.com/v2-c00816df0e00b88174822bf48b4ec502_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/64618090", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "Python爬虫入门教程第十二讲： 半次元COS图爬取", 
            "content": "<h2><b>半次元COS图爬取-写在前面</b></h2><p>今天在浏览网站的时候，忽然一个莫名的链接指引着我跳转到了半次元网站 <code>https://bcy.net/</code> 打开之后，发现也没有什么有意思的内容，职业的敏感让我瞬间联想到了 <code>cosplay</code> ，这种网站必然会有这个的存在啊，于是乎，我准备好我的大爬虫了。<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-2b3af1a648194849b1d7eb2017b6f791_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"382\" data-rawheight=\"404\" class=\"content_image\" width=\"382\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;382&#39; height=&#39;404&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"382\" data-rawheight=\"404\" class=\"content_image lazy\" width=\"382\" data-actualsrc=\"https://pic2.zhimg.com/v2-2b3af1a648194849b1d7eb2017b6f791_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f3cd2b4306e57b9c3d31bb295506e461_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"870\" data-rawheight=\"428\" class=\"origin_image zh-lightbox-thumb\" width=\"870\" data-original=\"https://pic2.zhimg.com/v2-f3cd2b4306e57b9c3d31bb295506e461_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;870&#39; height=&#39;428&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"870\" data-rawheight=\"428\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"870\" data-original=\"https://pic2.zhimg.com/v2-f3cd2b4306e57b9c3d31bb295506e461_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f3cd2b4306e57b9c3d31bb295506e461_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>把上面的链接打开之后，被我发现了吧，就知道我的第八感不错滴。接下来就是找入口，一定要找到图片链接的入口才可以做下面的操作<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f73c5be13d70260fdf6fb9e6ed3dcaa0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1526\" data-rawheight=\"407\" class=\"origin_image zh-lightbox-thumb\" width=\"1526\" data-original=\"https://pic1.zhimg.com/v2-f73c5be13d70260fdf6fb9e6ed3dcaa0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1526&#39; height=&#39;407&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1526\" data-rawheight=\"407\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1526\" data-original=\"https://pic1.zhimg.com/v2-f73c5be13d70260fdf6fb9e6ed3dcaa0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f73c5be13d70260fdf6fb9e6ed3dcaa0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这个页面不断往下拖拽，页面会一直加载，当时当你拖拽一会，就停下来了，就是这个时机<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-715b5b411eecd8da38e999359e92e30e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"669\" data-rawheight=\"342\" class=\"origin_image zh-lightbox-thumb\" width=\"669\" data-original=\"https://pic3.zhimg.com/v2-715b5b411eecd8da38e999359e92e30e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;669&#39; height=&#39;342&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"669\" data-rawheight=\"342\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"669\" data-original=\"https://pic3.zhimg.com/v2-715b5b411eecd8da38e999359e92e30e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-715b5b411eecd8da38e999359e92e30e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>发现入口，在我实际的操作中，其实还发现了很多其他的入口，这个就不一一的解释了，赶紧上车，进入 <code>view more</code> 之后，发现了页面依旧是一个下拉刷新的布局方式，专业术语 <code>瀑布流</code> 。</p><h2><b>半次元COS图爬取-python爬虫第一步</b></h2><p>打开开发者工具，切换到<code>network</code>之后，发现 很多<code>xhr</code>请求，发现这个，就代表这个网站很容易爬取了</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-de509b7418e724a95cc55213883657cf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1029\" data-rawheight=\"470\" class=\"origin_image zh-lightbox-thumb\" width=\"1029\" data-original=\"https://pic4.zhimg.com/v2-de509b7418e724a95cc55213883657cf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1029&#39; height=&#39;470&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1029\" data-rawheight=\"470\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1029\" data-original=\"https://pic4.zhimg.com/v2-de509b7418e724a95cc55213883657cf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-de509b7418e724a95cc55213883657cf_b.jpg\"/></figure><p><br/>提取待爬取的链接，分析规律</p><div class=\"highlight\"><pre><code class=\"language-text\">https://bcy.net/circle/timeline/loadtag?since=0&amp;grid_type=timeline&amp;tag_id=1482&amp;sort=hot\nhttps://bcy.net/circle/timeline/loadtag?since=26499.779&amp;grid_type=timeline&amp;tag_id=1482&amp;sort=hot\nhttps://bcy.net/circle/timeline/loadtag?since=26497.945&amp;grid_type=timeline&amp;tag_id=1482&amp;sort=hot</code></pre></div><p>发现只有一个参数在变，而且这变化好像没有任何规律可以寻找，没事，看数据，你就可以发现其中的奥妙了</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7a9e16af3d81202d25d5b946c9ac2262_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1509\" data-rawheight=\"556\" class=\"origin_image zh-lightbox-thumb\" width=\"1509\" data-original=\"https://pic3.zhimg.com/v2-7a9e16af3d81202d25d5b946c9ac2262_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1509&#39; height=&#39;556&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1509\" data-rawheight=\"556\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1509\" data-original=\"https://pic3.zhimg.com/v2-7a9e16af3d81202d25d5b946c9ac2262_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7a9e16af3d81202d25d5b946c9ac2262_b.jpg\"/></figure><p><br/>这个网站的原理很简单，就是通过不断获取每次数据的最后一条的<code>since</code>然后获取接下来的数据，那么我们按照它的规律实现代码就可以了，不要多线程了，这种规律是没有办法进行实操的。<br/>这次的数据我把它存储到<code>mongodb</code>里面，因为没有办法一次全部获取到，所以可能需要下次在继续使用</p><div class=\"highlight\"><pre><code class=\"language-text\">if __name__ == &#39;__main__&#39;:\n    ###  mongodb 的一些基本操作   \n    DATABASE_IP = &#39;127.0.0.1&#39;\n    DATABASE_PORT = 27017\n    DATABASE_NAME = &#39;sun&#39;\n    start_url = &#34;https://bcy.net/circle/timeline/loadtag?since={}&amp;grid_type=timeline&amp;tag_id=399&amp;sort=recent&#34;\n    client = MongoClient(DATABASE_IP, DATABASE_PORT)\n\n    db = client.sun\n    db.authenticate(&#34;dba&#34;, &#34;dba&#34;)\n    collection  =  db.bcy  # 准备插入数据\n    #####################################3333\n    get_data(start_url,collection)</code></pre></div><p>获取网页数据这个地方，由我们前面的经验就变得很简单了</p><div class=\"highlight\"><pre><code class=\"language-text\">## 半次元COS图爬取-获取数据函数  \ndef get_data(start_url,collection):\n    since = 0\n    while 1:\n        try:\n            with requests.Session() as s:\n                response = s.get(start_url.format(str(since)),headers=headers,timeout=3)\n                res_data = response.json()\n                if res_data[&#34;status&#34;] == 1:\n                    data = res_data[&#34;data&#34;]  # 获取Data数组\n                    time.sleep(0.5)\n                ## 数据处理\n                since = data[-1][&#34;since&#34;]  # 获取20条数据的最后一条json数据中的since\n                ret = json_handle(data)   # 代码实现在下面\n                try:\n                    print(ret)\n                    collection.insert_many(ret)   # 批量出入数据库\n                    print(&#34;上述数据插入成功！！！！！！！！&#34;)\n                except Exception as e:\n                    print(&#34;插入失败&#34;)\n                    print(ret)\n\n                ##\n        except Exception as e:\n            print(&#34;!&#34;,end=&#34;异常，请注意&#34;)\n            print(e,end=&#34; &#34;)\n    else:\n        print(&#34;循环完毕&#34;)</code></pre></div><p>网页解析代码</p><div class=\"highlight\"><pre><code class=\"language-text\"># 对JSON数据进行处理\ndef json_handle(data):\n    # 提取关键数据\n    list_infos = []\n    for item in data:\n        item = item[&#34;item_detail&#34;]\n        try:\n            avatar = item[&#34;avatar&#34;] # 用户头像\n            item_id = item[&#34;item_id&#34;] # 图片详情页面\n            like_count = item[&#34;like_count&#34;] # 喜欢数目\n            pic_num = item[&#34;pic_num&#34;] if &#34;pic_num&#34; in item else 0 # 图片总数\n            reply_count =item[&#34;reply_count&#34;]\n            share_count =item[&#34;share_count&#34;]\n            uid = item[&#34;uid&#34;]\n            plain = item[&#34;plain&#34;]\n            uname = item[&#34;uname&#34;]\n            list_infos.append({&#34;avatar&#34;:avatar,\n                               &#34;item_id&#34;:item_id,\n                               &#34;like_count&#34;:like_count,\n                               &#34;pic_num&#34;:pic_num,\n                               &#34;reply_count&#34;:reply_count,\n                               &#34;share_count&#34;:share_count,\n                               &#34;uid&#34;:uid,\n                               &#34;plain&#34;:plain,\n                               &#34;uname&#34;:uname})\n        except Exception as e:\n            print(e)\n            continue\n        return list_infos</code></pre></div><p>到现在就实现了，代码跑起来<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-006dcc61b2b4054b72561a9590bca4a0_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1150\" data-rawheight=\"403\" data-thumbnail=\"https://pic1.zhimg.com/v2-006dcc61b2b4054b72561a9590bca4a0_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1150\" data-original=\"https://pic1.zhimg.com/v2-006dcc61b2b4054b72561a9590bca4a0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1150&#39; height=&#39;403&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1150\" data-rawheight=\"403\" data-thumbnail=\"https://pic1.zhimg.com/v2-006dcc61b2b4054b72561a9590bca4a0_b.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1150\" data-original=\"https://pic1.zhimg.com/v2-006dcc61b2b4054b72561a9590bca4a0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-006dcc61b2b4054b72561a9590bca4a0_b.gif\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-91e205d2e10b2ef7bc45dc68744ae256_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"394\" data-rawheight=\"384\" class=\"content_image\" width=\"394\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;394&#39; height=&#39;384&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"394\" data-rawheight=\"384\" class=\"content_image lazy\" width=\"394\" data-actualsrc=\"https://pic3.zhimg.com/v2-91e205d2e10b2ef7bc45dc68744ae256_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/64606504", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "Python爬虫入门教程第十一讲： 行行网电子书多线程爬取", 
            "content": "<h2><b>行行网电子书多线程爬取-写在前面</b></h2><p>最近想找几本电子书看看，就翻啊翻，然后呢，找到了一个 叫做 <code>周读</code>的网站 ，网站特别好，简单清爽，书籍很多，而且打开都是百度网盘可以直接下载，更新速度也还可以，于是乎，<b>我给爬了</b>。本篇文章学习即可，这么好的分享网站，尽量不要去爬，影响人家访问速度就不好了 <code>http://www.ireadweek.com/</code> ,想要数据的，可以在我博客下面评论，我发给你，QQ，邮箱，啥的都可以。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-de1bdbe4f31c96bfa8ba108dd287c373_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1124\" data-rawheight=\"352\" class=\"origin_image zh-lightbox-thumb\" width=\"1124\" data-original=\"https://pic4.zhimg.com/v2-de1bdbe4f31c96bfa8ba108dd287c373_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1124&#39; height=&#39;352&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1124\" data-rawheight=\"352\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1124\" data-original=\"https://pic4.zhimg.com/v2-de1bdbe4f31c96bfa8ba108dd287c373_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-de1bdbe4f31c96bfa8ba108dd287c373_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4e25376cd5919a6f1a17008d20d6eae2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"381\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic3.zhimg.com/v2-4e25376cd5919a6f1a17008d20d6eae2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;381&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"381\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic3.zhimg.com/v2-4e25376cd5919a6f1a17008d20d6eae2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4e25376cd5919a6f1a17008d20d6eae2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这个网站页面逻辑特别简单 ，我翻了翻 书籍详情页面 ，就是下面这个样子的，我们只需要循环生成这些页面的链接，然后去爬就可以了，为了速度，我采用的多线程，你试试就可以了，想要爬取之后的数据，就在本篇博客下面评论，不要搞坏别人服务器。</p><div class=\"highlight\"><pre><code class=\"language-text\">http://www.ireadweek.com/index.php/bookInfo/11393.html\nhttp://www.ireadweek.com/index.php/bookInfo/11.html\n....</code></pre></div><h2><b>行行网电子书多线程爬取-撸代码</b></h2><p>代码非常简单，有咱们前面的教程做铺垫，很少的代码就可以实现完整的功能了，最后把采集到的内容写到 <code>csv</code> 文件里面，(<code>csv</code> 是啥，你百度一下就知道了) 这段代码是<code>IO密集操作</code> 我们采用<code>aiohttp</code>模块编写。</p><h2><b>第1步</b></h2><p>拼接URL，开启线程。</p><div class=\"highlight\"><pre><code class=\"language-text\">import requests\n\n# 导入协程模块\nimport asyncio\nimport aiohttp\n\n\nheaders = {&#34;User-Agent&#34;: &#34;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36&#34;,\n           &#34;Host&#34;: &#34;www.ireadweek.com&#34;,\n           &#34;Accept&#34;: &#34;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&#34;}\n\nasync def get_content(url):\n    print(&#34;正在操作:{}&#34;.format(url))\n    # 创建一个session 去获取数据 \n    async with aiohttp.ClientSession() as session:\n        async with session.get(url,headers=headers,timeout=3) as res:\n            if res.status == 200:\n                source = await res.text()  # 等待获取文本\n                print(source)\n\n\nif __name__ == &#39;__main__&#39;:\n    url_format = &#34;http://www.ireadweek.com/index.php/bookInfo/{}.html&#34;\n    full_urllist = [url_format.format(i) for i in range(1,11394)]  # 11394\n    loop = asyncio.get_event_loop()\n    tasks = [get_content(url) for url in full_urllist]\n    results = loop.run_until_complete(asyncio.wait(tasks))</code></pre></div><p>上面的代码可以同步开启N多个线程，但是这样子很容易造成别人的服务器瘫痪，所以，我们必须要限制一下并发次数，下面的代码，你自己尝试放到指定的位置吧。</p><div class=\"highlight\"><pre><code class=\"language-text\">sema = asyncio.Semaphore(5)\n# 为避免爬虫一次性请求次数太多，控制一下\nasync def x_get_source(url):\n    with(await sema):\n        await get_content(url)</code></pre></div><h2><b>第2步</b></h2><p>处理抓取到的网页源码，提取我们想要的元素，我新增了一个方法，采用<code>lxml</code>进行数据提取。</p><div class=\"highlight\"><pre><code class=\"language-text\">def async_content(tree):\n    title = tree.xpath(&#34;//div[@class=&#39;hanghang-za-title&#39;]&#34;)[0].text\n    # 如果页面没有信息，直接返回即可\n    if title == &#39;&#39;:\n        return\n    else:\n        try:\n            description = tree.xpath(&#34;//div[@class=&#39;hanghang-shu-content-font&#39;]&#34;)\n            author = description[0].xpath(&#34;p[1]/text()&#34;)[0].replace(&#34;作者：&#34;,&#34;&#34;) if description[0].xpath(&#34;p[1]/text()&#34;)[0] is not None else None\n            cate = description[0].xpath(&#34;p[2]/text()&#34;)[0].replace(&#34;分类：&#34;,&#34;&#34;) if description[0].xpath(&#34;p[2]/text()&#34;)[0] is not None else None\n            douban = description[0].xpath(&#34;p[3]/text()&#34;)[0].replace(&#34;豆瓣评分：&#34;,&#34;&#34;) if description[0].xpath(&#34;p[3]/text()&#34;)[0] is not None else None\n            # 这部分内容不明确，不做记录\n            #des = description[0].xpath(&#34;p[5]/text()&#34;)[0] if description[0].xpath(&#34;p[5]/text()&#34;)[0] is not None else None\n            download = tree.xpath(&#34;//a[@class=&#39;downloads&#39;]&#34;)\n        except Exception as e:\n            print(title)\n            return\n\n    ls = [\n        title,author,cate,douban,download[0].get(&#39;href&#39;)\n    ]\n    return ls</code></pre></div><h2><b>第3步</b></h2><p>数据格式化之后，保存到<code>csv</code>文件，收工！</p><div class=\"highlight\"><pre><code class=\"language-text\">print(data)\n with open(&#39;hang.csv&#39;, &#39;a+&#39;, encoding=&#39;utf-8&#39;) as fw:\n     writer = csv.writer(fw)\n     writer.writerow(data)\n print(&#34;插入成功！&#34;)</code></pre></div><h2><b>行行网电子书多线程爬取-运行代码，查看结果</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d5b476ac09846276365a46824f47408d_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"886\" data-rawheight=\"595\" data-thumbnail=\"https://pic2.zhimg.com/v2-d5b476ac09846276365a46824f47408d_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"886\" data-original=\"https://pic2.zhimg.com/v2-d5b476ac09846276365a46824f47408d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;886&#39; height=&#39;595&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"886\" data-rawheight=\"595\" data-thumbnail=\"https://pic2.zhimg.com/v2-d5b476ac09846276365a46824f47408d_b.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"886\" data-original=\"https://pic2.zhimg.com/v2-d5b476ac09846276365a46824f47408d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d5b476ac09846276365a46824f47408d_b.gif\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-45564aab5b6d93fb46b3699a8695bcb9_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"270\" data-rawheight=\"270\" data-thumbnail=\"https://pic2.zhimg.com/v2-45564aab5b6d93fb46b3699a8695bcb9_b.jpg\" class=\"content_image\" width=\"270\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;270&#39; height=&#39;270&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"270\" data-rawheight=\"270\" data-thumbnail=\"https://pic2.zhimg.com/v2-45564aab5b6d93fb46b3699a8695bcb9_b.jpg\" class=\"content_image lazy\" width=\"270\" data-actualsrc=\"https://pic2.zhimg.com/v2-45564aab5b6d93fb46b3699a8695bcb9_b.gif\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/64605878", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "Python爬虫入门教程第十讲：图虫网多线程爬取", 
            "content": "<h2><b>图虫网-写在前面</b></h2><p>经历了一顿噼里啪啦的操作之后，终于我把博客写到了第10篇，后面，慢慢的会涉及到更多的爬虫模块，有人问<code>scrapy</code> 啥时候开始用，这个我预计要在30篇以后了吧，后面的套路依旧慢节奏的，所以莫着急了，100篇呢，预计4~5个月写完，常见的反反爬后面也会写的，还有fuck login类的内容。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-fcd8703209bb058f5e4e3cb8d5e4350c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"175\" data-rawheight=\"175\" class=\"content_image\" width=\"175\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;175&#39; height=&#39;175&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"175\" data-rawheight=\"175\" class=\"content_image lazy\" width=\"175\" data-actualsrc=\"https://pic1.zhimg.com/v2-fcd8703209bb058f5e4e3cb8d5e4350c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>图虫网-爬取图虫网</b></h2><p>为什么要爬取这个网站，不知道哎~ 莫名奇妙的收到了，感觉图片质量不错，不是那些<code>妖艳贱货</code> 可以比的，所以就开始爬了，搜了一下网上有人也在爬，但是基本都是py2，py3的还没有人写，所以顺手写一篇吧。</p><h3>起始页面</h3><p><a href=\"https://link.zhihu.com/?target=https%3A//tuchong.com/explore/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">tuchong.com/explore/</span><span class=\"invisible\"></span></a><br/>这个页面中有很多的标签，每个标签下面都有很多图片，为了和谐，我选择了一个非常好的标签<code>花卉</code> 你可以选择其他的，甚至，你可以把所有的都爬取下来。</p><div class=\"highlight\"><pre><code class=\"language-text\">https://tuchong.com/tags/%E8%8A%B1%E5%8D%89/  # 花卉编码成了  %E8%8A%B1%E5%8D%89  这个无所谓</code></pre></div><p>我们这次也玩点以前没写过的，使用python中的queue，也就是队列</p><p>下面是我从别人那顺来的一些解释，基本爬虫初期也就用到这么多</p><div class=\"highlight\"><pre><code class=\"language-text\">1. 初始化： class Queue.Queue(maxsize) FIFO 先进先出\n\n2. 包中的常用方法:\n\n    - queue.qsize() 返回队列的大小\n    - queue.empty() 如果队列为空，返回True,反之False\n    - queue.full() 如果队列满了，返回True,反之False\n    - queue.full 与 maxsize 大小对应\n    - queue.get([block[, timeout]])获取队列，timeout等待时间\n\n3. 创建一个“队列”对象\n    import queue\n    myqueue = queue.Queue(maxsize = 10)\n\n4. 将一个值放入队列中\n    myqueue.put(10)\n\n5. 将一个值从队列中取出\n    myqueue.get()</code></pre></div><h3>开始编码</h3><p>首先我们先实现主要方法的框架，我依旧是把一些核心的点，都写在注释上面</p><div class=\"highlight\"><pre><code class=\"language-text\">def main():\n    # 声明一个队列，使用循环在里面存入100个页码\n    page_queue  = Queue(100)\n    for i in range(1,101):\n        page_queue.put(i)\n\n\n    # 采集结果(等待下载的图片地址)\n    data_queue = Queue()\n\n    # 记录线程的列表\n    thread_crawl = []\n    # 每次开启4个线程\n    craw_list = [&#39;采集线程1号&#39;,&#39;采集线程2号&#39;,&#39;采集线程3号&#39;,&#39;采集线程4号&#39;]\n    for thread_name in craw_list:\n        c_thread = ThreadCrawl(thread_name, page_queue, data_queue)\n        c_thread.start()\n        thread_crawl.append(c_thread)\n\n    # 等待page_queue队列为空，也就是等待之前的操作执行完毕\n    while not page_queue.empty():\n        pass\n\nif __name__ == &#39;__main__&#39;:\n    main()</code></pre></div><p>代码运行之后，成功启动了4个线程，然后等待线程结束，这个地方注意，你需要把 <code>ThreadCrawl</code> 类补充完整</p><div class=\"highlight\"><pre><code class=\"language-text\">class ThreadCrawl(threading.Thread):\n\n    def __init__(self, thread_name, page_queue, data_queue):\n        # threading.Thread.__init__(self)\n        # 调用父类初始化方法\n        super(ThreadCrawl, self).__init__()\n        self.threadName = thread_name\n        self.page_queue = page_queue\n        self.data_queue = data_queue\n\n    def run(self):\n        print(self.threadName + &#39; 启动************&#39;)</code></pre></div><p>运行结果<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e6ac98de5dfd525a78bdf13545ba925b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"492\" data-rawheight=\"121\" class=\"origin_image zh-lightbox-thumb\" width=\"492\" data-original=\"https://pic4.zhimg.com/v2-e6ac98de5dfd525a78bdf13545ba925b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;492&#39; height=&#39;121&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"492\" data-rawheight=\"121\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"492\" data-original=\"https://pic4.zhimg.com/v2-e6ac98de5dfd525a78bdf13545ba925b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e6ac98de5dfd525a78bdf13545ba925b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>线程已经开启，在run方法中，补充爬取数据的代码就好了,这个地方引入一个全局变量，用来标识爬取状态<br/><code>CRAWL_EXIT = False</code></p><p>先在<code>main</code>方法中加入如下代码</p><div class=\"highlight\"><pre><code class=\"language-text\">CRAWL_EXIT = False  # 这个变量声明在这个位置\nclass ThreadCrawl(threading.Thread):\n\n    def __init__(self, thread_name, page_queue, data_queue):\n        # threading.Thread.__init__(self)\n        # 调用父类初始化方法\n        super(ThreadCrawl, self).__init__()\n        self.threadName = thread_name\n        self.page_queue = page_queue\n        self.data_queue = data_queue\n\n    def run(self):\n        print(self.threadName + &#39; 启动************&#39;)\n        while not CRAWL_EXIT:\n            try:\n                global tag, url, headers,img_format  # 把全局的值拿过来\n                # 队列为空 产生异常\n                page = self.page_queue.get(block=False)   # 从里面获取值\n                spider_url = url_format.format(tag,page,100)   # 拼接要爬取的URL\n                print(spider_url)\n            except:\n                break\n\n            timeout = 4   # 合格地方是尝试获取3次，3次都失败，就跳出\n            while timeout &gt; 0:\n                timeout -= 1\n                try:\n                    with requests.Session() as s:\n                        response = s.get(spider_url, headers=headers, timeout=3)\n                        json_data = response.json()\n                        if json_data is not None:\n                            imgs = json_data[&#34;postList&#34;]\n                            for i in imgs:\n                                imgs = i[&#34;images&#34;]\n                                for img in imgs:\n                                    img = img_format.format(img[&#34;user_id&#34;],img[&#34;img_id&#34;])\n                                    self.data_queue.put(img)  # 捕获到图片链接，之后，存入一个新的队列里面，等待下一步的操作\n\n                    break\n\n                except Exception as e:\n                    print(e)\n\n\n            if timeout &lt;= 0:\n                print(&#39;time out!&#39;)\ndef main():\n    # 代码在上面\n\n    # 等待page_queue队列为空，也就是等待之前的操作执行完毕\n    while not page_queue.empty():\n        pass\n\n    # 如果page_queue为空，采集线程退出循环\n    global CRAWL_EXIT\n    CRAWL_EXIT = True\n    \n    # 测试一下队列里面是否有值\n    print(data_queue)</code></pre></div><p>经过测试，data_queue 里面有数据啦！！，哈哈，下面在使用相同的操作，去下载图片就好喽<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-bbcbd54ca52c3f3b647f31855bb56254_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"620\" data-rawheight=\"195\" class=\"origin_image zh-lightbox-thumb\" width=\"620\" data-original=\"https://pic1.zhimg.com/v2-bbcbd54ca52c3f3b647f31855bb56254_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;620&#39; height=&#39;195&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"620\" data-rawheight=\"195\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"620\" data-original=\"https://pic1.zhimg.com/v2-bbcbd54ca52c3f3b647f31855bb56254_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-bbcbd54ca52c3f3b647f31855bb56254_b.jpg\"/></figure><p><br/>完善<code>main</code>方法</p><div class=\"highlight\"><pre><code class=\"language-text\">def main():\n    # 代码在上面\n\n    for thread in thread_crawl:\n        thread.join()\n        print(&#34;抓取线程结束&#34;)\n\n    thread_image = []\n    image_list = [&#39;下载线程1号&#39;, &#39;下载线程2号&#39;, &#39;下载线程3号&#39;, &#39;下载线程4号&#39;]\n    for thread_name in image_list:\n        Ithread = ThreadDown(thread_name, data_queue)\n        Ithread.start()\n        thread_image.append(Ithread)\n\n\n\n    while not data_queue.empty():\n        pass\n\n    global DOWN_EXIT\n    DOWN_EXIT = True\n\n    for thread in thread_image:\n        thread.join()\n        print(&#34;下载线程结束&#34;)</code></pre></div><p>还是补充一个 <code>ThreadDown</code> 类，这个类就是用来下载图片的。</p><div class=\"highlight\"><pre><code class=\"language-text\">class ThreadDown(threading.Thread):\n    def __init__(self, thread_name, data_queue):\n        super(ThreadDown, self).__init__()\n        self.thread_name = thread_name\n        self.data_queue = data_queue\n\n    def run(self):\n        print(self.thread_name + &#39; 启动************&#39;)\n        while not DOWN_EXIT:\n            try:\n                img_link = self.data_queue.get(block=False)\n                self.write_image(img_link)\n            except Exception as e:\n                pass\n\n    def write_image(self, url):\n\n        with requests.Session() as s:\n            response = s.get(url, timeout=3)\n            img = response.content   # 获取二进制流\n\n        try:\n            file = open(&#39;image/&#39; + str(time.time())+&#39;.jpg&#39;, &#39;wb&#39;)\n            file.write(img)\n            file.close()\n            print(&#39;image/&#39; + str(time.time())+&#39;.jpg 图片下载完毕&#39;)\n\n        except Exception as e:\n            print(e)\n            return</code></pre></div><p>运行之后,等待图片下载就可以啦~~</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a993e0b8c97407c56c29993e8d80cf03_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"628\" data-rawheight=\"546\" class=\"origin_image zh-lightbox-thumb\" width=\"628\" data-original=\"https://pic4.zhimg.com/v2-a993e0b8c97407c56c29993e8d80cf03_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;628&#39; height=&#39;546&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"628\" data-rawheight=\"546\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"628\" data-original=\"https://pic4.zhimg.com/v2-a993e0b8c97407c56c29993e8d80cf03_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a993e0b8c97407c56c29993e8d80cf03_b.jpg\"/></figure><p><br/>关键注释已经添加到代码里面了,收图吧 (◕ᴗ◕✿)，这次代码回头在上传到<code>github</code>上 因为比较简单<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1e111e71b00a66eceffb484407d8d7d9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1094\" data-rawheight=\"301\" class=\"origin_image zh-lightbox-thumb\" width=\"1094\" data-original=\"https://pic2.zhimg.com/v2-1e111e71b00a66eceffb484407d8d7d9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1094&#39; height=&#39;301&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1094\" data-rawheight=\"301\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1094\" data-original=\"https://pic2.zhimg.com/v2-1e111e71b00a66eceffb484407d8d7d9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1e111e71b00a66eceffb484407d8d7d9_b.jpg\"/></figure><p><br/>当你把上面的花卉修改成比如<code>xx</code>啥的~，就是<code>天外飞仙</code>了，小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4c633601d1b1a4ff15c975cb082f9950_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"534\" data-rawheight=\"445\" class=\"origin_image zh-lightbox-thumb\" width=\"534\" data-original=\"https://pic1.zhimg.com/v2-4c633601d1b1a4ff15c975cb082f9950_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;534&#39; height=&#39;445&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"534\" data-rawheight=\"445\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"534\" data-original=\"https://pic1.zhimg.com/v2-4c633601d1b1a4ff15c975cb082f9950_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-4c633601d1b1a4ff15c975cb082f9950_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/64605122", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 1, 
            "title": "Python爬虫入门教程第九讲： 河北阳光理政投诉板块", 
            "content": "<h2><b>河北阳光理政投诉板块-写在前面</b></h2><p>之前几篇文章都是在写图片相关的爬虫，今天写个留言板爬出，为另一套数据分析案例的教程做做准备，作为一个河北人，遵纪守法，有事投诉是必备的技能，那么咱看看我们大河北人都因为什么投诉过呢？</p><p>今天要爬取的网站地址 <code>http://yglz.tousu.hebnews.cn/l-1001-5-</code>，一遍爬取一遍嘀咕，别因为爬这个网站在去喝茶，再次声明，学习目的，切勿把人家网站爬瘫痪了。<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-94792e03f32a2d9d683d5142eda2034a_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"320\" data-rawheight=\"320\" data-thumbnail=\"https://pic3.zhimg.com/v2-94792e03f32a2d9d683d5142eda2034a_b.jpg\" class=\"content_image\" width=\"320\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;320&#39; height=&#39;320&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"320\" data-rawheight=\"320\" data-thumbnail=\"https://pic3.zhimg.com/v2-94792e03f32a2d9d683d5142eda2034a_b.jpg\" class=\"content_image lazy\" width=\"320\" data-actualsrc=\"https://pic3.zhimg.com/v2-94792e03f32a2d9d683d5142eda2034a_b.gif\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>河北阳光理政投诉板块-开始撸代码</b></h2><p>今天再次尝试使用一个新的模块 <code>lxml</code> ，它可以配合<code>xpath</code>快速解析HTML文档，官网网站 <a href=\"https://link.zhihu.com/?target=https%3A//lxml.de/index.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">lxml.de/index.html</span><span class=\"invisible\"></span></a><br/>利用pip安装lxml，如果安装失败，可以在搜索引擎多搜搜，内容很多，100%有解决方案。</p><div class=\"highlight\"><pre><code class=\"language-text\">pip install lxml</code></pre></div><p>废话不多说，直接通过<code>requests</code>模块获取百度首页，然后用<code>lxml</code>进行解析</p><div class=\"highlight\"><pre><code class=\"language-text\">import requests\nfrom lxml import etree  # 从lxml中导入etree\n\nresponse = requests.get(&#34;http://www.baidu.com&#34;)\nhtml = response.content.decode(&#34;utf-8&#34;)\ntree=etree.HTML(html)  # 解析html\n\nprint(tree)</code></pre></div><p>当你打印的内容为下图所示，你就接近成功了！<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-68e8540d7bc635e1001479dd5ecc4ca2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"343\" data-rawheight=\"73\" class=\"content_image\" width=\"343\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;343&#39; height=&#39;73&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"343\" data-rawheight=\"73\" class=\"content_image lazy\" width=\"343\" data-actualsrc=\"https://pic3.zhimg.com/v2-68e8540d7bc635e1001479dd5ecc4ca2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>下面就是 配合<code>xpath</code> 语法获取网页元素了，关于<code>xpath</code> 这个你也可以自行去学习，非常简单，搜索一下全都是资料，咱就不讲了。<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2796ee8ed1229c6a74336d006162723a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"451\" data-rawheight=\"349\" class=\"origin_image zh-lightbox-thumb\" width=\"451\" data-original=\"https://pic3.zhimg.com/v2-2796ee8ed1229c6a74336d006162723a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;451&#39; height=&#39;349&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"451\" data-rawheight=\"349\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"451\" data-original=\"https://pic3.zhimg.com/v2-2796ee8ed1229c6a74336d006162723a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2796ee8ed1229c6a74336d006162723a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>通过<code>xpath</code>我们进行下一步的操作，代码注释可以多看一下。</p><div class=\"highlight\"><pre><code class=\"language-text\">tree=etree.HTML(html)  # 解析html\nhrefs = tree.xpath(&#39;//a&#39;)  #通过xpath获取所有的a元素\n# 注意网页中有很多的a标签，所以获取到的是一个数组，那么我们需要用循环进行操作\nfor href in hrefs:\n    print(href)</code></pre></div><p>打印结果如下</p><div class=\"highlight\"><pre><code class=\"language-text\">&lt;Element a at 0x1cf64252408&gt;\n&lt;Element a at 0x1cf642523c8&gt;\n&lt;Element a at 0x1cf64252288&gt;\n&lt;Element a at 0x1cf64252308&gt;\n&lt;Element a at 0x1cf64285708&gt;\n&lt;Element a at 0x1cf642aa108&gt;\n&lt;Element a at 0x1cf642aa0c8&gt;\n&lt;Element a at 0x1cf642aa148&gt;\n&lt;Element a at 0x1cf642aa048&gt;\n&lt;Element a at 0x1cf64285848&gt;\n&lt;Element a at 0x1cf642aa188&gt;</code></pre></div><p>在使用<code>xpath</code>配合<code>lxml</code>中，记住只要输出上述内容，就代表获取到东西了，当然这个不一定是你需要的，不过代码至少是没有错误的。</p><p>继续编写代码</p><div class=\"highlight\"><pre><code class=\"language-text\"># 注意网页中有很多的a标签，所以获取到的是一个数组，那么我们需要用循环进行操作\nfor href in hrefs:\n    print(href)\n    print(href.get(&#34;href&#34;)) # 获取html元素属性\n    print(href.text)  # 获取a标签内部文字</code></pre></div><p>输出结果</p><div class=\"highlight\"><pre><code class=\"language-text\">&lt;Element a at 0x1c7b76c2408&gt;\nhttp://news.baidu.com\n新闻\n&lt;Element a at 0x1c7b76c23c8&gt;\nhttp://www.hao123.com\nhao123\n&lt;Element a at 0x1c7b76c2288&gt;\nhttp://map.baidu.com\n地图\n&lt;Element a at 0x1c7b76c2308&gt;\nhttp://v.baidu.com\n视频\n&lt;Element a at 0x1c7b76f5708&gt;\nhttp://tieba.baidu.com\n贴吧</code></pre></div><p>现在你已经看到，我们已经获取到了百度首页的所有a标签，并且获取到了a标签的href属性和a标签的文字。有这些内容，你就能很容易的去获取我们的目标网站了。</p><h2><b>河北阳光理政投诉板块-爬取投诉数据</b></h2><p>找到我们的目标网页，结果发现，出事情了，页面竟然是用aspx动态生成的，技术你就不需要研究了，总之，碰到了一个比较小的问题。</p><p>首先，点击下一页的时候，页面是局部刷新的<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ae531b58a0cb2263fd4b8a748176c47d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1143\" data-rawheight=\"228\" class=\"origin_image zh-lightbox-thumb\" width=\"1143\" data-original=\"https://pic2.zhimg.com/v2-ae531b58a0cb2263fd4b8a748176c47d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1143&#39; height=&#39;228&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1143\" data-rawheight=\"228\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1143\" data-original=\"https://pic2.zhimg.com/v2-ae531b58a0cb2263fd4b8a748176c47d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ae531b58a0cb2263fd4b8a748176c47d_b.jpg\"/></figure><p><br/>刷新的同时，捕获了一下发送的请求，是<code>post</code>方式，这个需要留意一下，最要紧的是下面第2张图片和第3张图片。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8e16c5fe7d89f623b9c923bc21f52fad_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"465\" data-rawheight=\"215\" class=\"origin_image zh-lightbox-thumb\" width=\"465\" data-original=\"https://pic2.zhimg.com/v2-8e16c5fe7d89f623b9c923bc21f52fad_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;465&#39; height=&#39;215&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"465\" data-rawheight=\"215\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"465\" data-original=\"https://pic2.zhimg.com/v2-8e16c5fe7d89f623b9c923bc21f52fad_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8e16c5fe7d89f623b9c923bc21f52fad_b.jpg\"/></figure><p><br/>这张图片中的<code>viewstate</code><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e6cd8d7adb92ed4fd8d193be10a38cb9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"741\" data-rawheight=\"339\" class=\"origin_image zh-lightbox-thumb\" width=\"741\" data-original=\"https://pic2.zhimg.com/v2-e6cd8d7adb92ed4fd8d193be10a38cb9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;741&#39; height=&#39;339&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"741\" data-rawheight=\"339\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"741\" data-original=\"https://pic2.zhimg.com/v2-e6cd8d7adb92ed4fd8d193be10a38cb9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e6cd8d7adb92ed4fd8d193be10a38cb9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这张图片也有一些奇怪的参数<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3af67a33731c78802d279085601e96ee_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"836\" data-rawheight=\"306\" class=\"origin_image zh-lightbox-thumb\" width=\"836\" data-original=\"https://pic3.zhimg.com/v2-3af67a33731c78802d279085601e96ee_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;836&#39; height=&#39;306&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"836\" data-rawheight=\"306\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"836\" data-original=\"https://pic3.zhimg.com/v2-3af67a33731c78802d279085601e96ee_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-3af67a33731c78802d279085601e96ee_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这些参数都是典型的动态网页参数。</p><p>解决这个问题，还要从源头抓起！<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f644a653603942b3488f5deed4af9477_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"465\" data-rawheight=\"459\" class=\"origin_image zh-lightbox-thumb\" width=\"465\" data-original=\"https://pic4.zhimg.com/v2-f644a653603942b3488f5deed4af9477_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;465&#39; height=&#39;459&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"465\" data-rawheight=\"459\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"465\" data-original=\"https://pic4.zhimg.com/v2-f644a653603942b3488f5deed4af9477_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f644a653603942b3488f5deed4af9477_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>打开我们要爬取的首页<code>http://yglz.tousu.hebnews.cn/l-1001-5-</code> 第1点需要确定，post的地址经过分析就是这个页面。</p><p>所以这段代码是必备的了，注意下面的post</p><div class=\"highlight\"><pre><code class=\"language-text\">response = requests.post(&#34;http://yglz.tousu.hebnews.cn/l-1001-5-&#34;)\nhtml = response.content.decode(&#34;utf-8&#34;)</code></pre></div><p>右键查看源码之后，发现源码中有一些比较重要的<code>隐藏域</code> 里面获取就是我们要的必备信息<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-251e409b88c5e4ff428207c5a62fc775_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"931\" data-rawheight=\"253\" class=\"origin_image zh-lightbox-thumb\" width=\"931\" data-original=\"https://pic2.zhimg.com/v2-251e409b88c5e4ff428207c5a62fc775_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;931&#39; height=&#39;253&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"931\" data-rawheight=\"253\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"931\" data-original=\"https://pic2.zhimg.com/v2-251e409b88c5e4ff428207c5a62fc775_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-251e409b88c5e4ff428207c5a62fc775_b.jpg\"/></figure><p><br/>没错，这些内容，我们想办法获取到就可以了<br/>基本步骤</p><ol><li>获取源码</li><li>lxml通过xpath解析隐藏域，取值</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">import requests\nfrom lxml import etree  # 从lxml中导入etree\ntry:\n    response = requests.post(&#34;http://yglz.tousu.hebnews.cn/l-1001-5-&#34;)\n    html = response.content.decode(&#34;utf-8&#34;)\nexcept Exception as e:\n    print(e)\n\ntree=etree.HTML(html)  # 解析html\nhids = tree.xpath(&#39;//input[@type=&#34;hidden&#34;]&#39;)  # 获取隐藏域\n\n# 声明一个字典，用来存储后面的数据\ncommon_param = {}\n# 循环取值\nfor ipt in hids:\n    common_param.update({ipt.get(&#34;name&#34;):ipt.get(&#34;value&#34;)})  # 这个地方可以分开写，应该会很清楚，我就不写了，总之，就是把上面获取到的隐藏域的name属性和value属性都获取到了</code></pre></div><p>上面的代码写完之后，其实已经完成了，非常核心的内容了，后面就是继续爬取了<br/>我们按照<code>post</code>要的参数补充完整其他的参数即可</p><div class=\"highlight\"><pre><code class=\"language-text\">import requests\nfrom lxml import etree  # 从lxml中导入etree\n\ntry:\n    response = requests.post(&#34;http://yglz.tousu.hebnews.cn/l-1001-5-&#34;)\n    html = response.content.decode(&#34;utf-8&#34;)\nexcept Exception as e:\n    print(e)\n\ntree=etree.HTML(html)  # 解析html\n\nhids = tree.xpath(&#39;//input[@type=&#34;hidden&#34;]&#39;)\ncommon_param = {}\nfor ipt in hids:\n    common_param.update({ipt.get(&#34;name&#34;):ipt.get(&#34;value&#34;)})\n\n##############################################################\nfor i in range(1,691):\n    common_param.update({&#34;__CALLBACKPARAM&#34;:f&#34;Load|*|{i}&#34;,  # 注意这个地方，由于我直接看到了总共有690页数据，所以直接写死了循环次数\n                       &#34;__CALLBACKID&#34;: &#34;__Page&#34;,\n                       &#34;__EVENTTARGET&#34;:&#34;&#34;,\n                       &#34;__EVENTARGUMENT&#34;:&#34;&#34;})</code></pre></div><p>到这一步，就可以抓取真实的数据了，我在下面的代码中最关键的一些地方加上注释，希望你能看懂</p><div class=\"highlight\"><pre><code class=\"language-text\">for i in range(1,691):\n    common_param.update({&#34;__CALLBACKPARAM&#34;:f&#34;Load|*|{i}&#34;,\n                       &#34;__CALLBACKID&#34;: &#34;__Page&#34;,\n                       &#34;__EVENTTARGET&#34;:&#34;&#34;,\n                       &#34;__EVENTARGUMENT&#34;:&#34;&#34;})\n\n\n    response = requests.post(&#34;http://yglz.tousu.hebnews.cn/l-1001-5-&#34;,data=common_param,headers=headers)\n    html = response.content.decode(&#34;utf-8&#34;)\n    print(&#34;*&#34;*200)\n\n    tree = etree.HTML(html)  # 解析html\n    divs = tree.xpath(&#39;//div[@class=&#34;listcon&#34;]&#39;)  # 解析列表区域div\n    for div in divs:  # 循环这个区域\n        try:\n            # 注意下面是通过div去进行的xpath查找，同时加上try方式报错\n            shouli = div.xpath(&#39;span[1]/p/a/text()&#39;)[0]  # 受理单位\n            type = div.xpath(&#39;span[2]/p/text()&#39;)[0].replace(&#34;\\n&#34;,&#34;&#34;)  # 投诉类型\n            content = div.xpath(&#39;span[3]/p/a/text()&#39;)[0]  # 投诉内容\n            datetime = div.xpath(&#39;span[4]/p/text()&#39;)[0].replace(&#34;\\n&#34;,&#34;&#34;)  # 时间\n            status = div.xpath(&#39;span[6]/p/text()&#39;)[0].replace(&#34;\\n&#34;,&#34;&#34;)  # 时间\n            one_data = {&#34;shouli&#34;:shouli,\n                        &#34;type&#34;:type,\n                        &#34;content&#34;:content,\n                        &#34;datetime&#34;:datetime,\n                        &#34;status&#34;:status,\n                        }\n            print(one_data)  # 打印数据，方便存储到mongodb里面，小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~\n            \n        except Exception as e:\n            print(&#34;内部数据报错&#34;)\n            print(div)\n            continue</code></pre></div><p>代码完成，非常爽<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-eb5a3c6444203650e4c1ac50f42ab850_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"950\" data-rawheight=\"293\" class=\"origin_image zh-lightbox-thumb\" width=\"950\" data-original=\"https://pic1.zhimg.com/v2-eb5a3c6444203650e4c1ac50f42ab850_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;950&#39; height=&#39;293&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"950\" data-rawheight=\"293\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"950\" data-original=\"https://pic1.zhimg.com/v2-eb5a3c6444203650e4c1ac50f42ab850_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-eb5a3c6444203650e4c1ac50f42ab850_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>最后抓取到了 <code>13765</code> 条数据，官方在我抓取的时候是13790，差了25条数据，没有大的影响~<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-06159dfc68cbeacde7fb7152607a37ae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"403\" data-rawheight=\"154\" class=\"content_image\" width=\"403\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;403&#39; height=&#39;154&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"403\" data-rawheight=\"154\" class=\"content_image lazy\" width=\"403\" data-actualsrc=\"https://pic3.zhimg.com/v2-06159dfc68cbeacde7fb7152607a37ae_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>数据我都存储在了 mongodb里面，关于这个如何使用，请去看我以前的代码吧~~~~</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-97aca8b94cb3f22917bf1d1f8c170f46_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"257\" data-rawheight=\"260\" class=\"content_image\" width=\"257\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;257&#39; height=&#39;260&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"257\" data-rawheight=\"260\" class=\"content_image lazy\" width=\"257\" data-actualsrc=\"https://pic3.zhimg.com/v2-97aca8b94cb3f22917bf1d1f8c170f46_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这些数据，放着以后做数据分析用了。</p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/64604491", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "Python爬虫入门教程第八讲： 蜂鸟网图片爬取之三", 
            "content": "<h2><b>蜂鸟网图片--啰嗦两句</b></h2><p>前面的教程内容量都比较大，今天写一个相对简单的，爬取的还是蜂鸟，依旧采用<code>aiohttp</code> 希望你喜欢<br/>爬取页面<code>https://tu.fengniao.com/15/</code> 本篇教程还是基于<b>学习</b>的目的，为啥选择蜂鸟，没办法，我瞎选的。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7c96008cada396a2cb3ee36244b0e03c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"282\" data-rawheight=\"248\" class=\"content_image\" width=\"282\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;282&#39; height=&#39;248&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"282\" data-rawheight=\"248\" class=\"content_image lazy\" width=\"282\" data-actualsrc=\"https://pic1.zhimg.com/v2-7c96008cada396a2cb3ee36244b0e03c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>一顿熟悉的操作之后，我找到了下面的链接<br/><code>https://tu.fengniao.com/ajax/ajaxTuPicList.php?page=2&amp;tagsId=15&amp;action=getPicLists</code></p><p>这个链接返回的是JSON格式的数据</p><ol><li>page =2页码，那么从1开始进行循环就好了</li><li>tags=15 标签名称，15是儿童，13是美女，6391是私房照，只能帮助你到这了，毕竟我这是<code>专业博客</code> ヾ(◍°∇°◍)ﾉﾞ</li><li>action=getPicLists接口地址，不变的地方</li></ol><h3>数据有了，开爬吧</h3><div class=\"highlight\"><pre><code class=\"language-text\">import aiohttp\nimport asyncio\n\nheaders = {&#34;User-Agent&#34;: &#34;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36&#34;,\n           &#34;X-Requested-With&#34;: &#34;XMLHttpRequest&#34;,\n           &#34;Accept&#34;: &#34;*/*&#34;}\n\nasync def get_source(url):\n    print(&#34;正在操作:{}&#34;.format(url))\n    conn = aiohttp.TCPConnector(verify_ssl=False)  # 防止ssl报错,其中一种写法\n    async with aiohttp.ClientSession(connector=conn) as session:  # 创建session\n        async with session.get(url, headers=headers, timeout=10) as response:  # 获得网络请求\n            if response.status == 200:  # 判断返回的请求码\n                source = await response.text()  # 使用await关键字获取返回结果\n                print(source)\n            else:\n                print(&#34;网页访问失败&#34;)\n\n\nif __name__==&#34;__main__&#34;:\n        url_format = &#34;https://tu.fengniao.com/ajax/ajaxTuPicList.php?page={}&amp;tagsId=15&amp;action=getPicLists&#34;\n        full_urllist= [url_format.format(i) for i in range(1,21)]\n        event_loop = asyncio.get_event_loop()   #创建事件循环\n        tasks = [get_source(url) for url in full_urllist]\n        results = event_loop.run_until_complete(asyncio.wait(tasks))   #等待任务结束</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fa74b73a5aa1e50060946669b823db4e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"610\" data-rawheight=\"383\" class=\"origin_image zh-lightbox-thumb\" width=\"610\" data-original=\"https://pic3.zhimg.com/v2-fa74b73a5aa1e50060946669b823db4e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;610&#39; height=&#39;383&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"610\" data-rawheight=\"383\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"610\" data-original=\"https://pic3.zhimg.com/v2-fa74b73a5aa1e50060946669b823db4e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fa74b73a5aa1e50060946669b823db4e_b.jpg\"/></figure><p><br/>上述代码在执行过程中发现，顺发了20个请求，这样子很容易就被人家判定为爬虫，可能会被封IP或者账号，我们需要对并发量进行一下控制。<br/>使<code>Semaphore</code>控制同时的并发量</p><div class=\"highlight\"><pre><code class=\"language-text\">import aiohttp\nimport asyncio\n# 代码在上面\nsema = asyncio.Semaphore(3)\nasync def get_source(url):\n    # 代码在上面\n    #######################\n# 为避免爬虫一次性请求次数太多，控制一下\nasync def x_get_source(url):\n    with(await sema):\n        await get_source(url)\n\nif __name__==&#34;__main__&#34;:\n        url_format = &#34;https://tu.fengniao.com/ajax/ajaxTuPicList.php?page={}&amp;tagsId=15&amp;action=getPicLists&#34;\n        full_urllist= [url_format.format(i) for i in range(1,21)]\n        event_loop = asyncio.get_event_loop()   #创建事件循环\n        tasks = [x_get_source(url) for url in full_urllist]\n        results = event_loop.run_until_complete(asyncio.wait(tasks))   #等待任务结束</code></pre></div><p>走一波代码，出现下面的结果，就可以啦！<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7ddd4896fb1016922a1e442037fe7761_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"868\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb\" width=\"868\" data-original=\"https://pic2.zhimg.com/v2-7ddd4896fb1016922a1e442037fe7761_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;868&#39; height=&#39;168&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"868\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"868\" data-original=\"https://pic2.zhimg.com/v2-7ddd4896fb1016922a1e442037fe7761_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7ddd4896fb1016922a1e442037fe7761_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在补充上图片下载的代码</p><div class=\"highlight\"><pre><code class=\"language-text\">import aiohttp\nimport asyncio\n\nimport json\n\n## 蜂鸟网图片--代码去上面找\nasync def get_source(url):\n    print(&#34;正在操作:{}&#34;.format(url))\n    conn = aiohttp.TCPConnector(verify_ssl=False)  # 防止ssl报错,其中一种写法\n    async with aiohttp.ClientSession(connector=conn) as session:  # 创建session\n        async with session.get(url, headers=headers, timeout=10) as response:  # 获得网络请求\n            if response.status == 200:  # 判断返回的请求码\n                source = await response.text()  # 使用await关键字获取返回结果\n                ############################################################\n                data = json.loads(source)\n                photos = data[&#34;photos&#34;][&#34;photo&#34;]\n                for p in photos:\n                    img = p[&#34;src&#34;].split(&#39;?&#39;)[0]\n                    try:\n                        async with session.get(img, headers=headers) as img_res:\n                            imgcode = await img_res.read()\n                            with open(&#34;photos/{}&#34;.format(img.split(&#39;/&#39;)[-1]), &#39;wb&#39;) as f:\n                                f.write(imgcode)\n                                f.close()\n                    except Exception as e:\n                        print(e)\n                ############################################################\n            else:\n                print(&#34;网页访问失败&#34;)\n\n\n# 为避免爬虫一次性请求次数太多，控制一下，小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~\nasync def x_get_source(url):\n    with(await sema):\n        await get_source(url)\n\n\nif __name__==&#34;__main__&#34;:\n        #### 代码去上面找</code></pre></div><p>图片下载成功，一个小爬虫，我们又写完了，美滋滋<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-dc1f84648d6611557596dccdf7f7387f_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"480\" data-rawheight=\"480\" data-thumbnail=\"https://pic4.zhimg.com/v2-dc1f84648d6611557596dccdf7f7387f_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"480\" data-original=\"https://pic4.zhimg.com/v2-dc1f84648d6611557596dccdf7f7387f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;480&#39; height=&#39;480&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"480\" data-rawheight=\"480\" data-thumbnail=\"https://pic4.zhimg.com/v2-dc1f84648d6611557596dccdf7f7387f_b.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"480\" data-original=\"https://pic4.zhimg.com/v2-dc1f84648d6611557596dccdf7f7387f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-dc1f84648d6611557596dccdf7f7387f_b.gif\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/64603029", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 0, 
            "title": "Python爬虫入门教程第七讲： 蜂鸟网图片爬取之二", 
            "content": "<h2><b>蜂鸟网图片--简介</b></h2><p>今天玩点新鲜的，使用一个新库 <code>aiohttp</code> ，利用它提高咱爬虫的爬取速度。</p><p>安装模块常规套路</p><div class=\"highlight\"><pre><code class=\"language-text\">pip install aiohttp</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-db629951c192941d11274857e892d926_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"245\" class=\"origin_image zh-lightbox-thumb\" width=\"550\" data-original=\"https://pic3.zhimg.com/v2-db629951c192941d11274857e892d926_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;550&#39; height=&#39;245&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"245\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"550\" data-original=\"https://pic3.zhimg.com/v2-db629951c192941d11274857e892d926_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-db629951c192941d11274857e892d926_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>运行之后等待，安装完毕，想要深造，那么官方文档必备 ：<a href=\"https://link.zhihu.com/?target=https%3A//aiohttp.readthedocs.io/en/stable/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">aiohttp.readthedocs.io/</span><span class=\"invisible\">en/stable/</span><span class=\"ellipsis\"></span></a></p><p>接下来就可以开始写代码了。</p><p>我们要爬取的页面，这一次选取的是</p><div class=\"highlight\"><pre><code class=\"language-text\">http://bbs.fengniao.com/forum/forum_101_1_lastpost.html</code></pre></div><p>打开页面，我们很容易就获取到了页码</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-29fdd082e2446bd4ed26eb60f32a9b68_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1215\" data-rawheight=\"617\" class=\"origin_image zh-lightbox-thumb\" width=\"1215\" data-original=\"https://pic1.zhimg.com/v2-29fdd082e2446bd4ed26eb60f32a9b68_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1215&#39; height=&#39;617&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1215\" data-rawheight=\"617\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1215\" data-original=\"https://pic1.zhimg.com/v2-29fdd082e2446bd4ed26eb60f32a9b68_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-29fdd082e2446bd4ed26eb60f32a9b68_b.jpg\"/></figure><p><br/>好久没有这么方便的看到页码了。<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b96158699aedc4a1577c84598644043e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"250\" data-rawheight=\"253\" class=\"content_image\" width=\"250\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;250&#39; height=&#39;253&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"250\" data-rawheight=\"253\" class=\"content_image lazy\" width=\"250\" data-actualsrc=\"https://pic3.zhimg.com/v2-b96158699aedc4a1577c84598644043e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>尝试用<code>aiohttp</code>访问这个页面吧，模块的引入，没有什么特殊的，采用<code>import</code>即可</p><p>如果我们需要 使用<code>Asyncio + Aiohttp</code>异步IO 编写爬虫，那么需要注意，你需要异步的方法前面加上<code>async</code></p><p class=\"ztext-empty-paragraph\"><br/></p><p>接下来，先尝试去获取一下上面那个地址的网页源码。</p><p>代码中，先声明一个fetch_img_url的函数，同时携带一个参数，这个参数也可以直接写死。</p><p><code>with</code> 上下文不在提示，自行搜索相关资料即可 (｀・ω・´)</p><p><code>aiohttp.ClientSession() as session:</code> 创建一个<code>session</code>对象，然后用该<code>session</code>对象去打开网页。<code>session</code>可以进行多项操作，比如<code>post</code>, <code>get</code>, <code>put</code>等</p><p>代码中 <code>await response.text()</code> 等待网页数据返回</p><p><code>asyncio.get_event_loop</code>创建线程，<code>run_until_complete</code>方法负责安排执行 <code>tasks</code>中的任务。<code>tasks</code>可以为单独的函数，也可以是列表。</p><div class=\"highlight\"><pre><code class=\"language-text\">import aiohttp  \nimport asyncio \n\n\nasync def fetch_img_url(num):\n    url = f&#39;http://bbs.fengniao.com/forum/forum_101_{num}_lastpost.html&#39;  # 字符串拼接\n    # 或者直接写成 url = &#39;http://bbs.fengniao.com/forum/forum_101_1_lastpost.html&#39;\n    print(url)\n    headers = {\n        &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.26 Safari/537.36 Core/1.63.6726.400 QQBrowser/10.2.2265.400&#39;,\n    }\n\n    async with aiohttp.ClientSession() as session:\n        # 获取轮播图地址\n        async with session.get(url,headers=headers) as response:\n            try:\n                html = await response.text()   # 获取到网页源码\n                print(html)\n                \n            except Exception as e:\n                print(&#34;基本错误&#34;)\n                print(e)\n\n# 这部分你可以直接临摹\nloop = asyncio.get_event_loop()\ntasks = asyncio.ensure_future(fetch_img_url(1))\nresults = loop.run_until_complete(tasks)</code></pre></div><p>上面代码最后一部分也可以写成</p><div class=\"highlight\"><pre><code class=\"language-text\">loop = asyncio.get_event_loop()\ntasks =  [fetch_img_url(1)]\nresults = loop.run_until_complete(asyncio.wait(tasks))</code></pre></div><p>好了，如果你已经成果的获取到了源码，那么距离最终的目的就差那么一丢丢了。<br/>修改代码为批量获取10页。<br/>只需要修改<code>tasks</code>即可,在此运行，看到如下结果</p><div class=\"highlight\"><pre><code class=\"language-text\">tasks =  [fetch_img_url(num) for num in range(1, 10)]</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1ca83fce22598578f1585fd568bf3d2b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"498\" data-rawheight=\"211\" class=\"origin_image zh-lightbox-thumb\" width=\"498\" data-original=\"https://pic4.zhimg.com/v2-1ca83fce22598578f1585fd568bf3d2b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;498&#39; height=&#39;211&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"498\" data-rawheight=\"211\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"498\" data-original=\"https://pic4.zhimg.com/v2-1ca83fce22598578f1585fd568bf3d2b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1ca83fce22598578f1585fd568bf3d2b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>下面的一系列操作和上一篇博客非常类似，找规律。<br/>随便打开一个页面</p><div class=\"highlight\"><pre><code class=\"language-text\">http://bbs.fengniao.com/forum/forum_101_4_lastpost.html</code></pre></div><p>点击一张图片，进入内页，在点击内页的一张图片，进入到一个轮播页面<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d548b41ab2872795f21f800e849c6dbc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1239\" data-rawheight=\"255\" class=\"origin_image zh-lightbox-thumb\" width=\"1239\" data-original=\"https://pic1.zhimg.com/v2-d548b41ab2872795f21f800e849c6dbc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1239&#39; height=&#39;255&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1239\" data-rawheight=\"255\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1239\" data-original=\"https://pic1.zhimg.com/v2-d548b41ab2872795f21f800e849c6dbc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d548b41ab2872795f21f800e849c6dbc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f9b98af7857ef37ae638d30e1df5c30f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"843\" data-rawheight=\"755\" class=\"origin_image zh-lightbox-thumb\" width=\"843\" data-original=\"https://pic4.zhimg.com/v2-f9b98af7857ef37ae638d30e1df5c30f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;843&#39; height=&#39;755&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"843\" data-rawheight=\"755\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"843\" data-original=\"https://pic4.zhimg.com/v2-f9b98af7857ef37ae638d30e1df5c30f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f9b98af7857ef37ae638d30e1df5c30f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>再次点击进入图片播放页面</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a65ef4781fcc727ace358200fb2716d1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1214\" data-rawheight=\"629\" class=\"origin_image zh-lightbox-thumb\" width=\"1214\" data-original=\"https://pic2.zhimg.com/v2-a65ef4781fcc727ace358200fb2716d1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1214&#39; height=&#39;629&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1214\" data-rawheight=\"629\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1214\" data-original=\"https://pic2.zhimg.com/v2-a65ef4781fcc727ace358200fb2716d1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a65ef4781fcc727ace358200fb2716d1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>最后我们在图片播放页面，找到源码中发现了所有的图片链接，那么问题出来了，如何从上面的第一个链接，转变成轮播图的链接？？？<br/>下面的源码是在 <code>http://bbs.fengniao.com/forum/pic/slide_101_10408464_89383854.html</code> 右键查看源码。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d624f598779d4b573fc1006e4d2422bf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"697\" data-rawheight=\"465\" class=\"origin_image zh-lightbox-thumb\" width=\"697\" data-original=\"https://pic4.zhimg.com/v2-d624f598779d4b573fc1006e4d2422bf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;697&#39; height=&#39;465&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"697\" data-rawheight=\"465\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"697\" data-original=\"https://pic4.zhimg.com/v2-d624f598779d4b573fc1006e4d2422bf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d624f598779d4b573fc1006e4d2422bf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>继续分析吧~~~~ <code>ヾ(=･ω･=)o</code></p><div class=\"highlight\"><pre><code class=\"language-text\">http://bbs.fengniao.com/forum/forum_101_4_lastpost.html\n转变成下面的链接？\nhttp://bbs.fengniao.com/forum/pic/slide_101_10408464_89383854.html</code></pre></div><p>继续看第一个链接，我们使用F12开发者工具，去抓取一个图片看看。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0c539e94999d77013f8c098456b02745_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"943\" data-rawheight=\"262\" class=\"origin_image zh-lightbox-thumb\" width=\"943\" data-original=\"https://pic2.zhimg.com/v2-0c539e94999d77013f8c098456b02745_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;943&#39; height=&#39;262&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"943\" data-rawheight=\"262\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"943\" data-original=\"https://pic2.zhimg.com/v2-0c539e94999d77013f8c098456b02745_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0c539e94999d77013f8c098456b02745_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>图片中标黄色框的位置，发现了我们想要的数字，那么好了，我们只需要通过正则表达式把他们匹配出来就好了。<br/>代码在下面<code>####</code>的位置，需要注意的是，我采用的原始的正则匹配，在编写正则表达式的过程中，我发现一步竟然没有完整匹配，只能分成两个步骤了，你可以看一下具体的细节<code>o(╥﹏╥)o</code></p><ol><li>查找所有的图片<code>&lt;div class=&#34;picList&#34;&gt;</code></li><li>获取我们想要的两部分数字</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">async def fetch_img_url(num):\n    url = f&#39;http://bbs.fengniao.com/forum/forum_101_{num}_lastpost.html&#39;\n    print(url)\n    headers = {\n        &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.26 Safari/537.36 Core/1.63.6726.400 QQBrowser/10.2.2265.400&#39;,\n    }\n\n    async with aiohttp.ClientSession() as session:\n        # 获取轮播图地址\n        async with session.get(url,headers=headers) as response:\n            try:\n                ###############################################\n                url_format = &#34;http://bbs.fengniao.com/forum/pic/slide_101_{0}_{1}.html&#34;\n                html = await response.text()   # 获取到网页源码\n                pattern = re.compile(&#39;&lt;div class=&#34;picList&#34;&gt;([\\s\\S.]*?)&lt;/div&gt;&#39;)\n                first_match = pattern.findall(html)\n                href_pattern = re.compile(&#39;href=&#34;/forum/(\\d+?)_p(\\d+?)\\.html&#39;)\n                urls = [url_format.format(href_pattern.search(url).group(1), href_pattern.search(url).group(2)) for url in first_match]\n                ##############################################\n\n            except Exception as e:\n                print(&#34;基本错误&#34;)\n                print(e)</code></pre></div><p>代码完成，我们已经获取到，我们想要的URL了，下面继续读取URL内部信息，然后匹配我们想要的图片链接</p><div class=\"highlight\"><pre><code class=\"language-text\">async def fetch_img_url(num):\n    # 去抄上面的代码\n    async with aiohttp.ClientSession() as session:\n        # 获取轮播图地址\n        async with session.get(url,headers=headers) as response:\n            try:\n                #去抄上面的代码去吧\n                ################################################################\n                for img_slider in urls:\n                    try:\n                        async with session.get(img_slider, headers=headers) as slider:\n                            slider_html = await slider.text()   # 获取到网页源码\n                            try:\n                                pic_list_pattern = re.compile(&#39;var picList = \\[(.*)?\\];&#39;)\n                                pic_list = &#34;[{}]&#34;.format(pic_list_pattern.search(slider_html).group(1))\n                                pic_json = json.loads(pic_list)  # 图片列表已经拿到\n                                print(pic_json)\n                            except Exception as e:\n                                print(&#34;代码调试错误&#34;)\n                                print(pic_list)\n                                print(&#34;*&#34;*100)\n                                print(e)\n\n                    except Exception as e:\n                        print(&#34;获取图片列表错误&#34;)\n                        print(img_slider)\n                        print(e)\n                        continue\n                ################################################################\n\n\n                print(&#34;{}已经操作完毕&#34;.format(url))\n            except Exception as e:\n                print(&#34;基本错误&#34;)\n                print(e)</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0ffedeb02a78179d189b78f3112179cb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"958\" data-rawheight=\"305\" class=\"origin_image zh-lightbox-thumb\" width=\"958\" data-original=\"https://pic4.zhimg.com/v2-0ffedeb02a78179d189b78f3112179cb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;958&#39; height=&#39;305&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"958\" data-rawheight=\"305\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"958\" data-original=\"https://pic4.zhimg.com/v2-0ffedeb02a78179d189b78f3112179cb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0ffedeb02a78179d189b78f3112179cb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>图片最终的JSON已经拿到，最后一步，下载图片，当当当~~~~，一顿迅猛的操作之后，图片就拿下来了。小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~</p><div class=\"highlight\"><pre><code class=\"language-text\">async def fetch_img_url(num):\n    # 代码去上面找\n    async with aiohttp.ClientSession() as session:\n        # 获取轮播图地址\n        async with session.get(url,headers=headers) as response:\n            try:\n                # 代码去上面找\n                for img_slider in urls:\n                    try:\n                        async with session.get(img_slider, headers=headers) as slider:\n                            # 代码去上面找\n                            ##########################################################\n                            for img in pic_json:\n                                try:\n                                    img = img[&#34;downloadPic&#34;]\n                                    async with session.get(img, headers=headers) as img_res:\n                                        imgcode = await img_res.read()  # 图片读取\n                                        with open(&#34;images/{}&#34;.format(img.split(&#39;/&#39;)[-1]), &#39;wb&#39;) as f:\n                                            f.write(imgcode)\n                                            f.close()\n                                except Exception as e:\n                                    print(&#34;图片下载错误&#34;)\n                                    print(e)\n                                    continue\n                            ###############################################################\n\n                    except Exception as e:\n                        print(&#34;获取图片列表错误&#34;)\n                        print(img_slider)\n                        print(e)\n                        continue\n                print(&#34;{}已经操作完毕&#34;.format(url))\n            except Exception as e:\n                print(&#34;基本错误&#34;)\n                print(e)</code></pre></div><p>图片会在你提前写好的<code>images</code>文件夹里面快速的生成<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1a44251f10b7e0465b918106878104d9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"745\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb\" width=\"745\" data-original=\"https://pic2.zhimg.com/v2-1a44251f10b7e0465b918106878104d9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;745&#39; height=&#39;356&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"745\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"745\" data-original=\"https://pic2.zhimg.com/v2-1a44251f10b7e0465b918106878104d9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1a44251f10b7e0465b918106878104d9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><code>tasks</code>最多可以开1024协程，但是建议你开100个就OK了，太多并发，人家服务器吃不消。</p><p>以上操作执行完毕，在添加一些细节，比如保存到指定文件夹，就OK了。</p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/64598778", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 1, 
            "title": "Python爬虫入门教程第六讲：蜂鸟网图片爬取之一", 
            "content": "<h2><b>1. 蜂鸟网图片--简介</b></h2><p>国庆假日结束了，新的工作又开始了，今天我们继续爬取一个网站，这个网站为 <code>http://image.fengniao.com/</code> ，蜂鸟一个摄影大牛聚集的地方，本教程请用来学习，不要用于商业目的，不出意外，蜂鸟是有版权保护的网站。<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-921f42c531b98eb568a6e589837d8294_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"228\" data-rawheight=\"180\" class=\"content_image\" width=\"228\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;228&#39; height=&#39;180&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"228\" data-rawheight=\"180\" class=\"content_image lazy\" width=\"228\" data-actualsrc=\"https://pic1.zhimg.com/v2-921f42c531b98eb568a6e589837d8294_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>2. 蜂鸟网图片--网站分析</b></h2><p>第一步，分析要爬取的网站有没有方法爬取，打开页面，找分页</p><div class=\"highlight\"><pre><code class=\"language-text\">http://image.fengniao.com/index.php?action=getList&amp;class_id=192&amp;sub_classid=0&amp;page=1&amp;not_in_id=5352384,5352410\nhttp://image.fengniao.com/index.php?action=getList&amp;class_id=192&amp;sub_classid=0&amp;page=2&amp;not_in_id=5352384,5352410\nhttp://image.fengniao.com/index.php?action=getList&amp;class_id=192&amp;sub_classid=0&amp;page=3&amp;not_in_id=5352384,5352410\nhttp://image.fengniao.com/index.php?action=getList&amp;class_id=192&amp;sub_classid=0&amp;page=4&amp;not_in_id=5352384,5352410</code></pre></div><p>上面的页面发现一个关键的参数<code>page=1</code>这个就是页码了，但是另一个比较头疼的问题是，他没有最后的页码，这样我们没有办法确定循环次数，所以后面的代码编写中，只能使用<code>while</code>了</p><p>这个地址返回的是JSON格式的数据，这个对爬虫来说，非常友好！省的我们用正则表达式分析了。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-743d5eb746f26f5c53ce3bf9489636da_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"802\" data-rawheight=\"326\" class=\"origin_image zh-lightbox-thumb\" width=\"802\" data-original=\"https://pic3.zhimg.com/v2-743d5eb746f26f5c53ce3bf9489636da_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;802&#39; height=&#39;326&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"802\" data-rawheight=\"326\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"802\" data-original=\"https://pic3.zhimg.com/v2-743d5eb746f26f5c53ce3bf9489636da_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-743d5eb746f26f5c53ce3bf9489636da_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>分析这个页面的头文件，查阅是否有反爬措施</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2d2fed5f2cab8cd0fa11199348d34706_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"793\" data-rawheight=\"237\" class=\"origin_image zh-lightbox-thumb\" width=\"793\" data-original=\"https://pic3.zhimg.com/v2-2d2fed5f2cab8cd0fa11199348d34706_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;793&#39; height=&#39;237&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"793\" data-rawheight=\"237\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"793\" data-original=\"https://pic3.zhimg.com/v2-2d2fed5f2cab8cd0fa11199348d34706_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2d2fed5f2cab8cd0fa11199348d34706_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>发现除了HOST和User-Agent以外，没有特殊的点，大网站就是任性，没啥反爬，可能压根不在乎这个事情。</p><p>第二步，分析图片详情页面，在我们上面获取到的JSON中，找到关键地址<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b07855fdf7c97873b1ccfd1939fef2d8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"696\" data-rawheight=\"320\" class=\"origin_image zh-lightbox-thumb\" width=\"696\" data-original=\"https://pic1.zhimg.com/v2-b07855fdf7c97873b1ccfd1939fef2d8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;696&#39; height=&#39;320&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"696\" data-rawheight=\"320\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"696\" data-original=\"https://pic1.zhimg.com/v2-b07855fdf7c97873b1ccfd1939fef2d8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b07855fdf7c97873b1ccfd1939fef2d8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>关键地址打开之后，这个地方有一个比较骚的操作了，上面图片中标注的URL选的不好，恰好是一个文章了，我们要的是组图，重新提供一个新链接 <code>http://image.fengniao.com/slide/535/5352130_1.html#p=1</code></p><p>打开页面，你可能直接去找规律了，找到下面的一堆链接，但是这个操作就有点复杂了，我们查阅上述页面的源码</p><div class=\"highlight\"><pre><code class=\"language-text\">http://image.fengniao.com/slide/535/5352130_1.html#p=1\nhttp://image.fengniao.com/slide/535/5352130_1.html#p=2\nhttp://image.fengniao.com/slide/535/5352130_1.html#p=3\n....</code></pre></div><p>网页源码中发现了，这么一块区域<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6becaf873dd65f5c4cf101f03ced4e12_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1397\" data-rawheight=\"483\" class=\"origin_image zh-lightbox-thumb\" width=\"1397\" data-original=\"https://pic3.zhimg.com/v2-6becaf873dd65f5c4cf101f03ced4e12_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1397&#39; height=&#39;483&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1397\" data-rawheight=\"483\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1397\" data-original=\"https://pic3.zhimg.com/v2-6becaf873dd65f5c4cf101f03ced4e12_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6becaf873dd65f5c4cf101f03ced4e12_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>大胆的猜测一下，这个应该是图片的JSON，只是他打印在了HTML中，我们只需要用正则表达式进行一下匹配就好了，匹配到之后，然后进行下载。</p><p>第三步，开始撸代码。<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-56ff7fcee0718346074c4b7e092b708d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"232\" data-rawheight=\"239\" class=\"content_image\" width=\"232\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;232&#39; height=&#39;239&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"232\" data-rawheight=\"239\" class=\"content_image lazy\" width=\"232\" data-actualsrc=\"https://pic2.zhimg.com/v2-56ff7fcee0718346074c4b7e092b708d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>3. 蜂鸟网图片--写代码</b></h2><div class=\"highlight\"><pre><code class=\"language-text\">from http_help import R  # 这个文件自己去上篇博客找，或者去github找\nimport threading\nimport time\nimport json\nimport re\n\nimg_list = []\nimgs_lock = threading.Lock()  #图片操作锁\n\n\n# 生产者类\nclass Product(threading.Thread):\n\n    def __init__(self):\n        threading.Thread.__init__(self)\n\n        self.__headers = {&#34;Referer&#34;:&#34;http://image.fengniao.com/&#34;,\n                          &#34;Host&#34;: &#34;image.fengniao.com&#34;,\n                          &#34;X-Requested-With&#34;:&#34;XMLHttpRequest&#34;\n                          }\n        #链接模板\n        self.__start = &#34;http://image.fengniao.com/index.php?action=getList&amp;class_id=192&amp;sub_classid=0&amp;page={}&amp;not_in_id={}&#34;\n        self.__res = R(headers=self.__headers)\n\n\n    def run(self):\n\n        # 因为不知道循环次数，所有采用while循环\n        index = 2 #起始页码设置为1\n        not_in = &#34;5352384,5352410&#34;\n        while True:\n            url  = self.__start.format(index,not_in)\n            print(&#34;开始操作:{}&#34;.format(url))\n            index += 1\n\n            content = self.__res.get_content(url,charset=&#34;gbk&#34;)\n\n            if content is None:\n                print(&#34;数据可能已经没有了====&#34;)\n                continue\n\n            time.sleep(3)  # 睡眠3秒\n            json_content = json.loads(content)\n\n            if json_content[&#34;status&#34;] == 1:\n                for item in json_content[&#34;data&#34;]:\n                    title = item[&#34;title&#34;]\n                    child_url =  item[&#34;url&#34;]   # 获取到链接之后\n\n                    img_content = self.__res.get_content(child_url,charset=&#34;gbk&#34;)\n\n                    pattern = re.compile(&#39;&#34;pic_url_1920_b&#34;:&#34;(.*?)&#34;&#39;)\n                    imgs_json = pattern.findall(img_content)\n                    if len(imgs_json) &gt; 0:\n\n                        if imgs_lock.acquire():\n                            img_list.append({&#34;title&#34;:title,&#34;urls&#34;:imgs_json})   # 这个地方，我用的是字典+列表的方式，主要是想后面生成文件夹用，你可以进行改造\n                            imgs_lock.release()</code></pre></div><p>上面的链接已经生成，下面就是下载图片了，也非常简单</p><div class=\"highlight\"><pre><code class=\"language-text\"># 消费者\nclass Consumer(threading.Thread):\n    def __init__(self):\n        threading.Thread.__init__(self)\n        self.__res = R()\n\n    def run(self):\n\n        while True:\n            if len(img_list) &lt;= 0:\n                continue  # 进入下一次循环\n\n            if imgs_lock.acquire():\n\n                data = img_list[0]\n                del img_list[0]  # 删除第一项\n\n                imgs_lock.release()\n\n            urls =[url.replace(&#34;\\\\&#34;,&#34;&#34;) for url in data[&#34;urls&#34;]]\n\n            # 创建文件目录\n            for item_url in urls:\n               try:\n                   file =  self.__res.get_file(item_url)\n                   # 记得在项目根目录先把fengniaos文件夹创建完毕，小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~\n                   with open(&#34;./fengniaos/{}&#34;.format(str(time.time())+&#34;.jpg&#34;), &#34;wb+&#34;) as f:\n                       f.write(file)\n               except Exception as e:\n                   print(e)</code></pre></div><p>代码走起，结果<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-520198714bd95a6e4b202b0a1687e2a8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1151\" data-rawheight=\"674\" class=\"origin_image zh-lightbox-thumb\" width=\"1151\" data-original=\"https://pic1.zhimg.com/v2-520198714bd95a6e4b202b0a1687e2a8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1151&#39; height=&#39;674&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1151\" data-rawheight=\"674\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1151\" data-original=\"https://pic1.zhimg.com/v2-520198714bd95a6e4b202b0a1687e2a8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-520198714bd95a6e4b202b0a1687e2a8_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/64597564", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 1, 
            "title": "Python爬虫入门教程第五讲： 27270图片爬取", 
            "content": "<p>今天继续爬取一个网站，<code>http://www.27270.com/ent/meinvtupian/</code> 这个网站具备反爬，so我们下载的代码有些地方处理的也不是很到位，大家重点学习思路，有啥建议可以在评论的地方跟我说说。<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-138614e3b24c2125bd6990796a922495_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"249\" data-rawheight=\"275\" class=\"content_image\" width=\"249\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;249&#39; height=&#39;275&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"249\" data-rawheight=\"275\" class=\"content_image lazy\" width=\"249\" data-actualsrc=\"https://pic2.zhimg.com/v2-138614e3b24c2125bd6990796a922495_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>为了以后的网络请求操作方向，我们这次简单的进行一些代码的封装操作。</p><p>在这里你可以先去安装一个叫做 <code>retrying</code> 的模块</p><div class=\"highlight\"><pre><code class=\"language-text\">pip install retrying</code></pre></div><p>这个模块的具体使用，自己去百度吧。嘿嘿哒~</p><p>在这里我使用了一个随机产生user_agent的方法</p><div class=\"highlight\"><pre><code class=\"language-text\">import requests\nfrom retrying import retry\nimport random\nimport datetime\n\nclass R:\n\n    def __init__(self,method=&#34;get&#34;,params=None,headers=None,cookies=None):\n        # do something\n\n\n    def get_headers(self):\n        user_agent_list = [ \\\n            &#34;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1&#34; \\\n            &#34;Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1&#34;, \\\n            &#34;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3&#34;, \\\n            &#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3&#34;, \\\n            &#34;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24&#34;\n        ]\n        UserAgent = random.choice(user_agent_list)\n        headers = {&#39;User-Agent&#39;: UserAgent}\n        return headers\n    #other code</code></pre></div><p><code>retrying</code> 最简单的使用就是给你想不断重试的方法加上 装饰器 @retry</p><p>在这里，我希望网络请求模块尝试3次之后，在报错！</p><p>同时在<code>R类</code>初始化方法中增加一些必备的参数，你可以直接看下面的代码</p><p><code>__retrying_requests</code> 方法为私有方法，其中根据<code>get</code>和<code>post</code>方式进行逻辑判断</p><div class=\"highlight\"><pre><code class=\"language-text\">import requests\nfrom retrying import retry\nimport random\nimport datetime\n\nclass R:\n\n    def __init__(self,method=&#34;get&#34;,params=None,headers=None,cookies=None):\n        #do something\n\n    def get_headers(self):\n        # do something\n    @retry(stop_max_attempt_number=3)\n    def __retrying_requests(self,url):\n        if self.__method == &#34;get&#34;:\n            response = requests.get(url,headers=self.__headers,cookies=self.__cookies,timeout=3)\n        else:\n            response = requests.post(url,params=self.__params,headers=self.__headers,cookies=self.__cookies,timeout=3)\n        return response.content\n\n   \n    # other code</code></pre></div><p>网络请求的方法已经声明完毕，并且返回 <code>response.content</code> 数据流</p><p>下面基于这个私有方法，增加一个获取网络文本的方法和一个获取网络文件的方法。同步完善类的初始化方法，在开发中发现，我们要爬取的网页编码是<code>gb2312</code> 所以还需要给某些方法增加一个<code>编码参数</code></p><div class=\"highlight\"><pre><code class=\"language-text\">import requests\nfrom retrying import retry\nimport random\nimport datetime\n\nclass R:\n    # 类的初始化方法\n    def __init__(self,method=&#34;get&#34;,params=None,headers=None,cookies=None):\n        self.__method = method\n        myheaders = self.get_headers()\n        if headers is not None:\n            myheaders.update(headers)\n        self.__headers = myheaders\n        self.__cookies = cookies\n        self.__params = params\n\n\n    def get_headers(self):\n       # do something\n\n    @retry(stop_max_attempt_number=3)\n    def __retrying_requests(self,url):\n        # do something\n\n    # get请求\n    def get_content(self,url,charset=&#34;utf-8&#34;):\n        try:\n            html_str = self.__retrying_requests(url).decode(charset)\n        except:\n            html_str = None\n        return html_str\n\n    def get_file(self,file_url):\n        try:\n            file = self.__retrying_requests(file_url)\n        except:\n            file = None\n        return file</code></pre></div><p>到此，这个<code>R类</code>已经被我们完善了，完整的代码，你应该从上面拼凑起来，你也可以直接翻到文章最后面，去github上直接查阅。</p><p>接下来，就是比较重要的爬虫代码部分了。这一次，我们可以简单的使用一下类和对象，并且加上简单的多线程操作。</p><p>首先，创建一个 <code>ImageList</code> 类，这个类第一件事情，需要获取我们爬取页面的总页码数目</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e8ec14c5457b46857d46a7334c5eda2d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1529\" data-rawheight=\"580\" class=\"origin_image zh-lightbox-thumb\" width=\"1529\" data-original=\"https://pic2.zhimg.com/v2-e8ec14c5457b46857d46a7334c5eda2d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1529&#39; height=&#39;580&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1529\" data-rawheight=\"580\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1529\" data-original=\"https://pic2.zhimg.com/v2-e8ec14c5457b46857d46a7334c5eda2d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e8ec14c5457b46857d46a7334c5eda2d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这个步骤比较简单</p><ol><li>获取网页源码</li><li>正则匹配末页元素</li><li>提取数字</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">import http_help as hh   # 这个http_help 是我上面写到的那个R类\nimport re\nimport threading\nimport time\nimport os\nimport requests\n\n# 获取所有待爬取的URL列表\nclass ImageList():\n    def __init__(self):\n        self.__start = &#34;http://www.27270.com/ent/meinvtupian/list_11_{}.html&#34;  # URL模板\n        # 头文件\n        self.__headers = {&#34;Referer&#34;:&#34;http://www.27270.com/ent/meinvtupian/&#34;,\n                          &#34;Host&#34;:&#34;www.27270.com&#34;\n                          }\n        self.__res = hh.R(headers=self.__headers)  # 初始化访问请求\n    def run(self):\n        page_count =  int(self.get_page_count())\n\n        if page_count==0:\n            return\n        urls = [self.__start.format(i) for i in range(1,page_count)]\n        return urls\n\n\n    # 正则表达式匹配末页，分析页码\n    def get_page_count(self):\n        # 注意这个地方需要传入编码\n        content = self.__res.get_content(self.__start.format(&#34;1&#34;),&#34;gb2312&#34;)\n        pattern = re.compile(&#34;&lt;li&gt;&lt;a href=&#39;list_11_(\\d+?).html&#39; target=&#39;_self&#39;&gt;末页&lt;/a&gt;&lt;/li&gt;&#34;)\n        search_text = pattern.search(content)\n        if search_text is not None:\n            count = search_text.group(1)\n            return count\n        else:\n            return 0\nif __name__ == &#39;__main__&#39;:\n    img = ImageList()\n    urls = img.run()</code></pre></div><p>上面的代码注意<code>get_page_count</code>方法，该方法已经获取到了末尾的页码</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3467811e445521aa7770ec0fc69e440a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"529\" data-rawheight=\"97\" class=\"origin_image zh-lightbox-thumb\" width=\"529\" data-original=\"https://pic3.zhimg.com/v2-3467811e445521aa7770ec0fc69e440a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;529&#39; height=&#39;97&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"529\" data-rawheight=\"97\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"529\" data-original=\"https://pic3.zhimg.com/v2-3467811e445521aa7770ec0fc69e440a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-3467811e445521aa7770ec0fc69e440a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们在<code>run</code>方法内部，通过一个列表生成器</p><div class=\"highlight\"><pre><code class=\"language-text\">urls = [self.__start.format(i) for i in range(1,page_count)]</code></pre></div><p>批量把要爬取的所有链接都生成完毕。</p><h2><b>27270图片----分析上面爬取到的URL列表，捕获详情页</b></h2><p>我们采用生产者和消费者模型，就是一个抓取链接图片，一个下载图片，采用多线程的方式进行操作，需要首先引入</p><div class=\"highlight\"><pre><code class=\"language-text\">import threading\nimport time</code></pre></div><p>完整代码如下</p><div class=\"highlight\"><pre><code class=\"language-text\">import http_help as hh\nimport re\nimport threading\nimport time\nimport os\nimport requests\n\nurls_lock = threading.Lock()  #url操作锁\nimgs_lock = threading.Lock()  #图片操作锁\n\nimgs_start_urls = []\n\n\nclass Product(threading.Thread):\n    # 类的初始化方法\n    def __init__(self,urls):\n        threading.Thread.__init__(self)\n        self.__urls = urls\n        self.__headers = {&#34;Referer&#34;:&#34;http://www.27270.com/ent/meinvtupian/&#34;,\n                          &#34;Host&#34;:&#34;www.27270.com&#34;\n                          }\n\n        self.__res = hh.R(headers=self.__headers)\n\n    # 链接抓取失败之后重新加入urls列表中\n    def add_fail_url(self,url):\n        print(&#34;{}该URL抓取失败&#34;.format(url))\n        global urls_lock\n        if urls_lock.acquire():\n            self.__urls.insert(0, url)\n            urls_lock.release()  # 解锁\n    \n    # 线程主要方法\n    def run(self):\n        print(&#34;*&#34;*100)\n        while True:\n            global urls_lock,imgs_start_urls\n            if len(self.__urls)&gt;0:\n                if urls_lock.acquire():   # 锁定\n                    last_url = self.__urls.pop()   # 获取urls里面最后一个url，并且删除\n                    urls_lock.release()  # 解锁\n\n                print(&#34;正在操作{}&#34;.format(last_url))\n\n                content = self.__res.get_content(last_url,&#34;gb2312&#34;)   # 页面注意编码是gb2312其他格式报错\n                if content is not  None:\n                    html = self.get_page_list(content)\n\n                    if len(html) == 0:\n                        self.add_fail_url(last_url)\n                    else:\n                        if imgs_lock.acquire():\n                            imgs_start_urls.extend(html)    # 爬取到图片之后，把他放在待下载的图片列表里面\n                            imgs_lock.release()\n\n                    time.sleep(5)\n                else:\n                    self.add_fail_url(last_url)\n\n            else:\n                print(&#34;所有链接已经运行完毕&#34;)\n                break\n\n\n\n\n\n    def get_page_list(self,content):\n        # 正则表达式\n        pattern = re.compile(&#39;&lt;li&gt; &lt;a href=&#34;(.*?)&#34; title=&#34;(.*?)&#34; class=&#34;MMPic&#34; target=&#34;_blank&#34;&gt;.*?&lt;/li&gt;&#39;)\n        list_page = re.findall(pattern, content)\n\n        return list_page</code></pre></div><p>上述代码中比较重要的有<br/>threading.Lock() 锁的使用，在多个线程之间操作全局变量，需要进行及时的锁定；<br/>其他的注意内容，我已经添加在注释里面，只要你按着步骤一点点的写，并且加入一些自己微妙的理解，就可以搞定。</p><p>到现在为止，我们已经抓取到了所有的图片地址，我把他存放在了一个全局的变量里面 <code>imgs_start_urls</code><br/>那么现在又来了</p><p>这个列表里面存放的是 <code>http://www.27270.com/ent/meinvtupian/2018/298392.html</code> 这样的地址，当你打开这个页面之后，你会发现只有一张图片 ，并且下面有个分页。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c4dcaa1451a84e8b8f046be028559c57_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1225\" data-rawheight=\"630\" class=\"origin_image zh-lightbox-thumb\" width=\"1225\" data-original=\"https://pic4.zhimg.com/v2-c4dcaa1451a84e8b8f046be028559c57_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1225&#39; height=&#39;630&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1225\" data-rawheight=\"630\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1225\" data-original=\"https://pic4.zhimg.com/v2-c4dcaa1451a84e8b8f046be028559c57_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c4dcaa1451a84e8b8f046be028559c57_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1432429de0f50113a002fcbc87564bd1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1153\" data-rawheight=\"447\" class=\"origin_image zh-lightbox-thumb\" width=\"1153\" data-original=\"https://pic2.zhimg.com/v2-1432429de0f50113a002fcbc87564bd1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1153&#39; height=&#39;447&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1153\" data-rawheight=\"447\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1153\" data-original=\"https://pic2.zhimg.com/v2-1432429de0f50113a002fcbc87564bd1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1432429de0f50113a002fcbc87564bd1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>点击分页之后，就知道规律了</p><div class=\"highlight\"><pre><code class=\"language-text\">http://www.27270.com/ent/meinvtupian/2018/298392.html \nhttp://www.27270.com/ent/meinvtupian/2018/298392_2.html \nhttp://www.27270.com/ent/meinvtupian/2018/298392_3.html \nhttp://www.27270.com/ent/meinvtupian/2018/298392_4.html \n....</code></pre></div><p>当你进行多次尝试之后，你会发现，后面的链接完全可以靠拼接完成，如果没有这个页面，那么他会显示？</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3135cb4932b77b81eb5c2eb0cea08f6b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1069\" data-rawheight=\"359\" class=\"origin_image zh-lightbox-thumb\" width=\"1069\" data-original=\"https://pic4.zhimg.com/v2-3135cb4932b77b81eb5c2eb0cea08f6b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1069&#39; height=&#39;359&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1069\" data-rawheight=\"359\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1069\" data-original=\"https://pic4.zhimg.com/v2-3135cb4932b77b81eb5c2eb0cea08f6b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3135cb4932b77b81eb5c2eb0cea08f6b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>好了，如果你进行了上面的操作，你应该知道接下来怎么实现啦！</p><p>我把所有的代码，都直接贴在下面，还是用注释的方式给大家把最重要的地方标注出来</p><div class=\"highlight\"><pre><code class=\"language-text\">class Consumer(threading.Thread):\n    # 初始化\n    def __init__(self):\n        threading.Thread.__init__(self)\n        self.__headers = {&#34;Referer&#34;: &#34;http://www.27270.com/ent/meinvtupian/&#34;,\n                          &#34;Host&#34;: &#34;www.27270.com&#34;}\n        self.__res = hh.R(headers=self.__headers)\n\n    # 图片下载方法\n    def download_img(self,filder,img_down_url,filename):\n        file_path = &#34;./downs/{}&#34;.format(filder)\n        \n        # 判断目录是否存在，存在创建\n        if not os.path.exists(file_path):\n            os.mkdir(file_path)  # 创建目录\n\n        if os.path.exists(&#34;./downs/{}/{}&#34;.format(filder,filename)):\n            return\n        else:\n            try:\n                # 这个地方host设置是个坑，因为图片为了防止盗链，存放在另一个服务器上面\n                img = requests.get(img_down_url,headers={&#34;Host&#34;:&#34;t2.hddhhn.com&#34;},timeout=3)\n            except Exception as e:\n                print(e)\n\n            print(&#34;{}写入图片&#34;.format(img_down_url))\n            try:\n                # 图片写入不在赘述\n                with open(&#34;./downs/{}/{}&#34;.format(filder,filename),&#34;wb+&#34;) as f:\n                    f.write(img.content)\n            except Exception as e:\n                print(e)\n                return\n\n\n\n\n\n    def run(self):\n\n        while True:\n            global imgs_start_urls,imgs_lock\n\n            if len(imgs_start_urls)&gt;0:\n                if imgs_lock.acquire():  # 锁定\n                    img_url = imgs_start_urls[0]   #获取到链接之后\n                    del imgs_start_urls[0]  # 删掉第0项\n                    imgs_lock.release()  # 解锁\n            else:\n                continue\n\n            # http://www.27270.com/ent/meinvtupian/2018/295631_1.html\n\n            #print(&#34;图片开始下载&#34;)\n            img_url = img_url[0]\n            start_index = 1\n            base_url = img_url[0:img_url.rindex(&#34;.&#34;)]    # 字符串可以当成列表进行切片操作\n\n            while True:\n\n                img_url =&#34;{}_{}.html&#34;.format(base_url,start_index)   # url拼接\n                content = self.__res.get_content(img_url,charset=&#34;gbk&#34;)   # 这个地方获取内容，采用了gbk编码\n                if content is not None:\n                    pattern = re.compile(&#39;&lt;div class=&#34;articleV4Body&#34; id=&#34;picBody&#34;&gt;[\\s\\S.]*?img alt=&#34;(.*?)&#34;.*? src=&#34;(.*?)&#34; /&gt;&#39;)\n                    # 匹配图片，匹配不到就代表本次操作已经完毕\n                    img_down_url = pattern.search(content)  # 获取到了图片地址\n\n                    if img_down_url is not None:\n                        filder = img_down_url.group(1)\n                        img_down_url = img_down_url.group(2)\n                        filename = img_down_url[img_down_url.rindex(&#34;/&#34;)+1:]\n                        self.download_img(filder,img_down_url,filename)  #下载图片\n\n                    else:\n                        print(&#34;-&#34;*100)\n                        print(content)\n                        break # 终止循环体\n\n                else:\n                    print(&#34;{}链接加载失败&#34;.format(img_url))\n\n                    if imgs_lock.acquire():  # 锁定\n                        imgs_start_urls.append(img_url)\n                        imgs_lock.release()  # 解锁\n\n                start_index+=1   # 上文描述中，这个地方需要不断进行+1操作小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~\n                 \n              </code></pre></div><p>所有的代码都在上面了，关键的地方我尽量加上了标注，你可以细细的看一下，实在看不明白，就多敲几遍，因为没有特别复杂的地方，好多都是逻辑。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-eba0909d6db56f14cb02c95ec443adfb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"258\" data-rawheight=\"281\" class=\"content_image\" width=\"258\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;258&#39; height=&#39;281&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"258\" data-rawheight=\"281\" class=\"content_image lazy\" width=\"258\" data-actualsrc=\"https://pic4.zhimg.com/v2-eba0909d6db56f14cb02c95ec443adfb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>最后附上main部分的代码，让我们的代码跑起来</p><div class=\"highlight\"><pre><code class=\"language-text\">if __name__ == &#39;__main__&#39;:\n\n    img = ImageList()\n    urls = img.run()\n    for i in range(1,2):\n        p = Product(urls)\n        p.start()\n\n    for i in range(1,2):\n        c = Consumer()\n        c.start()</code></pre></div><p>一会过后，就慢慢收图吧<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-00f66d55715348111a862c9a7a99839b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"783\" data-rawheight=\"451\" class=\"origin_image zh-lightbox-thumb\" width=\"783\" data-original=\"https://pic4.zhimg.com/v2-00f66d55715348111a862c9a7a99839b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;783&#39; height=&#39;451&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"783\" data-rawheight=\"451\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"783\" data-original=\"https://pic4.zhimg.com/v2-00f66d55715348111a862c9a7a99839b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-00f66d55715348111a862c9a7a99839b_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/64371713", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 1, 
            "title": "Python爬虫入门教程第三讲：美空网数据爬取", 
            "content": "<h2><b>美空网数据----简介</b></h2><p>从今天开始，我们尝试用2篇博客的内容量，搞定一个网站叫做“美空网”网址为：<a href=\"https://link.zhihu.com/?target=http%3A//www.moko.cc/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">moko.cc/</span><span class=\"invisible\"></span></a>， 这个网站我分析了一下，我们要爬取的图片在 下面这个网址</p><blockquote><i><a href=\"https://link.zhihu.com/?target=http%3A//www.moko.cc/post/1302075.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">moko.cc/post/1302075.ht</span><span class=\"invisible\">ml</span><span class=\"ellipsis\"></span></a></i></blockquote><p>然后在去分析一下，我需要找到一个图片列表页面是最好的，作为一个勤劳的爬虫coder，我找到了这个页面</p><blockquote><i><a href=\"https://link.zhihu.com/?target=http%3A//www.moko.cc/post/da39db43246047c79dcaef44c201492d/list.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">moko.cc/post/da39db4324</span><span class=\"invisible\">6047c79dcaef44c201492d/list.html</span><span class=\"ellipsis\"></span></a></i></blockquote><p>列表页面被我找到了，貌似没有分页，这就简单多了，但是刚想要爬，就翻车了，我发现一个严重的问题。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.moko.cc/post/%3D%3Dda39db43246047c79dcaef44c201492d%3D%3D/list.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">moko.cc/post/==da39db43</span><span class=\"invisible\">246047c79dcaef44c201492d==/list.html</span><span class=\"ellipsis\"></span></a></p><p>我要做的是一个自动化的爬虫，但是我发现，出问题了，上面那个黄色背景的位置是啥？</p><p>ID，昵称，个性首页，这个必须要搞定。</p><p>我接下来随机的找了一些图片列表页，试图找到规律到底是啥？</p><ol><li><a href=\"https://link.zhihu.com/?target=http%3A//www.moko.cc/post/978c74a0375f4edca114e87b0a45a0b5/list.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">moko.cc/post/978c74a037</span><span class=\"invisible\">5f4edca114e87b0a45a0b5/list.html</span><span class=\"ellipsis\"></span></a></li><li><a href=\"https://link.zhihu.com/?target=http%3A//www.moko.cc/post/jundayi/list.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">moko.cc/post/jundayi/li</span><span class=\"invisible\">st.html</span><span class=\"ellipsis\"></span></a></li><li><a href=\"https://link.zhihu.com/?target=http%3A//www.moko.cc/post/slavik/list.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">moko.cc/post/slavik/lis</span><span class=\"invisible\">t.html</span><span class=\"ellipsis\"></span></a><br/></li><li>......</li></ol><p>没什么问题，发现规律了</p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.moko.cc/post/%3D%3D\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">moko.cc/post/==</span><span class=\"invisible\"></span></a>个性昵称(中文昵称是一个加密的串)==/list.html</p><p>这就有点意思了，我要是能找到尽量多的昵称，不就能拼接出来我想要得所有地址了吗</p><p>开干！！！</p><p>手段，全站乱点，找入口，找切入点，找是否有API</p><p>.... .... 结果没找着</p><p>下面的一些备选方案</p><p>趴这个页面，发现只有 20页 <a href=\"https://link.zhihu.com/?target=http%3A//www.moko.cc/channels/post/23/1.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">moko.cc/channels/post/2</span><span class=\"invisible\">3/1.html</span><span class=\"ellipsis\"></span></a></p><p>每页48个模特，20页。那么也才960人啊，完全覆盖不到尽可能多的用户。</p><p>接着又找到</p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.moko.cc/catalog/index.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">moko.cc/catalog/index.h</span><span class=\"invisible\">tml</span><span class=\"ellipsis\"></span></a> 这个页面</p><p>确认了一下眼神，以为发现问题了，结果</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-38971e91c2e2f4248e8680a73c75041d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"613\" data-rawheight=\"374\" class=\"origin_image zh-lightbox-thumb\" width=\"613\" data-original=\"https://pic2.zhimg.com/v2-38971e91c2e2f4248e8680a73c75041d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;613&#39; height=&#39;374&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"613\" data-rawheight=\"374\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"613\" data-original=\"https://pic2.zhimg.com/v2-38971e91c2e2f4248e8680a73c75041d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-38971e91c2e2f4248e8680a73c75041d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>哎呀，还么有权限，谁有权限，可以跟我交流一下，一时激动，差点去下载他们的APP，然后进行抓包去。</p><p>上面两条路，都不好弄，接下来继续找路子。</p><p>无意中，我看到了一丝曙光</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-035856aaee6bd60d358a43baa2eb3689_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1010\" data-rawheight=\"379\" class=\"origin_image zh-lightbox-thumb\" width=\"1010\" data-original=\"https://pic2.zhimg.com/v2-035856aaee6bd60d358a43baa2eb3689_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1010&#39; height=&#39;379&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1010\" data-rawheight=\"379\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1010\" data-original=\"https://pic2.zhimg.com/v2-035856aaee6bd60d358a43baa2eb3689_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-035856aaee6bd60d358a43baa2eb3689_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>关注名单，点进去</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9e8788996adc9554f8f024e5ce1435a6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"753\" data-rawheight=\"565\" class=\"origin_image zh-lightbox-thumb\" width=\"753\" data-original=\"https://pic3.zhimg.com/v2-9e8788996adc9554f8f024e5ce1435a6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;753&#39; height=&#39;565&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"753\" data-rawheight=\"565\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"753\" data-original=\"https://pic3.zhimg.com/v2-9e8788996adc9554f8f024e5ce1435a6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9e8788996adc9554f8f024e5ce1435a6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>哈哈哈，OK了，这不就是，我要找到的东西吗？</p><p>不多说了，爬虫走起，测试一下他是否有反扒机制。</p><p>我找到了一个关注的人比较多的页面，1500多个人</p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.moko.cc/subscribe/chenhaoalex/1.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">moko.cc/subscribe/chenh</span><span class=\"invisible\">aoalex/1.html</span><span class=\"ellipsis\"></span></a></p><p>然后又是一波分析操作</p><h3>美空网数据----爬虫数据存储</h3><p>确定了爬虫的目标，接下来，我做了两件事情，看一下，是否对你也有帮助</p><ol><li>确定数据存储在哪里？最后我选择了MongoDB</li><li>用正则表达式去分析网页数据</li></ol><p>对此，我们需要安装一下MongoDB，安装的办法肯定是官网教程啦！</p><blockquote><i><a href=\"https://link.zhihu.com/?target=https%3A//docs.mongodb.com/master/tutorial/install-mongodb-on-red-hat/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">docs.mongodb.com/master</span><span class=\"invisible\">/tutorial/install-mongodb-on-red-hat/</span><span class=\"ellipsis\"></span></a></i></blockquote><p>如果官方文档没有帮助你安装成功。</p><p>那么我推荐下面这篇博客</p><blockquote><i><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/hackyo/p/7967170.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">cnblogs.com/hackyo/p/79</span><span class=\"invisible\">67170.html</span><span class=\"ellipsis\"></span></a></i></blockquote><p>安装MongoDB出现如下结果</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-15b896b0a04d8aebf7c970fcfb9580be_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"405\" data-rawheight=\"78\" class=\"content_image\" width=\"405\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;405&#39; height=&#39;78&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"405\" data-rawheight=\"78\" class=\"content_image lazy\" width=\"405\" data-actualsrc=\"https://pic3.zhimg.com/v2-15b896b0a04d8aebf7c970fcfb9580be_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>恭喜你安装成功了。</p><p>接下来，你要学习的是 关于mongodb用户权限的管理</p><blockquote><i><a href=\"https://link.zhihu.com/?target=http%3A//www.cnblogs.com/shiyiwen/p/5552750.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">cnblogs.com/shiyiwen/p/</span><span class=\"invisible\">5552750.html</span><span class=\"ellipsis\"></span></a></i></blockquote><p>mongodb索引的创建</p><blockquote><i><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/salmonellavaccine/article/details/53907535\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/salmonell</span><span class=\"invisible\">avaccine/article/details/53907535</span><span class=\"ellipsis\"></span></a></i></blockquote><p>别问为啥我不重新写一遍，懒呗~~~ 况且这些资料太多了，互联网大把大把的。</p><p>一些我经常用的mongdb的命令</p><div class=\"highlight\"><pre><code class=\"language-text\">链接 mongo --port &lt;端口号&gt;\n\n选择数据库 use admin \n\n展示当前数据库  db \n\n当前数据库授权  db.auth(&#34;用户名&#34;,&#34;密码&#34;)\n\n查看数据库  show dbs\n\n查看数据库中的列名  show collections \n\n创建列  db.createCollection(&#34;列名&#34;)\n\n创建索引 db.col.ensureIndex({&#34;列名字&#34;:1},{&#34;unique&#34;:true})\n\n展示所有索引 db.col.getIndexes()\n\n删除索引 db.col.dropIndex(&#34;索引名字&#34;)  \n\n查找数据  db.列名.find()\n\n查询数据总条数  db.列名.find().count()</code></pre></div><p>上面基本是我最常用的了，我们下面实际操作一把。</p><h3>美空网数据----用Python链接MongoDB</h3><p>使用 pip3 安装pymongo库</p><p>使用pymongo模块连接mongoDB数据库</p><p>一些准备工作</p><ol><li>创建dm数据库<br/>链接上mongodb 在终端使用命令 mongo --port 21111</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">[linuxboy@localhost ~]$ mongo --port 21111\nMongoDB shell version v3.6.5\nconnecting to: mongodb://127.0.0.1:21111/\nMongoDB server version: 3.6.5\n&gt;</code></pre></div><ol><li>配置用户权限：接着上面输入命令 show dbs 查看权限</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0be8c66f65d19d33410b0d01d7bdf92a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"937\" data-rawheight=\"308\" class=\"origin_image zh-lightbox-thumb\" width=\"937\" data-original=\"https://pic3.zhimg.com/v2-0be8c66f65d19d33410b0d01d7bdf92a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;937&#39; height=&#39;308&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"937\" data-rawheight=\"308\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"937\" data-original=\"https://pic3.zhimg.com/v2-0be8c66f65d19d33410b0d01d7bdf92a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0be8c66f65d19d33410b0d01d7bdf92a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>权限不足</p><ol><li>创建管理用户</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">db.createUser({user: &#34;userAdmin&#34;,pwd: &#34;123456&#34;, roles: [ { role: &#34;userAdminAnyDatabase&#34;, db: &#34;admin&#34; } ] } )</code></pre></div><ol><li>授权用户</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">db.auth(&#34;userAdmin&#34;,&#34;123456&#34;)</code></pre></div><ol><li>查看权限</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">&gt; db.auth(&#34;userAdmin&#34;,&#34;123456&#34;)\n1\n&gt; show dbs\nadmin   0.000GB\nconfig  0.000GB\nlocal   0.000GB\nmoko    0.013GB\ntest    0.000GB\n&gt; </code></pre></div><ol><li>接下来创建 dm数据库&lt;在这之前还需要创建一个读写用户&gt;</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">&gt; use dm\nswitched to db dm\n&gt; db\ndm\n&gt; db.createUser({user: &#34;dba&#34;,pwd: &#34;dba&#34;, roles: [ { role: &#34;readWrite&#34;, db: &#34;dm&#34; } ] } )\nSuccessfully added user: {\n    &#34;user&#34; : &#34;dba&#34;,\n    &#34;roles&#34; : [\n        {\n            &#34;role&#34; : &#34;readWrite&#34;,\n            &#34;db&#34; : &#34;dm&#34;\n        }\n    ]\n}\n&gt;</code></pre></div><ol><li>重新授权</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">db.auth(&#34;dba&#34;,&#34;dba&#34;)</code></pre></div><ol><li>创建一列数据</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">&gt; db.createCollection(&#34;demo&#34;)\n{ &#34;ok&#34; : 1 }\n&gt; db.collections\ndm.collections\n&gt; show collections\ndemo\n&gt; </code></pre></div><ol><li>Python实现插入操作</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">import pymongo as pm  #确保你已经安装过pymongo了\n\n # 获取连接\nclient = pm.MongoClient(&#39;localhost&#39;, 21111)  # 端口号是数值型\n\n# 连接目标数据库\ndb = client.dm\n\n# 数据库用户验证\ndb.authenticate(&#34;dba&#34;, &#34;dba&#34;)\npost = {\n        &#34;id&#34;: &#34;111111&#34;,\n        &#34;level&#34;: &#34;MVP&#34;,\n        &#34;real&#34;:1,\n        &#34;profile&#34;: &#39;111&#39;,\n        &#39;thumb&#39;:&#39;2222&#39;,\n        &#39;nikename&#39;:&#39;222&#39;,\n        &#39;follows&#39;:20\n}\n\ndb.col.insert_one(post) # 插入单个文档\n\n# 打印集合第1条记录\nprint (db.col.find_one())</code></pre></div><ol><li>编译执行</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">[linuxboy@bogon moocspider]$ python3 mongo.py\n{&#39;_id&#39;: ObjectId(&#39;5b15033cc3666e1e28ae5582&#39;), &#39;id&#39;: &#39;111111&#39;, &#39;level&#39;: &#39;MVP&#39;, &#39;real&#39;: 1, &#39;profile&#39;: &#39;111&#39;, &#39;thumb&#39;: &#39;2222&#39;, &#39;nikename&#39;: &#39;222&#39;, &#39;follows&#39;: 20}\n[linuxboy@bogon moocspider]$</code></pre></div><hr/><p>好了，我们到现在为止，实现了mongodb的插入问题。</p><h3>美空网数据----用Python 爬取关注对象</h3><p>首先，我需要创造一个不断抓取链接的类</p><p>这个类做的事情，就是分析</p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.moko.cc/subscribe/chenhaoalex/1.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">moko.cc/subscribe/chenh</span><span class=\"invisible\">aoalex/1.html</span><span class=\"ellipsis\"></span></a></p><p>这个页面，总共有多少页，然后生成链接</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fbd398f9b94d9033210a354e4006cd36_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"961\" data-rawheight=\"412\" class=\"origin_image zh-lightbox-thumb\" width=\"961\" data-original=\"https://pic3.zhimg.com/v2-fbd398f9b94d9033210a354e4006cd36_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;961&#39; height=&#39;412&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"961\" data-rawheight=\"412\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"961\" data-original=\"https://pic3.zhimg.com/v2-fbd398f9b94d9033210a354e4006cd36_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fbd398f9b94d9033210a354e4006cd36_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>抓取页面中的总页数为77</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-00f080e6729cd6727ba9133c25f709bc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1429\" data-rawheight=\"400\" class=\"origin_image zh-lightbox-thumb\" width=\"1429\" data-original=\"https://pic1.zhimg.com/v2-00f080e6729cd6727ba9133c25f709bc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1429&#39; height=&#39;400&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1429\" data-rawheight=\"400\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1429\" data-original=\"https://pic1.zhimg.com/v2-00f080e6729cd6727ba9133c25f709bc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-00f080e6729cd6727ba9133c25f709bc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>正则表达式如下</p><div class=\"highlight\"><pre><code class=\"language-text\">onfocus=\\&#34;this\\.blur\\(\\)\\&#34;&gt;(\\d*?)&lt;\n</code></pre></div><p>在这里，由所有的分页都一样，所以，我匹配了全部的页码，然后计算了数组中的最大值</p><div class=\"highlight\"><pre><code class=\"language-text\">#获取页码数组\npages = re.findall(r&#39;onfocus=\\&#34;this\\.blur\\(\\)\\&#34;&gt;(\\d*?)&lt;&#39;,content,re.S)   #获取总页数\npage_size = 1\nif pages:  #如果数组不为空\n    page_size = int(max(pages))   #获取最大页数</code></pre></div><p>接下来就是我们要搞定的<code>生产者</code>编码阶段了，我们需要打造一个不断获取连接的爬虫</p><p>简单的说就是</p><p>我们需要一个爬虫，不断的去爬取</p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.moko.cc/subscribe/chenhaoalex/1.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">moko.cc/subscribe/chenh</span><span class=\"invisible\">aoalex/1.html</span><span class=\"ellipsis\"></span></a> 这个页面中所有的用户，并且还要爬取到总页数。</p><p>比如查看上述页面中,我们要获取的关键点如下</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c1a3a6f1811329c1f32a53878a7f082e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1297\" data-rawheight=\"588\" class=\"origin_image zh-lightbox-thumb\" width=\"1297\" data-original=\"https://pic3.zhimg.com/v2-c1a3a6f1811329c1f32a53878a7f082e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1297&#39; height=&#39;588&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1297\" data-rawheight=\"588\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1297\" data-original=\"https://pic3.zhimg.com/v2-c1a3a6f1811329c1f32a53878a7f082e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c1a3a6f1811329c1f32a53878a7f082e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>通过这个页面，我们要得到，这样子的一个数组，注意下面数组中有个位置【我用爬虫爬到的】这个就是关键的地方了</p><div class=\"highlight\"><pre><code class=\"language-text\">all_urls = [\n    &#34;http://www.moko.cc/subscribe/chenhaoalex/1.html&#34;,\n    &#34;http://www.moko.cc/subscribe/chenhaoalex/2.html&#34;,\n    &#34;http://www.moko.cc/subscribe/chenhaoalex/3.html&#34;,\n    &#34;http://www.moko.cc/subscribe/chenhaoalex/4.html&#34;,\n    ......\n    &#34;http://www.moko.cc/subscribe/dde760d5dd6a4413aacb91d1b1d76721/1.html&#34;\n    &#34;http://www.moko.cc/subscribe/3cc82db2231a4449aaa97ed8016b917a/1.html&#34;\n    &#34;http://www.moko.cc/subscribe/d45c1e3069c24152abdc41c1fb342b8f/1.html&#34;\n    &#34;http://www.moko.cc/subscribe/【我用爬虫爬到的】/1.html&#34;\n    \n    \n    ]</code></pre></div><p>引入必备模块</p><div class=\"highlight\"><pre><code class=\"language-text\"># -*- coding: UTF-8 -*-\nimport requests   #网络请求模块\nimport random     #随机模块\nimport re         #正则表达式模块\nimport time       #时间模块\nimport threading  #线程模块\nimport pymongo as pm   #mongodb模块</code></pre></div><p>接下来，我们需要准备一个通用函数模拟<code>UserAgent</code>做一个简单的反爬处理</p><div class=\"highlight\"><pre><code class=\"language-text\">class Config():\n    def getHeaders(self):\n        user_agent_list = [ \\\n            &#34;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1&#34; \\\n            &#34;Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1&#34;, \\\n            &#34;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3&#34;, \\\n            &#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3&#34;, \\\n            &#34;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24&#34;, \\\n            &#34;Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24&#34;\n        ]\n        UserAgent=random.choice(user_agent_list)\n        headers = {&#39;User-Agent&#39;: UserAgent}\n        return headers\n</code></pre></div><p>编写生产者的类和核心代码,<code>Producer</code>继承<code>threading.Thread</code></p><div class=\"highlight\"><pre><code class=\"language-text\">#生产者\nclass Producer(threading.Thread):\n    \n    def run(self):\n        print(&#34;线程启动...&#34;)\n        headers = Config().getHeaders()\n\nif __name__ == &#34;__main__&#34;:\n    p = Producer()\n    p.start()</code></pre></div><p>测试运行，一下，看是否可以启动</p><div class=\"highlight\"><pre><code class=\"language-text\">[linuxboy@bogon moocspider]$ python3 demo.py\n线程启动...\n{&#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24&#39;}\n[linuxboy@bogon moocspider]$</code></pre></div><p>如果上面的代码没有问题，接下来就是我们爬虫代码部分了，为了方便多线程之间的调用，我们还是创建一个共享变量在N个线程之间调用</p><div class=\"highlight\"><pre><code class=\"language-text\"># -*- coding: UTF-8 -*-\nimport requests\nimport random\nimport re\nimport time\nimport threading\nimport pymongo as pm\n\n # 获取连接\nclient = pm.MongoClient(&#39;localhost&#39;, 21111)  # 端口号是数值型\n\n# 连接目标数据库\ndb = client.moko\n\n# 数据库用户验证\ndb.authenticate(&#34;moko&#34;, &#34;moko&#34;)\n\nurls = [&#34;http://www.moko.cc/subscribe/chenhaoalex/1.html&#34;]\nindex = 0   #索引\ng_lock = threading.Lock()  #初始化一个锁  \n\n\n\n#生产者\nclass Producer(threading.Thread):\n    \n    def run(self):\n        print(&#34;线程启动...&#34;)\n        headers = Config().getHeaders()\n        print(headers)\n        global urls\n        global index \n        while True:\n            g_lock.acquire() \n            if len(urls)==0:\n                g_lock.release() \n                continue\n            page_url = urls.pop()\n            g_lock.release() #使用完成之后及时把锁给释放，方便其他线程使用\n            response = &#34;&#34;\n            try:\n                response = requests.get(page_url,headers=headers,timeout=5)\n                \n            except Exception as http:\n                print(&#34;生产者异常&#34;)\n                print(http)\n                continue       \n            content = response.text \n            \n            rc = re.compile(r&#39;&lt;a class=\\&#34;imgBorder\\&#34; href=\\&#34;\\/(.*?)\\&#34; hidefocus=\\&#34;true\\&#34;&gt;&#39;)\n            follows = rc.findall(content)\n            print(follows)\n            fo_url = []\n            threading_links_2 = []\n            for u in follows:   \n                this_url = &#34;http://www.moko.cc/subscribe/%s/1.html&#34; % u \n                g_lock.acquire()\n                index += 1 \n                g_lock.release()\n                fo_url.append({&#34;index&#34;:index,&#34;link&#34;:this_url})\n                threading_links_2.append(this_url)\n           \n            g_lock.acquire()\n            urls += threading_links_2\n            g_lock.release()\n            print(fo_url)\n            \n            try:\n                db.text.insert_many(fo_url,ordered=False )\n            except:\n                continue\n\nif __name__ == &#34;__main__&#34;:\n    p = Producer()\n    p.start()</code></pre></div><p>上面代码除了基本操作以外，我做了一些细小的处理</p><p>现在说明如下</p><div class=\"highlight\"><pre><code class=\"language-text\">fo_url.append({&#34;index&#34;:index,&#34;link&#34;:this_url})</code></pre></div><p>这部分代码，是为了消费者使用时候，方便进行查找并且删除操作而特意改造的，增加了一个字段index作为标识</p><p>第二个部分，插入数据的时候，我进行了批量的操作使用的是<code>insert_many</code>函数，并且关键的地方，我增加了一个ordered=False的操作，这个地方大家可以自行研究一下，我的目的是去掉重复数据，默认情况下<code>insert_many</code>函数如果碰到数据重复，并且在mongodb中创建了索引==创建索引的办法，大家自行翻阅文章上面==，那么是无法插入的，但是这样子会插入一部分，只把重复的地方略过，非常方便。</p><p>关于pymongo的使用，大家可以参考官网手册</p><p>这个是 pymongo的官方教程</p><blockquote><i><a href=\"https://link.zhihu.com/?target=http%3A//api.mongodb.com/python/current/api/pymongo/collection.html%3Fhighlight%3Dinsert_many%23pymongo.collection.Collection.insert_many\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">api.mongodb.com/python/</span><span class=\"invisible\">current/api/pymongo/collection.html?highlight=insert_many#pymongo.collection.Collection.insert_many</span><span class=\"ellipsis\"></span></a></i></blockquote><p>MongoDB的手册大家也可以参考</p><blockquote><i><a href=\"https://link.zhihu.com/?target=https%3A//docs.mongodb.com/manual/reference/method/db.collection.insertMany/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">docs.mongodb.com/manual</span><span class=\"invisible\">/reference/method/db.collection.insertMany/</span><span class=\"ellipsis\"></span></a></i></blockquote><div class=\"highlight\"><pre><code class=\"language-text\">db.text.insert_many(fo_url,ordered=False )</code></pre></div><p>我们链接上MongoDB数据库，查询一下我们刚刚插入的数据</p><div class=\"highlight\"><pre><code class=\"language-text\">&gt; show collections\ncol\nlinks\ntext\n&gt; db.text\nmoko.text\n&gt; db.text.find()\n{ &#34;_id&#34; : ObjectId(&#34;5b1789e0c3666e642364a70b&#34;), &#34;index&#34; : 1, &#34;link&#34; : &#34;http://www.moko.cc/subscribe/dde760d5dd6a4413aacb91d1b1d76721/1.html&#34; }\n{ &#34;_id&#34; : ObjectId(&#34;5b1789e0c3666e642364a70c&#34;), &#34;index&#34; : 2, &#34;link&#34; : &#34;http://www.moko.cc/subscribe/3cc82db2231a4449aaa97ed8016b917a/1.html&#34; }\n.......\n{ &#34;_id&#34; : ObjectId(&#34;5b1789e0c3666e642364a71e&#34;), &#34;index&#34; : 20, &#34;link&#34; : &#34;http://www.moko.cc/subscribe/8c1e4c738e654aad85903572f9090adb/1.html&#34; }\nType &#34;it&#34; for more</code></pre></div><p>其实上面代码，有一个非常严重的BUG，就是当我们实际操作的时候，发现，我们每次获取到的都是我们使用<code>this_url = &#34;http://www.moko.cc/subscribe/%s/1.html&#34; % u</code> 进行拼接的结果。</p><p>也就是说，我们获取到的永远都是第1页。这个按照我们之前设计的就不符合逻辑了，</p><p>我们还要获取到分页的内容，那么这个地方需要做一个简单的判断，就是下面的逻辑了。</p><p>==如果完整代码，大家不知道如何观看，可以直接翻阅到文章底部，有对应的github链接==</p><div class=\"highlight\"><pre><code class=\"language-text\">#如果是第一页，那么需要判断一下\n#print(page_url)\nis_home =re.search(r&#39;(\\d*?)\\.html&#39;,page_url).group(1)\nif is_home == str(1):\n    pages = re.findall(r&#39;onfocus=\\&#34;this\\.blur\\(\\)\\&#34;&gt;(\\d*?)&lt;&#39;,content,re.S)   #获取总页数\n    page_size = 1\n    if pages:\n        page_size = int(max(pages))   #获取最大页数\n        if page_size &gt; 1:   #如果最大页数大于1，那么获取所有的页面\n            url_arr = []\n            threading_links_1 = []\n            for page in range(2,page_size+1):\n                url =  re.sub(r&#39;(\\d*?)\\.html&#39;,str(page)+&#34;.html&#34;,page_url)  \n                threading_links_1.append(url)\n                g_lock.acquire()\n                index += 1 \n                g_lock.release()\n\n                url_arr.append({ &#34;index&#34;:index, &#34;link&#34;: url})\n\n            g_lock.acquire()\n            urls += threading_links_1  #  URL数据添加\n            g_lock.release()\n            try:\n                db.text.insert_many(url_arr,ordered=False )\n            except Exception as e:\n                print(&#34;数据库输入异常&#34;)\n                print (e)\n                continue\n            \n        else:\n            pass\n    else:\n            pass</code></pre></div><p>截止到现在为止，其实你已经实现了链接的生产者了 。</p><p>我们在MongoDB中生成了一堆链接，接下来就是使用阶段了。</p><p>使用起来也是非常简单。</p><p>我先给大家看一个比较复杂的<code>正则表达式</code>爬虫写的好不好，正则表达式站很重要的比例哦~</p><div class=\"highlight\"><pre><code class=\"language-text\">divEditOperate_(?P&lt;ID&gt;\\d*)[\\&#34;] .*&gt;[\\s\\S]*?&lt;p class=\\&#34;state\\&#34;&gt;.*?(?P&lt;级别&gt;\\w*P).*&lt;/span&gt;&lt;/span&gt;(?P&lt;是否认证&gt;&lt;br/&gt;)?.*?&lt;/p&gt;[\\s\\S]*?&lt;div class=\\&#34;info clearfix\\&#34;&gt;[\\s\\S]*?&lt;a class=\\&#34;imgBorder\\&#34; href=\\&#34;\\/(?P&lt;主页&gt;.*?)\\&#34; hidefocus=\\&#34;true\\&#34;&gt;[\\s\\S]*?&lt;img .*?src=\\&#34;(?P&lt;头像&gt;.*?)\\&#34;.*?alt=\\&#34;.*?\\&#34; title=\\&#34;(?P&lt;昵称&gt;.*?)\\&#34; /&gt;[\\s\\S]*?&lt;p class=\\&#34;font12 lesserColor\\&#34;&gt;(?P&lt;地点&gt;.*?)&amp;nbsp.*?&lt;span class=\\&#34;font12 mainColor\\&#34;&gt;(?P&lt;粉丝数目&gt;\\d*?)&lt;/span&gt;\n</code></pre></div><p>上面这个正则表达式，就是我为</p><div class=\"highlight\"><pre><code class=\"language-text\">http://www.moko.cc/subscribe/chenhaoalex/1.html </code></pre></div><p>这个页面专门准备的。</p><p>这样子，我就可以直接获取到我想要的所有数据了。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-bb9f3b244c72509c3c2d896f0a6981ed_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"334\" data-rawheight=\"169\" class=\"content_image\" width=\"334\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;334&#39; height=&#39;169&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"334\" data-rawheight=\"169\" class=\"content_image lazy\" width=\"334\" data-actualsrc=\"https://pic2.zhimg.com/v2-bb9f3b244c72509c3c2d896f0a6981ed_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>消费者的代码如下</p><div class=\"highlight\"><pre><code class=\"language-text\">get_index = 0\n#消费者类\nclass Consumer(threading.Thread):\n    \n    def run(self):\n        headers = Config().getHeaders()\n\n        global get_index \n        while True:\n            \n            g_lock.acquire() \n            get_index += 1\n            g_lock.release()\n            #从刚才数据存储的列里面获取一条数据，这里用到find_one_and_delete方法\n            #get_index 需要声明成全局的变量\n            link = db.links.find_one_and_delete({&#34;index&#34;:get_index})\n            page_url = &#34;&#34;\n            if link:\n                page_url = link[&#34;link&#34;]\n                print(page_url+&#34;&gt;&gt;&gt;网页分析中...&#34;)\n            else:\n                continue\n\n            response = &#34;&#34;\n            try:\n                response = requests.get(page_url,headers=headers,timeout=5)\n                \n            except Exception as http:\n                print(&#34;消费者有异常&#34;)\n                print(http)\n                continue\n            \n            content = response.text \n            rc = re.compile(r&#39;divEditOperate_(?P&lt;ID&gt;\\d*)[\\&#34;] .*&gt;[\\s\\S]*?&lt;p class=\\&#34;state\\&#34;&gt;.*?(?P&lt;级别&gt;\\w*P).*&lt;/span&gt;&lt;/span&gt;(?P&lt;是否认证&gt;&lt;br/&gt;)?.*?&lt;/p&gt;[\\s\\S]*?&lt;div class=\\&#34;info clearfix\\&#34;&gt;[\\s\\S]*?&lt;a class=\\&#34;imgBorder\\&#34; href=\\&#34;\\/(?P&lt;主页&gt;.*?)\\&#34; hidefocus=\\&#34;true\\&#34;&gt;[\\s\\S]*?&lt;img .*?src=\\&#34;(?P&lt;头像&gt;.*?)\\&#34;.*?alt=\\&#34;.*?\\&#34; title=\\&#34;(?P&lt;昵称&gt;.*?)\\&#34; /&gt;[\\s\\S]*?&lt;p class=\\&#34;font12 lesserColor\\&#34;&gt;(?P&lt;地点&gt;.*?)&amp;nbsp.*?&lt;span class=\\&#34;font12 mainColor\\&#34;&gt;(?P&lt;粉丝数目&gt;\\d*?)&lt;/span&gt;&#39;)\n            user_info = rc.findall(content)\n            print(&#34;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&#34;)\n            users = []\n            for user in user_info:\n                post = {\n                    &#34;id&#34;: user[0],\n                    &#34;level&#34;: user[1],\n                    &#34;real&#34;:user[2],\n                    &#34;profile&#34;: user[3],\n                    &#39;thumb&#39;:user[4],\n                    &#39;nikename&#39;:user[5],\n                    &#39;address&#39;:user[6],\n                    &#39;follows&#39;:user[7]\n                }\n\n                users.append(post)\n            print(users)\n           \n            try:\n                db.mkusers.insert_many(users,ordered=False )\n            except Exception as e:\n                print(&#34;数据库输入异常&#34;)\n                print (e)\n                continue\n\n            time.sleep(1)\n\n            print(&#34;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&#34;)</code></pre></div><p>当你使用<code>python3 demo.py</code> 编译demo之后，屏幕滚动如下结果，那么你成功了。</p><p>接下来就可以去数据库查阅数据去了。</p><div class=\"highlight\"><pre><code class=\"language-text\">[linuxboy@bogon moocspider]$ python3 demo.py\n线程启动...\n{&#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3&#39;}\nhttp://www.moko.cc/subscribe/chenhaoalex/2.html&gt;&gt;&gt;网页分析中...\n[&#39;dde760d5dd6a4413aacb91d1b1d76721&#39;, &#39;3cc82db2231a4449aaa97ed8016b917a&#39;, &#39;a1835464ad874eec92ccbb31841a7590&#39;, &#39;c9ba6a47a246494398d4e26c1e0b7e54&#39;, &#39;902fe175e668417788a4fb5d4de7ab99&#39;, &#39;dcb8f11265594f17b821a6d90caf96a7&#39;, &#39;7ea0a96621eb4ed99c9c642936559c94&#39;, &#39;d45c1e3069c24152abdc41c1fb342b8f&#39;, &#39;chenyiqiu&#39;, &#39;798522844&#39;, &#39;MEERILLES&#39;, &#39;ddfd9e1f7dca4cffb2430caebd2494f8&#39;, &#39;d19cbd37c87e400e9da42e159560649b&#39;, &#39;ac07e7fbfde14922bb1d0246b9e4374d&#39;, &#39;05abc72ac7bb4f738f73028fed17ac23&#39;, &#39;hanzhuoer&#39;, &#39;e12e15aaee654b8aa9f528215bc3294c&#39;, &#39;3b6d8dc6fd814789bd484f393b5c9fa8&#39;, &#39;83256b93a2f94f449ab75c730cb80a7b&#39;, &#39;8c1e4c738e654aad85903572f9090adb&#39;]\n[{&#39;index&#39;: 77, &#39;link&#39;: &#39;http://www.moko.cc/subscribe/dde760d5dd6a4413aacb91d1b1d76721/1.html&#39;}, {&#39;index&#39;: 78, &#39;link&#39;: &#39;http://www.moko.cc/subscribe/3cc82db2231a4449aaa97ed8016b917a/1.html&#39;}, {&#39;index&#39;: 79, &#39;link&#39;: &#39;http://www.moko.cc/subscribe/a1835464ad874eec92ccbb31841a7590/1.html&#39;}, {&#39;index&#39;: 80, &#39;link&#39;: &#39;http://www.moko.cc/subscribe/c9ba6a47a246494398d4e26c1e0b7e54/1.html&#39;}, {]\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n[{&#39;id&#39;: &#39;3533155&#39;, &#39;level&#39;: &#39;MP&#39;, &#39;real&#39;: &#39;&#39;, &#39;profile&#39;: &#39;b1a7e76455cc4ca4b81ed800ab68b308&#39;, &#39;thumb&#39;: &#39;http://img.mb.moko.cc/2018-02-17/d7db42d4-7f34-46d2-a760-c88eb90d6e0d.jpg&#39;, &#39;nikename&#39;: &#39;模特九九&#39;, &#39;address&#39;: &#39;大连&#39;, &#39;follows&#39;: &#39;10&#39;}, {&#39;id&#39;: &#39;3189865&#39;, &#39;level&#39;: &#39;VIP&#39;, &#39;real&#39;: &#39;&#39;, &#39;profile&#39;: &#39;cfdf1482a9034f65a60bc6a1cf8d6a02&#39;, &#39;thumb&#39;: &#39;http://img.mb.moko.cc/2016-09-30/98c1ddd3-f9a8-4a15-a106-5d664fa7b558.jpg&#39;, &#39;nikename&#39;: &#39;何应77&#39;, &#39;address&#39;: &#39;杭州&#39;, &#39;follows&#39;: &#39;219&#39;}, {&#39;id&#39;: &#39;14886&#39;, &#39;level&#39;: &#39;VIP&#39;, &#39;real&#39;: &#39;&lt;br/&gt;&#39;, &#39;profile&#39;: &#39;cndp&#39;, &#39;thumb&#39;: &#39;http://img2.moko.cc/users/0/49/14886/logo/img2_des_x3_10100286.jpg&#39;, &#39;nikename&#39;: &#39;多拍PGirl&#39;, &#39;address&#39;: &#39;北京&#39;, &#39;follows&#39;: &#39;2331&#39;}, {&#39;id&#39;: &#39;3539257&#39;, &#39;level&#39;: &#39;MP&#39;, &#39;real&#39;: &#39;&lt;br/&gt;&#39;, &#39;profile&#39;: &#39;605c8fb2824049aa841f21858a7fd142&#39;, &#39;thumb&#39;: &#39;http://img.mb.moko.cc/2018-02&#39;:</code></pre></div><p>记得处理数据的时候去掉重复值</p><div class=\"highlight\"><pre><code class=\"language-text\">&gt;show collections\ncol\nlinks\nmkusers\ntext\n&gt; db.mkusers.find()\n{ &#34;_id&#34; : ObjectId(&#34;5b17931ec3666e6eff3953bc&#34;), &#34;id&#34; : &#34;3533155&#34;, &#34;level&#34; : &#34;MP&#34;, &#34;real&#34; : &#34;&#34;, &#34;profile&#34; : &#34;b1a7e76455cc4ca4b81ed800ab68b308&#34;, &#34;thumb&#34; : &#34;http://img.mb.moko.cc/2018-02-17/d7db42d4-7f34-46d2-a760-c88eb90d6e0d.jpg&#34;, &#34;nikename&#34; : &#34;模特九九&#34;, &#34;address&#34; : &#34;大连&#34;, &#34;follows&#34; : &#34;10&#34; }\n{ &#34;_id&#34; : ObjectId(&#34;5b17931ec3666e6eff3953bd&#34;), &#34;id&#34; : &#34;3189865&#34;, &#34;level&#34; : &#34;VIP&#34;, &#34;real&#34; : &#34;&#34;, &#34;profile&#34; : &#34;cfdf1482a9034f65a60bc6a1cf8d6a02&#34;, &#34;thumb&#34; : &#34;http://img.mb.moko.cc/2016-09-30/98c1ddd3-f9a8-4a15-a106-5d664fa7b558.jpg&#34;, &#34;nikename&#34; : &#34;何应77&#34;, &#34;address&#34; : &#34;杭州&#34;, &#34;follows&#34; : &#34;219&#34; }\n{ &#34;_id&#34; : ObjectId(&#34;5b17931ec3666e6eff3953be&#34;), &#34;id&#34; : &#34;14886&#34;, &#34;level&#34; : &#34;VIP&#34;, &#34;real&#34; : &#34;&lt;br/&gt;&#34;, &#34;profile&#34; : &#34;cndp&#34;, &#34;thumb&#34; : &#34;http://img2.moko.cc/users/0/49/14886/logo/img2_des_x3_10100286.jpg&#34;, &#34;nikename&#34; : &#34;多拍PGirl&#34;, &#34;address&#34; : &#34;北京&#34;, &#34;follows&#34; : &#34;2331&#34; }\n{ &#34;_\n</code></pre></div><p>最后一步，如果你想要把效率提高，修改线程就好了</p><div class=\"highlight\"><pre><code class=\"language-text\">if __name__ == &#34;__main__&#34;:\n\n    for i in range(5):\n        p = Producer()\n        p.start()\n\n    for i in range(7):\n        c = Consumer()\n        c.start()</code></pre></div><p>经过3个小时的爬取，我获取了70000多美空的用户ID，原则上，你可以获取到所有的被关注者的，不过这些数据对我们测试来说，已经足够使用。小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~</p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/64215130", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 8, 
            "title": "Python爬虫入门教程第二讲： 妹子图网站爬取", 
            "content": "<h2><b>妹子图网站爬取---前言</b></h2><p>从今天开始就要撸起袖子，直接写Python爬虫了，学习语言最好的办法就是有目的的进行，所以，接下来我将用10+篇的博客，写<code>爬图片</code>这一件事情。希望可以做好。</p><p>为了写好爬虫，我们需要准备一个火狐浏览器，还需要准备抓包工具，抓包工具，我使用的是CentOS自带的tcpdump，加上wireshark ，这两款软件的安装和使用，建议你还是学习一下，后面我们应该会用到。</p><h2><b>妹子图网站爬取---网络请求模块requests</b></h2><p>Python中的大量开源的模块使得编码变的特别简单，我们写爬虫第一个要了解的模块就是requests。</p><h3>妹子图网站爬取---安装requests</h3><p>打开终端：使用命令</p><blockquote><i>pip3 install requests</i></blockquote><p>等待安装完毕即可使用</p><p>接下来在终端中键入如下命令</p><div class=\"highlight\"><pre><code class=\"language-text\"># mkdir demo  \n# cd demo\n# touch down.py</code></pre></div><p>上面的linux命令是 创建一个名称为<code>demo</code>的文件夹，之后创建一个<code>down.py</code>文件,你也可以使用GUI工具，像操作windows一样，右键创建各种文件。</p><p>为了提高在linux上的开发效率，我们需要安装一个<code>visual studio code</code> 的开发工具</p><p>对于怎么安装vscode，参考官方的<a href=\"https://link.zhihu.com/?target=https%3A//code.visualstudio.com/docs/setup/linux\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">code.visualstudio.com/d</span><span class=\"invisible\">ocs/setup/linux</span><span class=\"ellipsis\"></span></a> 有详细的说明。</p><p>对于centos则如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc\nsudo sh -c &#39;echo -e &#34;[code]\\nname=Visual Studio Code\\nbaseurl=https://packages.microsoft.com/yumrepos/vscode\\nenabled=1\\ngpgcheck=1\\ngpgkey=https://packages.microsoft.com/keys/microsoft.asc&#34; &gt; /etc/yum.repos.d/vscode.repo&#39;</code></pre></div><p>然后用yum命令安装</p><div class=\"highlight\"><pre><code class=\"language-text\">yum check-update\nsudo yum install code</code></pre></div><p>安装成功之后，在你的CentOS中会出现如下画面</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-66e37d926c519669020d586b9479e00b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"467\" data-rawheight=\"222\" class=\"origin_image zh-lightbox-thumb\" width=\"467\" data-original=\"https://pic4.zhimg.com/v2-66e37d926c519669020d586b9479e00b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;467&#39; height=&#39;222&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"467\" data-rawheight=\"222\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"467\" data-original=\"https://pic4.zhimg.com/v2-66e37d926c519669020d586b9479e00b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-66e37d926c519669020d586b9479e00b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>接着说我们上面的操作 ，因为我们这边是用gnome图形界面，所以后面的有些操作，我直接用windows的操作风格讲解了</p><p>打开软件&gt;文件&gt;打开文件&gt;找到我们刚刚创建的<code>down.py</code>文件</p><p>之后，在VSCODE里面输入</p><div class=\"highlight\"><pre><code class=\"language-text\">import requests   #导入模块\n\ndef run():        #声明一个run方法\n    print(&#34;跑码文件&#34;)    #打印内容\n\nif __name__ == &#34;__main__&#34;:   #主程序入口\n    run()    #调用上面的run方法</code></pre></div><p><i>tips:本教程不是Python3的基础入门课，所以有些编码基础，默认你懂，比如Python没有分号结尾，需要对齐格式。我会尽量把注释写的完整</i></p><p>按键盘上的<code>ctrl+s</code>保存文件，如果提示权限不足，那么按照提示输入密码即可</p><p>通过终端进入demo目录，然后输入</p><blockquote><i>python3 down.py</i></blockquote><p>显示如下结果，代表编译没有问题</p><div class=\"highlight\"><pre><code class=\"language-text\">[root@bogon demo]# python3 down.py\n跑码文件</code></pre></div><p>接下来，我们开始测试<code>requests</code>模块是否可以使用</p><p>修改上述代码中的</p><div class=\"highlight\"><pre><code class=\"language-text\">import requests\n\ndef run():\n    response = requests.get(&#34;http://www.baidu.com&#34;)\n    print(response.text)\n\nif __name__ == &#34;__main__&#34;:\n    run()</code></pre></div><p>运行结果（出现下图代表你运行成功了）：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0f61fd277882f127b898e9570e2695ae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1043\" data-rawheight=\"452\" class=\"origin_image zh-lightbox-thumb\" width=\"1043\" data-original=\"https://pic3.zhimg.com/v2-0f61fd277882f127b898e9570e2695ae_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1043&#39; height=&#39;452&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1043\" data-rawheight=\"452\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1043\" data-original=\"https://pic3.zhimg.com/v2-0f61fd277882f127b898e9570e2695ae_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0f61fd277882f127b898e9570e2695ae_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>接下来，我们实际下载一张图片试试，比如下面这张图片</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9d868a545776b417fe2c67ff944805dc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1920\" data-rawheight=\"523\" class=\"origin_image zh-lightbox-thumb\" width=\"1920\" data-original=\"https://pic1.zhimg.com/v2-9d868a545776b417fe2c67ff944805dc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1920&#39; height=&#39;523&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1920\" data-rawheight=\"523\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1920\" data-original=\"https://pic1.zhimg.com/v2-9d868a545776b417fe2c67ff944805dc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-9d868a545776b417fe2c67ff944805dc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>修改代码，在这之前，我们修改一些内容</p><p>由于每次修改文件，都提示必须管理员权限，所以你可以使用linux命令修改权限。</p><blockquote><i>[root@bogon linuxboy]# chmod -R 777 demo/</i></blockquote><div class=\"highlight\"><pre><code class=\"language-text\">import requests\n\ndef run():\n    response = requests.get(&#34;http://www.newsimg.cn/big201710leaderreports/xibdj20171030.jpg&#34;) \n    with open(&#34;xijinping.jpg&#34;,&#34;wb&#34;) as f :\n        f.write(response.content)   \n        f.close\n\nif __name__ == &#34;__main__&#34;:\n    run()</code></pre></div><p>运行代码之后，发现在文件夹内部生成了一个文件</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-05374367b56ebca2d5af645a908297ae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"387\" data-rawheight=\"181\" class=\"content_image\" width=\"387\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;387&#39; height=&#39;181&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"387\" data-rawheight=\"181\" class=\"content_image lazy\" width=\"387\" data-actualsrc=\"https://pic3.zhimg.com/v2-05374367b56ebca2d5af645a908297ae_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>但是打开文件之后发现，这个文件并不能查阅，这代表这个文件压根没有下载下来</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1436f149207c2beca7019c5bb94c7e7d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"540\" data-rawheight=\"503\" class=\"origin_image zh-lightbox-thumb\" width=\"540\" data-original=\"https://pic2.zhimg.com/v2-1436f149207c2beca7019c5bb94c7e7d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;540&#39; height=&#39;503&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"540\" data-rawheight=\"503\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"540\" data-original=\"https://pic2.zhimg.com/v2-1436f149207c2beca7019c5bb94c7e7d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1436f149207c2beca7019c5bb94c7e7d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们继续修改代码，因为有的服务器图片，都做了一些限制，我们可以用浏览器打开，但是使用Python代码并不能完整的下载下来。</p><p>修改代码</p><div class=\"highlight\"><pre><code class=\"language-text\">import requests\n\ndef run():\n    # 头文件，header是字典类型\n    headers = {\n        &#34;Host&#34;:&#34;www.newsimg.cn&#34;,\n        &#34;User-Agent&#34;:&#34;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.26 Safari/537.36 Core/1.63.5383.400 QQBrowser/10.0.1313.400&#34;\n    }\n    response = requests.get(&#34;http://www.newsimg.cn/big201710leaderreports/xibdj20171030.jpg&#34;,headers=headers) \n    with open(&#34;xijinping.jpg&#34;,&#34;wb&#34;) as f :\n        f.write(response.content)   \n        f.close\n\nif __name__ == &#34;__main__&#34;:\n    run()</code></pre></div><p>好了，这次在终端编译一下python文件</p><blockquote><i>python3 down.py</i></blockquote><p>发现图片下载下来了</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0b8df7a771c6eea5bd09b6a3c1d8c5fd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"486\" data-rawheight=\"226\" class=\"origin_image zh-lightbox-thumb\" width=\"486\" data-original=\"https://pic2.zhimg.com/v2-0b8df7a771c6eea5bd09b6a3c1d8c5fd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;486&#39; height=&#39;226&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"486\" data-rawheight=\"226\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"486\" data-original=\"https://pic2.zhimg.com/v2-0b8df7a771c6eea5bd09b6a3c1d8c5fd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0b8df7a771c6eea5bd09b6a3c1d8c5fd_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们重点查看上述代码中 <code>requests.get</code>部分，添加了一个<code>headers</code>的实参。这样我们程序就下载下来了完整的图片。</p><h2><b>Python爬虫页面分析</b></h2><p>有了上面这个简单的案例，我们接下来的操作就变的简单多了。爬虫是如何进行的呢？</p><blockquote><i>输入域名-&gt;下载源代码-&gt;分析图片路径-&gt;下载图片</i></blockquote><p>上面就是他的步骤</p><h3>妹子图网站爬取---输入域名</h3><p>我们今天要爬的网站叫做 <a href=\"https://link.zhihu.com/?target=http%3A//www.meizitu.com/a/pure.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">meizitu.com/a/pure.html</span><span class=\"invisible\"></span></a></p><p>为啥爬取这个网站，因为好爬。</p><p>好了，接下来分析这个页面</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ce116f394cf259c3040d9cc0b4e427a9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"717\" data-rawheight=\"382\" class=\"origin_image zh-lightbox-thumb\" width=\"717\" data-original=\"https://pic2.zhimg.com/v2-ce116f394cf259c3040d9cc0b4e427a9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;717&#39; height=&#39;382&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"717\" data-rawheight=\"382\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"717\" data-original=\"https://pic2.zhimg.com/v2-ce116f394cf259c3040d9cc0b4e427a9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ce116f394cf259c3040d9cc0b4e427a9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>做爬虫很重要的一点，就是你要找到分页的地方，因为有分页代表着有规律，有规律，我们就好爬了(可以做的更智能一些，输入首页网址，爬虫自己就能分析到这个网站中的所有地址)</p><p>上面图片中，我们发现了分页，那么找规律吧</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e493b487c0b515e9fbeb13665d7e384d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"916\" data-rawheight=\"577\" class=\"origin_image zh-lightbox-thumb\" width=\"916\" data-original=\"https://pic2.zhimg.com/v2-e493b487c0b515e9fbeb13665d7e384d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;916&#39; height=&#39;577&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"916\" data-rawheight=\"577\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"916\" data-original=\"https://pic2.zhimg.com/v2-e493b487c0b515e9fbeb13665d7e384d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e493b487c0b515e9fbeb13665d7e384d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>使用火狐浏览器的开发者工具，发现分页规律</p><div class=\"highlight\"><pre><code class=\"language-text\">http://www.meizitu.com/a/pure_1.html\nhttp://www.meizitu.com/a/pure_2.html\nhttp://www.meizitu.com/a/pure_3.html\nhttp://www.meizitu.com/a/pure_4.html</code></pre></div><p>好了，接下来用Python实现这部分（以下写法有部分面向对象的写法，没有基础的同学，请百度找些基础来看，不过对于想学习的你来说，这些简单极了）</p><div class=\"highlight\"><pre><code class=\"language-text\">import requests\nall_urls = []  #我们拼接好的图片集和列表路径\nclass Spider():\n    #构造函数，初始化数据使用\n    def __init__(self,target_url,headers):\n        self.target_url = target_url\n        self.headers = headers\n\n    #获取所有的想要抓取的URL\n    def getUrls(self,start_page,page_num):\n        \n        global all_urls\n        #循环得到URL\n        for i in range(start_page,page_num+1):\n            url = self.target_url  % i\n            all_urls.append(url)\n\nif __name__ == &#34;__main__&#34;:\n    headers = {\n            &#39;User-Agent&#39;: &#39;Mozilla/5.0 (X11; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0&#39;,\n            &#39;HOST&#39;:&#39;www.meizitu.com&#39;\n    }\n    target_url = &#39;http://www.meizitu.com/a/pure_%d.html&#39; #图片集和列表规则\n    \n    spider = Spider(target_url,headers)\n    spider.getUrls(1,16)\n    print(all_urls)</code></pre></div><p>上面的代码，可能需要有一定的Python基础可以看懂，不过你其实仔细看一下，就几个要点</p><p>第一个是 <code>class Spider():</code> 我们声明了一个类,然后我们使用 <code>def __init__</code>去声明一个构造函数，这些我觉得你找个教程30分钟也就学会了。</p><p>拼接URL，我们可以用很多办法，我这里用的是最直接的，字符串拼接。</p><p>注意上述代码中有一个全局的变量 <code>all_urls</code> 我用它来存储我们的所有分页的URL</p><p>接下来，是爬虫最核心的部分代码了</p><p>我们需要分析页面中的逻辑。首先打开 <a href=\"https://link.zhihu.com/?target=http%3A//www.meizitu.com/a/pure_1.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">meizitu.com/a/pure_1.ht</span><span class=\"invisible\">ml</span><span class=\"ellipsis\"></span></a> ，右键审查元素。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3c25cbc2dd3512c53c69955deea9c6e8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"397\" data-rawheight=\"515\" class=\"content_image\" width=\"397\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;397&#39; height=&#39;515&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"397\" data-rawheight=\"515\" class=\"content_image lazy\" width=\"397\" data-actualsrc=\"https://pic1.zhimg.com/v2-3c25cbc2dd3512c53c69955deea9c6e8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d89645bdca1662d66795bd60ba0a0ca9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"873\" data-rawheight=\"442\" class=\"origin_image zh-lightbox-thumb\" width=\"873\" data-original=\"https://pic2.zhimg.com/v2-d89645bdca1662d66795bd60ba0a0ca9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;873&#39; height=&#39;442&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"873\" data-rawheight=\"442\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"873\" data-original=\"https://pic2.zhimg.com/v2-d89645bdca1662d66795bd60ba0a0ca9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d89645bdca1662d66795bd60ba0a0ca9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>发现上图红色框框里面的链接</p><p>点击图片之后，发现进入一个图片详情页面，发现竟然是一组图片，那么现在的问题是</p><p>我们要解决第一步，需要在 <a href=\"https://link.zhihu.com/?target=http%3A//www.meizitu.com/a/pure_1.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">meizitu.com/a/pure_1.ht</span><span class=\"invisible\">ml</span><span class=\"ellipsis\"></span></a> 这种页面中爬取所有的 <a href=\"https://link.zhihu.com/?target=http%3A//www.meizitu.com/a/5585.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">meizitu.com/a/5585.html</span><span class=\"invisible\"></span></a> 这种地址</p><p>这里我们采用多线程的方式爬取（这里还用了一种设计模式，叫观察者模式）</p><div class=\"highlight\"><pre><code class=\"language-text\">import threading   #多线程模块\nimport re #正则表达式模块\nimport time #时间模块</code></pre></div><p>首先引入三个模块，分别是多线程，正则表达式，时间模块</p><p>新增加一个全局的变量，并且由于是多线程操作，我们需要引入线程锁</p><div class=\"highlight\"><pre><code class=\"language-text\">all_img_urls = []       #图片列表页面的数组\n\ng_lock = threading.Lock()  #初始化一个锁  </code></pre></div><p>声明一个生产者的类，用来不断的获取图片详情页地址，然后添加到 <code>all_img_urls</code> 这个全局变量中</p><div class=\"highlight\"><pre><code class=\"language-text\">#生产者，负责从每个页面提取图片列表链接\nclass Producer(threading.Thread):   \n\n    def run(self):\n        headers = {\n            &#39;User-Agent&#39;: &#39;Mozilla/5.0 (X11; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0&#39;,\n            &#39;HOST&#39;:&#39;www.meizitu.com&#39;\n        }\n        global all_urls\n        while len(all_urls) &gt; 0 :\n            g_lock.acquire()  #在访问all_urls的时候，需要使用锁机制\n            page_url = all_urls.pop()   #通过pop方法移除最后一个元素，并且返回该值\n            \n            g_lock.release() #使用完成之后及时把锁给释放，方便其他线程使用\n            try:\n                print(&#34;分析&#34;+page_url)   \n                response = requests.get(page_url , headers = headers,timeout=3)\n                all_pic_link = re.findall(&#39;&lt;a target=\\&#39;_blank\\&#39; href=&#34;(.*?)&#34;&gt;&#39;,response.text,re.S)   \n                global all_img_urls\n                g_lock.acquire()   #这里还有一个锁\n                all_img_urls += all_pic_link   #这个地方注意数组的拼接，没有用append直接用的+=也算是python的一个新语法吧\n                print(all_img_urls)\n                g_lock.release()   #释放锁\n                time.sleep(0.5)\n            except:\n                pass</code></pre></div><p>上述代码用到了继承的概念，我从threading.Thread中继承了一个子类，继承的基础学习，你可以去翻翻 <a href=\"https://link.zhihu.com/?target=http%3A//www.runoob.com/python3/python3-class.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">runoob.com/python3/pyth</span><span class=\"invisible\">on3-class.html</span><span class=\"ellipsis\"></span></a> 菜鸟教程就行。</p><p>线程锁，在上面的代码中，当我们操作<code>all_urls.pop()</code>的时候，我们是不希望其他线程对他进行同时操作的，否则会出现意外，所以我们使用<code>g_lock.acquire()</code>锁定资源，然后使用完成之后，记住一定要立马释放<code>g_lock.release()</code>,否则这个资源就一直被占用着，程序无法进行下去了。</p><p>匹配网页中的URL，我使用的是正则表达式，后面我们会使用其他的办法，进行匹配。</p><p><code>re.findall()</code>方法是获取所有匹配到的内容，正则表达式，你可以找一个30分钟入门的教程，看看就行。</p><p>代码容易出错的地方，我放到了</p><p>try: except: 里面，当然，你也可以自定义错误。</p><p>如果上面的代码，都没有问题，那么我们就可以在程序入口的地方编写</p><div class=\"highlight\"><pre><code class=\"language-text\">for x in range(2):\n    t = Producer()\n    t.start()</code></pre></div><p>执行程序，因为我们的Producer继承自threading.Thread类，所以，你必须要实现的一个方法是 <code>def run</code> 这个我相信在上面的代码中，你已经看到了。然后我们可以执行啦~~~</p><p>运行结果：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4c87a806187c51fd734466b0a5d96a59_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1051\" data-rawheight=\"427\" class=\"origin_image zh-lightbox-thumb\" width=\"1051\" data-original=\"https://pic2.zhimg.com/v2-4c87a806187c51fd734466b0a5d96a59_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1051&#39; height=&#39;427&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1051\" data-rawheight=\"427\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1051\" data-original=\"https://pic2.zhimg.com/v2-4c87a806187c51fd734466b0a5d96a59_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-4c87a806187c51fd734466b0a5d96a59_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这样，图片详情页面的列表就已经被我们存储起来了。</p><p>接下来，我们需要执行这样一步操作，我想要等待图片详情页面全部获取完毕，在进行接下来的分析操作。</p><p>这里增加代码</p><div class=\"highlight\"><pre><code class=\"language-text\">#threads= []   \n#开启两个线程去访问\nfor x in range(2):\n    t = Producer()\n    t.start()\n    #threads.append(t)\n\n# for tt in threads:\n#     tt.join()\n\nprint(&#34;进行到我这里了&#34;)</code></pre></div><p>注释关键代码，运行如下</p><div class=\"highlight\"><pre><code class=\"language-text\">[linuxboy@bogon demo]$ python3 down.py\n分析http://www.meizitu.com/a/pure_2.html\n分析http://www.meizitu.com/a/pure_1.html\n进行到我这里了\n[&#39;http://www.meizitu.com/a/5585.html&#39;,</code></pre></div><p>把上面的tt.join等代码注释打开</p><div class=\"highlight\"><pre><code class=\"language-text\">[linuxboy@bogon demo]$ python3 down.py\n分析http://www.meizitu.com/a/pure_2.html\n分析http://www.meizitu.com/a/pure_1.html\n[&#39;http://www.meizitu.com/a/5429.html&#39;, ......\n进行到我这里了</code></pre></div><p>发现一个本质的区别，就是，我们由于是多线程的程序，所以，当程序跑起来之后，<code>print(&#34;进行到我这里了&#34;)</code>不会等到其他线程结束，就会运行到，但是当我们改造成上面的代码之后，也就是加入了关键的代码 <code>tt.join()</code> 那么主线程的代码会等到所以子线程运行完毕之后，在接着向下运行。这就满足了，我刚才说的，先获取到所有的图片详情页面的集合，这一条件了。</p><p>join所完成的工作就是线程同步，即主线程遇到join之后进入阻塞状态，一直等待其他的子线程执行结束之后，主线程在继续执行。这个大家在以后可能经常会碰到。</p><p>下面编写一个消费者/观察者，也就是不断关注刚才我们获取的那些图片详情页面的数组。</p><p>添加一个全局变量，用来存储获取到的图片链接</p><div class=\"highlight\"><pre><code class=\"language-text\">pic_links = []            #图片地址列表\n#消费者\nclass Consumer(threading.Thread) : \n    def run(self):\n        headers = {\n            &#39;User-Agent&#39;: &#39;Mozilla/5.0 (X11; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0&#39;,\n            &#39;HOST&#39;:&#39;www.meizitu.com&#39;\n        }\n        global all_img_urls   #调用全局的图片详情页面的数组\n        print(&#34;%s is running &#34; % threading.current_thread)\n        while len(all_img_urls) &gt;0 : \n            g_lock.acquire()\n            img_url = all_img_urls.pop()\n            g_lock.release()\n            try:\n                response = requests.get(img_url , headers = headers )\n                response.encoding=&#39;gb2312&#39;   #由于我们调用的页面编码是GB2312，所以需要设置一下编码\n                title = re.search(&#39;&lt;title&gt;(.*?) | 妹子图&lt;/title&gt;&#39;,response.text).group(1)\n                all_pic_src = re.findall(&#39;&lt;img alt=.*?src=&#34;(.*?)&#34; /&gt;&lt;br /&gt;&#39;,response.text,re.S)\n                \n                pic_dict = {title:all_pic_src}   #python字典\n                global pic_links\n                g_lock.acquire()\n                pic_links.append(pic_dict)    #字典数组\n                print(title+&#34; 获取成功&#34;)\n                g_lock.release()\n                \n            except:\n                pass\n            time.sleep(0.5)</code></pre></div><p>看到没有，上面的代码其实和我们刚才写的极其相似，后面，我会在github上面把这部分代码修改的更加简洁一些，不过这才是第二课，后面我们的路长着呢。</p><p>代码中比较重要的一些部分，我已经使用注释写好了，大家可以直接参考。大家一定要注意我上面使用了两个正则表达式，分别用来匹配title和图片的url这个title是为了后面创建不同的文件夹使用的，所以大家注意吧。</p><div class=\"highlight\"><pre><code class=\"language-text\">#开启10个线程去获取链接\nfor x in range(10):\n    ta = Consumer()\n    ta.start()</code></pre></div><p>运行结果：</p><div class=\"highlight\"><pre><code class=\"language-text\">[linuxboy@bogon demo]$ python3 down.py\n分析http://www.meizitu.com/a/pure_2.html\n分析http://www.meizitu.com/a/pure_1.html\n[&#39;http://www.meizitu.com/a/5585.html&#39;, ......\n&lt;function current_thread at 0x7f7caef851e0&gt; is running \n&lt;function current_thread at 0x7f7caef851e0&gt; is running \n&lt;function current_thread at 0x7f7caef851e0&gt; is running \n&lt;function current_thread at 0x7f7caef851e0&gt; is running \n&lt;function current_thread at 0x7f7caef851e0&gt; is running \n&lt;function current_thread at 0x7f7caef851e0&gt; is running \n&lt;function current_thread at 0x7f7caef851e0&gt; is running \n&lt;function current_thread at 0x7f7caef851e0&gt; is running \n进行到我这里了\n&lt;function current_thread at 0x7f7caef851e0&gt; is running \n&lt;function current_thread at 0x7f7caef851e0&gt; is running \n清纯美如画，摄影师的御用麻豆 获取成功\n宅男女神叶梓萱近日拍摄一组火爆写真 获取成功\n美（bao）胸（ru）女王带来制服诱惑 获取成功\n每天睁开眼看到美好的你，便是幸福 获取成功\n可爱女孩，愿暖风呵护纯真和执着 获取成功\n清纯妹子如一缕阳光温暖这个冬天 获取成功 .....</code></pre></div><p>是不是感觉距离成功有进了一大步</p><p>接下来就是，我们开篇提到的那个存储图片的操作了，还是同样的步骤，写一个自定义的类</p><div class=\"highlight\"><pre><code class=\"language-text\">class DownPic(threading.Thread) :\n\n    def run(self):\n        headers = {\n            &#39;User-Agent&#39;: &#39;Mozilla/5.0 (X11; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0&#39;,\n            &#39;HOST&#39;:&#39;mm.chinasareview.com&#39;\n        \n        }\n        while True:   #  这个地方写成死循环，为的是不断监控图片链接数组是否更新\n            global pic_links\n            # 上锁\n            g_lock.acquire()\n            if len(pic_links) == 0:   #如果没有图片了，就解锁\n                # 不管什么情况，都要释放锁\n                g_lock.release()\n                continue\n            else:\n                pic = pic_links.pop()\n                g_lock.release()\n                # 遍历字典列表\n                for key,values in  pic.items():\n                    path=key.rstrip(&#34;\\\\&#34;)\n                    is_exists=os.path.exists(path)\n                    # 判断结果\n                    if not is_exists:\n                        # 如果不存在则创建目录\n                        # 创建目录操作函数\n                        os.makedirs(path) \n                \n                        print (path+&#39;目录创建成功&#39;)\n                        \n                    else:\n                        # 如果目录存在则不创建，并提示目录已存在\n                        print(path+&#39; 目录已存在&#39;) \n                    for pic in values :\n                        filename = path+&#34;/&#34;+pic.split(&#39;/&#39;)[-1]\n                        if os.path.exists(filename):\n                            continue\n                        else:\n                            response = requests.get(pic,headers=headers)\n                            with open(filename,&#39;wb&#39;) as f :\n                                f.write(response.content)\n                                f.close</code></pre></div><p>我们获取图片链接之后，就需要下载了，我上面的代码是首先创建了一个之前获取到<code>title</code>的文件目录，然后在目录里面通过下面的代码,去创建一个文件。</p><p>涉及到文件操作，引入一个新的模块</p><div class=\"highlight\"><pre><code class=\"language-text\">import os  #目录操作模块\n# 遍历字典列表\nfor key,values in  pic.items():\n    path=key.rstrip(&#34;\\\\&#34;)\n    is_exists=os.path.exists(path)\n    # 判断结果\n    if not is_exists:\n        # 如果不存在则创建目录\n        # 创建目录操作函数\n        os.makedirs(path) \n\n        print (path+&#39;目录创建成功&#39;)\n        \n    else:\n        # 如果目录存在则不创建，并提示目录已存在\n        print(path+&#39; 目录已存在&#39;) \n    for pic in values :\n        filename = path+&#34;/&#34;+pic.split(&#39;/&#39;)[-1]\n        if os.path.exists(filename):\n            continue\n        else:\n            response = requests.get(pic,headers=headers)\n            with open(filename,&#39;wb&#39;) as f :\n                f.write(response.content)\n                f.close</code></pre></div><p>因为我们的图片链接数组，里面存放是的字典格式，也就是下面这种格式</p><div class=\"highlight\"><pre><code class=\"language-text\">[{&#34;妹子图1&#34;:[&#34;http://mm.chinasareview.com/wp-content/uploads/2016a/08/24/01.jpg&#34;,&#34;http://mm.chinasareview.com/wp-content/uploads/2016a/08/24/02.jpg&#34;.&#34;http://mm.chinasareview.com/wp-content/uploads/2016a/08/24/03.jpg&#34;]},{&#34;妹子图2&#34;:[&#34;http://mm.chinasareview.com/wp-content/uploads/2016a/08/24/01.jpg&#34;,&#34;http://mm.chinasareview.com/wp-content/uploads/2016a/08/24/02.jpg&#34;.&#34;http://mm.chinasareview.com/wp-content/uploads/2016a/08/24/03.jpg&#34;]},{&#34;妹子图3&#34;:[&#34;http://mm.chinasareview.com/wp-content/uploads/2016a/08/24/01.jpg&#34;,&#34;http://mm.chinasareview.com/wp-content/uploads/2016a/08/24/02.jpg&#34;.&#34;http://mm.chinasareview.com/wp-content/uploads/2016a/08/24/03.jpg&#34;]}]</code></pre></div><p>需要先循环第一层,获取title，创建目录之后，在循环第二层去下载图片，代码中，我们在修改一下，把异常处理添加上。</p><div class=\"highlight\"><pre><code class=\"language-text\">try:\n    response = requests.get(pic,headers=headers)\n    with open(filename,&#39;wb&#39;) as f :\n        f.write(response.content)\n        f.close\nexcept Exception as e:\n    print(e)\n    pass</code></pre></div><p>然后在主程序中编写代码</p><div class=\"highlight\"><pre><code class=\"language-text\">#开启10个线程保存图片\nfor x in range(10):\n    down = DownPic()\n    down.start()</code></pre></div><p>运行结果：</p><div class=\"highlight\"><pre><code class=\"language-text\">[linuxboy@bogon demo]$ python3 down.py\n分析http://www.meizitu.com/a/pure_2.html\n分析http://www.meizitu.com/a/pure_1.html\n[&#39;http://www.meizitu.com/a/5585.html&#39;, &#39;http://www.meizitu.com/a/5577.html&#39;, &#39;http://www.meizitu.com/a/5576.html&#39;, &#39;http://www.meizitu.com/a/5574.html&#39;, &#39;http://www.meizitu.com/a/5569.html&#39;, .......\n&lt;function current_thread at 0x7fa5121f2268&gt; is running \n&lt;function current_thread at 0x7fa5121f2268&gt; is running \n&lt;function current_thread at 0x7fa5121f2268&gt; is running \n进行到我这里了\n清纯妹子如一缕阳光温暖这个冬天 获取成功\n清纯妹子如一缕阳光温暖这个冬天目录创建成功\n可爱女孩，愿暖风呵护纯真和执着 获取成功\n可爱女孩，愿暖风呵护纯真和执着目录创建成功\n超美，纯纯的你与蓝蓝的天相得益彰 获取成功\n超美，纯纯的你与蓝蓝的天相得益彰目录创建成功\n美丽冻人，雪地里的跆拳道少女 获取成功\n五官精致的美眉，仿佛童话里的公主 获取成功\n有自信迷人的笑容，每天都是灿烂的 获取成功\n五官精致的美眉，仿佛童话里的公主目录创建成功\n有自信迷人的笑容，每天都是灿烂的目录创建成功\n清纯美如画，摄影师的御用麻豆 获取成功</code></pre></div><p>文件目录下面同时出现</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-879185ae67388dd1d687fcd0cf32235c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1101\" data-rawheight=\"524\" class=\"origin_image zh-lightbox-thumb\" width=\"1101\" data-original=\"https://pic1.zhimg.com/v2-879185ae67388dd1d687fcd0cf32235c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1101&#39; height=&#39;524&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1101\" data-rawheight=\"524\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1101\" data-original=\"https://pic1.zhimg.com/v2-879185ae67388dd1d687fcd0cf32235c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-879185ae67388dd1d687fcd0cf32235c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>点开一个目录</p><p>好了，今天的一个简单的爬虫成了</p><p>最后我们在代码的头部写上</p><div class=\"highlight\"><pre><code class=\"language-text\"># -*- coding: UTF-8 -*-   </code></pre></div><p>防止出现 <code>Non-ASCII character &#39;xe5&#39; in file</code>报错问题。</p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": [
                {
                    "userName": "Sample", 
                    "userLink": "https://www.zhihu.com/people/1bd2a3a68d5b1c182da17a2813b0ceee", 
                    "content": "<p>最后下载图片的时候，我的图片链接有重定向，一直报错requests.exceptions.TooManyRedirects:Exceeded 30 redirects</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/64213949", 
            "userName": "胡萝卜", 
            "userLink": "https://www.zhihu.com/people/d6ac390aa8733531ac26975d9fc69a9f", 
            "upvote": 19, 
            "title": "Python爬虫入门教程 1-100 CentOS环境安装", 
            "content": "<h2><b>简介</b></h2><p>你好，当你打开这个文档的时候，我知道，你想要的是什么！ Python爬虫，如何快速的学会Python爬虫，是你最期待的事情，可是这个事情应该没有想象中的那么容易，况且你的编程底子还不一定好，这套课程，没有你想要的Python基础，没有变量，循环，数组等基础知识，因为我不想在那些你可以直接快速学会的地方，去浪费你的时间。</p><p>好了，这套课程是基于Python3.0 以上写的，操作系统我使用的是CentOS7+ 所以里面的好多内容可能和你的不一样，当然也会导致许多问题的解决和你的不同，所以有的问题，需要你自己百度或者FQ解决啦，祝你碰到N多的BUG。O(∩_∩)O</p><p>接下来的第一步是什么？</p><p>安装一个虚拟机，因为你的电脑99%是windows的，所以你需要一个软件叫做 VMware 然后，下载地址 在2018年5月10日这一天，我百度到的是</p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.wuleba.com/309.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">wuleba.com/309.html</span><span class=\"invisible\"></span></a> 在这个网址里面有这款软件的下载和你懂的。</p><p>然后，我接着百度到了一个CentOS7的操作系统</p><p><i>软件下载之后的名字</i></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5abbff7fae0ff349c51544de7de09e31_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"808\" data-rawheight=\"23\" class=\"origin_image zh-lightbox-thumb\" width=\"808\" data-original=\"https://pic2.zhimg.com/v2-5abbff7fae0ff349c51544de7de09e31_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;808&#39; height=&#39;23&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"808\" data-rawheight=\"23\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"808\" data-original=\"https://pic2.zhimg.com/v2-5abbff7fae0ff349c51544de7de09e31_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5abbff7fae0ff349c51544de7de09e31_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>准备工作做好了，接下来就是需要你需要完成的操作了</p><p>首先，安装VM这款软件，并且把它&#34;pojie&#34;掉.（当有一天，你赚到了钱，记得在去买一下这款软件，支持一下）</p><p>安装软件，全部使用默认选项即可，这个地方因为简单到不需要我写了，有问题大家可以自行解决。</p><p><i>安装完毕之前的最后一步</i><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f0fca2dcb7e23df80e2eb361e56d086a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"497\" data-rawheight=\"426\" class=\"origin_image zh-lightbox-thumb\" width=\"497\" data-original=\"https://pic3.zhimg.com/v2-f0fca2dcb7e23df80e2eb361e56d086a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;497&#39; height=&#39;426&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"497\" data-rawheight=\"426\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"497\" data-original=\"https://pic3.zhimg.com/v2-f0fca2dcb7e23df80e2eb361e56d086a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f0fca2dcb7e23df80e2eb361e56d086a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>CentOS 7 安装</b></h2><p>接下来就进入CentOS的安装了</p><p>安装过程中几个关键点位，要确定好</p><p><i>选择中文，下一步</i></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-67b823c0f6f322943c265987b337ca9c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"785\" data-rawheight=\"591\" class=\"origin_image zh-lightbox-thumb\" width=\"785\" data-original=\"https://pic1.zhimg.com/v2-67b823c0f6f322943c265987b337ca9c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;785&#39; height=&#39;591&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"785\" data-rawheight=\"591\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"785\" data-original=\"https://pic1.zhimg.com/v2-67b823c0f6f322943c265987b337ca9c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-67b823c0f6f322943c265987b337ca9c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><i>红框的几个地方请注意</i></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3eca4afc5a700f97b4d81445883f8ec9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"794\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"794\" data-original=\"https://pic2.zhimg.com/v2-3eca4afc5a700f97b4d81445883f8ec9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;794&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"794\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"794\" data-original=\"https://pic2.zhimg.com/v2-3eca4afc5a700f97b4d81445883f8ec9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3eca4afc5a700f97b4d81445883f8ec9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><i>修改成下图的样子</i><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-593419f04074e8db53a8c8347fa7f491_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"583\" data-rawheight=\"306\" class=\"origin_image zh-lightbox-thumb\" width=\"583\" data-original=\"https://pic2.zhimg.com/v2-593419f04074e8db53a8c8347fa7f491_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;583&#39; height=&#39;306&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"583\" data-rawheight=\"306\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"583\" data-original=\"https://pic2.zhimg.com/v2-593419f04074e8db53a8c8347fa7f491_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-593419f04074e8db53a8c8347fa7f491_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>OK，我们已经做好配置了，接下来点击下一步，在下一步，你只需要配置一个root账户的密码就可以了</p><p>接下来就是几分钟的等待了，跟你电脑配置有关系，时间不等。</p><p>安装之后，打开CentOS操作系统，会出现一个嘿嘿的DOS命令窗口，输入账户root和你刚刚设置的密码，进入没有问题，完美~代表你的操作系统已经安装好了，小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~</p><p>接下来，为了方便我们后面的操作，我们要给我们的虚拟机安装一个GUI交互界面（也就是像windows一样的视窗操作软件）</p><p>安装的教程，我建议你观看 <a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/c-xiaohai/p/6509641.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">cnblogs.com/c-xiaohai/p</span><span class=\"invisible\">/6509641.html</span><span class=\"ellipsis\"></span></a> 这个博主的，简单，清楚，一次成功。</p><p>如果失败了，怎么办，百度“如何安装gnome图形界面程序” 想办法啃下来。</p><p><i>当出现这样子的一些图标的时候，你成功了</i></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a6d0c25cab3c70b87f427609f8671537_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"499\" data-rawheight=\"585\" class=\"origin_image zh-lightbox-thumb\" width=\"499\" data-original=\"https://pic4.zhimg.com/v2-a6d0c25cab3c70b87f427609f8671537_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;499&#39; height=&#39;585&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"499\" data-rawheight=\"585\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"499\" data-original=\"https://pic4.zhimg.com/v2-a6d0c25cab3c70b87f427609f8671537_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a6d0c25cab3c70b87f427609f8671537_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>可能碰到的问题</b></h2><p>好了，接下来说一下，你可能碰到的问题</p><ol><li>你点击应用程序，里面找到一个叫做火狐浏览器的软件，然后打开，发现竟然无法上网？！</li><li>你发现竟然无法使用中文输入法？！nice 这个问题，自行解决，随便找找一堆解决方案。</li></ol><p>好了，上面问题2解决了，那么解决一下问题1吧，这个问题首先确认一下你在一开始安装的时候，下面这个图配置的是否正确。</p><p><i>需要显示已连接</i><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5d72944a80e1b71b1ba9df4c329c3e0c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"327\" data-rawheight=\"93\" class=\"content_image\" width=\"327\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;327&#39; height=&#39;93&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"327\" data-rawheight=\"93\" class=\"content_image lazy\" width=\"327\" data-actualsrc=\"https://pic1.zhimg.com/v2-5d72944a80e1b71b1ba9df4c329c3e0c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如果上图没有问题，那么99%的童鞋是可以上网的，那么还是有无法上网的，这时候，打开一个叫做<code>终端</code>的软件</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ece5ebe52b0ff9a8b1df5aa4a45892a5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"431\" data-rawheight=\"359\" class=\"origin_image zh-lightbox-thumb\" width=\"431\" data-original=\"https://pic2.zhimg.com/v2-ece5ebe52b0ff9a8b1df5aa4a45892a5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;431&#39; height=&#39;359&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"431\" data-rawheight=\"359\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"431\" data-original=\"https://pic2.zhimg.com/v2-ece5ebe52b0ff9a8b1df5aa4a45892a5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ece5ebe52b0ff9a8b1df5aa4a45892a5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>输入代码</p><div class=\"highlight\"><pre><code class=\"language-text\">ping www.baidu.com</code></pre></div><p><i>网络通畅</i><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3afbb6a5dfa1cafec524f1611487eb2f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"747\" data-rawheight=\"153\" class=\"origin_image zh-lightbox-thumb\" width=\"747\" data-original=\"https://pic4.zhimg.com/v2-3afbb6a5dfa1cafec524f1611487eb2f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;747&#39; height=&#39;153&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"747\" data-rawheight=\"153\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"747\" data-original=\"https://pic4.zhimg.com/v2-3afbb6a5dfa1cafec524f1611487eb2f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3afbb6a5dfa1cafec524f1611487eb2f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如果联网失败，敲入下图中的命令，如果联网成功，那么下面的内容，你直接跳过，去查看安装Python部分吧。</p><h2><b>网络连接失败，修改方案</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-23ebd04ac3669c0fd495d3568463567a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"903\" data-rawheight=\"208\" class=\"origin_image zh-lightbox-thumb\" width=\"903\" data-original=\"https://pic3.zhimg.com/v2-23ebd04ac3669c0fd495d3568463567a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;903&#39; height=&#39;208&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"903\" data-rawheight=\"208\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"903\" data-original=\"https://pic3.zhimg.com/v2-23ebd04ac3669c0fd495d3568463567a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-23ebd04ac3669c0fd495d3568463567a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>使用命令 ，注意，你的文件不一定叫ifcfg-ens33，可能叫 ifcgf-xxx 操作都一样</p><div class=\"highlight\"><pre><code class=\"language-text\">vi ifcfg-ens33</code></pre></div><p>进入编辑页面 vi 的简单的操作</p><p>进入编辑状态请按键盘上的 i</p><p>退出编辑状态，请按键盘上的 ESC</p><p>然后输入 :wq 保存，退出，其他的命令用到在说吧。</p><p><i>图中有几个重点的地方，已经标注</i></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4847a739c73817df28becb6a5e12df00_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"437\" data-rawheight=\"360\" class=\"origin_image zh-lightbox-thumb\" width=\"437\" data-original=\"https://pic1.zhimg.com/v2-4847a739c73817df28becb6a5e12df00_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;437&#39; height=&#39;360&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"437\" data-rawheight=\"360\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"437\" data-original=\"https://pic1.zhimg.com/v2-4847a739c73817df28becb6a5e12df00_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-4847a739c73817df28becb6a5e12df00_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><blockquote><i>首先看到你的文件中是否有 HWADDR=mac地址 【MAC地址获取见下图】</i><br/><i>如果没有，自己手动加上，这里使用的DHCP连接网络的方式，所以，你需要把BOOTPROTO=&#34;dhcp&#34; 如果需要修改成 &#34;static&#34; 也可以，不过需要配置一些其他的内容</i><br/><i>保存文件退出</i></blockquote><p><i>获取MAC地址，注意图片中的标注</i></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-27f84d109406b94c09d61d4996b15385_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"891\" data-rawheight=\"361\" class=\"origin_image zh-lightbox-thumb\" width=\"891\" data-original=\"https://pic2.zhimg.com/v2-27f84d109406b94c09d61d4996b15385_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;891&#39; height=&#39;361&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"891\" data-rawheight=\"361\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"891\" data-original=\"https://pic2.zhimg.com/v2-27f84d109406b94c09d61d4996b15385_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-27f84d109406b94c09d61d4996b15385_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>接下来 使用下面的命令重启网络服务</p><div class=\"highlight\"><pre><code class=\"language-text\">service network restart</code></pre></div><p>在这里，你可能就重启成功了</p><p>如果失败，那么这个地方可能是一个非常大的坑了，网上的教程五花八门，但是大多数都是Copy一样的。</p><p>这里你可以按照各种教程去试一下，下面给大家几种常见的解决办法。</p><ol><li>你自己电脑上面的两个服务没有开启</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8a6c1237aa4ebc7aa26008eff8bb816e_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"53\" class=\"origin_image zh-lightbox-thumb\" width=\"750\" data-original=\"https://pic3.zhimg.com/v2-8a6c1237aa4ebc7aa26008eff8bb816e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;750&#39; height=&#39;53&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"53\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"750\" data-original=\"https://pic3.zhimg.com/v2-8a6c1237aa4ebc7aa26008eff8bb816e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8a6c1237aa4ebc7aa26008eff8bb816e_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ol><li>CentOS操作系统中 NetworkManager 服务有冲突</li></ol><p>这个需要在终端中解决</p><p>打开一个终端，输入如下命令</p><div class=\"highlight\"><pre><code class=\"language-text\">service NetworkManager stop  【回车】\n\nchkconfig NetworkManager off   【回车】   \n\n上面的命令是停止Network然后禁止开机启动，这样子在执行  service network restart  去尝试一下</code></pre></div><h2><b>安装Python</b></h2><p>接下来就是我们的开发工具了，Python的安装</p><p>默认我们的CentOS已经给我们带了一个Python2</p><p>你可以直接使用</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ca7b0ceb5b15c527484bc0dbd2bf6a86_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"660\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb\" width=\"660\" data-original=\"https://pic3.zhimg.com/v2-ca7b0ceb5b15c527484bc0dbd2bf6a86_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;660&#39; height=&#39;92&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"660\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"660\" data-original=\"https://pic3.zhimg.com/v2-ca7b0ceb5b15c527484bc0dbd2bf6a86_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ca7b0ceb5b15c527484bc0dbd2bf6a86_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们重新安装一下最新的Python3.0以上的版本，这里先安装一些基本的内容，CentOS使用 yum 安装程序</p><div class=\"highlight\"><pre><code class=\"language-text\">yum -y install zlib zlib-devel\nyum -y install bzip2 bzip2-devel\nyum -y install ncurses ncurses-devel\nyum -y install readline readline-devel\nyum -y install openssl openssl-devel\nyum -y install openssl-static\nyum -y install xz lzma xz-devel\nyum -y install sqlite sqlite-devel\nyum -y install gdbm gdbm-devel\nyum -y install tk tk-devel</code></pre></div><blockquote><i>里面的 -y 是代表所有需要用户确认的地方选择yes不需要用户输入了</i></blockquote><p>为了方便后面我们的Python3编译，还需要安装一个gcc编译器</p><div class=\"highlight\"><pre><code class=\"language-text\">yum -y install gcc</code></pre></div><p>之后到python官网下载 <a href=\"https://link.zhihu.com/?target=https%3A//www.python.org/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">https://www.python.org</a></p><p>终端运行以下命令</p><p>下载安装包</p><div class=\"highlight\"><pre><code class=\"language-text\">wget https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz</code></pre></div><p>解压下载好的Python-3.x.x.tgz包</p><div class=\"highlight\"><pre><code class=\"language-text\">tar -zxvf Python-3.6.5.tgz</code></pre></div><p>进入解压后的目录，编译安装。</p><div class=\"highlight\"><pre><code class=\"language-text\">cd Python-3.6.5/\n./configure --prefix=/usr/local/python3</code></pre></div><p>make</p><div class=\"highlight\"><pre><code class=\"language-text\">make</code></pre></div><p>make install</p><div class=\"highlight\"><pre><code class=\"language-text\">make install</code></pre></div><p>建立python3的软链</p><div class=\"highlight\"><pre><code class=\"language-text\">ln -s /usr/local/python3/bin/python3 /usr/bin/python3</code></pre></div><p>OK到现在为止，你已经创建好了python3的环境了，在终端中尝试一下吧</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-90e7e83466bc53b59f45373a6279357c_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"652\" data-rawheight=\"91\" class=\"origin_image zh-lightbox-thumb\" width=\"652\" data-original=\"https://pic1.zhimg.com/v2-90e7e83466bc53b59f45373a6279357c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;652&#39; height=&#39;91&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"652\" data-rawheight=\"91\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"652\" data-original=\"https://pic1.zhimg.com/v2-90e7e83466bc53b59f45373a6279357c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-90e7e83466bc53b59f45373a6279357c_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>接下来，你可能面临的一个问题就是安装setuptools和pip3的问题了，这个就留给大家自己解决去了。</p><p>最后输入一个 hello world 结束我们的1/100</p><div class=\"highlight\"><pre><code class=\"language-text\">print(&#34;hello,world&#34;)</code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": [
                {
                    "userName": "iuerQ", 
                    "userLink": "https://www.zhihu.com/people/a4f5eea0ef99206eacb9b2f799390837", 
                    "content": "你是个好人[爱]", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "bituplink", 
                    "userLink": "https://www.zhihu.com/people/d22bccce23974721e3497ae0d49ff4a4", 
                    "content": "在这里接触爬虫的人应该没啥太多基础，一上来就linux系统会增加学习负担的，Windows安装python和IDE坑会少很多的，建议新手在Windows平台开始学习爬虫", 
                    "likes": 2, 
                    "childComments": []
                }, 
                {
                    "userName": "Prince", 
                    "userLink": "https://www.zhihu.com/people/10217efaef2b8f0fccf748db9876fbc8", 
                    "content": "你这个CentOS7了还在按照CentOS6的使用习惯，另外CentOS8已经发布了！<br>建议安装的时候关闭kdump，减少资源占用。安装python3也不需要编译，直接使用yum就能安装！", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_1089466161258815488"
}
