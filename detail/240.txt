{
    "title": "小白的学习笔记", 
    "description": "", 
    "followers": [
        "https://www.zhihu.com/people/0564sxth-70", 
        "https://www.zhihu.com/people/han-yu-dong-10", 
        "https://www.zhihu.com/people/mo-cha-niu-ya-tang-4", 
        "https://www.zhihu.com/people/yiqi-da-da", 
        "https://www.zhihu.com/people/powertyuui", 
        "https://www.zhihu.com/people/wei-jin-jie-32", 
        "https://www.zhihu.com/people/cv2019", 
        "https://www.zhihu.com/people/jigenghua", 
        "https://www.zhihu.com/people/zhu-zhu-59-14-17", 
        "https://www.zhihu.com/people/zhao-rong-89-59", 
        "https://www.zhihu.com/people/xiao-yao-tong", 
        "https://www.zhihu.com/people/qi-che-ren-82", 
        "https://www.zhihu.com/people/nebel-51", 
        "https://www.zhihu.com/people/kkk-tttt", 
        "https://www.zhihu.com/people/qiqi-35", 
        "https://www.zhihu.com/people/zhi-bai-41-24", 
        "https://www.zhihu.com/people/zhang-guo-qing-57-96", 
        "https://www.zhihu.com/people/zhe-da-gai-jiu-shi-ren-sheng-ba", 
        "https://www.zhihu.com/people/whoisyourdaddy-2000", 
        "https://www.zhihu.com/people/hai-zhou-wan-81", 
        "https://www.zhihu.com/people/steven-62-63-84", 
        "https://www.zhihu.com/people/zhou-xin-45-21", 
        "https://www.zhihu.com/people/bu-ru-gui-qu-22-63", 
        "https://www.zhihu.com/people/wu-shang-lin-51", 
        "https://www.zhihu.com/people/wu-kong-80-98", 
        "https://www.zhihu.com/people/mao-xiao-miao-57-32", 
        "https://www.zhihu.com/people/pumpkin-14-30", 
        "https://www.zhihu.com/people/shmilymzg", 
        "https://www.zhihu.com/people/lixxx333", 
        "https://www.zhihu.com/people/zhao-heng-47", 
        "https://www.zhihu.com/people/luo-xiao-tong-46", 
        "https://www.zhihu.com/people/chen-shi-95-1", 
        "https://www.zhihu.com/people/super-skyos", 
        "https://www.zhihu.com/people/jie-sang-4"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/76872852", 
            "userName": "idejie", 
            "userLink": "https://www.zhihu.com/people/67d6bc7c8363bef16771d582f32ffc25", 
            "upvote": 6, 
            "title": "[论文阅读]Deep Cross-Modal Hashing", 
            "content": "<p> 原文：</p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1602.02255\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deep Cross-Modal Hashing</a><p><br/> 代码：</p><p class=\"ztext-empty-paragraph\"><br/></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/jiangqy/DCMH-CVPR2017\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">DCMH-CVPR2017 (Matlab Version)</a><p>古文师兄用PyTorch实现的：</p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/WendellGul/DCMH\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-e4feb619d71221ff83234235b45b37a0_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">WendellGul/DCMH</a><p class=\"ztext-empty-paragraph\"><br/></p><h2>摘要</h2><p>由于跨模态哈希（Cross-Modal Hashing，CMH）低存储成本和快速的查询速度，它已被广泛用于多媒体检索应用中的相似性搜索。 但是，大多数现有的CMH方法都是基于手工制作的特征，这些特征可能与哈希码学习过程不能最佳兼容。 因此，使用手工提取的特征的现有CMH方法可能无法实现令人满意的性能。 在本文中，我们通过将特征学习和哈希码学习集成到同一框架中，提出了一种新的CMH方法，称为深度跨模态哈希（Deep Cross-Modal Hashing,DCMH）。 DCMH是一个端到端的学习框架，具有深度神经网络，每个模态对应一个神经网络从头开始进行特征学习。 在具有图像-文本模态的三个真实数据集上的实验表明，DCMH可以胜过其他baseline，实现了跨模态检索应用中的最佳性能。</p><h2>1.Introduction</h2><p>近似最近邻（Approximate nearest neighbor ,ANN）搜索在机器学习和信息检索等相关应用中起着重要作用。 由于哈希的低存储成本和快速的检索速度，它最近引起了人工神经网络研究界的广泛关注。 哈希的目标是将来自原始空间的数据点映射到二进制代码的汉明（Hamming）空间，其中原始空间中的相似性保留在汉明空间中。 通过使用二进制哈希码来表示原始数据，可以显着降低存储成本。 此外，我们可以通过使用哈希码构建索引，从而实现搜索的常数级或亚线性级的时间复杂度。 因此，哈希在大规模数据集近似最近邻搜索中越来越受到欢迎。</p><p>在许多应用中，数据可以具有多种模态。 例如，除了图像内容之外，还存在文本信息，例如Flickr和许多其他社交网站中的图像的标签。 这种数据总是被称为多模态数据。 随着多模态数据在实际应用中的快速增长，尤其是多媒体应用，多模态哈希（Multi-Modal Hashing, MMH）最近被广泛用于多模态数据集的近似最近邻检索。</p><p>现有的MMH方法可分为两大类：多源哈希（Multi-Source Hashing，MSH）和跨模态哈希（Cross-Modal Hashing, CMH)。 MSH的目标是通过利用来自多种模态的所有信息来学习哈希码。因此，MSH要求所有数据点（包括查询数据点和数据库中的数据点）都应保留所有模态。在实际中，MSH的应用是有限的，因为在许多情况下难以获得所有数据点的所有模态。相反，CMH的应用场景比MSH的应用场景更灵活。在CMH中，查询点的模态与数据库中的点的模态不同。此外通常查询数据点只有一种模态，数据库中的数据点可以有一个或多个模态。例如，我们可以使用文本查询来检索数据库中的图像，我们还可以使用图像查询来检索数据库中的文本。由于CMH的广泛应用，CMH比MSH更受关注。</p><p>最近提出了许多CMH方法。 现有的代表性方法包括跨模态相似度敏感哈希（cross modality similarity sensitive hashing，CMSSH,通过最小化不同模态的相似样本之间的汉明距离, 最大化不同模态的不相似样本间的汉明距离, 学习哈希函数)，跨视图哈希（cross view hashing, CVH, 把谱哈希扩展到 跨模态检索, 通过最小化加权距离, 保持相似样本 (模态内和模态间) 的相似性），多模态潜在二进制嵌入（Multi-modal latent binary embedding, MLBE, 提出一个概率生成模型, 通过保持多模态样本的模 态内和模态间的相似度来学习哈希函数），联合正则化的哈希（co-regularized hashing，CRH）， 语义相关最大化（semantic correlation maximization,SCM），协同矩阵分解哈希（collective matrix factorization hashing,CMFH），语义主题多模态哈希（semantic topic multi-modal hashing, STMH）和语义保留哈希（semantics preserving hashing, SePH）。 几乎所有这些现有的CMH方法都基于手工制作的功能。 这些手工制作的基于特征的方法的一个缺点是特征提取过程独立于哈希码的学习过程，这意味着手工制作的特征可能与哈希码学习过程不是最佳兼容的。 因此，这些具有手工制作特征的现有CMH方法在实际应用中可能无法获得令人满意的性能。</p><p>最近，使用神经网络的深度学习被广泛用于从头开始进行的特征学习，并且具有很好的性能。 还有一些采用深度学习进行单模态哈希的方法。 这些方法表明端到端深度学习架构更适合于哈希学习。 对于CMH设置，还出现了一种方法，称为深度视觉语义哈希（deep visual-semantic hashing, DVSH），一个用于特征学习的深度神经网络。 但是，DVSH只能用于特殊的CMH情况，也就是其中一种模态必须是时间动态。</p><p>在本文中，我们提出了一种新的CMH方法，称为深度跨模态哈希（DCMH），用于跨模态的检索应用。 DCMH的主要贡献概述如下：</p><ul><li>DCMH是一个端到端的学习框架，具有深度神经网络，每个模态一个网络，用于从头开始进行特征学习。</li><li>哈希码学习问题本质上是一个离散的学习问题，很难学习。 因此，大多数现有的CMH方法通过将原始的离散学习问题松弛为连续学习问题来解决这个问题。 这种放松过程可能会降低学习的哈希码的准确性。 与这些基于松弛的方法不同，DCMH直接学习离散哈希码而不放松。</li><li>使用图像-文本真实数据集对模型进行的实验表明，DCMH可以胜过其他baseline，以实现跨模态检索应用程序中最先进的性能。</li></ul><h2>2.Problem Definition</h2><h3>2.1 Notation</h3><p>像 <img src=\"https://www.zhihu.com/equation?tex=%5Cbold+w\" alt=\"\\bold w\" eeimg=\"1\"/> 这样的粗体字小写字母用于表示向量。 像 <img src=\"https://www.zhihu.com/equation?tex=%5Cbold+W\" alt=\"\\bold W\" eeimg=\"1\"/> 这样的粗体大写字母用于表示矩阵， <img src=\"https://www.zhihu.com/equation?tex=%5Cbold+W\" alt=\"\\bold W\" eeimg=\"1\"/> 的第 <img src=\"https://www.zhihu.com/equation?tex=row\" alt=\"row\" eeimg=\"1\"/> 行和第 <img src=\"https://www.zhihu.com/equation?tex=column\" alt=\"column\" eeimg=\"1\"/> 列中的元素表示 <img src=\"https://www.zhihu.com/equation?tex=%5Cbold+W_%7Bi%2Cj%7D\" alt=\"\\bold W_{i,j}\" eeimg=\"1\"/> 。 <img src=\"https://www.zhihu.com/equation?tex=%5Cbold+W\" alt=\"\\bold W\" eeimg=\"1\"/> 的第 <img src=\"https://www.zhihu.com/equation?tex=row\" alt=\"row\" eeimg=\"1\"/> 行表示为 <img src=\"https://www.zhihu.com/equation?tex=%5Cbold+W_%7B%F0%9D%91%96%7D\" alt=\"\\bold W_{𝑖}\" eeimg=\"1\"/> <i>， </i><img src=\"https://www.zhihu.com/equation?tex=%5Cbold+W\" alt=\"\\bold W\" eeimg=\"1\"/> 的第 <img src=\"https://www.zhihu.com/equation?tex=column\" alt=\"column\" eeimg=\"1\"/> 行表示为 <img src=\"https://www.zhihu.com/equation?tex=%5Cbold+W_%7B%F0%9D%91%97%7D\" alt=\"\\bold W_{𝑗}\" eeimg=\"1\"/> 。 <img src=\"https://www.zhihu.com/equation?tex=%5Cbold+W%5E%7B%F0%9D%91%87%7D\" alt=\"\\bold W^{𝑇}\" eeimg=\"1\"/> 是 <img src=\"https://www.zhihu.com/equation?tex=%5Cbold+W\" alt=\"\\bold W\" eeimg=\"1\"/> 的转置。我们用 <img src=\"https://www.zhihu.com/equation?tex=%5Cbold+1\" alt=\"\\bold 1\" eeimg=\"1\"/> 来表示一个向量，其中所有元素都是1.  <img src=\"https://www.zhihu.com/equation?tex=%5Coperatorname%7Btr%7D%28%5Ccdot%29\" alt=\"\\operatorname{tr}(\\cdot)\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%7C%5Ccdot%5C%7C_%7BF%7D\" alt=\"\\|\\cdot\\|_{F}\" eeimg=\"1\"/> 分别表示矩阵的迹和矩阵的Frobenius范数。 <img src=\"https://www.zhihu.com/equation?tex=sign%EF%BC%88%E2%8B%85%EF%BC%89+\" alt=\"sign（⋅） \" eeimg=\"1\"/> 是一个元素符号函数，定义如下： </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Coperatorname%7Bsign%7D%28x%29%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Brl%7D%7B1%7D+%26+%7Bx+%5Cgeq+0%7D+%5C%5C+%7B-1%7D+%26+%7Bx%3C0%7D%5Cend%7Barray%7D%5Cright.\" alt=\"\\operatorname{sign}(x)=\\left\\{\\begin{array}{rl}{1} &amp; {x \\geq 0} \\\\ {-1} &amp; {x&lt;0}\\end{array}\\right.\" eeimg=\"1\"/> </p><h3>2.2Cross-Modal Hashing</h3><p>虽然本文提出的方法可以很容易地适用于具有两种以上模态的情况，但我们只关注这里有两种模态的情况。</p><p>假设我们有 <img src=\"https://www.zhihu.com/equation?tex=n\" alt=\"n\" eeimg=\"1\"/> 个训练实体（数据点），每个实体都有两种特征形式。 在不失一般性的情况下，我们在本文中使用图像-文本数据集进行说明，这意味着每个训练点都具有文本模态和图像模态。 我们使用<img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BX%7D%3D%5Cleft%5C%7B%5Cmathbf%7Bx%7D_%7Bi%7D%5Cright%5C%7D_%7Bi%3D1%7D%5E%7Bn%7D\" alt=\"\\mathbf{X}=\\left\\{\\mathbf{x}_{i}\\right\\}_{i=1}^{n}\" eeimg=\"1\"/> 来表示图像模态，其中 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D_%7Bi%7D\" alt=\"\\mathbf{x}_{i}\" eeimg=\"1\"/> 可以是图像 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 的手工制作的特征或原始像素。 此外，我们使用 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BY%7D%3D%5Cleft%5C%7B%5Cmathbf%7By%7D%7Bi%7D%5Cright%5C%7D%7Bi%3D1%7D%5E%7Bn%7D\" alt=\"\\mathbf{Y}=\\left\\{\\mathbf{y}{i}\\right\\}{i=1}^{n}\" eeimg=\"1\"/> 来表示文本模态，其中 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7By%7D_%7Bi%7D\" alt=\"\\mathbf{y}_{i}\" eeimg=\"1\"/> 通常是与图像 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 相关的标签信息。 此外，我们还给出了跨模态相似度矩阵 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BS%7D\" alt=\"\\mathbf{S}\" eeimg=\"1\"/> . <img src=\"https://www.zhihu.com/equation?tex=S_%7Bi+j%7D%3D1\" alt=\"S_{i j}=1\" eeimg=\"1\"/> 表示图像 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D_%7Bi%7D\" alt=\"\\mathbf{x}_{i}\" eeimg=\"1\"/> 和文本 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7By%7D_%7Bj%7D\" alt=\"\\mathbf{y}_{j}\" eeimg=\"1\"/> 相似的，否则 <img src=\"https://www.zhihu.com/equation?tex=S_%7Bi+j%7D%3D0\" alt=\"S_{i j}=0\" eeimg=\"1\"/> 。 这里，相似性通常由诸如类标签的一些语义信息来定义。 例如，我们可以说如果图像共享相同的类标签，则图像 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D_%7Bi%7D\" alt=\"\\mathbf{x}_{i}\" eeimg=\"1\"/> 和文本 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7By%7D_%7Bj%7D\" alt=\"\\mathbf{y}_{j}\" eeimg=\"1\"/> 是相似的。 否则图像 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D_%7Bi%7D\" alt=\"\\mathbf{x}_{i}\" eeimg=\"1\"/> 和文本 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7By%7D_%7Bj%7D\" alt=\"\\mathbf{y}_{j}\" eeimg=\"1\"/> 来自不同的类，则它们是不相似的。</p><p>给定上述训练信息 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BX%7D%2C+%5Cmathbf%7BY%7D\" alt=\"\\mathbf{X}, \\mathbf{Y}\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BS%7D\" alt=\"\\mathbf{S}\" eeimg=\"1\"/> ，跨模态哈希的目标是学习两种模态的两个散列函数： <img src=\"https://www.zhihu.com/equation?tex=h%5E%7B%28x%29%7D%28%5Cmathbf%7Bx%7D%29+%5Cin%5C%7B-1%2C%2B1%5C%7D%5Ec\" alt=\"h^{(x)}(\\mathbf{x}) \\in\\{-1,+1\\}^c\" eeimg=\"1\"/> 用于图像模态和 <img src=\"https://www.zhihu.com/equation?tex=h%5E%7B%28y%29%7D%28%5Cmathbf%7By%7D%29+%5Cin%5C%7B-1%2C%2B1%5C%7D%5E%7Bc%7D\" alt=\"h^{(y)}(\\mathbf{y}) \\in\\{-1,+1\\}^{c}\" eeimg=\"1\"/> 为文本模态，其中 <img src=\"https://www.zhihu.com/equation?tex=c\" alt=\"c\" eeimg=\"1\"/> 是二进制代码的长度。 这两个散列函数应该在 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BS%7D\" alt=\"\\mathbf{S}\" eeimg=\"1\"/> 中保留跨模态的相似性。更具体地说，如果 <img src=\"https://www.zhihu.com/equation?tex=S_%7Bi+j%7D%3D1\" alt=\"S_{i j}=1\" eeimg=\"1\"/> ，则为在二进制代码之间的汉明距离  <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bb%7D_%7Bi%7D%5E%7B%28x%29%7D%3Dh%5E%7B%28x%29%7D%5Cleft%28%5Cmathbf%7Bx%7D_%7Bi%7D%5Cright%29\" alt=\"\\mathbf{b}_{i}^{(x)}=h^{(x)}\\left(\\mathbf{x}_{i}\\right)\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bb%7D_%7Bi%7D%5E%7B%28y%29%7D%3Dh%5E%7B%28y%29%7D%5Cleft%28%5Cmathbf%7By%7D_%7Bi%7D%5Cright%29\" alt=\"\\mathbf{b}_{i}^{(y)}=h^{(y)}\\left(\\mathbf{y}_{i}\\right)\" eeimg=\"1\"/> 应该很小。 否则，如果 <img src=\"https://www.zhihu.com/equation?tex=S_%7Bi+j%7D%3D0\" alt=\"S_{i j}=0\" eeimg=\"1\"/> ，相应的汉明距离应该很大。</p><p>我们假设获取到训练集中每个点的两种特征模态，尽管我们的方法也可以很容易地适应其他一些训练点只能获取到一种特征形态的设置。 请注意，我们只对训练点做出此假设。 在我们训练模型之后，我们可以使用学习的模型为一个模态或两个模态的查询和数据库点生成哈希码，特别是完全可以匹配跨模态检索应用程序的设置。</p><h2>3.Deep Cross-Modal Hashing</h2><p>在本节中，我们将介绍有关深度CMH（DCMH）方法的详细信息，包括模型公式和学习算法。</p><h3>3.1 Model</h3><p>整个DCMH模型如图1所示，它是一个端到端的学习框架，它通过无缝集成两个部分：特征学习部分和哈希码学习部分。 在学习过程中，每个部分都可以向其他部分提供反馈。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-92614614d8fdb0ce68a56f766c5f98af_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2604\" data-rawheight=\"1022\" class=\"origin_image zh-lightbox-thumb\" width=\"2604\" data-original=\"https://pic4.zhimg.com/v2-92614614d8fdb0ce68a56f766c5f98af_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2604&#39; height=&#39;1022&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2604\" data-rawheight=\"1022\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2604\" data-original=\"https://pic4.zhimg.com/v2-92614614d8fdb0ce68a56f766c5f98af_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-92614614d8fdb0ce68a56f766c5f98af_b.jpg\"/></figure><h3>3.1.1 <b>Feature Learning Part</b></h3><p>特征学习部分包含两个深度神经网络，一个用于图像模态，另一个用于文本模态。 图像模态的深度神经网络是改编自卷积神经网络（CNN）。 这个CNN模型有八层, 前七层与CNN-F中的相同, 第八层是全连接层，其输出是学习到的图像特征。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0800178a5776149e9c60d5eaf39d55cc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1708\" data-rawheight=\"816\" class=\"origin_image zh-lightbox-thumb\" width=\"1708\" data-original=\"https://pic1.zhimg.com/v2-0800178a5776149e9c60d5eaf39d55cc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1708&#39; height=&#39;816&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1708\" data-rawheight=\"816\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1708\" data-original=\"https://pic1.zhimg.com/v2-0800178a5776149e9c60d5eaf39d55cc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0800178a5776149e9c60d5eaf39d55cc_b.jpg\"/></figure><p>表1显示了用于图像模态的CN-N的详细配置。 更具体地，八层被分成五个卷积层和三个全连接层，它们分别在表1中表示为“conv1-conv5”和“full6-full8”。每个卷积层都可以从以下几方面描述：</p><ul><li>&#34; <img src=\"https://www.zhihu.com/equation?tex=f\" alt=\"f\" eeimg=\"1\"/> 、 <img src=\"https://www.zhihu.com/equation?tex=%F0%9D%91%9B%F0%9D%91%A2%F0%9D%91%9A%C3%97%F0%9D%91%A0%F0%9D%91%96%F0%9D%91%A7%F0%9D%91%92%C3%97%F0%9D%91%A0%F0%9D%91%96%F0%9D%91%A7%F0%9D%91%92\" alt=\"𝑛𝑢𝑚×𝑠𝑖𝑧𝑒×𝑠𝑖𝑧𝑒\" eeimg=\"1\"/> &#34;表示卷积滤波器的数量及其感受野大小。</li><li>“st”表示卷积步长stride。</li><li>“pad”表示要添加（padding）到每个输入的像素数大小。</li><li>“LRN”表示是否应用了局部响应归一化（Local Response Normalization,LRN）。</li><li>“Pool”表示下采样参数。</li><li>全连接层中的数字，例如“4096”，表示该层中的节点数。 它也是该层输出的维数。</li></ul><p>所有前七层都使用线性整流函数（Rectified Linear Unit，Re-LU）[16]作为激活函数。 对于第八层，我们选择恒等函数作为激活函数。</p><p>为了从文本中执行特征学习，我们首先将每个文本 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7By%7D_%7Bj%7D\" alt=\"\\mathbf{y}_{j}\" eeimg=\"1\"/> 表示为词袋（BOW）表示模型的向量。 然后使用词袋向量作为具有两个完全连接层的深度神经网络（表示为“full1 -  full2”）的输入。 文本深度神经网络的详细配置如表2所示，其中配置显示了每层中的节点数。 第一层的激活函数是ReLU，第二层的激活功能是恒等函数。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-fa28545d7941b84e07f4566818c15cab_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1704\" data-rawheight=\"486\" class=\"origin_image zh-lightbox-thumb\" width=\"1704\" data-original=\"https://pic4.zhimg.com/v2-fa28545d7941b84e07f4566818c15cab_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1704&#39; height=&#39;486&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1704\" data-rawheight=\"486\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1704\" data-original=\"https://pic4.zhimg.com/v2-fa28545d7941b84e07f4566818c15cab_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-fa28545d7941b84e07f4566818c15cab_b.jpg\"/></figure><p>请注意，本文的主要目的是表明通过使用深度神经网络从头开始进行特征学习，可以设计一个用于跨模态哈希的端到端学习框架。 但是如何设计不同的神经网络并不是本文的重点。 其他深度神经网络也可用于为我们的DCMH模型执行特征学习，这将留待将来的研究。</p><h3>3.1.2 <b>Hash-Code Learning Part</b></h3><p>令 <img src=\"https://www.zhihu.com/equation?tex=f%5Cleft%28%5Cmathbf%7Bx%7D_%7Bi%7D+%3B+%5Ctheta_%7Bx%7D%5Cright%29+%5Cin+%5Cmathbb%7BR%7D%5E%7Bc%7D\" alt=\"f\\left(\\mathbf{x}_{i} ; \\theta_{x}\\right) \\in \\mathbb{R}^{c}\" eeimg=\"1\"/> 表示对于点 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 学习的图像特征，其对应于用于图像模态的CNN的输出。 此外，让 <img src=\"https://www.zhihu.com/equation?tex=g%5Cleft%28%5Cmathbf%7By%7D_%7Bj%7D+%3B+%5Ctheta_%7By%7D%5Cright%29+%5Cin+%5Cmathbb%7BR%7D%5E%7Bc%7D\" alt=\"g\\left(\\mathbf{y}_{j} ; \\theta_{y}\\right) \\in \\mathbb{R}^{c}\" eeimg=\"1\"/> 表示点 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 学习到的文本特征，它对应于文本模态的深度神经网络的输出。 这里， <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bx%7D\" alt=\"\\theta_{x}\" eeimg=\"1\"/> 是用于图像模态的CNN的网络参数， <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7By%7D\" alt=\"\\theta_{y}\" eeimg=\"1\"/> 是用于文本模态的深度神经网络的网络参数。</p><p>DCMH的目标函数定义如下： <img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cmin+_%7B%5Cmathbf%7BB%7D%5E%7B%28x%29%7D%2C+%5Cmathbf%7BB%7D%5E%7B%28y%29%7D%2C+%5Ctheta%7Bx%7D%2C+%5Ctheta_%7By%7D%7D+%5Cmathcal%7BJ%7D+%26%3D-%5Csum_%7Bi%2C+j%3D1%7D%5E%7Bn%7D%5Cleft%28S_%7Bi+j%7D+%5CTheta_%7Bi+j%7D-%5Clog+%5Cleft%281%2Be%5E%7B%5CTheta_%7Bi+j%7D%7D%5Cright%29%5Cright%29+%5C+%5C%5C%26%2B%5Cgamma%5Cleft%28%5Cleft%5C%7C%5Cmathbf%7BB%7D%5E%7B%28x%29%7D-%5Cmathbf%7BF%7D%5Cright%5C%7C_%7BF%7D%5E%7B2%7D%2B%5Cleft%5C%7C%5Cmathbf%7BB%7D%5E%7B%28y%29%7D-%5Cmathbf%7BG%7D%5Cright%5C%7C_%7BF%7D%5E%7B2%7D%5Cright%29+%5C+%5C%5C%26%2B%5Ceta%5Cleft%28%5C%7C%5Cmathbf%7BF%7D%5Cmathbf%7B1%7D%5C%7C_%7BF%7D%5E%7B2%7D%2B%5C%7C%5Cmathbf%7BG%7D+%5Cmathbf%7B1%7D%5C%7C_%7BF%7D%5E%7B2%7D%5Cright%29+%5C%5C+%5C%5C+%5Ctext+%7B+s.t.+%7D+%5Cquad+%5Cmathbf%7BB%7D%5E%7B%28x%29%7D+%26+%5Cin%5C%7B-1%2C%2B1%5C%7D%5E%7Bc+%5Ctimes+n%7D+%5C++%5C%5C+%5Cmathbf%7BB%7D%5E%7B%28y%29%7D%26+%5Cin%5C%7B-1%2C%2B1%5C%7D%5E%7Bc+%5Ctimes+n%7D+%5Cend%7Baligned%7D++%5Ctag+%7B1%7D+\" alt=\"\\begin{aligned} \\min _{\\mathbf{B}^{(x)}, \\mathbf{B}^{(y)}, \\theta{x}, \\theta_{y}} \\mathcal{J} &amp;=-\\sum_{i, j=1}^{n}\\left(S_{i j} \\Theta_{i j}-\\log \\left(1+e^{\\Theta_{i j}}\\right)\\right) \\ \\\\&amp;+\\gamma\\left(\\left\\|\\mathbf{B}^{(x)}-\\mathbf{F}\\right\\|_{F}^{2}+\\left\\|\\mathbf{B}^{(y)}-\\mathbf{G}\\right\\|_{F}^{2}\\right) \\ \\\\&amp;+\\eta\\left(\\|\\mathbf{F}\\mathbf{1}\\|_{F}^{2}+\\|\\mathbf{G} \\mathbf{1}\\|_{F}^{2}\\right) \\\\ \\\\ \\text { s.t. } \\quad \\mathbf{B}^{(x)} &amp; \\in\\{-1,+1\\}^{c \\times n} \\  \\\\ \\mathbf{B}^{(y)}&amp; \\in\\{-1,+1\\}^{c \\times n} \\end{aligned}  \\tag {1} \" eeimg=\"1\"/> </p><p>其中 </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BF%7D_%7B%2A+i%7D%3Df%5Cleft%28%5Cmathbf%7Bx%7D_%7Bi%7D+%3B+%5Ctheta_%7Bx%7D%5Cright%29\" alt=\"\\mathbf{F}_{* i}=f\\left(\\mathbf{x}_{i} ; \\theta_{x}\\right)\" eeimg=\"1\"/> 且 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BF%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bc+%5Ctimes+n%7D\" alt=\"\\mathbf{F} \\in \\mathbb{R}^{c \\times n}\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BG%7D_%7B%2A+j%7D%3Dg%5Cleft%28%5Cmathbf%7By%7D_%7Bj%7D+%3B+%5Ctheta_%7By%7D%5Cright%29\" alt=\"\\mathbf{G}_{* j}=g\\left(\\mathbf{y}_{j} ; \\theta_{y}\\right)\" eeimg=\"1\"/> 且 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BG%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bc+%5Ctimes+n%7D\" alt=\"\\mathbf{G} \\in \\mathbb{R}^{c \\times n}\" eeimg=\"1\"/> ，</p><p><img src=\"https://www.zhihu.com/equation?tex=%5CTheta_%7Bi+j%7D%3D%5Cfrac%7B1%7D%7B2%7D+%5Cmathbf%7BF%7D_%7B+i%7D%5E%7BT%7D+%5Cmathbf%7BG%7D_%7B+j%7D\" alt=\"\\Theta_{i j}=\\frac{1}{2} \\mathbf{F}_{ i}^{T} \\mathbf{G}_{ j}\" eeimg=\"1\"/> 。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BB%7D_%7B%2A+i%7D%5E%7B%28x%29%7D\" alt=\"\\mathbf{B}_{* i}^{(x)}\" eeimg=\"1\"/> 是图像 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D_%7Bi%7D\" alt=\"\\mathbf{x}_{i}\" eeimg=\"1\"/>的二进制哈希码。 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BB%7D_%7B%2A+j%7D%5E%7B%28y%29%7D\" alt=\"\\mathbf{B}_{* j}^{(y)}\" eeimg=\"1\"/> 是文本 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7By%7D_%7Bj%7D\" alt=\"\\mathbf{y}_{j}\" eeimg=\"1\"/> 的二进制哈希码</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"/>  和 <img src=\"https://www.zhihu.com/equation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"/> 是超参数。</p><p>等式（1）中 <img src=\"https://www.zhihu.com/equation?tex=-%5Csum_%7Bi%2C+j%3D1%7D%5E%7Bn%7D%5Cleft%28S_%7Bi+j%7D+%5CTheta_%7Bi+j%7D-%5Clog+%5Cleft%281%2Be%5E%7B%5CTheta_%7Bi+j%7D%7D%5Cright%29%5Cright%29\" alt=\"-\\sum_{i, j=1}^{n}\\left(S_{i j} \\Theta_{i j}-\\log \\left(1+e^{\\Theta_{i j}}\\right)\\right)\" eeimg=\"1\"/> 是跨模态相似性的负的log似然，似然函数定义如下：</p><p><img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28S_%7Bi+j%7D+%7C+%5Cmathbf%7BF%7D_%7B%2A+i%7D%2C+%5Cmathbf%7BG%7D_%7B%2A+j%7D%5Cright%29%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7D%7B%5Csigma%5Cleft%28%5CTheta_%7Bi+j%7D%5Cright%29%7D+%26+%7BS_%7Bi+j%7D%3D1%7D+%5C%5C+%7B1-%5Csigma%5Cleft%28%5CTheta_%7Bi+j%7D%5Cright%29%7D+%26+%7BS_%7Bi+j%7D%3D0%7D%5Cend%7Barray%7D%5Cright.\" alt=\"p\\left(S_{i j} | \\mathbf{F}_{* i}, \\mathbf{G}_{* j}\\right)=\\left\\{\\begin{array}{ll}{\\sigma\\left(\\Theta_{i j}\\right)} &amp; {S_{i j}=1} \\\\ {1-\\sigma\\left(\\Theta_{i j}\\right)} &amp; {S_{i j}=0}\\end{array}\\right.\" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=%5CTheta_%7Bi+j%7D%3D%5Cfrac%7B1%7D%7B2%7D+%5Cmathbf%7BF%7D_%7B%2A+i%7D%5E%7BT%7D+%5Cmathbf%7BG%7D_%7B%2A+j%7D\" alt=\"\\Theta_{i j}=\\frac{1}{2} \\mathbf{F}_{* i}^{T} \\mathbf{G}_{* j}\" eeimg=\"1\"/> <i>，</i> <img src=\"https://www.zhihu.com/equation?tex=%5Csigma%5Cleft%28%5CTheta_%7Bi+j%7D%5Cright%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5CTheta_%7Bi+j%7D%7D%7D\" alt=\"\\sigma\\left(\\Theta_{i j}\\right)=\\frac{1}{1+e^{-\\Theta_{i j}}}\" eeimg=\"1\"/> </p><p>很容易最小化负对数似然，等价于最大化似然，可以使 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BF%7D_%7B+i%7D\" alt=\"\\mathbf{F}_{ i}\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BG%7D_%7B+j%7D\" alt=\"\\mathbf{G}_{ j}\" eeimg=\"1\"/> 之间的相似性（内积）在<img src=\"https://www.zhihu.com/equation?tex=S_%7Bi+j%7D%3D1\" alt=\"S_{i j}=1\" eeimg=\"1\"/> 时大，当 <img src=\"https://www.zhihu.com/equation?tex=S_%7Bi+j%7D%3D0\" alt=\"S_{i j}=0\" eeimg=\"1\"/> 时小。因此，优化等式（1）中的第一项可以保留将 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BS%7D\" alt=\"\\mathbf{S}\" eeimg=\"1\"/> 中的图像特征表示 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BF%7D\" alt=\"\\mathbf{F}\" eeimg=\"1\"/> 和文本特征表示 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BG%7D\" alt=\"\\mathbf{G}\" eeimg=\"1\"/> 的跨模态相似性。</p><p>通过优化第二项 <img src=\"https://www.zhihu.com/equation?tex=%5Cgamma+%5Cleft%28+%5Cleft%5C%7C+%5Cmathbf%7BB%7D%5E%7B%28x%29%7D+-+%5Cmathbf%7BF%7D+%5Cright%5C%7C_%7BF%7D%5E%7B2%7D%2B%5C%7C+%5Cmathbf%7BB%7D%5E%7B%28y%29%7D-%5Cmathbf%7BG%7D+%5C%7C_%7BF%7D%5E%7B2%7D+%29%5Cmathbf%7BG%7D+%5C%7C_%7BF%7D%5E%7B2%7D%5Cright+%29\" alt=\"\\gamma \\left( \\left\\| \\mathbf{B}^{(x)} - \\mathbf{F} \\right\\|_{F}^{2}+\\| \\mathbf{B}^{(y)}-\\mathbf{G} \\|_{F}^{2} )\\mathbf{G} \\|_{F}^{2}\\right )\" eeimg=\"1\"/> ,我们可以得到 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BB%7D%5E%7B%28x%29%7D%3D%5Coperatorname%7Bsign%7D%28%5Cmathbf%7BF%7D%29\" alt=\"\\mathbf{B}^{(x)}=\\operatorname{sign}(\\mathbf{F})\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BB%7D%5E%7B%28y%29%7D%3D%5Coperatorname%7Bsign%7D%28%5Cmathbf%7BG%7D%29\" alt=\"\\mathbf{B}^{(y)}=\\operatorname{sign}(\\mathbf{G})\" eeimg=\"1\"/> .因此相对的，我们认为 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BF%7D\" alt=\"\\mathbf{F}\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BG%7D\" alt=\"\\mathbf{G}\" eeimg=\"1\"/> 分别是 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BB%7D%5E%7B%28x%29%7D\" alt=\"\\mathbf{B}^{(x)}\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BB%7D%5E%7B%28y%29%7D\" alt=\"\\mathbf{B}^{(y)}\" eeimg=\"1\"/> 的可连续替代.因为 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BF%7D\" alt=\"\\mathbf{F}\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BG%7D\" alt=\"\\mathbf{G}\" eeimg=\"1\"/> 能够保留 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BS%7D\" alt=\"\\mathbf{S}\" eeimg=\"1\"/> 中的跨模态相似性，所以二进制的哈希码 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BB%7D%5E%7B%28x%29%7D\" alt=\"\\mathbf{B}^{(x)}\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BB%7D%5E%7B%28y%29%7D\" alt=\"\\mathbf{B}^{(y)}\" eeimg=\"1\"/> 也被期望能够保留 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BS%7D\" alt=\"\\mathbf{S}\" eeimg=\"1\"/> 中的跨模态相似性，也就是说能够匹配跨模式哈希的目标。</p><p>第三项 <img src=\"https://www.zhihu.com/equation?tex=%5Ceta%5Cleft%28%5C%7C%5Cmathbf%7BF%7D+%5Cmathbf%7B1%7D%5C%7C_%7BF%7D%5E%7B2%7D%2B%5C%7C%5Cmathbf%7BG%7D+%5Cmathbf%7B1%7D%5C%7C_%7BF%7D%5E%7B2%7D%5Cright%29\" alt=\"\\eta\\left(\\|\\mathbf{F} \\mathbf{1}\\|_{F}^{2}+\\|\\mathbf{G} \\mathbf{1}\\|_{F}^{2}\\right)\" eeimg=\"1\"/> 被用作在所有训练点上使哈希码的每一位均衡。 更具体地说，所有训练点上每个比特的+1和-1的数量应该几乎相同。 该约束可用于最大化每位提供的信息。在我们的实验中，我们发现如果来自两个模态的二进制代码对于相同的训练点被设置为相同，则可以实现更好的性能。 因此，我们设置 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BB%7D%5E%7B%28x%29%7D%3D%5Cmathbf%7BB%7D%5E%7B%28y%29%7D%3D%5Cmathbf%7BB%7D\" alt=\"\\mathbf{B}^{(x)}=\\mathbf{B}^{(y)}=\\mathbf{B}\" eeimg=\"1\"/> .然后，等式（1）中的问题可以转换为以下公式：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cmin+_%7B%5Cmathbf%7BB%7D%2C+%5Ctheta%7Bx%7D%2C+%5Ctheta_%7By%7D%7D+%5Cmathcal%7BJ%7D+%26%3D-%5Csum_%7Bi%2C+j%3D1%7D%5E%7Bn%7D%5Cleft%28S_%7Bi+j%7D+%5CTheta_%7Bi+j%7D-%5Clog+%5Cleft%281%2Be%5E%7B%5CTheta_%7Bi+j%7D%7D%5Cright%29%5Cright%29+%5C%5C+%26%2B%5Cgamma%5Cleft%28%5C%7C%5Cmathbf%7BB%7D-%5Cmathbf%7BF%7D%5C%7C%7BF%7D%5E%7B2%7D%2B%5C%7C%5Cmathbf%7BB%7D-%5Cmathbf%7BG%7D%5C%7C%7BF%7D%5E%7B2%7D%5Cright%29+%5C%5C+%26%2B%5Ceta%5Cleft%28%5C%7C%5Cmathbf%7BF%7D+%5Cmathbf%7B1%7D%5C%7C%7BF%7D%5E%7B2%7D%2B%5C%7C%5Cmathbf%7BG%7D+%5Cmathbf%7B1%7D%5C%7C%7BF%7D%5E%7B2%7D%5Cright%29+%5C%5C+%5C%5C++%5Ctext+%7B+s.t.+%7D+%5Cmathbf%7BB%7D+%26+%5Cin%5C%7B-1%2C%2B1%5C%7D%5E%7Bc+%5Ctimes+n%7D+%5Cend%7Baligned%7D+%5Ctag+2+\" alt=\"\\begin{aligned} \\min _{\\mathbf{B}, \\theta{x}, \\theta_{y}} \\mathcal{J} &amp;=-\\sum_{i, j=1}^{n}\\left(S_{i j} \\Theta_{i j}-\\log \\left(1+e^{\\Theta_{i j}}\\right)\\right) \\\\ &amp;+\\gamma\\left(\\|\\mathbf{B}-\\mathbf{F}\\|{F}^{2}+\\|\\mathbf{B}-\\mathbf{G}\\|{F}^{2}\\right) \\\\ &amp;+\\eta\\left(\\|\\mathbf{F} \\mathbf{1}\\|{F}^{2}+\\|\\mathbf{G} \\mathbf{1}\\|{F}^{2}\\right) \\\\ \\\\  \\text { s.t. } \\mathbf{B} &amp; \\in\\{-1,+1\\}^{c \\times n} \\end{aligned} \\tag 2 \" eeimg=\"1\"/> </p><p>这是我们DCMH最终要学习的目标函数。</p><p>从（2）中，我们可以发现深度神经网络（ <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bx%7D\" alt=\"\\theta_{x}\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7By%7D\" alt=\"\\theta_{y}\" eeimg=\"1\"/> ）和二进制哈希码（ <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BB%7D\" alt=\"\\mathbf{B}\" eeimg=\"1\"/> ）的参数是从同一目标函数中学习的。 也就是说，DCMH将特征学习和哈希码学习集成到同一个深度学习框架中。</p><p>请注意，我们只训练点 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BB%7D%5E%7B%28x%29%7D%3D%5Cmathbf%7BB%7D%5E%7B%28y%29%7D\" alt=\"\\mathbf{B}^{(x)}=\\mathbf{B}^{(y)}\" eeimg=\"1\"/> 。 在我们学习了等式（2）中的问题后，我们对于相同点 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 的两个不同模态如果点 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 是查询点, 或该点来自数据库而不是训练点，仍然需要生成不同的二级制 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bb%7D_%7Bi%7D%5E%7B%28x%29%7D%3Dh%5E%7B%28x%29%7D%5Cleft%28%5Cmathbf%7Bx%7D_%7Bi%7D%5Cright%29\" alt=\"\\mathbf{b}_{i}^{(x)}=h^{(x)}\\left(\\mathbf{x}_{i}\\right)\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bb%7D_%7Bi%7D%5E%7B%28y%29%7D%3Dh%5E%7B%28y%29%7D%5Cleft%28%5Cmathbf%7By%7D_%7Bi%7D%5Cright%29\" alt=\"\\mathbf{b}_{i}^{(y)}=h^{(y)}\\left(\\mathbf{y}_{i}\\right)\" eeimg=\"1\"/> 。 这将在3.3节中进一步说明。</p><h3>3.2 Learning</h3><p>我们采用交替学习策略来学习 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bx%7D\" alt=\"\\theta_{x}\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7By%7D\" alt=\"\\theta_{y}\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BB%7D\" alt=\"\\mathbf{B}\" eeimg=\"1\"/> .每次我们学习一个参数，其他参数固定。 在算法1中简要概述了用于DCMH的整个交替学习算法，并且将在本小节的以下内容中引入详细的推导。</p><h3><b>3.2.1 Learn</b> 𝜃𝑥<b>, with</b> 𝜃𝑦 <b>and</b> B <b>Fixed</b></h3><p>当 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7By%7D\" alt=\"\\theta_{y}\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BB%7D\" alt=\"\\mathbf{B}\" eeimg=\"1\"/> 固定时，我们通过使用反向传播（BP）算法来学习图像模态的CNN参数 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bx%7D\" alt=\"\\theta_{x}\" eeimg=\"1\"/> 。 作为大多数现有的深度学习方法，我们利用随机梯度下降（SGD）和BP算法来学习 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bx%7D\" alt=\"\\theta_{x}\" eeimg=\"1\"/> 。 更具体地说，在每次迭代中，我们从训练集中采样mini-batch点，然后基于采样数据执行我们的学习算法。</p><p>特别是，对于每个采样点 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D_%7Bi%7D\" alt=\"\\mathbf{x}_{i}\" eeimg=\"1\"/> ，我们首先计算以下梯度： </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cfrac%7B%5Cpartial+%5Cmathcal%7BJ%7D%7D%7B%5Cpartial+%5Cmathbf%7BF%7D_%7B%2A+i%7D%7D%3D%26+%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bj%3D1%7D%5E%7Bn%7D%5Cleft%28%5Csigma%5Cleft%28%5CTheta_%7Bi+j%7D%5Cright%29+%5Cmathbf%7BG%7D_%7B%2A+j%7D-S_%7Bi+j%7D+%5Cmathbf%7BG%7D_%7B%2A+j%7D%5Cright%29+%5C%5C+%26%2B2+%5Cgamma%5Cleft%28%5Cmathbf%7BF%7D_%7B%2A+i%7D-%5Cmathbf%7BB%7D_%7B%2A+i%7D%5Cright%29%2B2+%5Ceta+%5Cmathbf%7BF%7D+%5Cmathbf%7B1%7D+%5Cend%7Baligned%7D+%5Ctag+3\" alt=\"\\begin{aligned} \\frac{\\partial \\mathcal{J}}{\\partial \\mathbf{F}_{* i}}=&amp; \\frac{1}{2} \\sum_{j=1}^{n}\\left(\\sigma\\left(\\Theta_{i j}\\right) \\mathbf{G}_{* j}-S_{i j} \\mathbf{G}_{* j}\\right) \\\\ &amp;+2 \\gamma\\left(\\mathbf{F}_{* i}-\\mathbf{B}_{* i}\\right)+2 \\eta \\mathbf{F} \\mathbf{1} \\end{aligned} \\tag 3\" eeimg=\"1\"/> </p><p> 然后我们使用链式法则（基于此法则BP可以用于更新参数 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bx%7D\" alt=\"\\theta_{x}\" eeimg=\"1\"/> ）通过 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Cmathcal%7BJ%7D%7D%7B%5Cpartial+%5Cmathbf%7BF%7D_%7B%2A+i%7D%7D\" alt=\"\\frac{\\partial \\mathcal{J}}{\\partial \\mathbf{F}_{* i}}\" eeimg=\"1\"/> 计算 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Cmathcal%7BJ%7D%7D%7B%5Cpartial+%5Ctheta_%7Bx%7D%7D\" alt=\"\\frac{\\partial \\mathcal{J}}{\\partial \\theta_{x}}\" eeimg=\"1\"/> </p><h3>3.2.2 <b>Learn</b> 𝜃𝑦<b>, with</b> 𝜃𝑥 <b>and</b> B <b>Fixed</b></h3><p>当 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bx%7D\" alt=\"\\theta_{x}\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BB%7D\" alt=\"\\mathbf{B}\" eeimg=\"1\"/> 固定时，我们还通过使用SGD和BP算法来学习文本模态的神经网络参数 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7By%7D\" alt=\"\\theta_{y}\" eeimg=\"1\"/> 。 更具体地说，对于每个采样点 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7By%7D_%7Bj%7D\" alt=\"\\mathbf{y}_{j}\" eeimg=\"1\"/> ，我们首先计算以下梯度：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cfrac%7B%5Cpartial+%5Cmathcal%7BJ%7D%7D%7B%5Cpartial+%5Cmathbf%7BG%7D_%7B%2A+j%7D%7D%3D%26+%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Cleft%28%5Csigma%5Cleft%28%5CTheta_%7Bi+j%7D%5Cright%29+%5Cmathbf%7BF%7D_%7B%2A+i%7D-S_%7Bi+j%7D+%5Cmathbf%7BF%7D_%7B%2A+i%7D%5Cright%29+%5C%5C+%26%2B2+%5Cgamma%5Cleft%28%5Cmathbf%7BG%7D_%7B%2A+j%7D-%5Cmathbf%7BB%7D_%7B%2A+j%7D%5Cright%29%2B2+%5Ceta+%5Cmathbf%7BG%7D+%5Cmathbf%7B1%7D+%5Cend%7Baligned%7D+%5Ctag+4\" alt=\"\\begin{aligned} \\frac{\\partial \\mathcal{J}}{\\partial \\mathbf{G}_{* j}}=&amp; \\frac{1}{2} \\sum_{i=1}^{n}\\left(\\sigma\\left(\\Theta_{i j}\\right) \\mathbf{F}_{* i}-S_{i j} \\mathbf{F}_{* i}\\right) \\\\ &amp;+2 \\gamma\\left(\\mathbf{G}_{* j}-\\mathbf{B}_{* j}\\right)+2 \\eta \\mathbf{G} \\mathbf{1} \\end{aligned} \\tag 4\" eeimg=\"1\"/> </p><p>然后我们使用链式法则（基于此法则BP可以用于更新参数 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7By%7D\" alt=\"\\theta_{y}\" eeimg=\"1\"/> ）通过 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Cmathcal%7BJ%7D%7D%7B%5Cpartial+%5Cmathbf%7BG%7D_%7B%2A+i%7D%7D\" alt=\"\\frac{\\partial \\mathcal{J}}{\\partial \\mathbf{G}_{* i}}\" eeimg=\"1\"/> 计算 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Cmathcal%7BJ%7D%7D%7B%5Cpartial+%5Ctheta%7By%7D%7D\" alt=\"\\frac{\\partial \\mathcal{J}}{\\partial \\theta{y}}\" eeimg=\"1\"/> .</p><h3>3.2.3 <b>Learn B,</b>, with 𝜃𝑦 and <b>𝜃𝑥 </b>Fixed</h3><p>当 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bx%7D\" alt=\"\\theta_{x}\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7By%7D\" alt=\"\\theta_{y}\" eeimg=\"1\"/> 固定时，前面提到的问题（如果点 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 是查询点, 或该点来自数据库而不是训练点，点 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 的两个不同模态仍然需要生成不同的二进制码 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bb%7D_%7Bi%7D%5E%7B%28x%29%7D%3Dh%5E%7B%28x%29%7D%5Cleft%28%5Cmathbf%7Bx%7D_%7Bi%7D%5Cright%29\" alt=\"\\mathbf{b}_{i}^{(x)}=h^{(x)}\\left(\\mathbf{x}_{i}\\right)\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bb%7D_%7Bi%7D%5E%7B%28y%29%7D%3Dh%5E%7B%28y%29%7D%5Cleft%28%5Cmathbf%7By%7D_%7Bi%7D%5Cright%29\" alt=\"\\mathbf{b}_{i}^{(y)}=h^{(y)}\\left(\\mathbf{y}_{i}\\right)\" eeimg=\"1\"/> ），可以重新计算为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmax+_%7B%5Cmathbf%7BB%7D%7D+%5Coperatorname%7Btr%7D%5Cleft%28%5Cmathbf%7BB%7D%5E%7BT%7D%28%5Cgamma%28%5Cmathbf%7BF%7D%2B%5Cmathbf%7BG%7D%29%29%5Cright%29%3D%5Coperatorname%7Btr%7D%5Cleft%28%5Cmathbf%7BB%7D%5E%7BT%7D+%5Cmathbf%7BV%7D%5Cright%29%3D%5Csum_%7Bi%2C+j%7D+B_%7Bi+j%7D+V_%7Bi+j%7D+%5C%5C+s.t.+%5Cmathbf%7BB%7D+%5Cin%5C%7B-1%2C%2B1%5C%7D%5E%7Bc+%5Ctimes+n%7D\" alt=\"\\max _{\\mathbf{B}} \\operatorname{tr}\\left(\\mathbf{B}^{T}(\\gamma(\\mathbf{F}+\\mathbf{G}))\\right)=\\operatorname{tr}\\left(\\mathbf{B}^{T} \\mathbf{V}\\right)=\\sum_{i, j} B_{i j} V_{i j} \\\\ s.t. \\mathbf{B} \\in\\{-1,+1\\}^{c \\times n}\" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BV%7D%3D%5Cgamma%28%5Cmathbf%7BF%7D%2B%5Cmathbf%7BG%7D%29\" alt=\"\\mathbf{V}=\\gamma(\\mathbf{F}+\\mathbf{G})\" eeimg=\"1\"/> 。</p><p>很容易发现二进制代码 <img src=\"https://www.zhihu.com/equation?tex=B_%7Bi+j%7D\" alt=\"B_{i j}\" eeimg=\"1\"/> 应该与 <img src=\"https://www.zhihu.com/equation?tex=V_%7Bi+j%7D\" alt=\"V_{i j}\" eeimg=\"1\"/> 保持相同的符号。 因此，我们有：</p><p><img src=\"https://www.zhihu.com/equation?tex=+%5Cmathbf%7BB%7D%3D%5Coperatorname%7Bsign%7D%28%5Cmathbf%7BV%7D%29%3D%5Coperatorname%7Bsign%7D%28%5Cgamma%28%5Cmathbf%7BF%7D%2B%5Cmathbf%7BG%7D%29%29+%5Ctag+5+\" alt=\" \\mathbf{B}=\\operatorname{sign}(\\mathbf{V})=\\operatorname{sign}(\\gamma(\\mathbf{F}+\\mathbf{G})) \\tag 5 \" eeimg=\"1\"/> </p><h3>3.3 Out-of-Sample Extension</h3><p>对于不在训练集中的任何点，只要观察到其中一个模态（图像或文本），我们就可以获得其哈希码。 特别是，给定点 <img src=\"https://www.zhihu.com/equation?tex=q\" alt=\"q\" eeimg=\"1\"/> 的图像模态 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D_%7Bq%7D\" alt=\"\\mathbf{x}_{q}\" eeimg=\"1\"/> ，我们可以采用前向传播来生成哈希码，如下所示： </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bb%7D_%7Bq%7D%5E%7B%28x%29%7D%3Dh%5E%7B%28x%29%7D%5Cleft%28%5Cmathbf%7Bx%7D_%7Bq%7D%5Cright%29%3D%5Coperatorname%7Bsign%7D%5Cleft%28f%5Cleft%28%5Cmathbf%7Bx%7D_%7Bq%7D+%3B+%5Ctheta_%7Bx%7D%5Cright%29%5Cright%29\" alt=\"\\mathbf{b}_{q}^{(x)}=h^{(x)}\\left(\\mathbf{x}_{q}\\right)=\\operatorname{sign}\\left(f\\left(\\mathbf{x}_{q} ; \\theta_{x}\\right)\\right)\" eeimg=\"1\"/> </p><p>特别的，如果点 <img src=\"https://www.zhihu.com/equation?tex=q\" alt=\"q\" eeimg=\"1\"/> 只有文本模态 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7By%7D_%7Bq%7D\" alt=\"\\mathbf{y}_{q}\" eeimg=\"1\"/> ，我们可以生成它的哈希码： </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bb%7D_%7Bq%7D%5E%7B%28y%29%7D%3Dh%5E%7B%28y%29%7D%5Cleft%28%5Cmathbf%7By%7D_%7Bq%7D%5Cright%29%3D%5Coperatorname%7Bsign%7D%5Cleft%28g%5Cleft%28%5Cmathbf%7By%7D_%7Bq%7D+%3B+%5Ctheta_%7By%7D%5Cright%29%5Cright%29\" alt=\"\\mathbf{b}_{q}^{(y)}=h^{(y)}\\left(\\mathbf{y}_{q}\\right)=\\operatorname{sign}\\left(g\\left(\\mathbf{y}_{q} ; \\theta_{y}\\right)\\right)\" eeimg=\"1\"/> </p><p> 因此，我们的DCMH模型可以用在跨模态搜索上，尤其是查询点只有一种模态，或者该数据点是在数据库中的点且具有其他模态。</p><div class=\"highlight\"><pre><code class=\"language-java\"><span class=\"n\">算法1</span><span class=\"err\">：</span> <span class=\"n\">DMCH的学习算法</span>\n\n<span class=\"n\">输入</span><span class=\"err\">：</span><span class=\"n\">图像集X</span><span class=\"o\">,</span><span class=\"n\">文本集Y和跨模态相似矩阵S</span>\n\n<span class=\"n\">输出</span><span class=\"err\">：</span><span class=\"n\">神经网络参数𝜃𝑥和𝜃𝑦</span><span class=\"err\">，</span><span class=\"n\">以及二进制码矩阵B</span>\n\n<span class=\"n\">初始化</span><span class=\"err\">：</span>\n    <span class=\"n\">初始化神经网络参数</span><span class=\"err\">：</span><span class=\"n\">𝜃𝑥和𝜃𝑦</span>\n  <span class=\"n\">最小批大小</span><span class=\"err\">：</span><span class=\"n\">𝑁𝑥</span> <span class=\"o\">=</span> <span class=\"n\">𝑁𝑦</span> <span class=\"o\">=</span> <span class=\"n\">128</span>\n  <span class=\"n\">迭代序号</span><span class=\"err\">：</span> <span class=\"n\">𝑡𝑥</span> <span class=\"o\">=</span> <span class=\"err\">⌈</span><span class=\"n\">𝑛</span><span class=\"o\">/</span><span class=\"n\">𝑁𝑥</span><span class=\"err\">⌉</span><span class=\"o\">,</span><span class=\"n\">𝑡𝑦</span> <span class=\"o\">=</span> <span class=\"err\">⌈</span><span class=\"n\">𝑛</span><span class=\"o\">/</span><span class=\"n\">𝑁𝑦</span> <span class=\"err\">⌉</span><span class=\"o\">.</span>\n\n<span class=\"n\">循环</span><span class=\"err\">（</span><span class=\"n\">知道迭代序号到达固定值</span><span class=\"err\">）：</span>\n  <span class=\"c1\">// 学习𝜃𝑥\n</span><span class=\"c1\"></span>    <span class=\"k\">for</span> <span class=\"n\">iter</span> <span class=\"o\">=</span> <span class=\"n\">1</span><span class=\"o\">,</span><span class=\"n\">2</span><span class=\"o\">,...,</span> <span class=\"n\">𝑡𝑥</span> <span class=\"k\">do</span>\n    <span class=\"n\">从X中随机采样𝑁𝑥个数据点构建一个mini</span><span class=\"o\">-</span><span class=\"n\">batch</span>\n    <span class=\"n\">对于mini</span><span class=\"o\">-</span><span class=\"n\">batch中采样的每个数据点都使用前向传播算法计算F</span><span class=\"err\">∗</span><span class=\"n\">𝑖</span> <span class=\"o\">=</span> <span class=\"n\">𝑓</span><span class=\"o\">(</span><span class=\"n\">x𝑖</span><span class=\"o\">;</span><span class=\"n\">𝜃𝑥</span><span class=\"o\">)</span>\n    <span class=\"n\">根据等式</span><span class=\"err\">（</span><span class=\"n\">3</span><span class=\"err\">）</span><span class=\"n\">求导</span>\n    <span class=\"n\">使用后向传播算法更新参数𝜃𝑥</span>\n  <span class=\"n\">end</span> <span class=\"k\">for</span>\n  <span class=\"c1\">//学习𝜃𝑦\n</span><span class=\"c1\"></span>  <span class=\"k\">for</span> <span class=\"n\">iter</span> <span class=\"o\">=</span> <span class=\"n\">1</span><span class=\"o\">,</span><span class=\"n\">2</span><span class=\"o\">,...,</span><span class=\"n\">𝑡𝑦</span> <span class=\"k\">do</span>\n    <span class=\"n\">从Y中随机采样𝑁𝑦个数据点构建一个mini</span><span class=\"o\">-</span><span class=\"n\">batch</span>\n    <span class=\"n\">对于mini</span><span class=\"o\">-</span><span class=\"n\">batch中采样的每个数据点都使用前向传播算法计算G</span><span class=\"err\">∗</span><span class=\"n\">𝑗</span> <span class=\"o\">=</span><span class=\"n\">𝑔</span><span class=\"o\">(</span><span class=\"n\">y𝑗</span><span class=\"o\">;</span><span class=\"n\">𝜃𝑦</span><span class=\"o\">)</span>\n    <span class=\"n\">根据等式</span><span class=\"err\">（</span><span class=\"n\">4</span><span class=\"err\">）</span><span class=\"n\">求导</span>\n    <span class=\"n\">使用后向传播算法更新参数𝜃𝑦</span>\n  <span class=\"n\">end</span> <span class=\"k\">for</span>\n  <span class=\"c1\">//学习矩阵B\n</span><span class=\"c1\"></span>  <span class=\"n\">根据等式</span><span class=\"err\">（</span><span class=\"n\">5</span><span class=\"err\">）</span><span class=\"n\">学习矩阵B</span></code></pre></div><h2>4.Experiment</h2><p>我们在图像-文本数据集上进行试验，以验证DCMH的高效性。DCMH使用开源深度学习工具包<code>MatConvNet</code>实现，在NVIDIA K80的GPU服务器上运行。</p><h3>4.1 Dataset</h3><p>三个数据集用作评估：<i>MIRFLICKR-25K</i> , <i>IAPR TC-12</i>  和 <i>NUS-WIDE</i> </p><p><i>MIRFLICKR-25K</i>原始数据包含了25000张从Flickr网站上收集的图片。每张图片都和几个文本标签关联。因此每个数据点都是图像-文本对。并且我们在试验中选择的每张图片都有至少20个文本标签。每个数据点的文本都用1386维的词袋向量表示。基于手工提取特征的方法，每张图片都用512维的GIST特征向量表示。而且，每个数据点都使用24个唯一标签之一手动注释。</p><p><i>IAPR TC-12</i>数据集包含了20000张图像-文本对，共有255类标签。我们实验使用了整个数据集。每个数据点的文本都用2912维的词袋向量表示。基于手工提取特征的方法，每张图片都用512维的GIST特征向量表示。</p><p><i>NUS-WIDE</i>数据集包含260648个网站图片，一些图片是关联了文本标签。他是一个多标签数据集，也就是说每个图片都有一个或者多个标签（共81个概念标签）。我们选取了21个最频繁的概念，其中包括195834个图像-文本数据对。每个数据点的文本都用1000、维的词袋向量表示。基于手工提取特征的方法，每张图片都用500维的视觉词袋特征向量表示。</p><p>对于所有的数据集，图像 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 和文本 <img src=\"https://www.zhihu.com/equation?tex=j\" alt=\"j\" eeimg=\"1\"/> 如果点 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 和点 <img src=\"https://www.zhihu.com/equation?tex=j\" alt=\"j\" eeimg=\"1\"/> 是有至少一个共同标签，我们就认为他们是相似的，否则我们认为不相似。</p><h3>4.2 <b>Evaluation Protocol and Baseline</b></h3><h3>4.2.1 <b>Evaluation Protocol</b></h3><p>对于<i>MIRFLICKR-25K</i>和<i>IAPR TC-12</i>数据集，我们随机抽样2,000个数据点作为测试（查询）集，其余点作为检索集（数据库）。对于<i>NUS-WIDE</i>数据集，我们将2,100个数据点作为测试集，其余数据作为检索集。此外，我们从检索集中采样10,000个数据点作为<i>MIRFLICKR-25K</i>和<i>IAPR TC-12</i>的训练集。对于NUS-WIDE数据集，我们从检索集中将10,500个数据点作为训练集进行采样。实际的上邻居是被定义为共享至少一个共同标签的那些图像-文本对。</p><p>对于基于哈希的检索，汉明排名（Hamming Ranking）和哈希查找（Hash look up）是两种广泛使用的检索协议。我们还采用这两个协议来评估我们的方法和其他baseline。汉明排名协议按照递增顺序对给定查询点的汉明距离对数据库中的点（检索集）进行排序。平均精度均值（MAP）是衡量汉明排名协议准确性而广泛使用的度量标准。哈希查找协议返回的是离查询点在某个汉明半径内的所有点。准确率-召回率PR曲线是用于衡量哈希查找协议准确性的广泛使用的度量标准。</p><h3>4.2.2 Baseline</h3><p>采用六种最先进的跨模式散列方法作为比较基线，包括SePH ，STMH ，SCM ，CMFH ，CCA 和DVSH 。。 由于DVSH只能用于其中一种模态必须是时间动态的特殊CMH情况，我们仅在IAPR TC-12数据集上比较DCMH和DVSH，其中原始文本是可被视为时间动态的句子。 <i>MIRFLICKR-25K</i>和<i>NUS-WIDE</i>中的文本是不适合DVSH的标签。 请注意，除了DVSH之外，所有评估方法的文本都表示为BOW向量。</p><p>SePH，STMH和SCM的源代码由相应的作者提供。 对于没有代码的CMFH和CCA，我们自己仔细实现的。 SePH是一种基于内核的方法，我们使用RBF内核，并按照作者的建议，将500个随机选择的点作为内核基础。 在SePH中，作者提出了两种策略来根据是否观察到两个点的模态来构造检索（数据库）点的哈希码。 然而，在本文中，我们只使用一种模态用于数据库（检索）点，因为本文的重点是跨模态检索。 所有基线的所有其他参数都是根据这些基线的原始论文的建议设定的。</p><p>对于DCMH，我们使用验证集来选择超参数 <img src=\"https://www.zhihu.com/equation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"/> ，并发现在 <img src=\"https://www.zhihu.com/equation?tex=%5Cgamma%3D%5Ceta%3D1\" alt=\"\\gamma=\\eta=1\" eeimg=\"1\"/> 时可以实现良好的性能。 因此，对于DCMH，我们设置 <img src=\"https://www.zhihu.com/equation?tex=%5Cgamma%3D%5Ceta%3D1\" alt=\"\\gamma=\\eta=1\" eeimg=\"1\"/> 。 我们利用在ImageNet数据集上预训练的CNN-F网络来初始化CNN的前七层并用于图像模态。 DCMH中的深度神经网络的所有其他参数都是被随机初始化的。 图像模态的输入是原始像素，文本模态的输入是BOW矢量。 我们将mini-batch大小固定为128，并将算法1中外环的迭代次数设置为500.学习率从 <img src=\"https://www.zhihu.com/equation?tex=10%5E%7B-6%7D\" alt=\"10^{-6}\" eeimg=\"1\"/> 到<img src=\"https://www.zhihu.com/equation?tex=10%5E%7B-1%7D\" alt=\"10^{-1}\" eeimg=\"1\"/> 中选择，并带有验证集。 所有实验运行五次，并报告平均性能。</p><h3><b>4.3. Accuracy</b></h3><h3><b>4.3.1 Hamming Ranking</b></h3><p>在<i>MIRFLICKR-25K</i>，<i>IAPR TC-12</i>和<i>NUS-WIDE</i>数据集上DCMH和其他具有手工制作特征的基线的MAP结果在表3中报告。这里，“ <img src=\"https://www.zhihu.com/equation?tex=I+%5Crightarrow+T\" alt=\"I \\rightarrow T\" eeimg=\"1\"/> ”表示查询是图像的情况而数据库是文本，&#34; <img src=\"https://www.zhihu.com/equation?tex=I%5Crightarrow+T\" alt=\"I\\rightarrow T\" eeimg=\"1\"/> &#34;表示查询是文本而数据库是图像的情况。 我们可以发现DCMH可以超越所有其他通过手工制作的特征方法的基线。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-52c0f788d466b6fe9c155b0eff10b62d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1852\" data-rawheight=\"1422\" class=\"origin_image zh-lightbox-thumb\" width=\"1852\" data-original=\"https://pic2.zhimg.com/v2-52c0f788d466b6fe9c155b0eff10b62d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1852&#39; height=&#39;1422&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1852\" data-rawheight=\"1422\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1852\" data-original=\"https://pic2.zhimg.com/v2-52c0f788d466b6fe9c155b0eff10b62d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-52c0f788d466b6fe9c155b0eff10b62d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>为了进一步验证DCMH的有效性，我们利用在ImageNet数据集上预训练的CNN-F深度网络，以提取CNN特征，该网络与DCMH中图像模态的初始CNN相同。 所有基线都基于这些CNN特征进行训练。 表4中报告了DCMH和其他具有CNN特征的基线在三个数据集上的MAP结果。我们可以发现在NUS-WIDE上图像到文本的检索，DCMH可以胜过除SePH之外的所有其他基线。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-40d8d26944bc8b38a4d0ac0547f022d4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1874\" data-rawheight=\"1418\" class=\"origin_image zh-lightbox-thumb\" width=\"1874\" data-original=\"https://pic1.zhimg.com/v2-40d8d26944bc8b38a4d0ac0547f022d4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1874&#39; height=&#39;1418&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1874\" data-rawheight=\"1418\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1874\" data-original=\"https://pic1.zhimg.com/v2-40d8d26944bc8b38a4d0ac0547f022d4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-40d8d26944bc8b38a4d0ac0547f022d4_b.jpg\"/></figure><h3><b>4.3.2 Hash Lookup</b></h3><p>在哈希查询协议中，我们可以计算给定任何汉明半径内的返回点的精度和召回率。 通过使用步长1将汉明半径从0变为 <img src=\"https://www.zhihu.com/equation?tex=c\" alt=\"c\" eeimg=\"1\"/> ，我们可以得到准确率-召回率曲线。</p><p>图2显示了三个数据集在编码长度是16位个准确率-召回率PR曲线，在图中的每行，其中前两个子图基于手工制作的特征，最后两个子图基于CNN-F特征。 我们可以发现DCMH可以显着优于其他方法无论是基于手工制作的特征和CNN-F特征。 我们的DCMH还可以在具有不同编码长度值的其他情况下实现最佳性能，例如32位和64位。 由于空间限制，省略了这些结果。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h3><b>4.4. Comparison with DVSH</b></h3><p>由于DVSH的源代码不公开，并且重新实现DVSH也很困难，我们采用与DVSH相同的实验设置来评估DCMH并直接将结果用于DVSH进行比较。 表5中列出了<i>IAPR TC-12</i>数据集上的前500个MAP比较结果。请注意，DVSH的文本输入是句子，我们将句子表示为DCMH的相同的BOW向量。 我们可以发现DCMH在大多数情况下可以胜过DVSH。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e8f5a71c854971062a45c2dfbd24a614_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2312\" data-rawheight=\"1706\" class=\"origin_image zh-lightbox-thumb\" width=\"2312\" data-original=\"https://pic1.zhimg.com/v2-e8f5a71c854971062a45c2dfbd24a614_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2312&#39; height=&#39;1706&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2312\" data-rawheight=\"1706\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2312\" data-original=\"https://pic1.zhimg.com/v2-e8f5a71c854971062a45c2dfbd24a614_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e8f5a71c854971062a45c2dfbd24a614_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h3><b>4.5. Sensitivity to Parameters</b></h3><p>我们探讨了超参数 <img src=\"https://www.zhihu.com/equation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"/> 的影响。 图3列出了具有不同 <img src=\"https://www.zhihu.com/equation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"/>值的在<i>MIRFLICKR-25K</i>数据集的MAP结果，其中编码长度为16位。 我们可以看出DCMH对<img src=\"https://www.zhihu.com/equation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"/>不敏感，其中 <img src=\"https://www.zhihu.com/equation?tex=0.01%3C%5Cgamma%3C2\" alt=\"0.01&lt;\\gamma&lt;2\" eeimg=\"1\"/> 且 <img src=\"https://www.zhihu.com/equation?tex=0.01%3C%5Ceta%3C2\" alt=\"0.01&lt;\\eta&lt;2\" eeimg=\"1\"/> 。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c10e15da1c1ffeb2c891706ede7e9838_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2092\" data-rawheight=\"944\" class=\"origin_image zh-lightbox-thumb\" width=\"2092\" data-original=\"https://pic1.zhimg.com/v2-c10e15da1c1ffeb2c891706ede7e9838_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2092&#39; height=&#39;944&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2092\" data-rawheight=\"944\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2092\" data-original=\"https://pic1.zhimg.com/v2-c10e15da1c1ffeb2c891706ede7e9838_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c10e15da1c1ffeb2c891706ede7e9838_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h3><b>4.6. Further Analysis</b></h3><p>为了进一步验证特征学习的有效性，我们评估了DCMH的一些变体，即DCMH-I，DCMH-T，DCMH-IT。 DCMH-I表示没有图像特征学习的变体，其中我们在训练期间固定图像模态的前七层的参数。 DCMH-T表示没有文本特征学习的变体，其中我们将文本模态的深度神经网络替换为线性投影。 DCMH-IT表示没有图像和文本特征学习的变体。</p><p>图4报告了<i>IAPR TC-12</i>上的MAP结果。 我们可以发现DCMH可以实现比DCMH-I，DCMH-T和DCMH-IT更高的准确性，这证明了同时进行哈希码学习和特征学习的重要性。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0ed2038faa2d25e5eb62047f9cacdd7c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2040\" data-rawheight=\"1028\" class=\"origin_image zh-lightbox-thumb\" width=\"2040\" data-original=\"https://pic1.zhimg.com/v2-0ed2038faa2d25e5eb62047f9cacdd7c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2040&#39; height=&#39;1028&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2040\" data-rawheight=\"1028\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2040\" data-original=\"https://pic1.zhimg.com/v2-0ed2038faa2d25e5eb62047f9cacdd7c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0ed2038faa2d25e5eb62047f9cacdd7c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>5.Conclusion</h2><p>在本文中，我们提出了一种新的哈希方法，称为DCMH，用于跨模态检索应用。 DCMH是一个端到端的深度学习框架，可以同时执行特征学习和哈希码学习。 对三个数据集的实验表明，DCMH可以明显优于其他基线，以在实际应用中实现最先进的性能。</p><h2>总结与思考</h2><p>1.用深度学习提取特征，并且每种模态都用一组NN去处理，然后hash码蕴含的相似度也是学习生成的。</p><p>2.这是一个end2end的模型，这个框架具有很强的通用性，方向很棒。</p><p>3.还有很多可以去完善的地方，比如一些小细节，像文中提到的网络的设计。</p><p>4.只能解决部分问题（CMH），感觉是CMH+DL，把文本和图像（多个模态）独立去做的处理，未考虑之间的关联关系。</p><p> More........</p>", 
            "topic": [
                {
                    "tag": "信息检索", 
                    "tagLink": "https://api.zhihu.com/topics/19580199"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "多模态学习", 
                    "tagLink": "https://api.zhihu.com/topics/20688199"
                }
            ], 
            "comments": [
                {
                    "userName": "毛豆大白", 
                    "userLink": "https://www.zhihu.com/people/dedb4b4be9fb137282ee80b21ea9cb11", 
                    "content": "<p>很清楚！谢谢大佬</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50769588", 
            "userName": "idejie", 
            "userLink": "https://www.zhihu.com/people/67d6bc7c8363bef16771d582f32ffc25", 
            "upvote": 0, 
            "title": "《信息检索》Boolean Retrieval", 
            "content": "<h2>(1)信息检索概述</h2><p>①<b>信息检索</b>是从大规模非结构化数据(通常是文本)的集合 (通常保存在计算机上)中找出满足用户信息需求的资料 (通常是文档)的过程。 </p><p>②特点：文档（结构化、半结构化、非结构化）、信息需求、文档集合语料库</p><p>③主要应用：<b>文本检索</b></p><p>④数据：</p><p>i. IR vs数据库: 结构化 vs ⾮非结构化数据(结构化数据即指“表”中的数据,数据库常常支持范围或者精确匹配查询 。e.g.,Salary &lt; 60000 AND Manager = Smith.)</p><p>ii. ⾮非结构化数据:通常指自由文本(free text),允许关键词加上操作符号的查询(奥运会AND游泳),允许更加复杂的概念性的查询找(出所有的有关药物滥用(drug abuse)的网页);经典的检索模型一般都针对自由文本进行处理</p><p>iii. 半结构化数据:没有数据是完全无结构的，比如网页就是一种半结构化数据.半结构化查询( Title contains data AND Bullets contain search) 严格来说，即使是文本也是有“语言”结构, 比如主谓宾结构 </p><p>⑤传统信息检索 vs. 现代信息检索：</p><ul><li> 传统信息检索主要关注非结构化、半结构化数据   </li><li> 现代信息检索中也处理结构化数据   </li></ul><p>⑥布尔检索</p><p>针对布尔查询的检索，布尔查询是指利用 AND,OR 或者 NOT操作符将词项 连接起来的查询</p><h2>(2)倒排索引</h2><p>①《莎士比亚全集》查找一个布尔表达式的结果</p><ul><li>暴力方法：从头到尾查</li><li>优点：方法简单；支持文档的动态变化</li><li>缺点：慢；不容易处理not；不支持其他操作（近义词）；不支持排序</li><li>词项-文档的关联矩阵：行（人名）在列（章节）是否出现，是为1，反之为0</li><li>能够对布尔表达式进行向量运算</li></ul><p>②信息检索的基本假设：</p><ul><li> 文档集（Collection）：由固定数量的文档组成（不变）  </li><li> 目标：返回与用户需求相关的文档并辅助用户来完成某项任务  </li><li> 相关性：  </li><li> 主观的概念  </li><li>反映对象的匹配程度</li><li> 不同应用的相关性不同  </li><li> 典型的搜索过程：  </li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5b5afc20267ecbd1b80ab84371ea5b1d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1308\" data-rawheight=\"830\" class=\"origin_image zh-lightbox-thumb\" width=\"1308\" data-original=\"https://pic2.zhimg.com/v2-5b5afc20267ecbd1b80ab84371ea5b1d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1308&#39; height=&#39;830&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1308\" data-rawheight=\"830\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1308\" data-original=\"https://pic2.zhimg.com/v2-5b5afc20267ecbd1b80ab84371ea5b1d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5b5afc20267ecbd1b80ab84371ea5b1d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ul><li> 检索效果的评价  </li><li> 正确率（Precision）：返回中正确结果占返回结果的比例。  </li><li>召回率（Recall）：返回中正确结果占所有正确结果的比例。</li><li>二者缺一不可：</li><li>全部返回，正确率低，召回率100%</li><li>只返回一个正确结果，正确率100%，召回率低</li></ul><p>③倒排索引</p><ul><li> 场景：词项-文档矩阵非常大，而且是非常稀疏的矩阵，如何检索  </li><li> 通常采用变长表方式   </li></ul><p>▪ 磁盘上，顺序存储方式比较好，便于快速读取 </p><p>▪ 内存中，采用链表或者可变长数组方式 </p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-fc3c77285197e05d8bf7ddef830a02e1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1312\" data-rawheight=\"656\" class=\"origin_image zh-lightbox-thumb\" width=\"1312\" data-original=\"https://pic2.zhimg.com/v2-fc3c77285197e05d8bf7ddef830a02e1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1312&#39; height=&#39;656&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1312\" data-rawheight=\"656\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1312\" data-original=\"https://pic2.zhimg.com/v2-fc3c77285197e05d8bf7ddef830a02e1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-fc3c77285197e05d8bf7ddef830a02e1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>倒排索引的构建</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e176cac17662426e223482b80dce3c1d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1362\" data-rawheight=\"794\" class=\"origin_image zh-lightbox-thumb\" width=\"1362\" data-original=\"https://pic2.zhimg.com/v2-e176cac17662426e223482b80dce3c1d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1362&#39; height=&#39;794&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1362\" data-rawheight=\"794\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1362\" data-original=\"https://pic2.zhimg.com/v2-e176cac17662426e223482b80dce3c1d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e176cac17662426e223482b80dce3c1d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ul><li> 索引构建过程  </li><li> 构建词条序列：&lt;词条，docID&gt;二元组  </li><li> 排序：按词项排序，然后每个词项按docID排序   </li><li> 合并：某个词项在单篇文档出现多次会被合并  </li><li> 拆分：拆分成词典和倒排记录表两部分  </li><li> 最终：&lt;（词项，频率），docIDs&gt;  </li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-2130fa4031b89d3dbcc3de889381151c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"221\" data-rawheight=\"402\" class=\"content_image\" width=\"221\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;221&#39; height=&#39;402&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"221\" data-rawheight=\"402\" class=\"content_image lazy\" width=\"221\" data-actualsrc=\"https://pic1.zhimg.com/v2-2130fa4031b89d3dbcc3de889381151c_b.jpg\"/></figure><h2>(3)布尔查询处理</h2><ul><li> 场景：如何用倒排索引来处理查询  </li><li> AND：  </li><li> 在索引中定位A，返回A的倒排记录表  </li><li>在索引中定位B，返回B的倒排记录表</li><li> Merge两个倒排记录表，求交集  </li><li> Merge的方法  </li><li> 线性搜索法：每个倒排记录表都有一个定位指针，两个指针同时从前往后扫描, 每次比较当前指针对应倒排记录，然后移动某个或两个指针。合并时间为两个表长之和的线性时间  </li><li> 调表指针法：  </li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9974dcca89bf1031f4c67a0409352653_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"848\" data-rawheight=\"224\" class=\"origin_image zh-lightbox-thumb\" width=\"848\" data-original=\"https://pic4.zhimg.com/v2-9974dcca89bf1031f4c67a0409352653_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;848&#39; height=&#39;224&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"848\" data-rawheight=\"224\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"848\" data-original=\"https://pic4.zhimg.com/v2-9974dcca89bf1031f4c67a0409352653_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-9974dcca89bf1031f4c67a0409352653_b.jpg\"/></figure><p>  查询52时，直接先查2处，比2和41都大，那就要调到41处再往后查48，64  <b>调表指针的位置：</b> </p><ul><li>简单的启发式策略：对于长度为$L$的倒排记录表，每$ \\sqrt L$ 处放一个跳表指针，即均匀放置。均匀放置方法忽略了查询词项的分布情况</li><li>如果索引相对静态，均匀方式方法是一种很简便的方法，但是如果索引经常更新造成L经常变化，均匀方式方式就很不方便</li><li>跳表方式在过去肯定是有用的，但是对于现代的硬件设备而言，如果合并的倒排记录表不能全部放入内存的话，上述方式不一定有用,更大的倒排记录表(含跳表)的 I/O开销可能远远超过内存中合并带来的好处</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li> 查询优化  </li><li>按照表从小到大合并（这也是为什么倒排索引要按DF排序的原因）</li><li>转换成合取范式，获得每个词项的df，将词项相加，估计每个or表达式对应的大小，再从大到小处理or</li><li> 优点  </li><li>构建过程简单</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li> 缺点  </li><li>布尔查询构建复杂，不适合普通用户。构建不当，检索结果过多或者过少</li><li>没有充分利用词项的频率信息，通常出现的越多越好，需要利用词项在文档中的词项频率(term frequency, tf)信息</li><li>不能对检索结果进行排序</li></ul>", 
            "topic": [
                {
                    "tag": "信息检索", 
                    "tagLink": "https://api.zhihu.com/topics/19580199"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50769609", 
            "userName": "idejie", 
            "userLink": "https://www.zhihu.com/people/67d6bc7c8363bef16771d582f32ffc25", 
            "upvote": 0, 
            "title": "《信息检索》Dictionary", 
            "content": "<h2>（1）文档</h2><ul><li>确定文档的格式：额外的转换过程（&amp;amp；）</li><li>文档的编码方式：多语言并存（该判断过程看成一个基于机器学习的分类问题 1(我们将在第 13   章讨论)来处理，但在实际中往往通过启发式方法来实现，也可以利用文档的元信息或者直接   由用户手工选择来确定。）</li><li>字符序列化：不一定线性</li><li>文档单位选择：</li><li>如果索引粒度太小，那么由于词项散布在多个细粒度文档中，我们就很可能错过那些重要的段落，也就是说此时正确率高而召回率低;反之，如果索引粒度太大，我们就很可能找到很多不相关的匹配结果，即正确率低而召回率高</li><li>索引粒度过大造成的问题也可以通过采用显式或隐式的邻近搜索方法来缓解</li></ul><h2>（2）词条</h2><ul><li>词条化：</li><li> 在这个过程中，可能会同时去掉一些特殊字符，如标点符号等。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-16b50240b4449fe4ca3b38ef531920da_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1400\" data-rawheight=\"208\" class=\"origin_image zh-lightbox-thumb\" width=\"1400\" data-original=\"https://pic3.zhimg.com/v2-16b50240b4449fe4ca3b38ef531920da_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1400&#39; height=&#39;208&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1400\" data-rawheight=\"208\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1400\" data-original=\"https://pic3.zhimg.com/v2-16b50240b4449fe4ca3b38ef531920da_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-16b50240b4449fe4ca3b38ef531920da_b.jpg\"/></figure><ul><li> 词条：在文档中出现的字符序列的一个实例  </li><li> 词条类：相同词条构成的集合  </li><li> 词项：在信息检索系统词典中所包含的某个可能经过归一化处理的词条类  </li><li> to sleep perchance to dream：五个词条、4个词条类、3个词项（停用词to会被去掉）  </li><li> 词条化的主要任务就是确定哪些才是正确的词条，际上即使对于单词之间存在空格的英文来说也存在很多难以处理的问题。比如，英文中的上撇号“ ’” 既可以代表所有关系也可以代表缩写  </li><li>一个非常简单的做法就是在既非字母也非数字的字符处进行拆分（并不好）</li><li>词条化处理往往与语言本身有关，不同语言下的词条化处理并不相同。很多语言的分词方法不通用，歧义。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li> 问题  </li><li>多语言分词</li><li>单个语言里面，索引单位的选择，一个字？一个词 ？怎么确定词（歧义，多种分割）？</li><li>重音符/时间格式</li><li>大小写</li><li> 中文分词：是否使用词典；是否基于规则或者 统计的方法  </li><li> 正向最大匹配；逆向最大匹配的方法  </li><li> 问题：新词，歧义（两种多种分法）  </li><li> 解决歧义和未登录词识别的基本方法:  </li><li>规则方法：分词过程中或者分词结束后根据规则进行处理；</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>统计方法：分词过程中或者分词结束后根据统计训练信息进行处理。</li><li> 规则+统计  </li><li> 结论：  </li><li> 并非分词精度高一定检索精度高  </li><li> 检索中的分词，查询和文档都分词  </li><li> 搜索引擎中的分词方法  </li><li>猜想：大词典+统计+启发式规则</li><li> 去停用词  </li><li> 一个常用的生成停用词表的方法就是将词项按照文档集频率(collection frequency，每个词项在文档集中出现的频率)从高到低排列，然后手工选择那些语义内容与文档主题关系不大的高频词作为停用词。  </li><li> 词项归一化  </li><li> 词条归一化(token normalization)就是将看起来不完全一致的多个词条归纳成一个等价类，以便在它们之间进行匹配的过程 1。最常规的做法是隐式地建立等价类 2，每类可以用其中的某个元素来命名。  </li><li> 问题：  </li><li>重音及变音符号问题。</li><li>大小写转换问题。</li><li>英语中的其他问题。colour color（拼写错误：soundex）</li><li>其他语言的问题。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li> 词干还原和词形归并  </li><li> 词干还原(stemming)和词形归并(lemmatization)这两个术语所代表的意义是不同的。前者通常指的是一个很粗略的<b>去除单词两端词缀</b>的启发式过程，并且希望大部分时间它 都能达到这个正确目的，这个过程也常常包括去除派生词缀。而词形归并通常指利用词汇表和 词形分析来去除<b>屈折</b>（？）词缀，从而返回词的原形或词典中的词的过程，返回的结果称为词元 (lemma)。   </li><li>Porter 算法</li></ul><h2>（3）跳表</h2><ul><li>基于跳表的倒排记录表快速合并算法</li><li>跳表指针只对 AND 类型的查询有用</li><li>如果索引相对固定的话，建立有效的跳表指针则比较容易。</li><li>含位置信息的倒排记录表及短语查询</li><li>二元词索引：将每个接续词对看成词项，这样马上就能处理两个词构成的短语查询，更长的查询可以分成多个短查询来处理 。用名词和名词短语来表述用户所查询的概念具有相当特殊的地位</li><li>the abolition of slavery       renegotiation of the constitution</li><li>对文本进行词条化</li><li>词性标注</li><li>将形式为NX*N非词项序列看成一个扩展的二元词</li><li>二元词索引的概念可以扩展到更长的词序列，如果索引中包含变长的词序列，通常就称为<b>短语索引</b></li></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>位置索引：不只是简单地判断两个词项是否出现在同一文档中，而且还需要检查它们出现的位置关系和查询短语的一致性。这就需要计算出词之间的偏移距离。（距离跳远不是词组）</li><li>混合索引机制：对某些查询使用短语索引或只使用二元词索引，而对其他短语查询则采用位置索引。短语索引所收录的那些较好的查询可以根据用户最近的访问行为日志统计得到，也就是说，它们往往是那些高频常见的查询。当然，这并不是唯一的准则。处理开销最大的短语查询往往是这样一些短语，它们中的每个词都非常常见，但是组合起来却相对很少见 </li></ul>", 
            "topic": [
                {
                    "tag": "信息检索", 
                    "tagLink": "https://api.zhihu.com/topics/19580199"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50769514", 
            "userName": "idejie", 
            "userLink": "https://www.zhihu.com/people/67d6bc7c8363bef16771d582f32ffc25", 
            "upvote": 0, 
            "title": "《信息检索》Introduction", 
            "content": "<p></p><h2>(1)什么是信息检索</h2><p>①首先，互联网大部分应用（搜索引擎、在线购物、婚恋网站）所共同具有的特征：</p><ul><li>给定需求(或者是对象)，从信息库中找出与之最匹配的信息(或对象)，如在Google中搜“信息检索”应该返回相关的研究网站和书籍，在Amazon上搜“天王表”应该返回手表……</li><li>不同场景下的“匹配”定义不同。如，你在网上买了微波炉之后，再给你推荐的不应该还是微波炉了；微博推荐好友时，不应该向你老婆推荐前女友了……</li><li>返回的数据形式都是以结构化的数据和无固定结构的自由文本。</li></ul><p>②然后，信息检索就是要满足上述需求的一门技术和学科。</p><ul><li>给定用户需求返回满足该需求信息的一门学科。通常涉及信息的获取、存储、组织和访问。</li><li>从大规模非结构化数据(通常是文本)的集合(通常保存在计算机上)中找出满足用户信息需求的资料(通常是文档)的过程。</li><li>“找对象”的学科，即定义并计算某种匹配“相似度”的学科。</li></ul><p>③信息检索应用广泛，如搜索引擎、推荐系统、舆情分析 、情报分析、数据挖掘、内容安全等等。</p><p>④信息检索分类。</p><ul><li>从规模上</li><li>个人信息检索：个人相关信息的组织、整理、搜索等。桌面搜索(Desktop Search)、个人信息管理(PIM = Personal Information Management)、个人数字记忆(Personal Digital Memory)</li><li>企业级信息检索：在企业内容文档的组织、管理、搜索等。企业级信息检索是内容管理(Content Management)的重要组成部分。局域网/内网搜索</li><li>Web信息检索：在超大规模数据集上的检索。</li></ul><h2>(2)为什么要信息检索</h2><p>①市场发展的需求</p><ul><li> 用户(国家、企业、个人等)需要信息检索技术：互联网的信息量太大、噪音太多，寻找所需要的信息非常不容易<br/> </li><li> 公司需要信息检索技术：<br/> </li><ul><li>搜索引擎改变了很多传统的生活方式，Yahoo、Google、Baidu，还有一些公司如Microsoft、Sina、Sohu、Tencent、Netease、360、Facebook都加入到这个搜索技术的竞争。</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>目前搜索引擎公司甚至整个互联网正常运转的计算广告的核心技术是信息检索技术</li><li>不只是搜索引擎才需要信息检索技术，电子商务(如亚马逊网站、淘宝等)、社交网(微博、Facebook、twitter、校内网)、数字图书馆、大规模数据分析(金融证券行业等)等都需要信息检索技术</li><li>人才的竞争：搜索相关人才人数出现缺口，他们非常抢手，待遇如日中天</li><li>是不是泡沫：2000年左右出现的网络泡沫和现在的互联网有什么不同，搜索引擎在其中占什么位置？</li></ul><p>②众多应用的需求</p><ul><li>移动搜索</li><li>产品搜索</li><li>专利搜索</li><li>广告推荐</li><li>社会网络分析</li><li>消费行为分析</li><li>网络评论分析</li><li>SEO营销</li><li>……</li></ul><h2>(3)信息检索的三个层次</h2><p>①应用层次：搜索是一项非常重要的应用！（搜索引擎）</p><p>②中间层次：搜索是极其重要的API（站内检索，内容检索，数据分析）</p><p>③核心层次：搜索是未来操作系统的重要组成部分！</p><p></p>", 
            "topic": [
                {
                    "tag": "信息检索", 
                    "tagLink": "https://api.zhihu.com/topics/19580199"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/48141856", 
            "userName": "idejie", 
            "userLink": "https://www.zhihu.com/people/67d6bc7c8363bef16771d582f32ffc25", 
            "upvote": 4, 
            "title": "[译]一种非负矩阵分解算法", 
            "content": "<blockquote>原文：</blockquote><a href=\"https://link.zhihu.com/?target=https%3A//www.google.com/url%3Fsa%3Dt%26rct%3Dj%26q%3D%26esrc%3Ds%26source%3Dweb%26cd%3D1%26cad%3Drja%26uact%3D8%26ved%3D2ahUKEwjOl-6BpLDeAhULUrwKHfIPBDQQFjAAegQICRAC%26url%3Dhttps%253A%252F%252Fpapers.nips.cc%252Fpaper%252F1861-algorithms-for-non-negative-matrix-factorization.pdf%26usg%3DAOvVaw0TSBcQUBYPJTzg3EnpXe2A\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Algorithms for Non-negative Matrix Factorization</a><h2><b>摘要</b></h2><p>非负矩阵分解是已知的对多元数据降维的有效方法。本文分析了两类不同的NMF多元算法。发现他们只是在更新规则中乘法因子的有轻微的不同。一类算法是最小化传统最小二乘法产生的误差，另一种是最小化泛化的KL散度。这两算法的单调收敛可以使用一个辅助函数来证明，类似于证明EM算法收敛时的辅助函数。算法也可被认为重新调节的梯度下降，其中重新调节因子选择最优来保证收敛。 </p><h2><b>1.介绍</b></h2><p>诸如主成分分析和矢量量化的无监督学习算法可以被理解为将受到不同约束的数据矩阵分解。根据所使用的约束，所得因子具有代表的性质也不同。主成分分析法仅强调弱正交约束，从而产生一个非常分散的表达式用来消除生成的可变性。[1,2] 另一方面，矢量量化使用了一个严格的&#39;winner-take-all&#39;的约束，使数据聚类成一个互斥的原型。[3]</p><p>我们之前已经证明，非负性是矩阵分解的有用约束，它可以学习数据的部分表示。 [4,5] 学习的非负基础向量用于分散但仍稀疏的组合，以在重建中产生表达。 [6,7]在本次提交中，我们详细分析了两种数值算法，用于从数据中学习最优非负因子。</p><h2><b>2.非负矩阵分解</b></h2><p>我们一般考虑这些算法解决一下问题：</p><p><b>非负矩阵分解NMF</b></p><p>给定一个非负矩阵 <img src=\"https://www.zhihu.com/equation?tex=V\" alt=\"V\" eeimg=\"1\"/> ，找到非负矩阵因子 <img src=\"https://www.zhihu.com/equation?tex=W\" alt=\"W\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=H\" alt=\"H\" eeimg=\"1\"/> ,如 </p><p><img src=\"https://www.zhihu.com/equation?tex=V%5Capprox+WH++%5Ctag%7B1%7D\" alt=\"V\\approx WH  \\tag{1}\" eeimg=\"1\"/> </p><p> NMF可以以下列方式应用于多变量数据的统计分析。</p><p>给定一组具有多元 <img src=\"https://www.zhihu.com/equation?tex=n+\" alt=\"n \" eeimg=\"1\"/> 维数据的向量，这些向量被放在一个 <img src=\"https://www.zhihu.com/equation?tex=n+%5Ctimes+m\" alt=\"n \\times m\" eeimg=\"1\"/> 的矩阵 <img src=\"https://www.zhihu.com/equation?tex=V\" alt=\"V\" eeimg=\"1\"/> ，其中 <img src=\"https://www.zhihu.com/equation?tex=m\" alt=\"m\" eeimg=\"1\"/> 是数据集中例子的数量。 然后将该矩阵近似地分解为 <img src=\"https://www.zhihu.com/equation?tex=n%5Ctimes+r\" alt=\"n\\times r\" eeimg=\"1\"/> 的矩阵 <img src=\"https://www.zhihu.com/equation?tex=W\" alt=\"W\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=r%5Ctimes+m\" alt=\"r\\times m\" eeimg=\"1\"/> 矩阵 <img src=\"https://www.zhihu.com/equation?tex=H\" alt=\"H\" eeimg=\"1\"/> .通常选择 <img src=\"https://www.zhihu.com/equation?tex=r\" alt=\"r\" eeimg=\"1\"/> 小于或小于 <img src=\"https://www.zhihu.com/equation?tex=m\" alt=\"m\" eeimg=\"1\"/> ，使得 <img src=\"https://www.zhihu.com/equation?tex=W\" alt=\"W\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=H\" alt=\"H\" eeimg=\"1\"/> 小于原始矩阵 <img src=\"https://www.zhihu.com/equation?tex=V\" alt=\"V\" eeimg=\"1\"/> .这个结果就是原始数据矩阵的压缩版本。</p><p><b>方程（1）中</b> <img src=\"https://www.zhihu.com/equation?tex=%5Capprox\" alt=\"\\approx\" eeimg=\"1\"/> <b>的意义是什么？</b>它可以逐列重写为 <img src=\"https://www.zhihu.com/equation?tex=v%5Capprox+Wh\" alt=\"v\\approx Wh\" eeimg=\"1\"/> ，其中 <img src=\"https://www.zhihu.com/equation?tex=v\" alt=\"v\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=h\" alt=\"h\" eeimg=\"1\"/> 是 <img src=\"https://www.zhihu.com/equation?tex=V\" alt=\"V\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=H\" alt=\"H\" eeimg=\"1\"/> 的对应列。换句话说，每个数据向量 <img src=\"https://www.zhihu.com/equation?tex=v\" alt=\"v\" eeimg=\"1\"/> 由 <img src=\"https://www.zhihu.com/equation?tex=W\" alt=\"W\" eeimg=\"1\"/> 的列的线性组合近似和， <img src=\"https://www.zhihu.com/equation?tex=h\" alt=\"h\" eeimg=\"1\"/> 中的元素代表权重。 因此， <img src=\"https://www.zhihu.com/equation?tex=W\" alt=\"W\" eeimg=\"1\"/> 可以被视为包含针对V中的数据的线性近似而优化的基础。由于使用相对较少的基矢量来表示许多数据矢量，所以只有在数据中基矢量发现潜在的结构时才能实现良好的近似。</p><p>本提交的内容不是关于NMF的应用，而是关注于寻找非负矩阵因子分解的技术问题。 当然，在数值类型的线性代数中已经广泛研究了其他类型的矩阵因子分解，但非负性约束使得此前的工作大部分不适用于本案例。</p><p>在这里，我们讨论了基于 <img src=\"https://www.zhihu.com/equation?tex=W\" alt=\"W\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=H\" alt=\"H\" eeimg=\"1\"/> 的迭代更新的两种NMF算法。由于这些算法易于实现并且它们的收敛性能得到保证，我们发现它们在实际应用中非常有用。 其他算法可能在总体计算时间上更有效，但是更难以实现，并且可能不会推广到不同的成本函数。 类似于我们的曾被用于发射断层扫描和天文图像的反卷积算法，其中只有一个因子被改编。[9,10,11,12]</p><p>我们的算法在每次迭代中，通过将当前值乘以取决于等式(1)中的近似程度的某个因子来找到$W$或$H$的新值。 我们证明了近似的程度随着这些乘法更新规则的应用而单调改善。 实际上，这意味着保证更新规则的重复迭代收敛到局部最优矩阵分解。</p><h2><b>3.成本函数</b></h2><p>为了找到近似分解 <img src=\"https://www.zhihu.com/equation?tex=V%5Capprox+WH\" alt=\"V\\approx WH\" eeimg=\"1\"/> ，我们首先需要定义用于量化近似质量的成本函数。这里可以使用两个非负矩阵 <img src=\"https://www.zhihu.com/equation?tex=A\" alt=\"A\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=B\" alt=\"B\" eeimg=\"1\"/> 之间的一些距离度量来构造这样的成本函数。 一个有用的衡量标准就是A和B之间欧氏距离的平方，[13] </p><p><img src=\"https://www.zhihu.com/equation?tex=%7B%5Cleft+%5C%7C+A-B+%5Cright+%5C%7C%7D%5E%7B2%7D+%3D+%5Csum_%7Bij%7D%5E%7B%7D%28A_%7Bij%7D-B_%7Bij%7D%29%5E2+%5Ctag%7B2%7D\" alt=\"{\\left \\| A-B \\right \\|}^{2} = \\sum_{ij}^{}(A_{ij}-B_{ij})^2 \\tag{2}\" eeimg=\"1\"/> </p><p>这等式是由受零限制的，当且仅当$A = B$时明显消失。</p><p>另一个有用的措施是 </p><p><img src=\"https://www.zhihu.com/equation?tex=D%28A+%5Cleft+%7C+%5Cright+%7C+B%29%3D%5Csum%7Bij%7D%28A_%7Bij%7Dlog%5Cfrac%7BA_%7Bij%7D%7D%7BB_%7Bij%7D%7D-A_%7Bij%7D%2BB_%7Bij%7D%29+%5Ctag%7B3%7D+\" alt=\"D(A \\left | \\right | B)=\\sum{ij}(A_{ij}log\\frac{A_{ij}}{B_{ij}}-A_{ij}+B_{ij}) \\tag{3} \" eeimg=\"1\"/> </p><p>与欧几里德距离一样，这也是由零限定的，并且当且仅当$A = B$时消失。 但它不能被称为“距离”，因为它在 <img src=\"https://www.zhihu.com/equation?tex=A\" alt=\"A\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=B\" alt=\"B\" eeimg=\"1\"/> 中不对称，所以我们将它称为 <img src=\"https://www.zhihu.com/equation?tex=A\" alt=\"A\" eeimg=\"1\"/> 中 <img src=\"https://www.zhihu.com/equation?tex=B\" alt=\"B\" eeimg=\"1\"/> 的“发散”。当 <img src=\"https://www.zhihu.com/equation?tex=%5Csum_%7Bij%7D%5E%7B%7DA_%7Bij%7D%3D%5Csum_%7Bij%7D%5E%7B%7DB_%7Bij%7D%3D1\" alt=\"\\sum_{ij}^{}A_{ij}=\\sum_{ij}^{}B_{ij}=1\" eeimg=\"1\"/> 时它减少到 <img src=\"https://www.zhihu.com/equation?tex=Kullback-Leibler\" alt=\"Kullback-Leibler\" eeimg=\"1\"/> 发散，或相对熵，因此 <img src=\"https://www.zhihu.com/equation?tex=A\" alt=\"A\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=B\" alt=\"B\" eeimg=\"1\"/> 可以被视为归一化概率分布。</p><p>我们现在考虑NMF的两种替代公式作为优化问题：</p><p><b>问题1</b>最小化 <img src=\"https://www.zhihu.com/equation?tex=%5Cleft+%5C%7C+V-W+H%5Cright+%5C%7C%5E2\" alt=\"\\left \\| V-W H\\right \\|^2\" eeimg=\"1\"/> ，对于 <img src=\"https://www.zhihu.com/equation?tex=W\" alt=\"W\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=H\" alt=\"H\" eeimg=\"1\"/> ，满足 <img src=\"https://www.zhihu.com/equation?tex=W%2CH+%5Cgeq+0\" alt=\"W,H \\geq 0\" eeimg=\"1\"/> 。 </p><p><b>问题2</b>最小化 <img src=\"https://www.zhihu.com/equation?tex=D%28V%5Cleft+%7C+%5Cright+%7CWH%29\" alt=\"D(V\\left | \\right |WH)\" eeimg=\"1\"/> ，对于 <img src=\"https://www.zhihu.com/equation?tex=W\" alt=\"W\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=H\" alt=\"H\" eeimg=\"1\"/> ，受满足 <img src=\"https://www.zhihu.com/equation?tex=W%2CH+%5Cgeq+0\" alt=\"W,H \\geq 0\" eeimg=\"1\"/> 。 尽管公式 <img src=\"https://www.zhihu.com/equation?tex=%5Cleft+%5C%7C+V-W+H%5Cright+%5C%7C%5E2\" alt=\"\\left \\| V-W H\\right \\|^2\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=D%28V%5Cleft+%7C+%5Cright+%7CWH%29\" alt=\"D(V\\left | \\right |WH)\" eeimg=\"1\"/> 只对于 <img src=\"https://www.zhihu.com/equation?tex=W\" alt=\"W\" eeimg=\"1\"/> 或只对于 <img src=\"https://www.zhihu.com/equation?tex=H\" alt=\"H\" eeimg=\"1\"/> 是凸的，但对于两个变量并不凸起。因此，期望算法在找到全局最小值的意义上解决问题1和2是不现实的。然而，有许多数值优化技术可用于寻找局部最小值。 梯度下降可能是最简单的实现方法，但收敛速度可能很慢。其他方法如共轭梯度具有更快的收敛性，至少在局部最小值附近，但实现比梯度下降更复杂[8]。基于梯度的方法的收敛还具有对步长的选择非常敏感的缺点，这对于大型应用来说非常不方便。</p><h2><b>4.乘法更新规则</b></h2><p>我们发现以下“乘法更新规则”是解决问题1和2的速度和易于实现之间的良好折衷。</p><p><b>定理1：</b>欧氏距离 <img src=\"https://www.zhihu.com/equation?tex=%5Cleft+%5C%7C+V-WH+%5Cright+%5C%7C\" alt=\"\\left \\| V-WH \\right \\|\" eeimg=\"1\"/> 根据更新规则是非增的。</p><p><img src=\"https://www.zhihu.com/equation?tex=+H_%7Ba%CE%BC%7D%E2%86%90+H_%7Ba%CE%BC%7D%5Cfrac%7B%28W%5ETV%29%7Ba%CE%BC%7D%7D%7B%28W%5ETWH%29%7Ba%CE%BC%7D%7D+%5Cqquad+W_%7Bia%7D%E2%86%90+W_%7Bia%7D%5Cfrac%7B%28VH%5ET%29%7Bia%7D%7D%7B%28WHH%5ET%29%7Bia%7D%7D+%5Ctag%7B4%7D+\" alt=\" H_{aμ}← H_{aμ}\\frac{(W^TV){aμ}}{(W^TWH){aμ}} \\qquad W_{ia}← W_{ia}\\frac{(VH^T){ia}}{(WHH^T){ia}} \\tag{4} \" eeimg=\"1\"/> </p><p>当且仅当 <img src=\"https://www.zhihu.com/equation?tex=W\" alt=\"W\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=H\" alt=\"H\" eeimg=\"1\"/> 处于距离的静止点时，欧几里德距离在这些更新下是不变的。</p><p><b>定理2：</b>散度 <img src=\"https://www.zhihu.com/equation?tex=D%28V%5Cleft+%7C+%5Cright+%7C+W+H%29\" alt=\"D(V\\left | \\right | W H)\" eeimg=\"1\"/> 根据更新规则是非增的。</p><p><img src=\"https://www.zhihu.com/equation?tex=H_%7Ba%CE%BC%7D%E2%86%90H_%7Ba%CE%BC%7D%5Cfrac+%7B%5Csum_%7Bi%7DW_%7Bia%7DV_%7Bia%7D%2F%28WH%29%7Bi%CE%BC%7D%7D%7B%5Csum+%7Bk%7D%7BW%7D%7Bka%7D%7D+%5Cqquad+W%7Bia%7D%E2%86%90W_%7Bia%7D%5Cfrac+%7B%5Csum_%7B%CE%BC%7DH_%7Ba%CE%BC%7DV_%7Bi%CE%BC%7D%2F%28WH%29%7Bi%CE%BC%7D%7D%7B%5Csum+%7Bv%7D%7BH%7D_%7Bav%7D%7D+%5Ctag%7B5%7D+\" alt=\"H_{aμ}←H_{aμ}\\frac {\\sum_{i}W_{ia}V_{ia}/(WH){iμ}}{\\sum {k}{W}{ka}} \\qquad W{ia}←W_{ia}\\frac {\\sum_{μ}H_{aμ}V_{iμ}/(WH){iμ}}{\\sum {v}{H}_{av}} \\tag{5} \" eeimg=\"1\"/></p><p>在这个散度中，如果 <img src=\"https://www.zhihu.com/equation?tex=W\" alt=\"W\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=H\" alt=\"H\" eeimg=\"1\"/> 处于静止点，则在这些更新下散度是不变的。 这些定理的证明在后面的部分中给出。 目前，我们注意到每次更新都包含一个乘法因子。 特别是，当 <img src=\"https://www.zhihu.com/equation?tex=V+%3D+WH\" alt=\"V = WH\" eeimg=\"1\"/> 时，可以直接看到该乘法因子是一个单位，因此完美重建必然是更新规则的固定点。</p><h2><b>5.乘法与加法更新规则</b></h2><p>将这些乘法更新与梯度下降产生的更新进行对比是有用的[14]。 特别是，可以将用于减小平方距离的$H$的简单加法更新写为 </p><p><img src=\"https://www.zhihu.com/equation?tex=H_%7Ba%CE%BC%7D%E2%86%90H_%7Ba%CE%BC%7D%2B%CE%B7_%7Ba%CE%BC%7D%5B%28W%5ETV%29%7Ba%CE%BC%7D-%28W%5ETWH%29%7Ba%CE%BC%7D%5D+%5Ctag%7B6%7D+\" alt=\"H_{aμ}←H_{aμ}+η_{aμ}[(W^TV){aμ}-(W^TWH){aμ}] \\tag{6} \" eeimg=\"1\"/> </p><p>如果把 <img src=\"https://www.zhihu.com/equation?tex=%CE%B7_%7Ba%CE%BC%7D\" alt=\"η_{aμ}\" eeimg=\"1\"/> 设置成一些较小的正数，就相当于实现了传统的梯度下降。这些数足够小，更新规则就能减少 <img src=\"https://www.zhihu.com/equation?tex=%5Cleft+%5C%7C+V-W+H%5Cright+%5C%7C\" alt=\"\\left \\| V-W H\\right \\|\" eeimg=\"1\"/> 。</p><p>现在，如果我们讲对角重新缩放变量并设置</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cqquad+%CE%B7_%7Ba%CE%BC%7D%3D%5Cfrac+%7BH_%7Ba%CE%BC%7D%7D%7B%28W%5ETWH%29_%7Ba%CE%BC%7D%7D+%5Ctag%7B7%7D+\" alt=\"\\qquad η_{aμ}=\\frac {H_{aμ}}{(W^TWH)_{aμ}} \\tag{7} \" eeimg=\"1\"/> </p><p>然后我们得到定理1中给出的 <img src=\"https://www.zhihu.com/equation?tex=H\" alt=\"H\" eeimg=\"1\"/> 的更新规则。注意，重新缩放产生的这个乘法因子的分母是梯度的正分量和分子是负分量的绝对值。</p><p>对于散度，对角重新缩放的梯度下降采用这种形式 </p><p><img src=\"https://www.zhihu.com/equation?tex=H_%7Ba%CE%BC%7D%E2%86%90H_%7Ba%CE%BC%7D%2B%CE%B7_%7Ba%CE%BC%7D%5B%5Csum_%7Bi%7DW_%7Bia%7D%5Cfrac+%7BV_%7Bi%CE%BC%7D%7D%7B%7BWH%7D%7Bi%CE%BC%7D%7D-%5Csum%7Bi%7DW_%7Bia%7D%5D+%5Ctag%7B8%7D\" alt=\"H_{aμ}←H_{aμ}+η_{aμ}[\\sum_{i}W_{ia}\\frac {V_{iμ}}{{WH}{iμ}}-\\sum{i}W_{ia}] \\tag{8}\" eeimg=\"1\"/> </p><p> 同样的，如果把$η_{aμ}$设置成正数且足够小，更新规则就能减少 <img src=\"https://www.zhihu.com/equation?tex=D%28V%5Cleft+%7C+%5Cright+%7CWH%29\" alt=\"D(V\\left | \\right |WH)\" eeimg=\"1\"/> 。</p><p>我们现在设置 </p><p><img src=\"https://www.zhihu.com/equation?tex=%CE%B7_%7Ba%CE%BC%7D%3D%5Cfrac+%7BH_%7Ba%CE%BC%7D%7D%7B%5Csum_%7Bi%7DW_%7Bia%7D%7D+%5Ctag%7B9%7D\" alt=\"η_{aμ}=\\frac {H_{aμ}}{\\sum_{i}W_{ia}} \\tag{9}\" eeimg=\"1\"/> </p><p> 然后我们得到定理2中给出的 <img src=\"https://www.zhihu.com/equation?tex=H\" alt=\"H\" eeimg=\"1\"/> 的更新规则。这种重新缩放也可以解释为乘法规则，其中乘法因子分母中是梯度的正分量和负分量作为的分子。 由于我们对 <img src=\"https://www.zhihu.com/equation?tex=%CE%B7_%7Ba%CE%BC%7D\" alt=\"η_{aμ}\" eeimg=\"1\"/> 的选择不小，似乎无法保证这种重新调整的梯度下降会导致成本函数降低。令人惊讶的是，确实如下节所示。</p><h2><b>6.收敛性证明</b></h2><p>为了证明定理1和2，我们将使用类似于期望最大化算法[15,16]中使用的辅助函数。</p><p><b>定义1</b> <img src=\"https://www.zhihu.com/equation?tex=G%28h%2Ch%27%29\" alt=\"G(h,h&#39;)\" eeimg=\"1\"/> 是条件 <img src=\"https://www.zhihu.com/equation?tex=F%28h%29\" alt=\"F(h)\" eeimg=\"1\"/> 的辅助函数当</p><p><img src=\"https://www.zhihu.com/equation?tex=+G%28h%2Ch%27%29+%5Cgeq+F%28H%29+%5Cquad+G%28h%2Ch%29%3DF%28h%29+%5Ctag%7B10%7D+\" alt=\" G(h,h&#39;) \\geq F(H) \\quad G(h,h)=F(h) \\tag{10} \" eeimg=\"1\"/> </p><p> 辅助函数是一个有用的概念，因为下面的引理，也在图1中以图形方式说明。</p><p><b>引理1</b>如果 <img src=\"https://www.zhihu.com/equation?tex=G+\" alt=\"G \" eeimg=\"1\"/> 是辅助函数，则$F$在更新规则下是非增的 </p><p><img src=\"https://www.zhihu.com/equation?tex=+h%5E%7Bt%2B1%7D+%3D+%5Carg+%5Cmin_%7Bh%7D+G+%28h%2Ch%5Et%29+%5Ctag%7B11%7D+\" alt=\" h^{t+1} = \\arg \\min_{h} G (h,h^t) \\tag{11} \" eeimg=\"1\"/> </p><p><b>证明</b>： <img src=\"https://www.zhihu.com/equation?tex=F%28h%5E%7Bt%2B1%7D%29+%5Cleq+G%28h%5E%7Bt%2B1%7D%2Ch%5Et%29+%5Cleq+G%28h%5Et%2Ch%5E%7Bt%2B1%7D%29%3DF%28h%5Et%29\" alt=\"F(h^{t+1}) \\leq G(h^{t+1},h^t) \\leq G(h^t,h^{t+1})=F(h^t)\" eeimg=\"1\"/> </p><p>注意，只有当 <img src=\"https://www.zhihu.com/equation?tex=h%5Et\" alt=\"h^t\" eeimg=\"1\"/> 是 <img src=\"https://www.zhihu.com/equation?tex=G%28h%2Ch%5Et%29\" alt=\"G(h,h^t)\" eeimg=\"1\"/> 的局部最小值时， <img src=\"https://www.zhihu.com/equation?tex=F%28h%5E%7Bt+%2B+1%7D%29%3DF%28h%5Et%29\" alt=\"F(h^{t + 1})=F(h^t)\" eeimg=\"1\"/> 。 如果在 <img src=\"https://www.zhihu.com/equation?tex=h%5Et\" alt=\"h^t\" eeimg=\"1\"/> 的小邻域内 <img src=\"https://www.zhihu.com/equation?tex=F\" alt=\"F\" eeimg=\"1\"/> 连续可导，这也意味着导数 <img src=\"https://www.zhihu.com/equation?tex=%5Cbigtriangledown+F%28h%27%29+%3D0\" alt=\"\\bigtriangledown F(h&#39;) =0\" eeimg=\"1\"/> .因此，通过迭代等式(11)中的更新规则，我们得到一系列收敛到目标函数的局部最小值 <img src=\"https://www.zhihu.com/equation?tex=h_%7Bmin%7D+%3D+%5Carg+%5Cmin_%7Bh%7D+F+%28h%29\" alt=\"h_{min} = \\arg \\min_{h} F (h)\" eeimg=\"1\"/> 的估计,目标函数如下：</p><p><img src=\"https://www.zhihu.com/equation?tex=F%28h_%7Bmin%7D%29+%5Cleq+%5Ccdots+%5Cleq+F%28h%5E%7Bt%2B1%7D%29+%5Cleq+F%28h%5Et%29%5Cleq+%5Ccdots+%5Cleq+F%28h_%7B2%7D%29+%5Cleq+F%28h_%7B1%7D%29+%5Cleq+F%28h_%7B0%7D%29+%5Ctag%7B12%7D\" alt=\"F(h_{min}) \\leq \\cdots \\leq F(h^{t+1}) \\leq F(h^t)\\leq \\cdots \\leq F(h_{2}) \\leq F(h_{1}) \\leq F(h_{0}) \\tag{12}\" eeimg=\"1\"/> </p><p> 我们将通过为 <img src=\"https://www.zhihu.com/equation?tex=%5Cleft+%5C%7C+V-WH+%5Cright+%5C%7C\" alt=\"\\left \\| V-WH \\right \\|\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=D%28V%5Cleft+%7C+%5Cright+%7C+W+H%29\" alt=\"D(V\\left | \\right | W H)\" eeimg=\"1\"/> 定义适当的辅助函数 <img src=\"https://www.zhihu.com/equation?tex=G%28h%2Ch%5Et%29\" alt=\"G(h,h^t)\" eeimg=\"1\"/> 来证明，定理1和2中的更新规则很容易遵循等式(11)。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d2471d6936e341974b91d45dd2deb6bc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"968\" data-rawheight=\"438\" class=\"origin_image zh-lightbox-thumb\" width=\"968\" data-original=\"https://pic1.zhimg.com/v2-d2471d6936e341974b91d45dd2deb6bc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;968&#39; height=&#39;438&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"968\" data-rawheight=\"438\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"968\" data-original=\"https://pic1.zhimg.com/v2-d2471d6936e341974b91d45dd2deb6bc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d2471d6936e341974b91d45dd2deb6bc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>引理2</b> 如果 <img src=\"https://www.zhihu.com/equation?tex=K%28h%5Et%29\" alt=\"K(h^t)\" eeimg=\"1\"/> 是对角矩阵 </p><p><img src=\"https://www.zhihu.com/equation?tex=K_%7Bab%7D%28h%5Et%29%3D%5Cdelta_%7Bab%7D%28W%5ETWh%5Et%29%7Ba%7D%2Fh%7Ba%7D%5E%7Bt%7D+%5Ctag%7B13%7D\" alt=\"K_{ab}(h^t)=\\delta_{ab}(W^TWh^t){a}/h{a}^{t} \\tag{13}\" eeimg=\"1\"/> </p><p>然后</p><p><img src=\"https://www.zhihu.com/equation?tex=G%28h%2Ch%5Et%29%3DF%28h%5Et%29%2B%28h-h%5Et%29++%5Cbigtriangledown+F%28h%5Et%29%2B%5Cfrac%7B1%7D%7B2%7D%28h-h%5Et%29%5ETK%28h%5Et%29%28h-h%5Et%29+%5Ctag%7B14%7D+\" alt=\"G(h,h^t)=F(h^t)+(h-h^t)  \\bigtriangledown F(h^t)+\\frac{1}{2}(h-h^t)^TK(h^t)(h-h^t) \\tag{14} \" eeimg=\"1\"/> </p><p> 是 </p><p><img src=\"https://www.zhihu.com/equation?tex=F%28h%29%3D+%5Cfrac+%7B1%7D%7B2%7D%5Csum_%7Bi%7D%28v_i-%5Csum_aW_%7Bia%7Dh_%7Ba%7D%29%5E2+%5Ctag%7B15%7D+\" alt=\"F(h)= \\frac {1}{2}\\sum_{i}(v_i-\\sum_aW_{ia}h_{a})^2 \\tag{15} \" eeimg=\"1\"/> </p><p> 的一个辅助函数。</p><p><b>证明</b>：由于 <img src=\"https://www.zhihu.com/equation?tex=G%28h%2Ch%29%3D+F%28h%29\" alt=\"G(h,h)= F(h)\" eeimg=\"1\"/> 是显而易见的，我们只需要证明 <img src=\"https://www.zhihu.com/equation?tex=G%28h%2Ch%5Et%29%5Cleq+F%28h%29\" alt=\"G(h,h^t)\\leq F(h)\" eeimg=\"1\"/> 。 为此，我们进行比较 </p><p><img src=\"https://www.zhihu.com/equation?tex=F%28h%29%3DF%28h%5Et%29%2B%28h-h%5Et%29++%5Cbigtriangledown+F%28h%5Et%29%2B%5Cfrac%7B1%7D%7B2%7D%28h-h%5Et%29%5ET%28W%5ETW%29%28h-h%5Et%29+%5Ctag%7B16%7D+\" alt=\"F(h)=F(h^t)+(h-h^t)  \\bigtriangledown F(h^t)+\\frac{1}{2}(h-h^t)^T(W^TW)(h-h^t) \\tag{16} \" eeimg=\"1\"/> </p><p> 通过等式（14）我们发现$ <img src=\"https://www.zhihu.com/equation?tex=G%28h%2Ch%5Et%29+%5Cgeq+F%28h%29\" alt=\"G(h,h^t) \\geq F(h)\" eeimg=\"1\"/> 等价于 </p><p><img src=\"https://www.zhihu.com/equation?tex=+0+%5Cleq+%28h-h%5Et%29%5ETK%28h%5Et%29-W%5ETW+%5Ctag%7B17%7D+\" alt=\" 0 \\leq (h-h^t)^TK(h^t)-W^TW \\tag{17} \" eeimg=\"1\"/> </p><p> 为了证明正半定矩阵，（通过考虑矩阵 <img src=\"https://www.zhihu.com/equation?tex=K%5E%7B%5Cfrac%7B1%7D%7B2%7D%7D%28I-K%5E%7B-%5Cfrac%7B1%7D%7B2%7D%7DW%5ETWk%5E%7B-%5Cfrac%7B1%7D%7B2%7D%7D%29\" alt=\"K^{\\frac{1}{2}}(I-K^{-\\frac{1}{2}}W^TWk^{-\\frac{1}{2}})\" eeimg=\"1\"/> 可以证明 <img src=\"https://www.zhihu.com/equation?tex=K-W%5ETW\" alt=\"K-W^TW\" eeimg=\"1\"/> 是正半定的。然后 <img src=\"https://www.zhihu.com/equation?tex=%5Csqrt%7Bh_%7Ba%7D%5E%7Bt%7D%28W%5ETWh%5Et%29%7D\" alt=\"\\sqrt{h_{a}^{t}(W^TWh^t)}\" eeimg=\"1\"/> 是具有单位特征值的 <img src=\"https://www.zhihu.com/equation?tex=K%5E%7B-+%5Cfrac+%7B1%7D%7B2%7D%7D+W%5ET+WK%5E%7B-%5Cfrac%7B1%7D%7B2%7D%7D\" alt=\"K^{- \\frac {1}{2}} W^T WK^{-\\frac{1}{2}}\" eeimg=\"1\"/> 的一个正的特征向量并且应用 <img src=\"https://www.zhihu.com/equation?tex=Frobenius-Perron\" alt=\"Frobenius-Perron\" eeimg=\"1\"/> 定理能够证明等式(17)），考虑这个矩阵 </p><p><img src=\"https://www.zhihu.com/equation?tex=M_%7Bab%7D%28h%5Et%29%3Dh_%7Ba%7D%5E%7Bt%7D%28K%28h%5Et%29-W%5ETW%29%7Bab%7Dh%5Et%7Bb%7D+%5Ctag%7B18%7D+\" alt=\"M_{ab}(h^t)=h_{a}^{t}(K(h^t)-W^TW){ab}h^t{b} \\tag{18} \" eeimg=\"1\"/> </p><p> 这只是 <img src=\"https://www.zhihu.com/equation?tex=K+-+W%5ET+W\" alt=\"K - W^T W\" eeimg=\"1\"/> 的组成部分重新缩放。 然后 <img src=\"https://www.zhihu.com/equation?tex=K+-+W%5ET+W\" alt=\"K - W^T W\" eeimg=\"1\"/> 是正定矩阵，当且仅当 <img src=\"https://www.zhihu.com/equation?tex=M\" alt=\"M\" eeimg=\"1\"/> 满足</p><p><img src=\"https://www.zhihu.com/equation?tex=v%5ETMv+%3D+%5Csum_%7Bab%7Dv_aM_%7Bab%7Dv_b+%5Ctag%7B19%7D+\" alt=\"v^TMv = \\sum_{ab}v_aM_{ab}v_b \\tag{19} \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cqquad+%3D%5Csum_%7Bab%7Dh_a%5Et%28W%5ETW%29%7Bab%7DH%5Et_av%5E2_a+-+v_ah_a%5Et%28W%5ETW%29%7Bab%7Dh%5Et_bv_b+%5Ctag%7B20%7D+\" alt=\"\\qquad =\\sum_{ab}h_a^t(W^TW){ab}H^t_av^2_a - v_ah_a^t(W^TW){ab}h^t_bv_b \\tag{20} \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=+%5Cqquad+%5Cqquad+%5Cqquad+%3D%5Csum_%7Bab%7D%28W%5ETW%29_%7Bab%7Dh%5Et_ah%5Et_b%5B%5Cfrac%7B1%7D%7B2%7Dv_a%5E2%2B%5Cfrac%7B1%7D%7B2%7Dv_b%5E2-v_av_b%29%5D+%5Ctag%7B21%7D\" alt=\" \\qquad \\qquad \\qquad =\\sum_{ab}(W^TW)_{ab}h^t_ah^t_b[\\frac{1}{2}v_a^2+\\frac{1}{2}v_b^2-v_av_b)] \\tag{21}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cqquad+%3D+%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bab%7D%28W%5ETW%29_%7Bab%7Dh%5Et_ah_b%5Et%28v_a-v_b%29%5E2+%5Ctag%7B22%7D+\" alt=\"\\qquad = \\frac{1}{2}\\sum_{ab}(W^TW)_{ab}h^t_ah_b^t(v_a-v_b)^2 \\tag{22} \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=+%5Cqquad+%5Cgeq0+%5Ctag%7B23%7D\" alt=\" \\qquad \\geq0 \\tag{23}\" eeimg=\"1\"/> </p><p>我们现在可以证明定理1的收敛性：</p><p><b>定理1证明 </b>替换方程式(11)中的 <img src=\"https://www.zhihu.com/equation?tex=G%28h%2Ch%5Et+%29\" alt=\"G(h,h^t )\" eeimg=\"1\"/> 生成更新规则： </p><p><img src=\"https://www.zhihu.com/equation?tex=h%5E%7Bt%2B1%7D%3Dh%5E%7Bt%7D-K%28h%5Et%29%5E%7B-1%7D%5Cbigtriangledown+F%28h%5Et%29+%5Ctag%7B24%7D++\" alt=\"h^{t+1}=h^{t}-K(h^t)^{-1}\\bigtriangledown F(h^t) \\tag{24}  \" eeimg=\"1\"/> </p><p> 因为等式(14)是一个辅助函数，根据引理1， <img src=\"https://www.zhihu.com/equation?tex=F\" alt=\"F\" eeimg=\"1\"/> 在更新规则下是非增的。明确地写出这个等式的组成部分，我们能得到</p><p><img src=\"https://www.zhihu.com/equation?tex=h_a%5E%7Bt%2B1%7D+%3D+h_a%5E%7Bt%7D%5Cfrac+%7B%28W%5ETv%29_a%7D%7BW%5ETWh%5Et%7D_a+%5Ctag%7B25%7D\" alt=\"h_a^{t+1} = h_a^{t}\\frac {(W^Tv)_a}{W^TWh^t}_a \\tag{25}\" eeimg=\"1\"/> </p><p>通过在引理1和2中反转 <img src=\"https://www.zhihu.com/equation?tex=W\" alt=\"W\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=T\" alt=\"T\" eeimg=\"1\"/> ，可以类似地证明$F$在对$W$更新规则时是非增的。</p><p>我们现在考虑下面用于散度成本函数的辅助函数：</p><p><b>引理3</b> 令 </p><p><img src=\"https://www.zhihu.com/equation?tex=G%28h%2Ch%5Et%29+%3D+%5Csum_%7Bi%7D%28v_i%5Clog+v_i-v_i%29%2B%5Csum_%7Bia%7DW_%7Bia%7Dh_a+%5Ctag%7B26%7D\" alt=\"G(h,h^t) = \\sum_{i}(v_i\\log v_i-v_i)+\\sum_{ia}W_{ia}h_a \\tag{26}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=-+%5Csum_%7Bia%7Dv_i%5Cfrac%7BW_%7Bia%7Dh_a%5Et%7D%7B%5Csum_b+W_%7Bib%7Dh_b%5Et%7D%28%5Clog+W_%7Bia%7Dh_a-%5Clog%5Cfrac%7BW_%7Bia%7Dh_a%5Et%7D%7B%5Csum_bW_%7Bib%7Dh_b%5Et%7D%29+%5Ctag%7B27%7D\" alt=\"- \\sum_{ia}v_i\\frac{W_{ia}h_a^t}{\\sum_b W_{ib}h_b^t}(\\log W_{ia}h_a-\\log\\frac{W_{ia}h_a^t}{\\sum_bW_{ib}h_b^t}) \\tag{27}\" eeimg=\"1\"/> </p><p>它是等式(28)的辅助函数 </p><p><img src=\"https://www.zhihu.com/equation?tex=+F%28h%29+%3D+%5Csum_iv_i+%5Clog%28%5Cfrac%7Bv_i%7D%7B%5Csum_aW_%7Bia%7Dh_%7Ba%7D%7D-v_i%2B%5Csum_aW_%7Bia%7Dh_a%29+%5Ctag%7B28%7D+\" alt=\" F(h) = \\sum_iv_i \\log(\\frac{v_i}{\\sum_aW_{ia}h_{a}}-v_i+\\sum_aW_{ia}h_a) \\tag{28} \" eeimg=\"1\"/> </p><p><b>证明：</b>很容易验证 <img src=\"https://www.zhihu.com/equation?tex=G%28h%2Ch%29%3D+F%28h%29\" alt=\"G(h,h)= F(h)\" eeimg=\"1\"/> 。 为了证明 <img src=\"https://www.zhihu.com/equation?tex=G%28h%2Ch%5Et%29%5Cgeq+F%28h%29\" alt=\"G(h,h^t)\\geq F(h)\" eeimg=\"1\"/> ，我们使用对数函数的凸性来推导不等式 </p><p><img src=\"https://www.zhihu.com/equation?tex=-%5Clog+%5Csum_aW_%7Bia%7Dh_a%5Cleq+%5Calpha_a%5Clog+%5Cfrac%7BW_%7Bia%7Dh_a%7D%7B%5Calpha_a%7D+%5Ctag%7B29%7D+\" alt=\"-\\log \\sum_aW_{ia}h_a\\leq \\alpha_a\\log \\frac{W_{ia}h_a}{\\alpha_a} \\tag{29} \" eeimg=\"1\"/> </p><p>这适用于所有总和为 <img src=\"https://www.zhihu.com/equation?tex=1\" alt=\"1\" eeimg=\"1\"/> 非负 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha_a\" alt=\"\\alpha_a\" eeimg=\"1\"/> 。 设置 </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Calpha+a+%3D+%5Cfrac%7BW%7Bia%7Dh_a%5Et%7D%7B%5Csum_bW_%7Bib%7Dh_b%5Et%7D+%5Ctag%7B30%7D+\" alt=\"\\alpha a = \\frac{W{ia}h_a^t}{\\sum_bW_{ib}h_b^t} \\tag{30} \" eeimg=\"1\"/> </p><p> 我们可以得到 </p><p><img src=\"https://www.zhihu.com/equation?tex=-%5Clog+%5Csum_aW_%7Bia%7Dh_a+%5Cleq-+%5Csum_a+%5Cfrac%7BW_%7Bia%7Dh_a%5Et%7D%7B%5Csum_bW_%7Bib%7Dh_b%5Et%7D%28%5Clog+W_%7Bia%7Dh_a-%5Clog+%5Cfrac%7BW_%7Bia%7Dh_a%5Et%7D%7B%5Csum_b+W_%7Bib%7Dh_b%5Et%7D%29+%5Ctag%7B31%7D+\" alt=\"-\\log \\sum_aW_{ia}h_a \\leq- \\sum_a \\frac{W_{ia}h_a^t}{\\sum_bW_{ib}h_b^t}(\\log W_{ia}h_a-\\log \\frac{W_{ia}h_a^t}{\\sum_b W_{ib}h_b^t}) \\tag{31} \" eeimg=\"1\"/> </p><p>由此可得 <img src=\"https://www.zhihu.com/equation?tex=F%28h%29%5Cleq+G%28h%2Ch%5Et%29\" alt=\"F(h)\\leq G(h,h^t)\" eeimg=\"1\"/> </p><p>有引理1的应用可以证明定理2</p><p><b>定理2证明：</b>通过将梯度设置为零来确定关于 <img src=\"https://www.zhihu.com/equation?tex=h\" alt=\"h\" eeimg=\"1\"/> 的 <img src=\"https://www.zhihu.com/equation?tex=G%28h%2Ch%5Et%29\" alt=\"G(h,h^t)\" eeimg=\"1\"/> 的最小值： </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7BdG%28h%2Ch%5Et%29%7D%7Bdh_a%7D%3D-%5Csum_iv_i%5Cfrac%7BW_%7Bia%7Dh_a%5Et%7D%7BW_%7Bib%7Dh_b%5Et%7D%5Cfrac%7B1%7D%7Bh_b%5Et%7D%2B%5Csum_iW_%7Bia%7D%3D0+%5Ctag%7B32%7D+\" alt=\"\\frac{dG(h,h^t)}{dh_a}=-\\sum_iv_i\\frac{W_{ia}h_a^t}{W_{ib}h_b^t}\\frac{1}{h_b^t}+\\sum_iW_{ia}=0 \\tag{32} \" eeimg=\"1\"/> </p><p> 因此等式(11)可以使用： </p><p><img src=\"https://www.zhihu.com/equation?tex=h_a%5E%7Bt%2B1%7D%3D%5Cfrac%7Bh_a%5Et%7D%7B%5Csum_bW_%7Bkb%7D%7D%5Csum_i%5Cfrac%7Bv_i%7D%7B%5Csum_bW_%7Bib%7Dh_b%5Et%7DW_%7Bia%7D+%5Ctag%7B33%7D\" alt=\"h_a^{t+1}=\\frac{h_a^t}{\\sum_bW_{kb}}\\sum_i\\frac{v_i}{\\sum_bW_{ib}h_b^t}W_{ia} \\tag{33}\" eeimg=\"1\"/> </p><p> 由于 <img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> 是一个辅助函数，因此在方程式(28)中在此更新非增。 以矩阵形式重写，等价于方程式(5)中的更新规则。 通过反转 <img src=\"https://www.zhihu.com/equation?tex=H\" alt=\"H\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=W\" alt=\"W\" eeimg=\"1\"/> ，对于 <img src=\"https://www.zhihu.com/equation?tex=W\" alt=\"W\" eeimg=\"1\"/> 的更新规则可以类似地证明其为非增的。</p><h2><b>7.讨论</b></h2><p>我们已经在等式（4）和（5）中展示了更新规则的应用， 能够保证分别找到问题1和2的局部最优解。 收敛证明依赖于定义适当的辅助函数。 我们目前正在努力将这些定理推广到更复杂的约束。 更新规则本身在计算上非常容易实现，并且有望被其他人用于各种各样的应用程序。 我们感谢贝尔实验室的支持。 我们还要感谢Carlos Brody，Ken Clarkson，Corinna Cortes，Roland Freund，Linda Kaufman，Yann Le Cun，Sam Roweis，Larry Saul和Margaret Wright的有益讨论。</p><h2><b>参考</b></h2><p>略（见原文）</p>", 
            "topic": [
                {
                    "tag": "矩阵", 
                    "tagLink": "https://api.zhihu.com/topics/19650614"
                }, 
                {
                    "tag": "算法", 
                    "tagLink": "https://api.zhihu.com/topics/19553510"
                }, 
                {
                    "tag": "数据挖掘", 
                    "tagLink": "https://api.zhihu.com/topics/19553534"
                }
            ], 
            "comments": [
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>收敛性证明部分，辅助函数G(h,h')中的h'有什么特别的含义吗，为什么用这种形式，我能暂时理解的是h'是现估计值，也就是本次迭代值</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "知乎用户", 
                            "userLink": "https://www.zhihu.com/people/0", 
                            "content": "抱歉，之前翻译的，有些忘了，我重新看后，再予你回答。", 
                            "likes": 0, 
                            "replyToAuthor": "知乎用户"
                        }, 
                        {
                            "userName": "知乎用户", 
                            "userLink": "https://www.zhihu.com/people/0", 
                            "content": "<p>好的，先谢谢作者了</p>", 
                            "likes": 0, 
                            "replyToAuthor": "知乎用户"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44421512", 
            "userName": "idejie", 
            "userLink": "https://www.zhihu.com/people/67d6bc7c8363bef16771d582f32ffc25", 
            "upvote": 0, 
            "title": "数据挖掘算法（一）：Aproiri", 
            "content": "<p>   Aproiri算法是数据关联分析最著名的算法之一。所谓的关联分析就是在大数据集中找到频繁出现的时间也叫频繁项集和事件与事件之间的条件概率关系也叫关联规则。（比如，你经常买可乐，那么如果把你的购物清单当做数据集，那么买可乐这个事件就是一个频繁项集；如果再仔细观察，在你买可乐的同时也会买薯片，那么既买可乐也买薯片对于在买了可乐这个事件发生的概率就较大，这就是一个关联规则）。 </p><p>   非常经典的一个案例就是沃尔玛超市的“啤酒尿布”故事，故事是这样的：在一家超市中，人们发现了一个特别有趣的现象，尿布与啤酒这两种风马牛不相及的商品居然摆在一起。但这一奇怪的举措居然使尿布和啤酒的销量大幅增加了。这可不是一个笑话，而是一直被商家所津津乐道的发生在美国沃尔玛连锁超市的真实案例。原来，美国的妇女通常在家照顾孩子，所以她们经常会嘱咐丈夫在下班回家的路上为孩子买尿布，而丈夫在买尿布的同时又会顺手购买自己爱喝的啤酒。这个发现为商家带来了大量的利润</p><h2>1.条件概率</h2><p>条件概率是指事件 <img src=\"https://www.zhihu.com/equation?tex=B\" alt=\"B\" eeimg=\"1\"/> 在另外一个事件  <img src=\"https://www.zhihu.com/equation?tex=A\" alt=\"A\" eeimg=\"1\"/>  已经发生条件下的发生概率。条件概率表示为： <img src=\"https://www.zhihu.com/equation?tex=P+%28+B%7C+A%29\" alt=\"P ( B| A)\" eeimg=\"1\"/> ，读作“在 <img src=\"https://www.zhihu.com/equation?tex=A\" alt=\"A\" eeimg=\"1\"/> 的条件下 <img src=\"https://www.zhihu.com/equation?tex=B\" alt=\"B\" eeimg=\"1\"/> 的概率”。</p><ul><li>从项（iterm）的角度：</li></ul><p><img src=\"https://www.zhihu.com/equation?tex=P%28A%29%3D%E2%80%9C%E5%8F%AF%E4%B9%90%E2%80%9D+%5C%5C+P%28B%29%3D%E2%80%9C%E8%96%AF%E7%89%87%E2%80%9D+%5C%5C+P%28A%5Ccup+B%29+%3D+%E2%80%9C%E5%8F%AF%E4%B9%90%5C%26%E8%96%AF%E7%89%87%E2%80%9D+%5C%5C+P%28B%7CA%29%3D%5Cfrac%7BP%28A%5Ccup+B%29%7D%7BP%28A%29%7D\" alt=\"P(A)=“可乐” \\\\ P(B)=“薯片” \\\\ P(A\\cup B) = “可乐\\&amp;薯片” \\\\ P(B|A)=\\frac{P(A\\cup B)}{P(A)}\" eeimg=\"1\"/> </p><ul><li>从事件的角度：</li></ul><p><img src=\"https://www.zhihu.com/equation?tex=A+%3D+%E2%80%9C%E4%B9%B0%E5%8F%AF%E4%B9%90%E2%80%9D+%5C%5C+B%3D%E2%80%9C%E4%B9%B0%E8%96%AF%E7%89%87%E2%80%9D+%5C%5C+A%5Ccap+B%3D+%E2%80%9C%E6%97%A2%E4%B9%B0%E8%96%AF%E7%89%87%E5%8F%88%E4%B9%B0%E5%8F%AF%E4%B9%90%E2%80%9D+%5C%5C+P%28B%7CA%29+%3D+%5Cfrac%7BP%28A%5Ccap+B%29%7D%7BP%28A%29%7D+\" alt=\"A = “买可乐” \\\\ B=“买薯片” \\\\ A\\cap B= “既买薯片又买可乐” \\\\ P(B|A) = \\frac{P(A\\cap B)}{P(A)} \" eeimg=\"1\"/> </p><h2>2.支持度和置信度</h2><p>支持度就是一个事件发生的可能大小的衡量。一般用这个事件发生概率表示。 <img src=\"https://www.zhihu.com/equation?tex=sup%28A%29%3DP%28A%29\" alt=\"sup(A)=P(A)\" eeimg=\"1\"/> </p><p>置信度就是事件 <img src=\"https://www.zhihu.com/equation?tex=B\" alt=\"B\" eeimg=\"1\"/> 在 <img src=\"https://www.zhihu.com/equation?tex=A\" alt=\"A\" eeimg=\"1\"/> 已经发生的情况下发生的概率大小。 <img src=\"https://www.zhihu.com/equation?tex=P%28B%7CA%29%3D%5Cfrac%7BP%28A%5Ccup+B%29%7D%7BP%28A%29%7D\" alt=\"P(B|A)=\\frac{P(A\\cup B)}{P(A)}\" eeimg=\"1\"/> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6e4c2bd3cb1f53f8a600aaa038495eee_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"749\" data-rawheight=\"270\" class=\"origin_image zh-lightbox-thumb\" width=\"749\" data-original=\"https://pic3.zhimg.com/v2-6e4c2bd3cb1f53f8a600aaa038495eee_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;749&#39; height=&#39;270&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"749\" data-rawheight=\"270\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"749\" data-original=\"https://pic3.zhimg.com/v2-6e4c2bd3cb1f53f8a600aaa038495eee_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6e4c2bd3cb1f53f8a600aaa038495eee_b.jpg\"/></figure><p><img src=\"https://www.zhihu.com/equation?tex=X+%3D+%E2%80%9C%E7%BD%91%E7%90%83%E6%8B%8D%E2%80%9D\" alt=\"X = “网球拍”\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=+Y+%3D%E2%80%9C%E7%BD%91%E7%90%83%E2%80%9D\" alt=\" Y =“网球”\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=sup%28X%29%3DP%28X%29%3D+%5Cfrac+%7B6%7D+%7B6%7D\" alt=\"sup(X)=P(X)= \\frac {6} {6}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=sup%28Y%29+%3D+P%28Y%29+%3D++%5Cfrac+%7B4%7D+%7B6%7D\" alt=\"sup(Y) = P(Y) =  \\frac {4} {6}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=sup%28X+%5Ccup+Y%29+%3D+P%28X+%5Ccup+Y%29+%3D+%5Cfrac%7B4%7D%7B6%7D\" alt=\"sup(X \\cup Y) = P(X \\cup Y) = \\frac{4}{6}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=conf%28Y%7CX%29+%3D+P%28Y%7CX%29+%3D+%5Cfrac%7BP%28X+%5Ccup+Y%29%7D%7BP%28X%29%7D+%3D+%5Cfrac+%7B4%7D%7B6%7D\" alt=\"conf(Y|X) = P(Y|X) = \\frac{P(X \\cup Y)}{P(X)} = \\frac {4}{6}\" eeimg=\"1\"/> </p><h2>3.Aproiri算法</h2><p>算法分为两步：</p><div class=\"highlight\"><pre><code class=\"language-text\">寻找满足最小支持度min_sup的频繁项集\n在频繁项集中寻找规则</code></pre></div><p>举例：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-86d398792dd4ed3a2da14f82dd8248b0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"801\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb\" width=\"801\" data-original=\"https://pic1.zhimg.com/v2-86d398792dd4ed3a2da14f82dd8248b0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;801&#39; height=&#39;204&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"801\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"801\" data-original=\"https://pic1.zhimg.com/v2-86d398792dd4ed3a2da14f82dd8248b0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-86d398792dd4ed3a2da14f82dd8248b0_b.jpg\"/></figure><p>假定最小支持度 <img src=\"https://www.zhihu.com/equation?tex=sup_%7Bmin%7D+%3D+0.5\" alt=\"sup_{min} = 0.5\" eeimg=\"1\"/> 和最小置信度 <img src=\"https://www.zhihu.com/equation?tex=conf_%7Bmin%7D%3D0.6\" alt=\"conf_{min}=0.6\" eeimg=\"1\"/> </p><p>最小出现的次数： <img src=\"https://www.zhihu.com/equation?tex=+%7CT%7C%2Asup_%7Bmin%7D+%3D+2+\" alt=\" |T|*sup_{min} = 2 \" eeimg=\"1\"/></p><p><b>a.寻找频繁项集</b></p><p>首先寻找候选集 <img src=\"https://www.zhihu.com/equation?tex=+C_%7B1%7D+%3D+%5C%7B1%3A2%2C2%3A3%2C3%3A3%2C4%3A1%2C5%3A3%5C%7D\" alt=\" C_{1} = \\{1:2,2:3,3:3,4:1,5:3\\}\" eeimg=\"1\"/> </p><p>i.寻找只有一项的频繁集:</p><p>在 <img src=\"https://www.zhihu.com/equation?tex=C_%7B1%7D\" alt=\"C_{1}\" eeimg=\"1\"/> 找到只有一项的频繁项集 <img src=\"https://www.zhihu.com/equation?tex=F_%7B1%7D%3D%5C%7B+1%3A2%2C2%3A3%2C3%3A3%2C5%3A3+%5C%7D\" alt=\"F_{1}=\\{ 1:2,2:3,3:3,5:3 \\}\" eeimg=\"1\"/> </p><p>同时在 <img src=\"https://www.zhihu.com/equation?tex=F_%7B1%7D\" alt=\"F_{1}\" eeimg=\"1\"/> 中组合出两项的候选集 <img src=\"https://www.zhihu.com/equation?tex=C_%7B2%7D%3D+%5C%7B%281%2C2%29%3A1%2C%281%2C3%29%3A2%2C%281%2C5%29%3A1%2C%282%2C3%29%3A2%2C%282%2C5%29%3A3%2C+%283%2C5%29%3A2%5C%7D\" alt=\"C_{2}= \\{(1,2):1,(1,3):2,(1,5):1,(2,3):2,(2,5):3, (3,5):2\\}\" eeimg=\"1\"/> </p><p>ii.寻找两项的频繁集:</p><p>在 <img src=\"https://www.zhihu.com/equation?tex=C_%7B2%7D\" alt=\"C_{2}\" eeimg=\"1\"/> 找到两项的频繁项集 <img src=\"https://www.zhihu.com/equation?tex=F_%7B2%7D%3D%5C%7B+%281%2C3%29%3A2%2C%282%2C3%29%3A2%2C%282%2C5%29%3A3%2C+%283%2C5%29%3A2+%5C%7D\" alt=\"F_{2}=\\{ (1,3):2,(2,3):2,(2,5):3, (3,5):2 \\}\" eeimg=\"1\"/> </p><p>同时在 <img src=\"https://www.zhihu.com/equation?tex=F_%7B2%7D\" alt=\"F_{2}\" eeimg=\"1\"/> 中组合出两项的候选集 <img src=\"https://www.zhihu.com/equation?tex=C_%7B3%7D%3D+%5C%7B%281%2C2%2C3%29%3A1%2C%281%2C3%2C5%29%3A21%282%2C3%2C5%29%3A2%5C%7D\" alt=\"C_{3}= \\{(1,2,3):1,(1,3,5):21(2,3,5):2\\}\" eeimg=\"1\"/> </p><p>iii.寻找三项的频繁项集：</p><p>在 <img src=\"https://www.zhihu.com/equation?tex=C_%7B3%7D\" alt=\"C_{3}\" eeimg=\"1\"/> 找到两项的频繁项集 <img src=\"https://www.zhihu.com/equation?tex=F_%7B3%7D%3D%5C%7B++%282%2C3%2C5%29%3A2+%5C%7D\" alt=\"F_{3}=\\{  (2,3,5):2 \\}\" eeimg=\"1\"/> </p><p>同时在 <img src=\"https://www.zhihu.com/equation?tex=F_%7B3%7D\" alt=\"F_{3}\" eeimg=\"1\"/> 中组合不出四项的候选集，依次类推。</p><p><i>注：</i> <img src=\"https://www.zhihu.com/equation?tex=P%28A%5Ccup+B%29++%5Cleq+P%28A%29+\" alt=\"P(A\\cup B)  \\leq P(A) \" eeimg=\"1\"/> ，二项频繁集只可能由一项频繁集的组合，依次类推。</p><p><b>b.在频繁项集里面找寻找管理规则</b></p><p>一项集没法寻找关系，从二项集开始。</p><p>i.二项频繁集对应子集(除去空集和全集)</p><p><img src=\"https://www.zhihu.com/equation?tex=C2_%7Bsub%7D+%3D%5C%7B+%5C%7B1%2C3%5C%7D%2C+%5C%7B2%2C3%5C%7D%EF%BC%8C%5C%7B2%2C5%5C%7D%2C%5C%7B3%2C5%5C%7D%5C%7D+\" alt=\"C2_{sub} =\\{ \\{1,3\\}, \\{2,3\\}，\\{2,5\\},\\{3,5\\}\\} \" eeimg=\"1\"/> </p><p>计算对应关系的置信度： <img src=\"https://www.zhihu.com/equation?tex=conf%281%7C3%29%3D1.0+%5C%5C+conf%283%7C1%29%3D0.67+%5C%5C+conf%282%7C3%29%3D0.67+%5C%5C+conf%283%7C2%29%3D0.67+%5C%5C+conf%282%7C5%29%3D1.0+%5C%5C+conf%285%7C2%29%3D1.0+%5C%5C+conf%283%7C5%29%3D0.67+%5C%5C+conf%285%7C3%29%3D0.67\" alt=\"conf(1|3)=1.0 \\\\ conf(3|1)=0.67 \\\\ conf(2|3)=0.67 \\\\ conf(3|2)=0.67 \\\\ conf(2|5)=1.0 \\\\ conf(5|2)=1.0 \\\\ conf(3|5)=0.67 \\\\ conf(5|3)=0.67\" eeimg=\"1\"/> </p><p>找出置信度 <img src=\"https://www.zhihu.com/equation?tex=conf%28B%7CA%29%5Cgeq+conf_%7Bmin%7D+\" alt=\"conf(B|A)\\geq conf_{min} \" eeimg=\"1\"/> 的关系：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%7B1%5C%7D%5CRightarrow%5C%7B3%5C%7D+%5C%5C+%5C%7B3%5C%7D%5CRightarrow%5C%7B1%5C%7D+%5C%5C+%5C%7B2%5C%7D%5CRightarrow%5C%7B3%5C%7D+%5C%5C+%5C%7B3%5C%7D%5CRightarrow%5C%7B2%5C%7D+%5C%5C+%5C%7B2%5C%7D%5CRightarrow%5C%7B5%5C%7D+%5C%5C+%5C%7B5%5C%7D%5CRightarrow%5C%7B2%5C%7D+%5C%5C+%5C%7B3%5C%7D%5CRightarrow%5C%7B5%5C%7D+%5C%5C+%5C%7B5%5C%7D%5CRightarrow%5C%7B3%5C%7D\" alt=\"\\{1\\}\\Rightarrow\\{3\\} \\\\ \\{3\\}\\Rightarrow\\{1\\} \\\\ \\{2\\}\\Rightarrow\\{3\\} \\\\ \\{3\\}\\Rightarrow\\{2\\} \\\\ \\{2\\}\\Rightarrow\\{5\\} \\\\ \\{5\\}\\Rightarrow\\{2\\} \\\\ \\{3\\}\\Rightarrow\\{5\\} \\\\ \\{5\\}\\Rightarrow\\{3\\}\" eeimg=\"1\"/> </p><p>ii.同理依次类推：</p><p>找到所有的管理关系：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%7B1%5C%7D%5CRightarrow%5C%7B3%5C%7D+%5C%5C+%5C%7B3%5C%7D%5CRightarrow%5C%7B1%5C%7D+%5C%5C+%5C%7B2%5C%7D%5CRightarrow%5C%7B3%5C%7D+%5C%5C+%5C%7B3%5C%7D%5CRightarrow%5C%7B2%5C%7D+%5C%5C+%5C%7B2%5C%7D%5CRightarrow%5C%7B5%5C%7D+%5C%5C+%5C%7B5%5C%7D%5CRightarrow%5C%7B2%5C%7D+%5C%5C+%5C%7B3%5C%7D%5CRightarrow%5C%7B5%5C%7D+%5C%5C+%5C%7B5%5C%7D%5CRightarrow%5C%7B3%5C%7D+%5C%5C+%5C%7B2%5C%7D%5CRightarrow%5C%7B3%2C5%5C%7D+%5C%5C+%5C%7B3%5C%7D%5CRightarrow%5C%7B2%2C5%5C%7D+%5C%5C+%5C%7B2%2C3%5C%7D%5CRightarrow%5C%7B5%5C%7D+%5C%5C+%5C%7B5%5C%7D%5CRightarrow%5C%7B2%2C3%5C%7D+%5C%5C+%5C%7B2%2C5%5C%7D%5CRightarrow%5C%7B3%5C%7D\" alt=\"\\{1\\}\\Rightarrow\\{3\\} \\\\ \\{3\\}\\Rightarrow\\{1\\} \\\\ \\{2\\}\\Rightarrow\\{3\\} \\\\ \\{3\\}\\Rightarrow\\{2\\} \\\\ \\{2\\}\\Rightarrow\\{5\\} \\\\ \\{5\\}\\Rightarrow\\{2\\} \\\\ \\{3\\}\\Rightarrow\\{5\\} \\\\ \\{5\\}\\Rightarrow\\{3\\} \\\\ \\{2\\}\\Rightarrow\\{3,5\\} \\\\ \\{3\\}\\Rightarrow\\{2,5\\} \\\\ \\{2,3\\}\\Rightarrow\\{5\\} \\\\ \\{5\\}\\Rightarrow\\{2,3\\} \\\\ \\{2,5\\}\\Rightarrow\\{3\\}\" eeimg=\"1\"/> </p><h2>4.Python实现</h2><p>1.初始化数据</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">load_list_data</span><span class=\"p\">():</span>\n    <span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span>\n        <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">4</span><span class=\"p\">],</span>\n        <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">5</span><span class=\"p\">],</span>\n        <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">5</span><span class=\"p\">],</span>\n        <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">5</span><span class=\"p\">]</span>\n    <span class=\"p\">])</span>\n    <span class=\"k\">return</span> <span class=\"n\">data</span></code></pre></div><p>2.初始化一元频繁集</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">get_frequent_1</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">min_sup</span><span class=\"p\">):</span>\n    <span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"k\">for</span> <span class=\"n\">item</span> <span class=\"ow\">in</span> <span class=\"n\">data</span><span class=\"p\">:</span>\n        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"n\">item</span><span class=\"p\">:</span>\n            <span class=\"k\">if</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"n\">c</span><span class=\"p\">:</span>\n                <span class=\"n\">c</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>\n            <span class=\"k\">else</span><span class=\"p\">:</span>\n                <span class=\"n\">c</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n    <span class=\"k\">for</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span> <span class=\"ow\">in</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">c</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">()):</span>\n        <span class=\"k\">if</span> <span class=\"n\">v</span> <span class=\"o\">&lt;</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"o\">*</span><span class=\"n\">min_sup</span><span class=\"p\">:</span>\n            <span class=\"n\">c</span><span class=\"o\">.</span><span class=\"n\">pop</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">c</span></code></pre></div><p>3.如果没有一元频繁项，则无关联规则</p><p>4.找到其他的 <img src=\"https://www.zhihu.com/equation?tex=n\" alt=\"n\" eeimg=\"1\"/> 元频繁项集</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># 获取所有的频繁项集</span>\n<span class=\"k\">def</span> <span class=\"nf\">get_all_frequent_sets</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">min_sup</span><span class=\"p\">):</span>\n    <span class=\"n\">f1</span> <span class=\"o\">=</span> <span class=\"n\">get_frequent_1</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">min_sup</span><span class=\"p\">)</span>\n    <span class=\"n\">indexs_f1</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"n\">f1</span><span class=\"p\">):</span>\n        <span class=\"n\">indexs_f1</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">([</span><span class=\"n\">i</span><span class=\"p\">])</span>\n    <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"mi\">2</span>\n    <span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">f1</span><span class=\"p\">]</span>\n    <span class=\"n\">indexs</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">indexs_f1</span><span class=\"p\">]</span>\n    <span class=\"c1\"># 寻找n元频繁集，找不到为止</span>\n    <span class=\"k\">while</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">[</span><span class=\"n\">k</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">]):</span>\n        <span class=\"n\">candidates</span> <span class=\"o\">=</span> <span class=\"n\">get_candidates_n</span><span class=\"p\">(</span><span class=\"n\">indexs</span><span class=\"p\">[</span><span class=\"n\">k</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"n\">k</span><span class=\"p\">)</span>\n        <span class=\"n\">f_tmp</span> <span class=\"o\">=</span> <span class=\"n\">get_frequent_n</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">candidates</span><span class=\"p\">,</span> <span class=\"n\">min_sup</span><span class=\"p\">)</span>\n        <span class=\"n\">indexs_f</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"n\">f_tmp</span><span class=\"p\">):</span>\n            <span class=\"n\">indexs_f</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">))</span>\n        <span class=\"n\">indexs</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">indexs_f</span><span class=\"p\">)</span>\n        <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">f_tmp</span><span class=\"p\">)</span>\n        <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"n\">k</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n    <span class=\"k\">return</span> <span class=\"n\">f</span>\n\n\n<span class=\"c1\"># 获得n元频繁项集</span>\n<span class=\"k\">def</span> <span class=\"nf\">get_frequent_n</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">candidates</span><span class=\"p\">,</span> <span class=\"n\">min_sup</span><span class=\"p\">):</span>\n    <span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"c1\"># 计算候选集的支持度</span>\n    <span class=\"k\">for</span> <span class=\"n\">candidate</span> <span class=\"ow\">in</span> <span class=\"n\">candidates</span><span class=\"p\">:</span>\n        <span class=\"k\">for</span> <span class=\"n\">item</span> <span class=\"ow\">in</span> <span class=\"n\">data</span><span class=\"p\">:</span>\n            <span class=\"k\">if</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">candidate</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">issubset</span><span class=\"p\">(</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">item</span><span class=\"p\">)):</span>\n                <span class=\"k\">if</span> <span class=\"nb\">tuple</span><span class=\"p\">(</span><span class=\"n\">candidate</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n                    <span class=\"n\">f</span><span class=\"p\">[</span><span class=\"nb\">tuple</span><span class=\"p\">(</span><span class=\"n\">candidate</span><span class=\"p\">)]</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>\n                <span class=\"k\">else</span><span class=\"p\">:</span>\n                    <span class=\"n\">f</span><span class=\"p\">[</span><span class=\"nb\">tuple</span><span class=\"p\">(</span><span class=\"n\">candidate</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n    <span class=\"k\">for</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span> <span class=\"ow\">in</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">()):</span>\n        <span class=\"k\">if</span> <span class=\"n\">v</span> <span class=\"o\">&lt;</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"o\">*</span><span class=\"n\">min_sup</span><span class=\"p\">:</span>\n            <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">pop</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">f</span>\n\n\n<span class=\"c1\"># 获得n元候选集</span>\n<span class=\"k\">def</span> <span class=\"nf\">get_candidates_n</span><span class=\"p\">(</span><span class=\"n\">indexs_f</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">):</span>\n    <span class=\"n\">candidates</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">l</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">indexs_f</span><span class=\"p\">)</span>\n    <span class=\"c1\"># 对n-1元频繁集的key进行组合</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">):</span>\n        <span class=\"k\">if</span> <span class=\"n\">i</span> <span class=\"o\">&lt;</span> <span class=\"n\">l</span><span class=\"p\">:</span>\n            <span class=\"k\">for</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">l</span><span class=\"p\">):</span>\n                <span class=\"n\">fi</span> <span class=\"o\">=</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">indexs_f</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]))</span>\n                <span class=\"n\">fj</span> <span class=\"o\">=</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">indexs_f</span><span class=\"p\">[</span><span class=\"n\">j</span><span class=\"p\">]))</span>\n                <span class=\"n\">tmp</span> <span class=\"o\">=</span> <span class=\"n\">fi</span> <span class=\"o\">|</span> <span class=\"n\">fj</span>\n                <span class=\"n\">tmp</span> <span class=\"o\">=</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">tmp</span><span class=\"p\">))</span>\n                <span class=\"k\">if</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">tmp</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"n\">n</span> <span class=\"ow\">and</span> <span class=\"n\">tmp</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"n\">candidates</span><span class=\"p\">:</span>  <span class=\"c1\"># 只找n元的</span>\n                    <span class=\"n\">candidates</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">tmp</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">candidates</span></code></pre></div><p>5.找出所有关联规则</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># 获得一个集合非空子集</span>\n<span class=\"k\">def</span> <span class=\"nf\">get_all_subsets</span><span class=\"p\">(</span><span class=\"n\">items</span><span class=\"p\">):</span>\n    <span class=\"n\">result</span> <span class=\"o\">=</span>  <span class=\"p\">[]</span>\n    <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">items</span><span class=\"p\">:</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"nb\">set</span><span class=\"p\">([</span><span class=\"n\">x</span><span class=\"p\">])</span>\n        <span class=\"n\">result</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"k\">for</span> <span class=\"n\">item</span> <span class=\"ow\">in</span> <span class=\"n\">result</span><span class=\"p\">:</span>\n            <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">item</span><span class=\"p\">)</span> <span class=\"o\">|</span> <span class=\"n\">x</span>\n            <span class=\"k\">if</span> <span class=\"n\">x</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"n\">result</span><span class=\"p\">:</span>\n                <span class=\"n\">result</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">result</span>\n\n\n<span class=\"c1\"># 获得所有关联关系</span>\n<span class=\"k\">def</span> <span class=\"nf\">get_all_rules</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">min_conf</span><span class=\"p\">,</span> <span class=\"n\">fs</span><span class=\"p\">):</span>\n    <span class=\"n\">rules</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"n\">subsets</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"c1\"># 找到所有非空子集</span>\n    <span class=\"k\">for</span> <span class=\"n\">fn</span> <span class=\"ow\">in</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n        <span class=\"k\">for</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">fn</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">()):</span>\n            <span class=\"n\">subsets</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">get_all_subsets</span><span class=\"p\">(</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">)))</span>\n    <span class=\"k\">for</span> <span class=\"n\">subset</span> <span class=\"ow\">in</span> <span class=\"n\">subsets</span><span class=\"p\">:</span>\n        <span class=\"n\">sum_sup</span> <span class=\"o\">=</span> <span class=\"n\">get_conf</span><span class=\"p\">(</span><span class=\"n\">subset</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">fs</span><span class=\"p\">)</span> <span class=\"c1\"># 全集的支持度</span>\n        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">subset</span><span class=\"p\">)</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">):</span>\n            <span class=\"n\">now_sup</span> <span class=\"o\">=</span> <span class=\"n\">get_conf</span><span class=\"p\">(</span><span class=\"n\">subset</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">],</span> <span class=\"n\">fs</span><span class=\"p\">)</span>\n            <span class=\"n\">conf</span> <span class=\"o\">=</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">sum_sup</span><span class=\"o\">/</span><span class=\"n\">now_sup</span><span class=\"p\">)</span>\n            <span class=\"k\">if</span> <span class=\"n\">conf</span> <span class=\"o\">&gt;=</span> <span class=\"n\">min_conf</span><span class=\"p\">:</span>\n                <span class=\"n\">cha</span> <span class=\"o\">=</span> <span class=\"n\">subset</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">-</span><span class=\"n\">subset</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span>  <span class=\"c1\"># 差集</span>\n                <span class=\"n\">cha</span> <span class=\"o\">=</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">cha</span><span class=\"p\">)</span>\n                <span class=\"n\">rules</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">subset</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">])</span><span class=\"o\">+</span><span class=\"s2\">&#34;-&gt;&#34;</span><span class=\"o\">+</span><span class=\"n\">cha</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">conf</span>\n    <span class=\"k\">return</span> <span class=\"n\">rules</span></code></pre></div><p>6.测试整个程序</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;__main__&#34;</span><span class=\"p\">:</span>\n    <span class=\"c1\"># 设置数据、最小置信度、最小支持度</span>\n    <span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">load_list_data</span><span class=\"p\">()</span> \n    <span class=\"n\">min_conf</span> <span class=\"o\">=</span> <span class=\"mf\">0.6</span>\n    <span class=\"n\">min_sup</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>\n    <span class=\"c1\"># 寻找一元频繁集</span>\n    <span class=\"n\">f1</span> <span class=\"o\">=</span> <span class=\"n\">get_frequent_1</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">min_sup</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">f1</span><span class=\"p\">:</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;no frenquent 1-item&#34;</span><span class=\"p\">)</span>\n        <span class=\"nb\">exit</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;=========== all frequent items  ==========&#34;</span><span class=\"p\">)</span>\n    <span class=\"c1\"># 对f1排序</span>\n    <span class=\"n\">index</span> <span class=\"o\">=</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"n\">f1</span><span class=\"p\">)</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;{&#34;</span><span class=\"p\">,</span><span class=\"n\">end</span><span class=\"o\">=</span><span class=\"s2\">&#34;&#34;</span><span class=\"p\">)</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">index</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"s2\">&#34;:&#34;</span><span class=\"p\">,</span><span class=\"n\">f1</span><span class=\"p\">[</span><span class=\"n\">index</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]],</span><span class=\"n\">end</span><span class=\"o\">=</span><span class=\"s2\">&#34;&#34;</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"n\">index</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">:]:</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;,&#34;</span><span class=\"p\">,</span><span class=\"n\">i</span><span class=\"p\">,</span><span class=\"s2\">&#34;:&#34;</span><span class=\"p\">,</span><span class=\"n\">f1</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">],</span><span class=\"n\">end</span><span class=\"o\">=</span><span class=\"s2\">&#34;&#34;</span><span class=\"p\">)</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;}&#34;</span><span class=\"p\">)</span>\n    <span class=\"c1\"># 寻找所有n元频繁集</span>\n    <span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"n\">get_all_frequent_sets</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">min_sup</span><span class=\"p\">)</span>\n    <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">remove</span><span class=\"p\">({})</span>   <span class=\"c1\"># get_all_frequent_sets中最后一个是空集 </span>\n    <span class=\"c1\"># 把所有n元频繁项放入一个字典</span>\n    <span class=\"n\">fs</span> <span class=\"o\">=</span> <span class=\"p\">{}</span> \n    <span class=\"k\">for</span> <span class=\"n\">fn</span> <span class=\"ow\">in</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n        <span class=\"k\">for</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span> <span class=\"ow\">in</span> <span class=\"n\">fn</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">():</span>\n            <span class=\"n\">fs</span><span class=\"p\">[</span><span class=\"n\">k</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">v</span>\n    <span class=\"c1\"># 打印2项以上的频繁集</span>\n    <span class=\"k\">if</span> <span class=\"n\">f1</span> <span class=\"ow\">in</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n        <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">remove</span><span class=\"p\">(</span><span class=\"n\">f1</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">item</span> <span class=\"ow\">in</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">item</span><span class=\"p\">)</span>\n    <span class=\"c1\"># 寻找所有的关联规则</span>\n    <span class=\"n\">rules</span> <span class=\"o\">=</span> <span class=\"n\">get_all_rules</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">min_conf</span><span class=\"p\">,</span> <span class=\"n\">fs</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"n\">rules</span><span class=\"p\">:</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;===========      all rules     ============&#34;</span><span class=\"p\">)</span>\n        <span class=\"k\">for</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span> <span class=\"ow\">in</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">rules</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">()):</span>\n            <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"s2\">&#34;:&#34;</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">)</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;no rules found!&#34;</span><span class=\"p\">)</span></code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "数据挖掘", 
                    "tagLink": "https://api.zhihu.com/topics/19553534"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "算法", 
                    "tagLink": "https://api.zhihu.com/topics/19553510"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/26424592", 
            "userName": "idejie", 
            "userLink": "https://www.zhihu.com/people/67d6bc7c8363bef16771d582f32ffc25", 
            "upvote": 1, 
            "title": "【笔记】21天实战 Caffe：第五天", 
            "content": "<h2>第五天：Caffe依赖包解析</h2><h2>1.ProtoBuffer</h2><p>protoBuffer 是Google开发的一种可以实现内存和非易失存储介质交换的协议接口。</p><p>caffe大量使用它作为权值和模型参数的载体。</p><h2>2.Boost</h2><p>Caffe主要是使用它的只能指针，避免共享指针时造成内存泄漏或者多次释放</p><h2>3.GFlags</h2><p>起到命令行参数解析的作用，与protoBuffer相似，但是输入源不同</p><h2>4.GLog</h2><p>Googlr开发的用于记录应用程序日志的使用库，提供基于C++标准输入输出流形式的接口</p><h2>5.BLAS</h2><p>卷积神经网络中用到数学计算。BLAS。OpenBLAS在Caffe中主要负责CPU端的数值计算</p><h2>6.HDF5</h2><p>NCSA 为了满足各种科研领域需求而研制的一种高效存储和分发科学数据的新型数据形式。他可以存储不同类型的图像和数码数据的文件，并且可以在不同类型的机器上传输，同时还有统一的文件格式的函数库。</p><h2>7.OpenCV</h2><p>开源计算机视觉库。</p><h2>8.LMDB和LevelDB</h2><ul><li><p>LMDB—— 闪电般的内存映射型数据管理器，在 Caffe 的作用主要是提供数据管理，将形形色色的原始数据转换成统一的 Key-Value存储。</p></li><li><p>LevelDB 是 Caffe 早期版本使用的数据存储方式，由 Google 开发。他是一种持续的键值对存储方式</p></li></ul><h2>9.Snappy</h2><p>Snappy 是一个用来压缩和解压缩的 C++库</p>", 
            "topic": [
                {
                    "tag": "Caffe（深度学习框架）", 
                    "tagLink": "https://api.zhihu.com/topics/20019488"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/26364603", 
            "userName": "idejie", 
            "userLink": "https://www.zhihu.com/people/67d6bc7c8363bef16771d582f32ffc25", 
            "upvote": 1, 
            "title": "【笔记】21天实战Caffe：第四天", 
            "content": "<h1>第四天：准备Caffe环境</h1><h2>1.Mac</h2><ul><li><p>安装homebrew</p></li><li><p>安装caffe</p></li><li><p>下载Caffe源码</p><br/></li><li><p>修改config</p><p>#CPU_ONLY :=1前面的#去掉</p><p><em>注：</em></p><p> 主要是因为我的机器不是N卡 我是黑苹果TnT A卡</p><p> 其实还是关了吧，caffe就是入门，用不到GPU加速</p></li><li><p>编译</p><p>make -j</p></li></ul><h2>2.Ubuntu</h2><blockquote><p>由于本人用的是黑苹果，并没有下载Xcode这种与我无关 “TnT” 的App</p><p>故选择Ubuntu了</p></blockquote><ul><li><p>apt-get</p><br/><br/><br/><br/><br/><br/><div class=\"highlight\"><pre><code class=\"language-bash\">$ sudo apt-get git\n$ sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler\n$ sudo apt-get install -no-install-recommends libboost-all-dev\n$ sudo apt-get install libatlas-base-dev\n$ sudo apt-get install python-dev\n$ sudo apt-get install libflags-dev libgoogle-glog-dev liblmdb-dev\n</code></pre></div></li><li><p>下载源码、修改config、编译同上</p></li></ul><h2>3.CentOS、Windows等</h2>", 
            "topic": [
                {
                    "tag": "Caffe（深度学习框架）", 
                    "tagLink": "https://api.zhihu.com/topics/20019488"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/26364541", 
            "userName": "idejie", 
            "userLink": "https://www.zhihu.com/people/67d6bc7c8363bef16771d582f32ffc25", 
            "upvote": 2, 
            "title": "【笔记】21天实战Caffe：第三天", 
            "content": "<h1>第三天：深度学习工具汇总</h1><h2>1.Caffe</h2><ul><li>实现了前馈卷积神经网络架构</li><li>速度快，因为用了MKL，openBLAS、cuBLAS，支持GPU加速</li><li>特别适合做特征提取，实际上适合做二维图像数据的特征提取</li><li>caffe完全开源</li></ul><h2>2.Torch &amp; Overfeat</h2><ul><li>支持嵌入式设备：Android、iOS、FPGA</li><li>内置8个包：torch、lab&amp;plot、qt、nn、image、optim、unsup、third-party</li><li>overfea是在imageNet数据集中使用Torch7训练的特征提取器</li></ul><h2>3.MxNet</h2><ul><li>N维数组接口</li><li>符合接口</li></ul><h2>4.TensorFlow</h2><ul><li>大规模机器学习框架、移植性好、支持多种深度学习模型</li></ul><h2>5.Theano</h2><p>基于Python的一款</p><h2>6.CNTK</h2><p>微软旗下的，单机4GPU性能强</p>", 
            "topic": [
                {
                    "tag": "Caffe（深度学习框架）", 
                    "tagLink": "https://api.zhihu.com/topics/20019488"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/26308688", 
            "userName": "idejie", 
            "userLink": "https://www.zhihu.com/people/67d6bc7c8363bef16771d582f32ffc25", 
            "upvote": 0, 
            "title": "【笔记】21天实战Caffe：第二天", 
            "content": "<h1>第二天：深度学习的过往</h1><h2>1.传统机器学习的局限性</h2><p>传统的深度学习需要人工设计特征提取器</p><h2>2.从表示学习到深度学习</h2><p>表示学习：直接以原始数据形式提供给机器输入，自动发现用于检测和分类的傲视方式。</p><p>深度学习：多层次表示学习分方法，那个简单的非线性的模块构建而成。</p><h2>3.监督学习</h2><ul><li>随机梯度下降</li><li>线性分类器</li><li>特征提取器</li><li>一个深度学习架构是将简单模块多层堆叠，大多数模块具备学习能力，能计算非线性输入-输出映射</li></ul><h2>4.反向传播算法</h2><ul><li>导数链式法则</li><li>双隐层深度前馈网络</li></ul><h2>5.卷积神经网络</h2><p>局部互联、共享权值、下采样以及使用多个卷积</p><h2>6.深度学习反思</h2><p>模型参数和数据量的关系</p>", 
            "topic": [
                {
                    "tag": "Caffe（深度学习框架）", 
                    "tagLink": "https://api.zhihu.com/topics/20019488"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/26300341", 
            "userName": "idejie", 
            "userLink": "https://www.zhihu.com/people/67d6bc7c8363bef16771d582f32ffc25", 
            "upvote": 3, 
            "title": "【笔记】21天实战Caffe：第一天", 
            "content": "<h1>第一天：什么是深度学习</h1><ul><li>学习分为上课和写作业，上课时有监督的学习，写作业就是无监督的学习。</li></ul><p>带学习功能的机器仅仅通过“看”未知系统的输入-输出对（称为训练样本），自动实现该系统内部算法，并具有举一反三的能力（称为泛化），对于不在训练样本中的位置输入也能正确产生输出。</p><ul><li>机器学习需要的三份数据：</li></ul><p>（1）训练集，机器学习的样例</p><p>（2）验证集，机器学习阶段，用于评估得分和损失是否达到预期要求</p><p>（3）测试集，机器学习之后，实战阶段的评估得分</p><ul><li>深度学习是由多个处理层组成的计算模型，可以通过学习获得数据的抽象层表示。该方法显著提高了语音识别、视觉目标识别和检测结果。</li></ul><h2>1.1星星之火，可以燎原</h2><ul><li><p>时间：2012年</p></li><li><p>事件：Alex在ILSVRC中使用了一个叫做AlexNet的深度学习模型，分别运行在两块GPU上</p></li><li><p>2012年爆发的原因：</p><p>更大的数据集</p><p>新的深度学习技术</p><p>新的计算机硬件</p></li></ul><h2>1.2师夷长技</h2><ul><li>谷歌&amp;微软<ul><li>GoogLeNet</li><li>Alpha Go</li><li>Master</li><li>Deep Residual Learning Framwork</li><li>Azure Machine Learning Studio</li></ul></li><li>Fb,Amazon ,Nvida<ul><li>Torch</li><li>Amazon Machine Learning</li><li>GPU</li></ul></li></ul><h2>1.3中国崛起</h2><ul><li>BAT</li><li>中科院</li><li>企业（商汤科技、Face++、涂鸦、格林深瞳、Dress++、LinkFace）</li></ul>", 
            "topic": [
                {
                    "tag": "Caffe（深度学习框架）", 
                    "tagLink": "https://api.zhihu.com/topics/20019488"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/idejie"
}
