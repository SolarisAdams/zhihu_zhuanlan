{
    "title": "深度学习一步步", 
    "description": "", 
    "followers": [
        "https://www.zhihu.com/people/roger-gou", 
        "https://www.zhihu.com/people/wo-kan-kan-59-65", 
        "https://www.zhihu.com/people/nan-yi-80", 
        "https://www.zhihu.com/people/liu-fei-94-95", 
        "https://www.zhihu.com/people/huo-chen-87", 
        "https://www.zhihu.com/people/zhu-ying-10-67", 
        "https://www.zhihu.com/people/mu-zi-13-91-44", 
        "https://www.zhihu.com/people/yang-zhi-gang-70-1", 
        "https://www.zhihu.com/people/ju-shang-38", 
        "https://www.zhihu.com/people/qinlibo_nlp", 
        "https://www.zhihu.com/people/heidy-46", 
        "https://www.zhihu.com/people/sun-lin-42-29", 
        "https://www.zhihu.com/people/yu-er-68-87", 
        "https://www.zhihu.com/people/kuai-le-123-93", 
        "https://www.zhihu.com/people/yu-ji-shi-17", 
        "https://www.zhihu.com/people/po-xin-yang-81", 
        "https://www.zhihu.com/people/ren-pang-lu-zi-hu", 
        "https://www.zhihu.com/people/lshhh", 
        "https://www.zhihu.com/people/wang-qin-ze-30", 
        "https://www.zhihu.com/people/lai-ge-tu-zi", 
        "https://www.zhihu.com/people/tianqin91", 
        "https://www.zhihu.com/people/zhang-long-91-36", 
        "https://www.zhihu.com/people/wgj-47-46", 
        "https://www.zhihu.com/people/xue-shao-jia", 
        "https://www.zhihu.com/people/shi-huai-71", 
        "https://www.zhihu.com/people/wj2014-59", 
        "https://www.zhihu.com/people/luweikxy", 
        "https://www.zhihu.com/people/hua-long-35-72", 
        "https://www.zhihu.com/people/deng-hui-yang-23", 
        "https://www.zhihu.com/people/cai-cai-hua-53", 
        "https://www.zhihu.com/people/scottdc-31", 
        "https://www.zhihu.com/people/cgaoan", 
        "https://www.zhihu.com/people/lizzy-chan-24", 
        "https://www.zhihu.com/people/feng-kou-shang-de-zhu-hou", 
        "https://www.zhihu.com/people/yu-shun-zhe", 
        "https://www.zhihu.com/people/lucky-45-9-77", 
        "https://www.zhihu.com/people/michael-wu-25", 
        "https://www.zhihu.com/people/eathon-5", 
        "https://www.zhihu.com/people/jennifer-57-69-21", 
        "https://www.zhihu.com/people/blueman", 
        "https://www.zhihu.com/people/joyce-58-73-53", 
        "https://www.zhihu.com/people/zhhhzhang", 
        "https://www.zhihu.com/people/feng-zhi-fan", 
        "https://www.zhihu.com/people/liang-yuan-88", 
        "https://www.zhihu.com/people/an-ran-57", 
        "https://www.zhihu.com/people/touxiangniubi", 
        "https://www.zhihu.com/people/xie-chu-xin-52", 
        "https://www.zhihu.com/people/hlhl", 
        "https://www.zhihu.com/people/tou-wen-a-meng", 
        "https://www.zhihu.com/people/ye-zhe-hu-zhi-13-3", 
        "https://www.zhihu.com/people/zhangyuteng", 
        "https://www.zhihu.com/people/frances-23-74", 
        "https://www.zhihu.com/people/TtForward", 
        "https://www.zhihu.com/people/LoveMixiaoMi", 
        "https://www.zhihu.com/people/liu-meng-yuan-72-95", 
        "https://www.zhihu.com/people/zhang-shi-xin-bo", 
        "https://www.zhihu.com/people/shine-peng-35", 
        "https://www.zhihu.com/people/huang-li-an", 
        "https://www.zhihu.com/people/tang-xin-76-79", 
        "https://www.zhihu.com/people/jqqq", 
        "https://www.zhihu.com/people/zhou-wen-wen-1-47", 
        "https://www.zhihu.com/people/na-lan-56-89", 
        "https://www.zhihu.com/people/yue-osmondy", 
        "https://www.zhihu.com/people/wang-jian-zhou", 
        "https://www.zhihu.com/people/min-wei", 
        "https://www.zhihu.com/people/chenlongzhen.tech", 
        "https://www.zhihu.com/people/xiao-ya-94-24", 
        "https://www.zhihu.com/people/sxsjt", 
        "https://www.zhihu.com/people/lan-lan-23-25", 
        "https://www.zhihu.com/people/yi-zhai-1991", 
        "https://www.zhihu.com/people/xie-wan-68-79", 
        "https://www.zhihu.com/people/tigereye3010", 
        "https://www.zhihu.com/people/memento-mori", 
        "https://www.zhihu.com/people/yan-xu-ran", 
        "https://www.zhihu.com/people/gavin-28-80-51", 
        "https://www.zhihu.com/people/chu-yun-fei-42-59", 
        "https://www.zhihu.com/people/foster-22", 
        "https://www.zhihu.com/people/hao-hao-gan-huo-95", 
        "https://www.zhihu.com/people/jiu-ye-20-63", 
        "https://www.zhihu.com/people/yildhd-wang", 
        "https://www.zhihu.com/people/tian-bian-de-yun-79"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/71618473", 
            "userName": "卡门", 
            "userLink": "https://www.zhihu.com/people/23533079660fe77ddb1abcf0b15c72b2", 
            "upvote": 1, 
            "title": "pyspark中RDD一些函数的解释", 
            "content": "<p>通常我们在进行一些数据处理的时候，会使用一些库，例如pandas等，来进行数据的处理工作。但是当数据量真的很大，达到所谓的大数据的规模之后，使用pandas就有些力不从心了：要么爆掉内存，要么跑的巨慢，无法接受。所以这时候就需要使用spark来进行处理了。</p><p>本文主要讲pyspark中RDD的一些api函数，不断更新。方便理解和查阅。</p><p><b>1、aggregate</b>(<i>zeroValue</i>,<i>seqOp</i>,<i>combOp</i>)</p><p>首先看他的英文解释：</p><p>The functions op(t1, t2) is allowed to modify <b>t1</b> and return it as its result value to avoid object allocation; however, it should not modify <b>t2</b>.</p><p>The first function (seqOp) can return a different result type, U, than the type of this RDD. Thus, we need one operation for merging a T into an U and one operation for merging two U。</p><p>什么意思呢？</p><p>seqOP和combOP实际是两个阶段的函数：</p><p>seqOP首先作用在各个partition, (U, T) -&gt; U 不断的更新U, 然后各个partition最终生成一个U；在这里， U的初始值是zeroValue, 而T则是不断传进来的各个partition内的值。U是我们希望最终生成的格式，所以U的格式，跟一开始各partition内的格式T是不一样的。</p><p>而combOP是把各个partition的U聚合起来， （U, U） -&gt; U。</p><p>举例来说，以api中代码为例：</p><div class=\"highlight\"><pre><code class=\"language-text\">&gt;&gt;&gt; seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n&gt;&gt;&gt; combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n(10, 4)\n&gt;&gt;&gt; sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n(0, 0)</code></pre></div><p>假如说通过parallelize分成了两个partition: [1,2], [3,4],</p><p>那么在partition1中， seqOP中的lambda x, y 一开始分别代表：</p><p>x = (0,0), y = (1) #也可是2</p><p>然后不断迭代：</p><div class=\"highlight\"><pre><code class=\"language-text\">x[0] + y = 0 + 1 = 1, x[1] + 1 = 0 + 1 = 1  =&gt; x = (1, 1)\nx[0] + y = 1 + 2 = 3, x[1] + 1 = 1 + 1 = 2  =&gt; x = (3, 2)</code></pre></div><p>在partition2中， </p><p>seqOP中的lambda x, y 一开始分别代表：</p><p>x = (0,0), y = (3) #也可是4</p><p>然后不断迭代：</p><div class=\"highlight\"><pre><code class=\"language-text\">x[0] + y = 0 + 3 = 3, x[1] + 1 = 0 + 1 = 1  =&gt; x = (3, 1)\nx[0] + y = 3 + 4 = 7, x[1] + 1 = 1 + 1 = 2  =&gt; x = (7, 2)</code></pre></div><p>两个partiton计算完毕后，分别是（3，2）和（7，2）</p><p>然后combOP合并计算<br/></p><div class=\"highlight\"><pre><code class=\"language-text\">x[0] + y[0] = 3 + 7 = 10, x[1] + y[1] = 4 = &gt; x = (10,4)</code></pre></div><p>在这个计算中， 该算子计算了两个值：一个是所有值的累加和，一个是所有值的个数，然后生成了两个值放在了（）中可以方便后续的计算。</p><p><b>2、aggregateByKey</b>(<i>zeroValue</i>,<i>seqFunc</i>,<i>combFunc</i>,<i>numPartitions=None</i>,<i>partitionFunc=&lt;function portable_hash at 0x7f51f1ac0668&gt;</i>)</p><p>跟aggregate是相似的功能，只不过他针对的是key-value的情况。</p><p>在这里，输入值是pair对（k, v），而这里的T则是v,在aggregate中T则是整个的（k,v）值。主要是这里的区别。</p><p>到最后生成 的，也是针对各个key内聚合的结果。</p><p>3、<b>cache</b>()</p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/qq_20641565/article/details/76216417\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">为什么用cache</a><p><b>4、cartesian（）</b></p><p>RDD数中的卡特兰乘积。</p><p><b>5</b>、<b>coalesce</b>(<i>numPartitions</i>,<i>shuffle=False</i>)</p><a href=\"https://link.zhihu.com/?target=https%3A//www.jianshu.com/p/ee069b6bc273\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-cbf37a8b5fe5a85a266394167833259e_180x120.jpg\" data-image-width=\"638\" data-image-height=\"321\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark 重分区函数：coalesce和repartition区别</a><p>repartition()方法就是coalesce()方法shuffle为true的情况。</p><p><b>6、cogroup</b>(<i>other</i>,<i>numPartitions=None</i>)</p><p>返回两个RDD当中，共同含有k的部分。如果没有，则填上null。</p><div class=\"highlight\"><pre><code class=\"language-text\">&gt;&gt;&gt; x = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 4)])\n&gt;&gt;&gt; y = sc.parallelize([(&#34;a&#34;, 2)])\n&gt;&gt;&gt; [(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]\n[(&#39;a&#39;, ([1], [2])), (&#39;b&#39;, ([4], []))]</code></pre></div><p><b>7、collect</b>()</p><p>这个比较容易理解， 把RDD所有的数据放到一个list里面去。</p><p>但是数据量不能大。</p><p>8、<b>collectAsMap</b>()</p><p>跟collect差不多， 只不多返回的是map形式。</p><p><b>9   、combineByKey</b>(<i>createCombiner</i>,<i>mergeValue</i>,<i>mergeCombiners</i>,<i>numPartitions=None</i>,<i>partitionFunc</i>)</p><p>这里有一个介绍：</p><a href=\"https://link.zhihu.com/?target=https%3A//www.jianshu.com/p/b77a6294f31c\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark核心RDD：combineByKey函数详解</a><p>感觉这个函数应该是很多相类似的聚合函数的鼻祖了。我们根据上面链接的介绍再重复一下，通过这个介绍，其实我们也能看到前面介绍的aggrate函数的影子。</p><p>这个函数里面有三个函数：</p><p>Users provide three functions:</p><ul><li><b>createCombiner</b>, which turns a V into a C (e.g., creates a one-element list)</li><li><b>mergeValue</b>, to merge a V into a C (e.g., adds it to the end of a list)</li><li><b>mergeCombiners</b>, to combine two C’s into a single one (e.g., merges the lists)</li></ul><p>这三个函数跟aggregate函数非常类似，主要的不同在于初始值的不同：</p><p>aggregate是一开始设定了初始值zeroValue，而createCombiner 则是通过一开始的V值进行转化得到初始值。然后再跟aggregate 类似，不断的进行处理。</p><div class=\"highlight\"><pre><code class=\"language-text\">&gt;&gt;&gt; x = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 1), (&#34;a&#34;, 2)])\n&gt;&gt;&gt; def to_list(a):\n...     return [a]\n...\n&gt;&gt;&gt; def append(a, b):\n...     a.append(b)\n...     return a\n...\n&gt;&gt;&gt; def extend(a, b):\n...     a.extend(b)\n...     return a\n...\n&gt;&gt;&gt; sorted(x.combineByKey(to_list, append, extend).collect())\n[(&#39;a&#39;, [1, 2]), (&#39;b&#39;, [1])]</code></pre></div><p>10、<b>count</b>()</p><p>返回RDD中的元素数。</p><p>11、<b>countByValue</b>()， <b>countBykey</b>()</p><p><b>countByValue</b>() 是返回里面每个值的个数</p><p><b>countBykey</b>()里面是map的形式，返回的是key的个数。</p><p>12、flatMap, flatMapvalues</p><p>就是把一个list里面的内容压扁打平。flatMapValues()则是把key-value形式的数据打平。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>13、fullOuterJoin</b>(<i>other</i>,<i>numPartitions=None</i>)</p><p>看代码即可</p><div class=\"highlight\"><pre><code class=\"language-text\">&gt;&gt;&gt; x = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 4)])\n&gt;&gt;&gt; y = sc.parallelize([(&#34;a&#34;, 2), (&#34;c&#34;, 8)])\n&gt;&gt;&gt; sorted(x.fullOuterJoin(y).collect())\n[(&#39;a&#39;, (1, 2)), (&#39;b&#39;, (4, None)), (&#39;c&#39;, (None, 8))]</code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/69771204", 
            "userName": "卡门", 
            "userLink": "https://www.zhihu.com/people/23533079660fe77ddb1abcf0b15c72b2", 
            "upvote": 0, 
            "title": "真正例、假正例、ROC、AUC--记起来真难啊", 
            "content": "<p>在机器学习的日常中，经常会用到这些TP、FP、ROC、AUC的一些概念，但是我对于这些东西一直没有仔细的记住，导致有时候会发生混淆。在这里再重新梳理一下，希望能有助于记忆，毕竟，记忆是理解的基础嘛。</p><p>首先，我们从常见的图表开始说起</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-be4373e0cff4c73205ca224b4d4a1860_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"575\" data-rawheight=\"196\" class=\"origin_image zh-lightbox-thumb\" width=\"575\" data-original=\"https://pic1.zhimg.com/v2-be4373e0cff4c73205ca224b4d4a1860_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;575&#39; height=&#39;196&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"575\" data-rawheight=\"196\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"575\" data-original=\"https://pic1.zhimg.com/v2-be4373e0cff4c73205ca224b4d4a1860_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-be4373e0cff4c73205ca224b4d4a1860_b.jpg\"/></figure><p>在这里主要有四个概念：</p><p>TP(真正例)、FP（假正例）、FN（假负例）、TN(真负例)</p><p>这四个概念真真假假的，的确有点晕啊。其实这里最重要的一点：</p><p>整个概念是对<b>预测样例结果  正确性</b>的判断。</p><p>P/N是预测结果的正负， T/F是对预测结果正确与否的判断。也就是说：</p><p>TP是判断为正例(P), 判断是正确的(T)     (也就是实际就是正例)；</p><p>TN是判断为负例(N)， 判断是正确的(T)   (也就是实际就是负例);</p><p>FP是判断为正例(N)， 判断是错误的(T)    (也就是实际是负例);</p><p>FN是判断为负例(N)， 判断是错误的(T)    (也就是实际是正例);</p><p>这样就相对要好记一些了。</p><p>基于这些概念，产出了其他的一些概念：</p><ol><li><b>正确率（accuracy）：</b></li></ol><p>(TP+TN)/(P+N), 也就是所有判断正确的样例在所有样子中的占比。P+N即为所有样例数， T* 即为判断正确的样例。</p><p><b>2、错误率（error rate)</b><br/>跟正确率相对，(FP+FN)/(P+N)， 即所有判断错误的样例在所有样子中的占比， error rate = 1 - accuracy</p><p><b>3、精度（precision） </b></p><p>precision=TP/(TP+FP)， 也就是在模型判断为正例的样本中，判断正确的个数占比。</p><p><b>4、召回率（recall） </b></p><p>recall=TP/(TP+FN)=TP/P=sensitivity，表示在所有的正例中，我判断为正例的有多少个，也就是我召回了多少个。</p><p>在精度和召回率中，分子都是正确判断为正例的个数，有区别的地方在分母：分母是所有判断为正例的样本，则是精度；如果分母是所有的真正的的正例，则是召回。</p><p><b>5、F1-score</b></p><p><img src=\"https://www.zhihu.com/equation?tex=F1%3D%5Cfrac%7B2+%5Ctimes+precision+%5Ctimes+recall%7D%7Bprecision+%2B+recall%7D%E2%80%8B\" alt=\"F1=\\frac{2 \\times precision \\times recall}{precision + recall}​\" eeimg=\"1\"/> <br/></p><p>在这些评价指标中，均对模型做出了一定的评价，但是只是对某些方面的指标做出了判断。真正常用的指标，还包括ROC,AUC等。</p><p><b>ROC和AUC</b></p><p>ROC是受试者工作特征曲线的简写。</p><p>ROC曲线的两个坐标轴分别是：纵坐标是真阳性率， TPR；横坐标是假阳性率，FPR。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-394cf77b9e449a1f07bc2ff92c8743b4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"417\" data-rawheight=\"364\" class=\"content_image\" width=\"417\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;417&#39; height=&#39;364&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"417\" data-rawheight=\"364\" class=\"content_image lazy\" width=\"417\" data-actualsrc=\"https://pic1.zhimg.com/v2-394cf77b9e449a1f07bc2ff92c8743b4_b.jpg\"/></figure><p>这两个指标的计算方法是：</p><p>真阳性率TPR是 TP/P ,也就是在所有正例当中，正确判断为正例的占的比例，实际就是精度；</p><p>假阳性率FPR是FP/N, 也就是在所有负例当中， 错误的判断为正例所占的比例。</p><p>可以看到两者的分母是不同的，所以两者之和实际上没什么意义。有意思的是分子部分，两者瓜分了判断为正例的部分。</p><p>设想两个极端情况：</p><p>假如所有的样例判断为负样本，那么TPR和FPR肯定都是0,所有在坐标轴上表现为起始点为（0，0）；假如所有的样例判断为正样本， 那么TP = P, FP = N, 因为都分到了正样本里。所以在坐标轴上表现为（1，1）； </p><p>那假如是随机进行的判断呢？</p><p>假设有样本点随机的进行了正负样例的判断，其中正样本有pM个， 负样本有（1-p）M个， 总共有M个。我们选取TOP M‘个，由于是随机做的判断， 那么在M’里面，同样有pM&#39;的是正样本， （1-p）M&#39;是负样本，那么TPR = FPR = M&#39;/M , 在坐标轴上表现为一条斜率为0.5的直线。</p><p>那么，假如我们的模型只要稍微的表现的好一点，就能把这条直线往左上方拉，也就是使得模型表现得更好了。</p><p><b>ROC的绘制</b></p><p>ROC有两种画法：</p><p>一种是将所有的样本按照预测的概率的大小按照从大到小的概率进行排列，然后逐个的扩大选取范围，比如：</p><p>我们按照概率大小，从大到小排列样本点，实际样本点为：</p><p>p(0.9), p(0.8), n(0.75), p(0.72), n(0.7)</p><p>总共有3个正样本， 2个负样本，那么应该有：</p><p>第一个点是（0，0）；</p><p>第二个位置点应该是（1/3, 0/2）;</p><p>第三个位置点应该是（2/3, 0/2）;</p><p>第四个位置点应该是（2/3, 0/2）;<br/>第五个位置点应该是（3/3, 1/2）;<br/>第六个位置点应该是（3/3, 2/2）（也就是点（1，1））；</p><p>还有一种画法，就是首先统计正负样本数的个数P,N</p><p>然后纵轴划分为1/P, 横轴划分为1/N， 同样按照概率值，按照概率由大到小的概率排序。逐个选取样本，当样本实际为正时，向纵轴画一条1/P长度的线；当样本实际为负时，向横轴画一条1/N长度的线。当所有的遍历完时，也就到达了（1，1），绘制完毕。</p><p><b>AUC曲线</b></p><p>AUC实际上是ROC曲线下面的面积。ROC在随机的时候是一条斜率为0.5的曲线，因此AUC最小也是0.5。</p><p>那么在实际中，我们如何计算AUC呢？总不能真的先画出ROC,然后再去积分求面积吧？</p><p>AUC有一个等价的性质，那就是：测试任意给一个正类样本和一个负类样本，正类样本的score有多大的概率大于负类样本的score。</p><p>根据这个性质，我们就可以进行计算：</p><blockquote>具体来说就是统计一下所有的 M×N(M为正类样本的数目，N为负类样本的数目)个正负样本对中，有多少个组中的正样本的score大于负样本的score。当二元组中正负样本的 score相等的时候，按照0.5计算。然后除以MN。实现这个方法的复杂度为O(n^2)。n为样本数（即n=M+N） </blockquote><div class=\"highlight\"><pre><code class=\"language-text\">用代码实现就是\ndef auc(labels,preds,n_bins=100):\n    postive_len = sum(labels)\n    negative_len = len(labels) - postive_len\n    total_case = postive_len * negative_len ##计算所有可能的个数\n    pos_histogram = [0 for _ in range(n_bins)]\n    neg_histogram = [0 for _ in range(n_bins)]\n    bin_width = 1.0 / n_bins\n    for i in range(len(labels)): ##在对应的位置上+1， 该位置表明该样本概率的大小，值表示有多少个样本落到了这个概率上。\n                                                ##在正负概率进行比价的时候，只需要比较位置就行了。\n        nth_bin = int(preds[i]/bin_width)\n        if labels[i]==1:\n            pos_histogram[nth_bin] += 1\n        else:\n            neg_histogram[nth_bin] += 1\n    accumulated_neg = 0\n    satisfied_pair = 0\n\n    for i in range(n_bins):\n##这里解释一下：\n##首先*0.5的部分就是两者的概率相等的时候，因为pos和neg都落在了同样的位置上，\n##所以是同样的概率，而两者相乘表示总共的可能性的多少，然后再*0.5即可；\n##而pos * neg累加的部分，是因为\n##这个位置的整理，概率可能大于之前所有位置的负例，因为“位置即概率”\n##所以把之前所有的负例都累加起来，跟这个位置正例相乘得到总的个数。\n        satisfied_pair += (pos_histogram[i]*accumulated_neg + pos_histogram[i]*neg_histogram[i]*0.5)\n        accumulated_neg += neg_histogram[i]\n## 统计出来的个数除以总个数就可以了。\n    return satisfied_pair / float(total_case)</code></pre></div><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/37204589", 
            "userName": "卡门", 
            "userLink": "https://www.zhihu.com/people/23533079660fe77ddb1abcf0b15c72b2", 
            "upvote": 11, 
            "title": "直觉理解LSTM和GRU", 
            "content": "<p>网上有很多对于LSTM以及GRU的介绍，主要从构造方面进行了进行了介绍。但是由于构造相对较复杂，而且涉及到的变量较多，往往不那么容易记住。下面我们从直觉的角度来对这两者进行介绍，方便理解和记忆。其中的更深刻的理论原理，可以参看相关的文章和论文。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>LSTM</b></p><p>对于LSTM的经常被提到的一篇文章就是 <a href=\"https://link.zhihu.com/?target=http%3A//colah.github.io/posts/2015-08-Understanding-LSTMs/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">理解LSTM</a> 。我们也基于此进行介绍。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>首先， LSTM最常见到的结构如下图：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e94d22b39cceee9f9a40e16a9be81e81_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2233\" data-rawheight=\"839\" class=\"origin_image zh-lightbox-thumb\" width=\"2233\" data-original=\"https://pic2.zhimg.com/v2-e94d22b39cceee9f9a40e16a9be81e81_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2233&#39; height=&#39;839&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2233\" data-rawheight=\"839\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2233\" data-original=\"https://pic2.zhimg.com/v2-e94d22b39cceee9f9a40e16a9be81e81_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e94d22b39cceee9f9a40e16a9be81e81_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>每个矩形称之为一个Unit。每个Unit有两个输入和两个输出： C 和 h。其中， C是Unit或者Cell的信息。这个信息不仅仅包含本unit的信息，还包含了之前所有Unit的信息。这种包含不是全部的包含，而是有所取舍的。这种取舍体现在了该Unit以及其余Unit的构造当中。而h里面包含的，主要是重点用于当前unit的信息，当前这部分的信息会进行输出。也就是说，会影响输出的，主要就是这部分的信息。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-06459a8772a76924608562113a7449a8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1826\" data-rawheight=\"564\" class=\"origin_image zh-lightbox-thumb\" width=\"1826\" data-original=\"https://pic1.zhimg.com/v2-06459a8772a76924608562113a7449a8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1826&#39; height=&#39;564&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1826\" data-rawheight=\"564\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1826\" data-original=\"https://pic1.zhimg.com/v2-06459a8772a76924608562113a7449a8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-06459a8772a76924608562113a7449a8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>C之间信息传递如图。在最上面的一层中， <img src=\"https://www.zhihu.com/equation?tex=C_%7Bt-1%7D\" alt=\"C_{t-1}\" eeimg=\"1\"/> 到 <img src=\"https://www.zhihu.com/equation?tex=C_t\" alt=\"C_t\" eeimg=\"1\"/> 主要有两步：丢弃一部分信息（对应乘号），加上一部分信息（对应加号）。这样就完成了上面一个Unit信息到本Unit信息的传递了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>那么要丢弃多少，就要由forget门来决定了：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-10bcec2996df5403769fe0570773b1c1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1826\" data-rawheight=\"564\" class=\"origin_image zh-lightbox-thumb\" width=\"1826\" data-original=\"https://pic2.zhimg.com/v2-10bcec2996df5403769fe0570773b1c1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1826&#39; height=&#39;564&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1826\" data-rawheight=\"564\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1826\" data-original=\"https://pic2.zhimg.com/v2-10bcec2996df5403769fe0570773b1c1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-10bcec2996df5403769fe0570773b1c1_b.jpg\"/></figure><p>这也是forget门的来历。通过学习，让forget门来学习究竟应该忘掉多少。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>那么添加的信息来自哪一部分呢？来自 <img src=\"https://www.zhihu.com/equation?tex=h_%7Bt-1%7D\" alt=\"h_{t-1}\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=x_t\" alt=\"x_t\" eeimg=\"1\"/> 。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f90f977f3a6a562800244fdd62a409e9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1826\" data-rawheight=\"564\" class=\"origin_image zh-lightbox-thumb\" width=\"1826\" data-original=\"https://pic2.zhimg.com/v2-f90f977f3a6a562800244fdd62a409e9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1826&#39; height=&#39;564&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1826\" data-rawheight=\"564\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1826\" data-original=\"https://pic2.zhimg.com/v2-f90f977f3a6a562800244fdd62a409e9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f90f977f3a6a562800244fdd62a409e9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>它分为了两部分：</p><p>一部分是生成新信息，也就是 <img src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7BC%7D_t\" alt=\"\\tilde{C}_t\" eeimg=\"1\"/> 。</p><p>但是这部分新生成的信息要把多少给 <img src=\"https://www.zhihu.com/equation?tex=%5Ctilde+C_t\" alt=\"\\tilde C_t\" eeimg=\"1\"/> , 这个要由 <img src=\"https://www.zhihu.com/equation?tex=i_t\" alt=\"i_t\" eeimg=\"1\"/> 输入门这部分来决定。</p><p>这样经过 <img src=\"https://www.zhihu.com/equation?tex=i_t\" alt=\"i_t\" eeimg=\"1\"/> 之后，跟之前保留的信息相加，  <img src=\"https://www.zhihu.com/equation?tex=+C_t\" alt=\" C_t\" eeimg=\"1\"/> 的信息就完成了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>但是 <img src=\"https://www.zhihu.com/equation?tex=h_t\" alt=\"h_t\" eeimg=\"1\"/> 还没有完成： 还需要搞清楚， 在所有的信息 <img src=\"https://www.zhihu.com/equation?tex=+C_t\" alt=\" C_t\" eeimg=\"1\"/> 里面，哪些是我本Unit需要关注，并用于输出的。</p><p>这一部分就是：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-34dac4cae7569175c60cc3abc1af7aa2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1826\" data-rawheight=\"564\" class=\"origin_image zh-lightbox-thumb\" width=\"1826\" data-original=\"https://pic3.zhimg.com/v2-34dac4cae7569175c60cc3abc1af7aa2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1826&#39; height=&#39;564&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1826\" data-rawheight=\"564\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1826\" data-original=\"https://pic3.zhimg.com/v2-34dac4cae7569175c60cc3abc1af7aa2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-34dac4cae7569175c60cc3abc1af7aa2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这个也是分为了两部分：</p><p>一部分是根据 <img src=\"https://www.zhihu.com/equation?tex=C_t\" alt=\"C_t\" eeimg=\"1\"/> 生成了信息，但是至于要多少， 让输出门 <img src=\"https://www.zhihu.com/equation?tex=o_t\" alt=\"o_t\" eeimg=\"1\"/> 来决定吧。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>GRU</b></p><p>在GRU里面，不再有C和h两部分，而是只用了h。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6cbb0ae8b5d0650649b1593b85398d23_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1520\" data-rawheight=\"699\" class=\"origin_image zh-lightbox-thumb\" width=\"1520\" data-original=\"https://pic4.zhimg.com/v2-6cbb0ae8b5d0650649b1593b85398d23_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1520&#39; height=&#39;699&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1520\" data-rawheight=\"699\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1520\" data-original=\"https://pic4.zhimg.com/v2-6cbb0ae8b5d0650649b1593b85398d23_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6cbb0ae8b5d0650649b1593b85398d23_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>h同样有一个忘记一部分旧信息、添加一部分新信息的过程，如上面红线圈出的部分。这个忘记、添加跟LSTM有点不同： 在LSTM中，忘记和添加的比例是学习来的，两者没什么相关；而在GRU中，这个比例是固定的：忘记了多少比例，那么新的信息就添加多少比例。这个由 （1-）来进行控制。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>而新的信息的生成，是由下图中红色部分来标识的。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8f1ac3ed831bb9da0421cba9510562b9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1373\" data-rawheight=\"570\" class=\"origin_image zh-lightbox-thumb\" width=\"1373\" data-original=\"https://pic2.zhimg.com/v2-8f1ac3ed831bb9da0421cba9510562b9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1373&#39; height=&#39;570&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1373\" data-rawheight=\"570\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1373\" data-original=\"https://pic2.zhimg.com/v2-8f1ac3ed831bb9da0421cba9510562b9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8f1ac3ed831bb9da0421cba9510562b9_b.jpg\"/></figure><p>可以看出 <img src=\"https://www.zhihu.com/equation?tex=z_t\" alt=\"z_t\" eeimg=\"1\"/> 作为update们，控制了忘记和输入。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>而新的信息时怎么生成的呢？</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6cd600ff0b21080a48f5197d6c55eec3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1500\" data-rawheight=\"596\" class=\"origin_image zh-lightbox-thumb\" width=\"1500\" data-original=\"https://pic4.zhimg.com/v2-6cd600ff0b21080a48f5197d6c55eec3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1500&#39; height=&#39;596&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1500\" data-rawheight=\"596\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1500\" data-original=\"https://pic4.zhimg.com/v2-6cd600ff0b21080a48f5197d6c55eec3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6cd600ff0b21080a48f5197d6c55eec3_b.jpg\"/></figure><p>由上图中可以看到，这部分是由 <img src=\"https://www.zhihu.com/equation?tex=h_%7Bt-1%7D\" alt=\"h_{t-1}\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=x_t+\" alt=\"x_t \" eeimg=\"1\"/> 共同生成的。但是 <img src=\"https://www.zhihu.com/equation?tex=h_%7Bt-1%7D\" alt=\"h_{t-1}\" eeimg=\"1\"/> 并不是用了全部，而是用了一部分。至于这个“一部分”究竟是多少，这个由 reset门<img src=\"https://www.zhihu.com/equation?tex=r_t\" alt=\"r_t\" eeimg=\"1\"/> 来控制。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>隐藏信息和输入信息决定了所有的“门”，通常一个sigmoid称之为一个门，它来控制信息的传递。</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "RNN", 
                    "tagLink": "https://api.zhihu.com/topics/20086967"
                }, 
                {
                    "tag": "LSTM", 
                    "tagLink": "https://api.zhihu.com/topics/20023220"
                }
            ], 
            "comments": [
                {
                    "userName": "haijin jiang", 
                    "userLink": "https://www.zhihu.com/people/91e2911ae6be0c3032eb1ed3b8e29aa9", 
                    "content": "从整体上描述整个网络的构建思路，通俗易懂", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "讲得很棒", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/37477611", 
            "userName": "卡门", 
            "userLink": "https://www.zhihu.com/people/23533079660fe77ddb1abcf0b15c72b2", 
            "upvote": 84, 
            "title": "cbow 与 skip-gram的比较", 
            "content": "<p>cbow和skip-gram都是在word2vec中用于将文本进行向量表示的实现方法，具体的算法实现细节可以去看word2vec的原理介绍文章。我们这里大体讲下两者的区别，尤其注意在使用当中的不同特点。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在cbow方法中，是用周围词预测中心词，从而利用中心词的预测结果情况，使用GradientDesent方法，不断的去调整周围词的向量。当训练完成之后，每个词都会作为中心词，把周围词的词向量进行了调整，这样也就获得了整个文本里面所有词的词向量。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>要注意的是， cbow的对周围词的调整是统一的：求出的gradient的值会同样的作用到每个周围词的词向量当中去。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>可以看到，cbow预测行为的次数跟整个文本的词数几乎是相等的（每次预测行为才会进行一次backpropgation, 而往往这也是最耗时的部分），复杂度大概是O(V);</p><p class=\"ztext-empty-paragraph\"><br/></p><p>而skip-gram是用中心词来预测周围的词。在skip-gram中，会利用周围的词的预测结果情况，使用GradientDecent来不断的调整中心词的词向量，最终所有的文本遍历完毕之后，也就得到了文本所有词的词向量。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>可以看出，skip-gram进行预测的次数是要多于cbow的：因为每个词在作为中心词时，都要使用周围词进行预测一次。这样相当于比cbow的方法多进行了K次（假设K为窗口大小），因此时间的复杂度为O(KV)，训练时间要比cbow要长。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>但是在skip-gram当中，每个词都要收到周围的词的影响，每个词在作为中心词的时候，都要进行K次的预测、调整。因此， 当数据量较少，或者词为生僻词出现次数较少时， 这种多次的调整会使得词向量相对的更加准确。因为尽管cbow从另外一个角度来说，某个词也是会受到多次周围词的影响（多次将其包含在内的窗口移动），进行词向量的跳帧，但是他的调整是跟周围的词一起调整的，grad的值会平均分到该词上， 相当于该生僻词没有收到专门的训练，它只是沾了周围词的光而已。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>因此，从更通俗的角度来说：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在skip-gram里面，每个词在作为中心词的时候，实际上是 1个学生 VS K个老师，K个老师（周围词）都会对学生（中心词）进行“专业”的训练，这样学生（中心词）的“能力”（向量结果）相对就会扎实（准确）一些，但是这样肯定会使用更长的时间；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>cbow是 1个老师 VS K个学生，K个学生（周围词）都会从老师（中心词）那里学习知识，但是老师（中心词）是一视同仁的，教给大家的一样的知识。至于你学到了多少，还要看下一轮（假如还在窗口内），或者以后的某一轮，你还有机会加入老师的课堂当中（再次出现作为周围词），跟着大家一起学习，然后进步一点。因此相对skip-gram，你的业务能力肯定没有人家强，但是对于整个训练营（训练过程）来说，这样肯定效率高，速度更快。</p><p>所以，这两者的取舍，要看你自己的需求是什么了。</p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "word2vec", 
                    "tagLink": "https://api.zhihu.com/topics/19886836"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": [
                {
                    "userName": "夏正", 
                    "userLink": "https://www.zhihu.com/people/cd817a35071d1568a6ac6ad243e3ae99", 
                    "content": "<p>鉴于skip-gram学习的词向量更细致，但语料库中有大量低频词时，使用skip-gram学习比较合适</p>", 
                    "likes": 10, 
                    "childComments": [
                        {
                            "userName": "chenwi", 
                            "userLink": "https://www.zhihu.com/people/f37f654f5642c5b50b16de376ef8647e", 
                            "content": "<p>确实，这博主也不改</p>", 
                            "likes": 0, 
                            "replyToAuthor": "夏正"
                        }
                    ]
                }, 
                {
                    "userName": "降空", 
                    "userLink": "https://www.zhihu.com/people/f06bb6ed1d1fd6cd54c3e4abd35944bc", 
                    "content": "<p>感谢！</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "晴蓝", 
                    "userLink": "https://www.zhihu.com/people/11b91d40508ba40e6cf98b9e128f3a5d", 
                    "content": "<p>感谢楼主分享经验</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "啦啦啦爱吃瓜", 
                    "userLink": "https://www.zhihu.com/people/46e498d121821d48b8ae39f74ece585b", 
                    "content": "<p>这比喻简直perfect！</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/36496458", 
            "userName": "卡门", 
            "userLink": "https://www.zhihu.com/people/23533079660fe77ddb1abcf0b15c72b2", 
            "upvote": 18, 
            "title": "LDA的理解", 
            "content": "<p>一直有点搞不懂LDA的原理，尤其里面吉布斯采样，共轭，以及他们和LDA的原理有什么关系。现在大概的将其他人的解释梳理一下，参考的文章见后文附录。</p><p>理解LDA需要理解两个部分：一个是原理部分，就是说他为什么要进行这么一个处理方法，这个相对来说比较容易懂；另外一个是实际操作部分，就是在实际处理的时候，我们要怎么去实现这个原理。实际操作部分需要数学上的推理来进行处理，主要是使用了采样方法。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>LDA原理</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>我们首先来讲原理部分。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-67fd06cbd15eddc1adf45c776c86a3c1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"494\" data-rawheight=\"244\" class=\"origin_image zh-lightbox-thumb\" width=\"494\" data-original=\"https://pic2.zhimg.com/v2-67fd06cbd15eddc1adf45c776c86a3c1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;494&#39; height=&#39;244&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"494\" data-rawheight=\"244\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"494\" data-original=\"https://pic2.zhimg.com/v2-67fd06cbd15eddc1adf45c776c86a3c1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-67fd06cbd15eddc1adf45c776c86a3c1_b.jpg\"/></figure><p>LDA的原理可以简单概括为：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>首先从文章里面进行主题的采样，也就是从文章里面选择一个主题，然后再从这个主题里面进行词的采样，也就是选出一个词来。这样不断的重复，最后形成了整个文档。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这里面的一些点展开说一下：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1、“从文章里面选择一个主题”，实际就是从文章的主题里面进行采样。进行主题的采样的时候，要按照其主题的分布情况进行。什么意思呢，就是说假定在文档d中， 主题z中n个主题[k1, k2, …kn]，每个主题是有不同的概率的，我们用 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 来标识 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta+%3D+%5B%5Ctheta_1%2C+%5Ctheta_2%2C+%E2%80%A6+%5Ctheta_n%5D\" alt=\" \\theta = [\\theta_1, \\theta_2, … \\theta_n]\" eeimg=\"1\"/> 。我们称 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 为主题的分布情况。我们在进行采样的时候，要按照这个分布（也就是其概率密度）进行采样。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>什么叫做按照分布采样，或者说按照概率密度采样呢？就是在其概率高的地方（概率密度函数值比较大的地方），采的样本多一点，在其概率小的地方（概率密度函数比较小的地方），采的样本少一点。这样我们采样所得的结果，才能跟真实的数据情况较吻合，也就是比较符合其概率分布情况。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>具体到LDA问题，就是在某个主题概率比较大的话，那么抽样出这个主题的次数应该比较多。反之，就应该较少。同样，从主题里面抽词的分布的时候，也是这个意思。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2、在1中提到的主题分布中，其分布的参数我们假定为了 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 。那么这个 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 该如何设置值呢？</p><p class=\"ztext-empty-paragraph\"><br/></p><p>比如说我们有3个主题，分别为[体育， 笑话，美妆]，每一个的概率为[0.6,0.3,0.1]，那么这个分布就是他的主题分布情况，其 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> = [0.6, 0.3, 0.1]。 假定我们有无数个主题，那么 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 密密麻麻，可能就更像一个连续的分布了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>但是在实际的问题当中，我们当然不会知道 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 是什么样的情况。 但是， 根据贝叶斯学派的理念，这个 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 应该也是服从某个分布的。它的这个分布决定了 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 应该是什么样子。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>那么在LDA里面， <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 应该服从什么样子的分布呢？首先，从主题分布里面抽样的时候，是多项式分布的。我们对 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 的分布选择的是参数为 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/> 的狄利克雷分布  Dirichlet分布 。为什么选择这个呢？首先这个分布可以模拟出很多的概率分布情况，还有就是狄利克雷分布跟多项式分布是共轭的。共轭的意思就是说： 原先 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 的分布是以 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/> 为参数的狄利克雷分布，以 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 为参数的主题分布是多项式分布，两者相乘之后， 得到的仍然是一个狄利克雷分布，只不过 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 分布的参数发生了变化而已。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>看下<a href=\"https://link.zhihu.com/?target=https%3A//yuedu.baidu.com/ebook/d0b441a8ccbff121dd36839a.html%3Ff%3Dread\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">LDA漫游指南</a> 对使用狄利克雷共轭好处的介绍（这里是用 二项分布和beta分布做的解释，但是原理是一样的。）</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f0ed98cfaf48c2cd36eaca1976e278ff_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"691\" class=\"origin_image zh-lightbox-thumb\" width=\"720\" data-original=\"https://pic4.zhimg.com/v2-f0ed98cfaf48c2cd36eaca1976e278ff_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;720&#39; height=&#39;691&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"691\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"720\" data-original=\"https://pic4.zhimg.com/v2-f0ed98cfaf48c2cd36eaca1976e278ff_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f0ed98cfaf48c2cd36eaca1976e278ff_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这样 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 的期望值就可以根据 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/> 来求了。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9bda554b7f8e923aa587aa59a54db9e2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"540\" data-rawheight=\"82\" class=\"origin_image zh-lightbox-thumb\" width=\"540\" data-original=\"https://pic3.zhimg.com/v2-9bda554b7f8e923aa587aa59a54db9e2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;540&#39; height=&#39;82&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"540\" data-rawheight=\"82\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"540\" data-original=\"https://pic3.zhimg.com/v2-9bda554b7f8e923aa587aa59a54db9e2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9bda554b7f8e923aa587aa59a54db9e2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>LDA的实现</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>上面我们介绍了LDA的原理，那么接下来的问题是：我们该如何实现呢？</p><p class=\"ztext-empty-paragraph\"><br/></p><p>首先P(w,z)我们是可以计算出来的</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5c5a1dabbbc2f2df1f275481cf6011f9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"333\" data-rawheight=\"60\" class=\"content_image\" width=\"333\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;333&#39; height=&#39;60&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"333\" data-rawheight=\"60\" class=\"content_image lazy\" width=\"333\" data-actualsrc=\"https://pic2.zhimg.com/v2-5c5a1dabbbc2f2df1f275481cf6011f9_b.jpg\"/></figure><p>但是我们实际上最想求的是：</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-076d04ce9e535f7d6df90999daa9da86_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"276\" data-rawheight=\"78\" class=\"content_image\" width=\"276\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;276&#39; height=&#39;78&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"276\" data-rawheight=\"78\" class=\"content_image lazy\" width=\"276\" data-actualsrc=\"https://pic3.zhimg.com/v2-076d04ce9e535f7d6df90999daa9da86_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>也就是在一篇文档当中的文本向量,所能得到的每个词对应的主题的概率分布情况,也就是 <img src=\"https://www.zhihu.com/equation?tex=p%28z_1%2C+z_2%2C+z_3%2C+%E2%80%A6+z_n%7Cw_1%2C+w_2%2C+w_3%2C+%E2%80%A6w_n%29\" alt=\"p(z_1, z_2, z_3, … z_n|w_1, w_2, w_3, …w_n)\" eeimg=\"1\"/> 。但是这个是很难求出来的，是因为</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-460b2b2a3954bb8b1d7835fd1b20f000_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"570\" data-rawheight=\"94\" class=\"origin_image zh-lightbox-thumb\" width=\"570\" data-original=\"https://pic1.zhimg.com/v2-460b2b2a3954bb8b1d7835fd1b20f000_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;570&#39; height=&#39;94&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"570\" data-rawheight=\"94\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"570\" data-original=\"https://pic1.zhimg.com/v2-460b2b2a3954bb8b1d7835fd1b20f000_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-460b2b2a3954bb8b1d7835fd1b20f000_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0839339ebf176db5e0aa956d466e8d72_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"549\" data-rawheight=\"86\" class=\"origin_image zh-lightbox-thumb\" width=\"549\" data-original=\"https://pic3.zhimg.com/v2-0839339ebf176db5e0aa956d466e8d72_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;549&#39; height=&#39;86&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"549\" data-rawheight=\"86\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"549\" data-original=\"https://pic3.zhimg.com/v2-0839339ebf176db5e0aa956d466e8d72_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0839339ebf176db5e0aa956d466e8d72_b.jpg\"/></figure><p>假设有V个词，每个词有K个topic的可能，那么这属于一个 <img src=\"https://www.zhihu.com/equation?tex=K%5EV\" alt=\"K^V\" eeimg=\"1\"/> 的问题，很难解。</p><p>因此我们可以采用吉布斯采样的方法，用采样的结果来模拟主题分布的情况。只要采样的结果符合主题分布，词分布的情况，那么求 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%EF%BC%8C%5Cphi\" alt=\"\\theta，\\phi\" eeimg=\"1\"/> 的参数估计就用这个采样的结果来统计就好了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>PS：参数估计的作用：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-fd40ebae7d668b211d0c7508f677b581_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"882\" data-rawheight=\"511\" class=\"origin_image zh-lightbox-thumb\" width=\"882\" data-original=\"https://pic2.zhimg.com/v2-fd40ebae7d668b211d0c7508f677b581_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;882&#39; height=&#39;511&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"882\" data-rawheight=\"511\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"882\" data-original=\"https://pic2.zhimg.com/v2-fd40ebae7d668b211d0c7508f677b581_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-fd40ebae7d668b211d0c7508f677b581_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>Gibbs 采样</h2><p>对于gibbs 采样，有很多的介绍，可以参见后面引用的文章。这里大体介绍一下。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>首先， gibbs 采样是基于马尔科夫转移链稳定状态的：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>给定一个转移矩阵，如果满足细致平稳条件的话，按照矩阵里面给定的概率进行转移，最终会到达一个稳定的分布。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>转移矩阵的形式：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>|      | p0   | p1   | p2   | p3   |<br/>| ---- | ---- | ---- | ---- | ---- |<br/>| p0   |       |        |      |      |<br/>| p1   |      |      |      |      |<br/>| p2   |      |      |      |      |<br/>| p3   |      |      |      |      |</p><p class=\"ztext-empty-paragraph\"><br/></p><p>最终的概率分布就是[p0, p1, p2, p3]， 每一列都代表列当前值到当前值，以及其他三个值的概率。当达到稳定的时候，这个分布就不再变化，四行的值就是一样的了。在这个分布中，只有一个变量p，对应的有四个不同的值。</p><p>如果有两维呢，可以理解成变成了</p><p>|       | x1, y1 | x1,y2 | x2,y1 | x2,y2 |<br/>| ----- | ------ | ----- | ----- | ----- |<br/>| x1,y1 |        |       |       |       |<br/>| x1,y2 |        |       |       |       |<br/>| x2,y1 |        |       |       |       |<br/>| x2,y2 |        |       |       |       |</p><p class=\"ztext-empty-paragraph\"><br/></p><p>每一维度取值的组合。那么概率分布函数也是类似的（x1, y1  x1,y2  x2,y1  x2,y2）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如果马尔科夫链能够达到稳定状态，我们用这个稳定的转移概率作为我们的分布概率，那么从这个点，跳到的下一个点，就是我们按照分布进行的采样：因为这个点的转移，转移到哪一个点，就是按照转移概率进行的，而这个转移概率就是分布概率。这样，不断的一个点一个点的往下转， 每个点都会按照稳定的转移概率进行转移，这样得到的结果就是按照我们理想的分布进行抽样的结果了！</p><p class=\"ztext-empty-paragraph\"><br/></p><p>但是有个问题：我们手里有了一个概率分布了，我们应该怎么设计转移概率矩阵，使得这个矩阵在不断的跳转之后，能够稳定，并且稳定在了我们期望的概率分布呢？</p><p class=\"ztext-empty-paragraph\"><br/></p><p><a href=\"https://link.zhihu.com/?target=http%3A//sunyi514.github.io/2016/03/05/mcmc%25E6%2596%25B9%25E6%25B3%2595%25E5%25B0%258F%25E8%25AE%25B0/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">这里</a> 进行了很详细的解释。大体来说就是要求满足细致平稳条件：<br/> <img src=\"https://www.zhihu.com/equation?tex=+%5Cpi%28i%29P%7Bij%7D+%3D+%5Cpi%28j%29P%7Bji%7D+\" alt=\" \\pi(i)P{ij} = \\pi(j)P{ji} \" eeimg=\"1\"/> <br/>这里的 <img src=\"https://www.zhihu.com/equation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"/> 就是我们想要的概率分布。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>但是这个公式还是没有解决我们的疑惑：应该怎么设计这个转移矩阵P。下面介绍了这个过程</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ba78330ae4e94b81996328ab5c47e274_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1528\" data-rawheight=\"1240\" class=\"origin_image zh-lightbox-thumb\" width=\"1528\" data-original=\"https://pic1.zhimg.com/v2-ba78330ae4e94b81996328ab5c47e274_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1528&#39; height=&#39;1240&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1528\" data-rawheight=\"1240\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1528\" data-original=\"https://pic1.zhimg.com/v2-ba78330ae4e94b81996328ab5c47e274_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ba78330ae4e94b81996328ab5c47e274_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>引入了接受率的概念之后，经过转化就可以使得转移矩阵最终的稳定状态就是我们期望的分布概率了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在这里主要介绍的是一维的情况。当数据有多维的时候呢？应该如何进行采样，得到按照概率分布的采样结果呢？假定我们有两个点A(x1, y1) B(x1, y2)， 那么就有：</p><p><img src=\"https://www.zhihu.com/equation?tex=+P%28A%29P%28y2%7Cx1%29%3DP%28x1%2Cy1%29P%28y2%7Cx1%29%3DP%28x1%29P%28y1%7Cx1%29P%28y2%7Cx1%29%3DP%28x1%2Cy2%29P%28y1%7Cx1%29%3DP%28B%29P%28y1%7Cx1%29\" alt=\" P(A)P(y2|x1)=P(x1,y1)P(y2|x1)=P(x1)P(y1|x1)P(y2|x1)=P(x1,y2)P(y1|x1)=P(B)P(y1|x1)\" eeimg=\"1\"/> </p><p><br/><i>也就是说：在多维的时候， 如果我们固定某个维度的数据，只在这一个维度上跳来跳去的话，这个跳来跳去的过程，天然的是符合细致平稳条件的，也就是说可以通过以下的方式来抽样：</i><br/> <img src=\"https://www.zhihu.com/equation?tex=+x_1%5E%7B%28t%2B1%29%7D+%5Csim+p%28x_1%7Cx_2%5E%7B%28t%29%7D%2C+x_3%5E%7B%28t%29%7D%2C%E2%80%A6%2Cx_n%5E%7B%28t%29%7D%29+%5C%5C+x_2%5E%7B%28t%2B1%29%7D+%5Csim+p%28x_2%7Cx_1%5E%7B%28t%2B1%29%7D%2C+x_3%5E%7B%28t%29%7D%2C%E2%80%A6%2Cx_n%5E%7B%28t%29%7D%29+%5C%5C+...+%5C%5C+x_n%5E%7B%28t%2B1%29%7D+%5Csim+p%28x_n%7Cx_1%5E%7B%28t%2B1%29%7D%2C+x_2%5E%7B%28t%2B1%29%7D%2C%E2%80%A6%2Cx_%7Bn-1%7D%5E%7B%28t%2B1%29%7D%29+\" alt=\" x_1^{(t+1)} \\sim p(x_1|x_2^{(t)}, x_3^{(t)},…,x_n^{(t)}) \\\\ x_2^{(t+1)} \\sim p(x_2|x_1^{(t+1)}, x_3^{(t)},…,x_n^{(t)}) \\\\ ... \\\\ x_n^{(t+1)} \\sim p(x_n|x_1^{(t+1)}, x_2^{(t+1)},…,x_{n-1}^{(t+1)}) \" eeimg=\"1\"/> <br/>原先的点是 <br/> <img src=\"https://www.zhihu.com/equation?tex=+%28x_1%5Et%2C+x_2%5Et%2C+...x_n%5Et%29+\" alt=\" (x_1^t, x_2^t, ...x_n^t) \" eeimg=\"1\"/> <br/><i>下一轮采样的点是：</i><br/> <img src=\"https://www.zhihu.com/equation?tex=+%28x_1%5E%7Bt%2B1%7D%2C+x_2%5E%7Bt%2B1%7D%2C+...x_n%5E%7Bt%2B1%7D%29+\" alt=\" (x_1^{t+1}, x_2^{t+1}, ...x_n^{t+1}) \" eeimg=\"1\"/> </p><p><br/>因此， 只要我们知道其他维到该维度的转移概率， 不断的轮询之后，就能得到下一个采样点了。这样多轮不断的采样之后，每一个维度上都会达到转移矩阵的稳定状态。从而在固定的某一维度上， 达到这一维度上的主题分布情况。这样，跳转的样本结果就是按照主题分布情况进行的采样了。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>LDA中吉布斯采样的运用</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>那么，在LDA中，是怎么应用到这个gibbs 采样的呢？</p><p class=\"ztext-empty-paragraph\"><br/></p><p>首先，我们回忆一下：我们想要得到的是 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Cvec+z+%7C+%5Cvec+w%29\" alt=\"p(\\vec z | \\vec w)\" eeimg=\"1\"/> ，也就是 <img src=\"https://www.zhihu.com/equation?tex=p%28z_1%2C+z_2%2C+z_3%2C+%E2%80%A6+z_n%7Cw_1%2C+w_2%2C+w_3%2C+%E2%80%A6w_n%29\" alt=\"p(z_1, z_2, z_3, … z_n|w_1, w_2, w_3, …w_n)\" eeimg=\"1\"/> 。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在这里，每个 <img src=\"https://www.zhihu.com/equation?tex=z_i\" alt=\"z_i\" eeimg=\"1\"/> 都对应着一个维度，维度的多少取决于你想要给 <img src=\"https://www.zhihu.com/equation?tex=z_i+\" alt=\"z_i \" eeimg=\"1\"/> 设定的主题个数，比如64个或者128个等。所以说在这里，就恰好是gibbs可以操作的地方了：在每个维度上进行跳转，达到马尔科夫链的平稳分布就可以了。然后我们就可以使用采样结果,来计算 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 的期望结果了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>所以我们需要知道的是：</p><p><img src=\"https://www.zhihu.com/equation?tex=p%28zi+%3D+k1+%7C+z%7B-i%7D%2C+%5Cvec+w%29%2C+p%28zi+%3D+k2+%7C+z%7B-i%7D%2C+%5Cvec+w%29%2C+p%28zi+%3D+kn+%7C+z%7B-i%7D%2C+%5Cvec+w%29\" alt=\"p(zi = k1 | z{-i}, \\vec w), p(zi = k2 | z{-i}, \\vec w), p(zi = kn | z{-i}, \\vec w)\" eeimg=\"1\"/> </p><p><i>的概率是多少，然后按照这个概率分布不断的跳转就行了。其中</i> <img src=\"https://www.zhihu.com/equation?tex=z_i\" alt=\"z_i\" eeimg=\"1\"/> 表示的是第i个词对应的主题。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这个概率是可以根据以下的方法求出来：</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f887358e5026ad06dbb1b771d07c9b6f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"860\" data-rawheight=\"638\" class=\"origin_image zh-lightbox-thumb\" width=\"860\" data-original=\"https://pic4.zhimg.com/v2-f887358e5026ad06dbb1b771d07c9b6f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;860&#39; height=&#39;638&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"860\" data-rawheight=\"638\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"860\" data-original=\"https://pic4.zhimg.com/v2-f887358e5026ad06dbb1b771d07c9b6f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f887358e5026ad06dbb1b771d07c9b6f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-aea798a4f82e4fbad7ce4cdedec07032_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"496\" data-rawheight=\"105\" class=\"origin_image zh-lightbox-thumb\" width=\"496\" data-original=\"https://pic3.zhimg.com/v2-aea798a4f82e4fbad7ce4cdedec07032_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;496&#39; height=&#39;105&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"496\" data-rawheight=\"105\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"496\" data-original=\"https://pic3.zhimg.com/v2-aea798a4f82e4fbad7ce4cdedec07032_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-aea798a4f82e4fbad7ce4cdedec07032_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这里的 <img src=\"https://www.zhihu.com/equation?tex=%5Cbeta+%2C+%5Calpha+\" alt=\"\\beta , \\alpha \" eeimg=\"1\"/> 都是伪计数，可以上来进行制定。一般设定 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha+%3D+50+%2Fk+%2C+%5Cbeta+%3D+0.1\" alt=\"\\alpha = 50 /k , \\beta = 0.1\" eeimg=\"1\"/> ,而其他的值，就是抽样统计得到的结果了。在一开始的时候，这些结果可以随机的进行初始化。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这个转移概率的意思是说，求出第i维上每个主题（K个主题中的一个）的条件概率，也就是求出这个主题上的概率分布情况，然后进行采样就行了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>下面是代码：</p><div class=\"highlight\"><pre><code class=\"language-text\">//Remove topic label for w_{m,n}\n        int oldTopic = z[m][n];\n        nmk[m][oldTopic]--;\n        nkt[oldTopic][doc[m][n]]--;\n        nmkSum[m]--;\n        nktSum[oldTopic]--;\n\n        //Compute p(z_i = k|z_-i, w)\n        double [] p = new double[K];\n        // 求出第n个单词，也就是第n维的概率分布情况。\n        for(int k = 0; k &lt; K; k++){\n            p[k] = (nkt[k][doc[m][n]] + beta) / (nktSum[k] + V * beta) * (nmk[m][k] + alpha) / (nmkSum[m] + K * alpha);\n        }\n\n        //Sample a new topic label for w_{m, n} like roulette\n        //Compute cumulated probability for p\n        for(int k = 1; k &lt; K; k++){\n            p[k] += p[k - 1];\n        }\n        //在这里要注意一点， 算出来的概率是没有归一化的，所以要乘以最大值。因为这时候算出来的是 第n个词 包含第1，...K个主题下，每个词的概率。计算是sum(n)/sum(z)， 这个分布是不一样的， 所以是没有归一的。\n\n        double u = Math.random() * p[K - 1]; //p[] is unnormalised\n        int newTopic;\n        for(newTopic = 0; newTopic &lt; K; newTopic++){\n            if(u &lt; p[newTopic]){\n                break;\n            }\n        }\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>这样，跑完一篇文档，才算是采样完一个文档中的点。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>到最后就可以计算 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%EF%BC%8C%5Cphi\" alt=\"\\theta，\\phi\" eeimg=\"1\"/> ，分别代表主题分布和词分布的参数情况了。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9eb81d9a8691038b6bf237f15491f273_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"572\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb\" width=\"572\" data-original=\"https://pic4.zhimg.com/v2-9eb81d9a8691038b6bf237f15491f273_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;572&#39; height=&#39;132&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"572\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"572\" data-original=\"https://pic4.zhimg.com/v2-9eb81d9a8691038b6bf237f15491f273_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-9eb81d9a8691038b6bf237f15491f273_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-32d137d88a3388d499929295a6624f26_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"561\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb\" width=\"561\" data-original=\"https://pic3.zhimg.com/v2-32d137d88a3388d499929295a6624f26_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;561&#39; height=&#39;134&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"561\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"561\" data-original=\"https://pic3.zhimg.com/v2-32d137d88a3388d499929295a6624f26_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-32d137d88a3388d499929295a6624f26_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>LDA的一些疑惑</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>1、这里之前一直会有一个混淆：概率分布和抽样，到底是谁想要谁呢？</p><p class=\"ztext-empty-paragraph\"><br/></p><p>简单的说就是：有概率分布，但是概率分布直接求值比较困难。于是就抽样，然后由抽样来模拟概率分布的一些值。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2、为什么在词很少的时候，对于LDA来说很不合适呢？</p><p class=\"ztext-empty-paragraph\"><br/></p><p>想一下，如果某一个词只出现了1次，在马尔科夫矩阵中初始化就只有跳这个topic下有可能性，而跳转到其他的topic的概率是0的，这种情况下，即使不断的跳转，它也不会跳到其他的主题之下，从而在多次迭代之后达到马尔科夫稳定状态。这样，它的准确性就大大降低了。当然，实际中可能 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha%EF%BC%8C+%5Cbeta\" alt=\"\\alpha， \\beta\" eeimg=\"1\"/> 不会设定为0，但是可能也是很小的值，这样也会很难达到稳定状态。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>一些数学知识</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>1、二项分布</p><p>就是多重的伯努利分布（0-1分布）， 其概率分布密度函数为：<br/> <img src=\"https://www.zhihu.com/equation?tex=+C_n%5Ekp%5Ek%281-p%29%5E%7Bn-k%7D+\" alt=\" C_n^kp^k(1-p)^{n-k} \" eeimg=\"1\"/> </p><p><br/>2、多项分布</p><p class=\"ztext-empty-paragraph\"><br/></p><p>二项分布的推广，也就是说：单次的测试中，不再是0-1的分布，而是有k种可能性的分布。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>其概率分布为：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4e9f08bd4dcd15fd7e3c4431bfe64143_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"485\" data-rawheight=\"123\" class=\"origin_image zh-lightbox-thumb\" width=\"485\" data-original=\"https://pic4.zhimg.com/v2-4e9f08bd4dcd15fd7e3c4431bfe64143_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;485&#39; height=&#39;123&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"485\" data-rawheight=\"123\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"485\" data-original=\"https://pic4.zhimg.com/v2-4e9f08bd4dcd15fd7e3c4431bfe64143_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4e9f08bd4dcd15fd7e3c4431bfe64143_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>3、共轭分布：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>什么叫做共轭呢？</p><p class=\"ztext-empty-paragraph\"><br/></p><p>我们选取一个函数作为似然函数的先验概率分布， 这种情况下， 其后验分布函数与先验分布函数是一致的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>其中， 二项分布的共轭函数是beta分布， 多项式分布的共轭分布是dirichlet分布。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>引用</h2><p><a href=\"https://link.zhihu.com/?target=http%3A//sunyi514.github.io/2016/03/05/mcmc%25E6%2596%25B9%25E6%25B3%2595%25E5%25B0%258F%25E8%25AE%25B0/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">mcmc方法小记</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/yangliuy/LDAGibbsSampling/tree/master/src/liuyang/nlp/lda/main\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">LDA实现</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DgK4L7ZtGbO0\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">徐亦达视频</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//yuedu.baidu.com/ebook/d0b441a8ccbff121dd36839a.html%3Ff%3Dread\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">LDA漫游指南</a></p><p>LDA数学八卦</p><p></p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "LDA", 
                    "tagLink": "https://api.zhihu.com/topics/19565464"
                }, 
                {
                    "tag": "主题模型", 
                    "tagLink": "https://api.zhihu.com/topics/19565468"
                }
            ], 
            "comments": [
                {
                    "userName": "王毅意", 
                    "userLink": "https://www.zhihu.com/people/154ad6b9881d13771a11ece5832735aa", 
                    "content": "<p>连续分布不是指主题是无数个就连续了(想一下自然数)，而是因为概率是连续的，所以是文档-话题的概率分布是连续的，服从Dirichlet distribution</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>推荐一本书，<a href=\"http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1908.03142\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/abs/1908.0314</span><span class=\"invisible\">2</span><span class=\"ellipsis\"></span></a></p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/35459698", 
            "userName": "卡门", 
            "userLink": "https://www.zhihu.com/people/23533079660fe77ddb1abcf0b15c72b2", 
            "upvote": 0, 
            "title": "如何理解backpropgation", 
            "content": "<p>首先要明确，我们进行backpropgation的目的是什么？</p><p>因为我们是用梯度下降 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac+%7B%5Cdelta+C%7D+%7B%5Cdelta+w%7D\" alt=\"\\frac {\\delta C} {\\delta w}\" eeimg=\"1\"/> 来逐步的计算我们的参数 <img src=\"https://www.zhihu.com/equation?tex=w\" alt=\"w\" eeimg=\"1\"/> ，所以backpropagation的目的就是求出 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac+%7B%5Cdelta+C%7D+%7B%5Cdelta+w%7D\" alt=\"\\frac {\\delta C} {\\delta w}\" eeimg=\"1\"/> 值，这里的 <img src=\"https://www.zhihu.com/equation?tex=%5Comega\" alt=\"\\omega\" eeimg=\"1\"/> 代表所有层的所有参数。求出它们以后，才能让梯度下降不断的进行下去。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>DNN 层的基本定义</h2><p>整个DNN的过程大体如下：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4ccb005ba58ca43f883292e7edeee934_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"492\" data-rawheight=\"211\" class=\"origin_image zh-lightbox-thumb\" width=\"492\" data-original=\"https://pic1.zhimg.com/v2-4ccb005ba58ca43f883292e7edeee934_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;492&#39; height=&#39;211&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"492\" data-rawheight=\"211\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"492\" data-original=\"https://pic1.zhimg.com/v2-4ccb005ba58ca43f883292e7edeee934_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-4ccb005ba58ca43f883292e7edeee934_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>为了避免混淆层，以及各层所包括的参数的概念， 我们对其中的某一层的定义如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0f81d2449900b1d47fd5506b3a90d424_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"729\" data-rawheight=\"303\" class=\"origin_image zh-lightbox-thumb\" width=\"729\" data-original=\"https://pic1.zhimg.com/v2-0f81d2449900b1d47fd5506b3a90d424_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;729&#39; height=&#39;303&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"729\" data-rawheight=\"303\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"729\" data-original=\"https://pic1.zhimg.com/v2-0f81d2449900b1d47fd5506b3a90d424_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0f81d2449900b1d47fd5506b3a90d424_b.jpg\"/></figure><p>对DNN的某一层来说，该层包含了这些参数以及forward计算值。 <img src=\"https://www.zhihu.com/equation?tex=+Z%5El\" alt=\" Z^l\" eeimg=\"1\"/> 是之前通过权重之后的线性值，然后经过一个非线性的整流层。这个整流层可以是simod， 也可以是relu等。因为这个的存在，从而使的DNN有了更多表达的可能。在最后一层的过程中，情况可能会略有不同：经过的最后一层可以不是simod，也可以是softmax。相当于最后一层中的每个节点的线性结果，都要经过一个“整流函数”，来得到一个新的非线性结果，用这个来跟实际结果进行衡量结果的好坏。</p><h2>backprop 计算</h2><p>如果我们将整个DNN过程分为两步来看：中间过程以及最后一步。那么可以推出不同的式子，因为最后一步的计算跟前面的计算是不同的：前面的计算要涉及到递归的运算，而最后一步要直接的计算值了。但是为了方便理解记忆，我们尝试把他们合并起来。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>首先在DNN中， 其前向传播过程是：</p><p><img src=\"https://www.zhihu.com/equation?tex=z_i%5El+%3D+%5Csum_j+W_%7Bji%7D%5ELa_j%5E%7Bl-1%7D+%2B+b_i%5El+%5C%5Ca_i%5El+%3D+f+%28z_i%5El%29+%5C%5C+\" alt=\"z_i^l = \\sum_j W_{ji}^La_j^{l-1} + b_i^l \\\\a_i^l = f (z_i^l) \\\\ \" eeimg=\"1\"/> </p><p>以上两步重复过程，到最后计算损失函数：</p><p><img src=\"https://www.zhihu.com/equation?tex=C+%3D+C_x%28a_i%5EL%2C+y_i%29+\" alt=\"C = C_x(a_i^L, y_i) \" eeimg=\"1\"/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>我们要求 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac+%7B%5Cdelta+C+%7D%7B%5Cdelta+w_%7Bij%7D%5El%7D\" alt=\"\\frac {\\delta C }{\\delta w_{ij}^l}\" eeimg=\"1\"/> ， 首先我们有下面的式子，这个式子是统一的：</p><p><img src=\"https://www.zhihu.com/equation?tex=+%5Cfrac+%7B%5Cdelta+C+%7D%7B%5Cdelta+w_%7Bij%7D%5El%7D+%3D%5B%5Csum_m%28%5Csum_k+%5Cfrac%7B%5Cdelta+C%7D%7B%5Cdelta+z_k%5E%7Bl%2B1%7D%7D%5Cfrac+%7B%5Cdelta+z_k%5E%7Bl%2B1%7D%7D%7B%5Cdelta+a_m%5El%7D+%29%5Cfrac+%7B%5Cdelta+a_m%5El%7D%7B%5Cdelta+z_i%5El%7D%5D+%5Cfrac+%7B%5Cdelta+z_i%5El%7D%7B%5Cdelta+w_%7Bij%7D%5E%7Bl%7D%7D+%5Ctag+1\" alt=\" \\frac {\\delta C }{\\delta w_{ij}^l} =[\\sum_m(\\sum_k \\frac{\\delta C}{\\delta z_k^{l+1}}\\frac {\\delta z_k^{l+1}}{\\delta a_m^l} )\\frac {\\delta a_m^l}{\\delta z_i^l}] \\frac {\\delta z_i^l}{\\delta w_{ij}^{l}} \\tag 1\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p>(1)式其实是通过链式求导，一直传递到最后的w的过程。这个式子里面有两个括号，分别代表的含义是：</p><p>1）（）代表的是，a会受到下一层的z的影响，有可能不仅仅是一个z，而是下一层的所有的z。因此，C对下一层的z所施加的影响，都要汇总到z对a的影响上面；</p><p>2） [] 跟1） 是同理的：z会受到同一层的a的影响，可能不仅仅是同一节点的a，还包括其他节点的a。当然，在实际中，只有最后一层z会受到其它节点a的影响,在中间层里面，还是只收同一节点的a的影响。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>下面我们逐个看括号里面的式子。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1、 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac+%7B%5Cdelta+z_i%5El%7D%7B%5Cdelta+w_%7Bij%7D%5E%7Bl%7D%7D\" alt=\"\\frac {\\delta z_i^l}{\\delta w_{ij}^{l}}\" eeimg=\"1\"/> </p><p>由于不管在那一层，都有 <img src=\"https://www.zhihu.com/equation?tex=z_i%5El+%3D+W_%7Bji%7D%5Ela_j%5E%7Bl-1%7D+%2B+b%5El+\" alt=\"z_i^l = W_{ji}^la_j^{l-1} + b^l \" eeimg=\"1\"/> ， 因此这个式子的值是统一的：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac+%7B%5Cdelta+z_i+%7D%7B%5Cdelta+w_%7Bij%7D%7D+%3D+a_j%5E%7Bl-1%7D\" alt=\"\\frac {\\delta z_i }{\\delta w_{ij}} = a_j^{l-1}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>2、 <img src=\"https://www.zhihu.com/equation?tex=%5Csum+%5Ctau+%5Cfrac+%7B%5Cdelta+a_k%5El%7D%7B%5Cdelta+z_i%5El%7D+\" alt=\"\\sum \\tau \\frac {\\delta a_k^l}{\\delta z_i^l} \" eeimg=\"1\"/> </p><p>我们先把（）内的值命为 <img src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/> ,我们先着重看最外层的求和。对于中间层还是最后一层， 这个式子可能会有不同：</p><p>在中间层中，使用的是整流函数（我们这里假定是sigmod）。从 <img src=\"https://www.zhihu.com/equation?tex=z_i+%5Cto+a_i\" alt=\"z_i \\to a_i\" eeimg=\"1\"/> 的式子是 <img src=\"https://www.zhihu.com/equation?tex=a_i+%3D+%5Csigma+%28z_i%29\" alt=\"a_i = \\sigma (z_i)\" eeimg=\"1\"/> ,两者的变化只会影响该节点的两者互相的值；</p><p>因此只有 k == i 的时候有：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Csum_k+%5Ctau+%5Cfrac+%7B%5Cdelta+a_k%5El%7D%7B%5Cdelta+z_i%5El%7D+%5C%5C+%3D+%5Ctau+%5Csum_k+%5Cfrac+%7B%5Cdelta+f%28z_k%29+%7D+%7B%5Cdelta+z_i%7D+%5C%5C+%3D%5Ctau+%5Csigma%27%28z_i%29+%5Ctag+2\" alt=\"\\sum_k \\tau \\frac {\\delta a_k^l}{\\delta z_i^l} \\\\ = \\tau \\sum_k \\frac {\\delta f(z_k) } {\\delta z_i} \\\\ =\\tau \\sigma&#39;(z_i) \\tag 2\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p>而在最后一层当中，使用的可能是softmax这种形式。在最后一层的softmax中，式子是： <img src=\"https://www.zhihu.com/equation?tex=a_i+%3D+f%28z_i%29+%3D+%5Cfrac+%7Bz_i%7D+%7B%5Csum_k%5EK+z_k%7D\" alt=\"a_i = f(z_i) = \\frac {z_i} {\\sum_k^K z_k}\" eeimg=\"1\"/> , 在这里， <img src=\"https://www.zhihu.com/equation?tex=a_i\" alt=\"a_i\" eeimg=\"1\"/> 的输出值不仅仅跟 <img src=\"https://www.zhihu.com/equation?tex=z_i\" alt=\"z_i\" eeimg=\"1\"/> 有关系，还跟其他所有的 <img src=\"https://www.zhihu.com/equation?tex=z_k\" alt=\"z_k\" eeimg=\"1\"/> 有关系。反过来说，对于某一个的 <img src=\"https://www.zhihu.com/equation?tex=z_k\" alt=\"z_k\" eeimg=\"1\"/> ,所有的输出 <img src=\"https://www.zhihu.com/equation?tex=a_i\" alt=\"a_i\" eeimg=\"1\"/> 的变化都会对 <img src=\"https://www.zhihu.com/equation?tex=z_k\" alt=\"z_k\" eeimg=\"1\"/> 施加影响。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>我们可以求出：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Csum_k+%5Ctau+%5Cfrac+%7B%5Cdelta+a_k%5El%7D%7B%5Cdelta+z_i%5El%7D+%3D+%5Ctau+%5Csum_k+%5Cfrac+%7B%5Cdelta+f%28z_k%29+%7D+%7B%5Cdelta+z_i%7D+%5C%5C+%3D+%5Ctau+%5Csum_k+f%28z_k%29+%28%5Cdelta_%7Bki%7D+-+f%28z_i%29%29+%5C%5C+when+%5Cspace+i+%3D%3D+k%2C+%5Cdelta_%7Bki%7D+%3D+1+%5Ctag+3+\" alt=\"\\sum_k \\tau \\frac {\\delta a_k^l}{\\delta z_i^l} = \\tau \\sum_k \\frac {\\delta f(z_k) } {\\delta z_i} \\\\ = \\tau \\sum_k f(z_k) (\\delta_{ki} - f(z_i)) \\\\ when \\space i == k, \\delta_{ki} = 1 \\tag 3 \" eeimg=\"1\"/> </p><p>(推导见附录)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>可以看出，在一开始的推导还是一样的，只不过在最后的实际求导的时候不再一样了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3、 <img src=\"https://www.zhihu.com/equation?tex=%5Ctau+%3D+%5Csum_k+%5Cfrac%7B%5Cdelta+C%7D%7B%5Cdelta+z_k%5E%7Bl%2B1%7D%7D%5Cfrac+%7B%5Cdelta+z_k%5E%7Bl%2B1%7D%7D%7B%5Cdelta+a_m%5El%7D\" alt=\"\\tau = \\sum_k \\frac{\\delta C}{\\delta z_k^{l+1}}\\frac {\\delta z_k^{l+1}}{\\delta a_m^l}\" eeimg=\"1\"/> </p><p> 在2中， 当前这个节点的值是否收到后面所有节点的影响，会根据是否是最后一层而有所不同。而在这里， 对于a的影响直接就是C的变化，因为最后一层中：</p><p><img src=\"https://www.zhihu.com/equation?tex=C+%3D+C_x%28a_i%5EL%2C+y_i%29+\" alt=\"C = C_x(a_i^L, y_i) \" eeimg=\"1\"/> </p><p>因此可以直接计算</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Ctau+%3D+%5Csum_k+%5Cfrac%7B%5Cdelta+C%7D%7B%5Cdelta+z_k%5E%7Bl%2B1%7D%7D%5Cfrac+%7B%5Cdelta+z_k%5E%7Bl%2B1%7D%7D%7B%5Cdelta+a_m%5El%7D+%5C%5C+%3D+%5Csum_k+%5Cfrac+%7B%5Cdelta+C%7D%7B%5Cdelta+a_m%5El%7D\" alt=\"\\tau = \\sum_k \\frac{\\delta C}{\\delta z_k^{l+1}}\\frac {\\delta z_k^{l+1}}{\\delta a_m^l} \\\\ = \\sum_k \\frac {\\delta C}{\\delta a_m^l}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p>而在中间层中，由于 <img src=\"https://www.zhihu.com/equation?tex=z_i%5El+%3D+%5Csum_j+W_%7Bji%7D%5ELa_j%5E%7Bl-1%7D+%2B+b_i%5El\" alt=\"z_i^l = \\sum_j W_{ji}^La_j^{l-1} + b_i^l\" eeimg=\"1\"/> </p><p>因此节点a的变化值，的确是会受到后面所有节点z变化的影响。分别计算：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac+%7B%5Cdelta+z_k%5E%7Bl%2B1%7D%7D%7B%5Cdelta+a_m%5El%7D+%3D+w_%7Bmk%7D%5E%7Bl%2B1%7D+%5Ctag+4\" alt=\"\\frac {\\delta z_k^{l+1}}{\\delta a_m^l} = w_{mk}^{l+1} \\tag 4\" eeimg=\"1\"/> </p><p>而</p><p><img src=\"https://www.zhihu.com/equation?tex=+%5Cdelta_k%5E%7Bl%2B1%7D+%3D%5Cfrac%7B%5Cdelta+C%7D%7B%5Cdelta+z_k%5E%7Bl%2B1%7D%7D+%5Ctag+5\" alt=\" \\delta_k^{l+1} =\\frac{\\delta C}{\\delta z_k^{l+1}} \\tag 5\" eeimg=\"1\"/> </p><p> 可以通过后面一层的值递归计算出来,因此有：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Ctau+%3D+%5Csum_k+%5Cfrac%7B%5Cdelta+C%7D%7B%5Cdelta+z_k%5E%7Bl%2B1%7D%7D%5Cfrac+%7B%5Cdelta+z_k%5E%7Bl%2B1%7D%7D%7B%5Cdelta+a_m%5El%7D+%5C%5C+%3D+%5Csum_k+w_%7Bmk%7D%5E%7Bl%2B1%7D+%2A+%5Cdelta_k%5E%7Bl%2B1%7D\" alt=\"\\tau = \\sum_k \\frac{\\delta C}{\\delta z_k^{l+1}}\\frac {\\delta z_k^{l+1}}{\\delta a_m^l} \\\\ = \\sum_k w_{mk}^{l+1} * \\delta_k^{l+1}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p>因此对于中间层有：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac+%7B%5Cdelta+C+%7D%7B%5Cdelta+w_%7Bij%7D%5El%7D+%3D+%EF%BC%88W%5E%7Bl%2B1%7D%5Cdelta%5E%7Bl%2B1%7D%EF%BC%89%5Csigma%27%28z_i%5El%29a_j%5E%7Bl-1%7D+%5C%5C+%3D%EF%BC%88W%5E%7Bl%2B1%7D%5Cdelta%5E%7Bl%2B1%7D%EF%BC%89a_i%27a_j%5E%7Bl-1%7D+%5C%5C+\" alt=\"\\frac {\\delta C }{\\delta w_{ij}^l} = （W^{l+1}\\delta^{l+1}）\\sigma&#39;(z_i^l)a_j^{l-1} \\\\ =（W^{l+1}\\delta^{l+1}）a_i&#39;a_j^{l-1} \\\\ \" eeimg=\"1\"/> </p><p>对于最后一层有：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac+%7B%5Cdelta+C+%7D%7B%5Cdelta+w_%7Bij%7D%5El%7D+%3D+%EF%BC%88%5Csum_k+%5Cfrac+%7B%5Cdelta+C%7D%7B%5Cdelta+a_m%7D%EF%BC%89%2A+soft%27%28z_i%5El%29a_j%5E%7Bl-1%7D\" alt=\"\\frac {\\delta C }{\\delta w_{ij}^l} = （\\sum_k \\frac {\\delta C}{\\delta a_m}）* soft&#39;(z_i^l)a_j^{l-1}\" eeimg=\"1\"/> </p><p>同理可以求出：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac+%7B%5Cdelta+C%7D%7B%5Cdelta+b_i%7D+%3D+%28+W_%7Bij%7D%5E%7BT%7D+%5Cdelta%5E%7Bl%2B1%7D+%29+%5Codot+%5Csigma%27%28z%5El%29+\" alt=\"\\frac {\\delta C}{\\delta b_i} = ( W_{ij}^{T} \\delta^{l+1} ) \\odot \\sigma&#39;(z^l) \" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>最后一步的计算</h2><p>在最后一步，我们使用cross_entropy作为损失函数，那么就有：</p><p><img src=\"https://www.zhihu.com/equation?tex=C+%3D+%5Csum_i%5EK+y_i+log+a_i%5EL\" alt=\"C = \\sum_i^K y_i log a_i^L\" eeimg=\"1\"/> </p><p>对于最后一层来说， 每个 <img src=\"https://www.zhihu.com/equation?tex=z_k\" alt=\"z_k\" eeimg=\"1\"/> 都会影响到输出 <img src=\"https://www.zhihu.com/equation?tex=a_i\" alt=\"a_i\" eeimg=\"1\"/> ,反过来说， 每个 <img src=\"https://www.zhihu.com/equation?tex=a_i\" alt=\"a_i\" eeimg=\"1\"/> 的变化都会影响到 <img src=\"https://www.zhihu.com/equation?tex=z_k\" alt=\"z_k\" eeimg=\"1\"/> ,因此对于 <img src=\"https://www.zhihu.com/equation?tex=z_i+\" alt=\"z_i \" eeimg=\"1\"/> 的偏导需要所有 <img src=\"https://www.zhihu.com/equation?tex=a+\" alt=\"a \" eeimg=\"1\"/> 的变化之和：<br/> <img src=\"https://www.zhihu.com/equation?tex=+%5Cfrac+%7B%5Cdelta+C+%7D%7B%5Cdelta+z_i%5EL%7D+%3D+%5Csum_k%5EK%5Cfrac+%7B%5Cdelta+C%7D%7B%5Cdelta+a_k%5EL%7D+%5Cfrac+%7B%5Cdelta+a_k%5EL%7D%7B%5Cdelta+z_i%5EL%7D+%5Ctag+2\" alt=\" \\frac {\\delta C }{\\delta z_i^L} = \\sum_k^K\\frac {\\delta C}{\\delta a_k^L} \\frac {\\delta a_k^L}{\\delta z_i^L} \\tag 2\" eeimg=\"1\"/> <br/></p><p>对于上式中的第一部分，有：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac+%7B%5Cdelta+C%7D%7B%5Cdelta+a_k%5EL%7D+%3D%5Cfrac+%7B+%5Cdelta+%28%5Csum+-y_iloga_i%29+%7D+%7B%5Cdelta+a_k%5EL%7D+%5C%5C+%3D+%5Cfrac+%7B-y_k%7D+%7B+a_k%7D\" alt=\"\\frac {\\delta C}{\\delta a_k^L} =\\frac { \\delta (\\sum -y_iloga_i) } {\\delta a_k^L} \\\\ = \\frac {-y_k} { a_k}\" eeimg=\"1\"/> </p><p>其中第二部分是对f(x) = softmax(x)的求导。</p><p>我们首先求</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac+%7B%5Cdelta+f%28x_i%29+%7D+%7B%5Cdelta+x_j%7D+%3D+%5Cfrac+%7B%5Cdelta+%5Cfrac+%7Be%5E%7Bx_i%7D%7D%7B%5Csum_k%5EK+e%5Ek%7D%7D%7B%5Cdelta+x_j%7D+%5C%5C%28%E4%BB%A4+%5Ctau+%3D+%5Csum_i%5EK+e%5Ek%2C+then+%5Cspace+f%28x_i%29+%3D+e%5E%7Bx_i%7D+%2F+%5Ctau%29%5C%5C+%3D+%28%5Cdelta+_%7Bij%7D+%2Ae%5E%7Bx_i%7D%2A+%5Ctau+-+e%5E%7Bx_i%7D+%2A+e%5E%7Bx_j%7D%29+%2F%5Ctau+%5E2+%5C%5C+%3D+f%28x_i%29+%28%5Cdelta_%7Bij%7D+-+f%28x_j%29%29+%5C%5C+when+%5Cspace+i+%3D%3D+j%2C+%5Cdelta+%3D+1+\" alt=\"\\frac {\\delta f(x_i) } {\\delta x_j} = \\frac {\\delta \\frac {e^{x_i}}{\\sum_k^K e^k}}{\\delta x_j} \\\\(令 \\tau = \\sum_i^K e^k, then \\space f(x_i) = e^{x_i} / \\tau)\\\\ = (\\delta _{ij} *e^{x_i}* \\tau - e^{x_i} * e^{x_j}) /\\tau ^2 \\\\ = f(x_i) (\\delta_{ij} - f(x_j)) \\\\ when \\space i == j, \\delta = 1 \" eeimg=\"1\"/> </p><p>那么就有</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac+%7B%5Cdelta+a_k%5EL%7D%7B%5Cdelta+z_i%5EL%7D+%3D+a_k%5EL%28%5Cdelta_%7Bki%7D+-+a_i%5EL%29\" alt=\"\\frac {\\delta a_k^L}{\\delta z_i^L} = a_k^L(\\delta_{ki} - a_i^L)\" eeimg=\"1\"/> </p><p>所以（2）可以写成</p><p><img src=\"https://www.zhihu.com/equation?tex=%3D%5Csum_k+-y_k+%2A+%28%5Cdelta+_%7Bki%7D+-+a_i%29%5C%5C+%3D%5Csum_k+-y_k%5Cdelta_%7Bki%7D+%2B+y_k%2A+a_i+%5C%5C+%3D+%28%5Csum+y_k%29a_i+-+y_i\" alt=\"=\\sum_k -y_k * (\\delta _{ki} - a_i)\\\\ =\\sum_k -y_k\\delta_{ki} + y_k* a_i \\\\ = (\\sum y_k)a_i - y_i\" eeimg=\"1\"/> </p><p>也就是说，最终的结果只跟 <img src=\"https://www.zhihu.com/equation?tex=z_i\" alt=\"z_i\" eeimg=\"1\"/> 对应的那个 <img src=\"https://www.zhihu.com/equation?tex=y_i\" alt=\"y_i\" eeimg=\"1\"/> 有关系。如果是单标签的，也就是说多个y中知会有一个为1， 那么就可以化为</p><p><img src=\"https://www.zhihu.com/equation?tex=a_i+-+y_i\" alt=\"a_i - y_i\" eeimg=\"1\"/> </p><p>而 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac+%7B%5Cdelta+z_i+%7D%7B%5Cdelta+w_%7Bij%7D%7D+%3D+a_j%5E%7BL-1%7D\" alt=\"\\frac {\\delta z_i }{\\delta w_{ij}} = a_j^{L-1}\" eeimg=\"1\"/> </p><p>因此有</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac+%7B%5Cdelta+C+%7D+%7Bw_%7Bij%7D%7D%3D+a_j%5E%7BL-1%7D%28a_i+-+y_i%29\" alt=\"\\frac {\\delta C } {w_{ij}}= a_j^{L-1}(a_i - y_i)\" eeimg=\"1\"/> </p><p>同理可有：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac+%7B%5Cdelta+C%7D%7B%5Cdelta+b_i%7D+%3D+a_i+-+y_i\" alt=\"\\frac {\\delta C}{\\delta b_i} = a_i - y_i\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>整个流程</h2><p>上面是我们推理的过程，那么整个的计算过程就是：</p><p>1、前向计算：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0f81d2449900b1d47fd5506b3a90d424_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"729\" data-rawheight=\"303\" class=\"origin_image zh-lightbox-thumb\" width=\"729\" data-original=\"https://pic1.zhimg.com/v2-0f81d2449900b1d47fd5506b3a90d424_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;729&#39; height=&#39;303&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"729\" data-rawheight=\"303\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"729\" data-original=\"https://pic1.zhimg.com/v2-0f81d2449900b1d47fd5506b3a90d424_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0f81d2449900b1d47fd5506b3a90d424_b.jpg\"/></figure><p>2、输出error：<br/> <img src=\"https://www.zhihu.com/equation?tex=+%5Cdelta%5E%7BL%7D+%3D+%5Cnabla_a+C+%5Codot+%5Csigma%27%28z%5EL%29+\" alt=\" \\delta^{L} = \\nabla_a C \\odot \\sigma&#39;(z^L) \" eeimg=\"1\"/> </p><p><br/><i>3、backpropagate error:</i><br/> <img src=\"https://www.zhihu.com/equation?tex=+%5Cdelta%5E%7Bl%7D+%3D+%28%28w%5E%7Bl%2B1%7D%29%5ET+%5Cdelta%5E%7Bl%2B1%7D%29+%5Codot+%5Csigma%27%28z%5E%7Bl%7D%29+\" alt=\" \\delta^{l} = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma&#39;(z^{l}) \" eeimg=\"1\"/> </p><p><br/><i>4、计算参数：</i><br/> <img src=\"https://www.zhihu.com/equation?tex=+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+w%5El_%7Bjk%7D%7D+%3D+a%5E%7Bl-1%7D_k+%5Cdelta%5El_j+%5C%5C+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+b%5El_j%7D+%3D+%5Cdelta%5El_j+\" alt=\" \\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j \\\\ \\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j \" eeimg=\"1\"/> </p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": [
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>有个typo： propgation -&gt; propagation</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/34771944", 
            "userName": "卡门", 
            "userLink": "https://www.zhihu.com/people/23533079660fe77ddb1abcf0b15c72b2", 
            "upvote": 2, 
            "title": "梯度下降（gradient desent）理解及各种优化", 
            "content": "<p>我们在机器学习过程中，目的实际上是为了找一个完美的、符合数据特征的模型。我们希望这个模型不管是在训练中，还是在实际的测试中，都可以完美的预测结果。但是，现实中我们当然找不到这个完美的模型。于是退而求其次，我们希望可以找到一个近似的、我们掌握的、符合当前数据特征的模型。在求解这个模型之后，使用这个模型来进行预测。这些模型就包括我们知道的LR， SVM，DNN等。</p><p>当我们选择了某种模型，然后在我们说我们要求解这个模型的时候，我们在说什么呢？可以理解为，我们实际上说的是要求出这个模型的参数。当我们找到了“完美”的参数的时候，我们实际就是求出了这个模型，它可以“完美”的拟合我们的训练数据，也可以“完美”的预测测试数据。当然，实际中，我们很难找到这么完美的参数，因为我们预定义的模型可能就是不完美的，当然也就很难找出完美的参数了。于是我们要求解的参数，实际上就是使得我们模型的预测跟实际值的误差，也就是误差函数最小的那些参数。</p><p>于是我们可以看到，在机器学习中，最重要的就是两个部分：</p><ol><li>定义模型：相当于我们建立了一个映射函数，建立从输入数据到结果的映射关系。</li><li>求解模型：实际上就是求出我们定义的模型，在训练数据中的参数是多少。这个求解的过程，要使用损失函数作为我们前进的方向的指导。在损失函数所告诉的好还是坏的正确指导下，我们才能朝着正确的参数方向前进。</li></ol><p>本文我们主要分析“求解模型”部分，也就是，如何求出我们既定模型的参数？</p><p><b>直接求解</b></p><p>对于线性损失函数来说， 当使用最小二乘法作为损失函数的时候，是可以有标准函数进行直接求解的。</p><p>假定我们选择的线性函数映射关系（矩阵形式）为：</p><p><img src=\"https://www.zhihu.com/equation?tex=Y+%3D+%5Ctheta+X\" alt=\"Y = \\theta X\" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=X\" alt=\"X\" eeimg=\"1\"/> 为n+1维参数， 因为包含了没有参数的 <img src=\"https://www.zhihu.com/equation?tex=x_0\" alt=\"x_0\" eeimg=\"1\"/> 。</p><p>我们将loss函数定义为：</p><p><img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29+%3D+%5Cfrac+%7B1%7D%7B2%7D+%5Csum+i%5En+%28h%5Ctheta%28x%5E%7B%28i%29%7D+-+y%5E%7B%28i%29%7D%29%5E2%29\" alt=\"J(\\theta) = \\frac {1}{2} \\sum i^n (h\\theta(x^{(i)} - y^{(i)})^2)\" eeimg=\"1\"/> </p><p>如果我们同样用矩阵形式来进行定义损失函数，那么损失函数可以写为：</p><p><img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29+%3D+%5Cfrac+%7B1%7D%7B2%7D+%28X%5Ctheta+-y%29%5ET+%28X%5Ctheta+-y%29\" alt=\"J(\\theta) = \\frac {1}{2} (X\\theta -y)^T (X\\theta -y)\" eeimg=\"1\"/> </p><p>接下来就是求解出最佳的 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 了，也就是求出使 <img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"/> 最小的 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 。于是通过求导我们就可以解出来：<br/> <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac+%7B%5Cdelta+J%7D%7B%5Cdelta+%5Ctheta%7D+%3D+%5Cfrac+%7B1%7D%7B2%7D+%5Cfrac+%7B%5Cdelta%7D%7B%5Cdelta+%5Ctheta%7D%28%5Ctheta+%5ET+X%5ET+X+%5Ctheta+-+%5Ctheta+%5ET+X%5ET+Y+-+Y%5ETX%5Ctheta+%2B+Y%5ETY%29+%5C%5C+%3D+%5Cfrac+%7B1%7D%7B2%7D+%282X%5ETX%5Ctheta+-X%5ETY+-+X%5ETY%29+%5C%5C+%3D+X%5ETX%5Ctheta+-+X%5ETY\" alt=\"\\frac {\\delta J}{\\delta \\theta} = \\frac {1}{2} \\frac {\\delta}{\\delta \\theta}(\\theta ^T X^T X \\theta - \\theta ^T X^T Y - Y^TX\\theta + Y^TY) \\\\ = \\frac {1}{2} (2X^TX\\theta -X^TY - X^TY) \\\\ = X^TX\\theta - X^TY\" eeimg=\"1\"/> <br/>令上式等于0，求解可以得到：<br/> <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta+%3D+%28X%5ETX%29%5E%7B-1%7DX%5ETY+\" alt=\"\\theta = (X^TX)^{-1}X^TY \" eeimg=\"1\"/> <br/>于是我们这样就求解出了 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> ，任务完成。</p><p>但是这里有两个问题：</p><ol><li>必须要求 <img src=\"https://www.zhihu.com/equation?tex=X%5ETX\" alt=\"X^TX\" eeimg=\"1\"/> 必须是满秩的，这样他们才会有逆。这一点是可以通过加参数来解决的。</li><li>当 <img src=\"https://www.zhihu.com/equation?tex=X\" alt=\"X\" eeimg=\"1\"/> 的维度很大时， 求解 <img src=\"https://www.zhihu.com/equation?tex=X%5ETX\" alt=\"X^TX\" eeimg=\"1\"/> 的结果是非常耗时间的。</li></ol><h2>迂回求解</h2><p>在直接求解当中，我们很幸运的得到了解。但是在很多机器学习算法中，我们是没有办法直接求解的。于是我们可以通过Gradient Descent的方法来解决这个问题。</p><p><b>直观理解</b></p><p>首先， 该如何理解直观的理解gradient descent呢？</p><p>比较常用的理解就是在山谷中放一个球，我们希望的结果是让球到达谷的底部。这个所谓的谷的底部，就是我们loss函数最小的点了。那么这个球会如何走呢？这个球会沿着最快速度下降的地方走，一直到谷底。如果这个不太好理解，那么我们就以我们自己的视角来看：</p><p>假定我们站在组成山谷地面的二维平面点上，所有的二维平面点对应的山谷高度值，所有这些值形成了山谷。我们所站位置的头顶上，恰好就是球所在的山谷中的位置，球的位置会随我们自己的移动而移动。只有这个球到达谷底的时候，我们才能拿到。我们的目标，是尽快的得到这个球。</p><p>那我们会往前后左右哪个方向走呢？由于我们迈出的步子大小是一样的，所以我们当然希望我们迈出一步，可以让球下降的最多，这样球才能尽快下来。而满足这个条件的，就是我们的要迈向的方向。而这个方向，就是这个点在各维度（这时候是二维地面）负梯度方向的综合。</p><p>于是我们可以知道：我们选择负梯度的方向，不仅仅是因为它可以收敛到我们希望的最低点（我们假如目前只有一个最低点），而且是因为它的收敛速度是最快的。</p><h2>求解</h2><p><b><i>Batch gradient descent</i></b> </p><p>于是我们就可以知道怎么求解了：随便选择一个点，然后让负梯度方向不断的前进，直到到达loss的最低点。到达最低点的时候， $\\theta$的值就是我们相求的参数的值。具体公式如下：<br/> <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta+%3D+%5Ctheta+-+%5Ceta+%2A+%5CDelta_%5Ctheta+J%28%5Ctheta%29+\" alt=\" \\theta = \\theta - \\eta * \\Delta_\\theta J(\\theta) \" eeimg=\"1\"/> <br/>这就是<i>Batch gradient descent</i> 。</p><p>这个公式有时候会没有表达清楚，我们不用矩阵的形式来看：<br/> <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta_%7Bi%2B1%7D+%3D+%5Ctheta_i+-+%5Ceta+%2A+%5Csum+k%5En+%28%5Cfrac+%7B%5Cdelta%7D%7B%5Cdelta+%5Ctheta%7D%28h%7B%5Ctheta%7D%28x%5E%7B%28i%29%7D%29+%2A%28h%5Ctheta+%28x%5E%7B%28i%29%7D+-+y%29+%29\" alt=\" \\theta_{i+1} = \\theta_i - \\eta * \\sum k^n (\\frac {\\delta}{\\delta \\theta}(h{\\theta}(x^{(i)}) *(h\\theta (x^{(i)} - y) )\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=h%7B%5Ctheta%7D%28x%5E%7B%28i%29%7D%29\" alt=\"h{\\theta}(x^{(i)})\" eeimg=\"1\"/> 就是我们的映射函数。</p><p>我们可以看到，每次更新参数的时候，我们都要求出所有的样本点对 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_i\" alt=\"\\theta_i\" eeimg=\"1\"/> 的梯度，然后求和进行更新。这样更新的问题是很明显的：太慢了，尤其是数据量大的时候，基本是不可行的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><i><b>Stochastic Gradient Descent  </b></i></p><p>于是我们可以选择一种偷懒的方法：<i>Stochastic Gradient Descent 算法。</i></p><p>在更新中，我们每次都计算一个点，只用一个点来更新参数：<br/> <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bi%2B1%7D%3D+%5Ctheta_i+-+%5Ceta+%2A+%28%5Cfrac+%7B%5Cdelta%7D%7B%5Cdelta+%5Ctheta%7D%28h_%7B%5Ctheta%7D%28x%5E%7B%28i%29%7D%29%29+%2A%28h%5Ctheta+%28x%5E%7B%28i%29%7D+-+y%29%29+\" alt=\"\\theta_{i+1}= \\theta_i - \\eta * (\\frac {\\delta}{\\delta \\theta}(h_{\\theta}(x^{(i)})) *(h\\theta (x^{(i)} - y)) \" eeimg=\"1\"/> <br/>首先有个问题：如何保证我选择一个个点的下降，最终的结果也会到达最低点？</p><p>根据人们时间来看，只要慢慢的调节 <img src=\"https://www.zhihu.com/equation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"/> ,最终还是能够收敛的,理论上也可以<a href=\"https://www.zhihu.com/question/27012077/answer/41067939\" class=\"internal\">证明</a>。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">nb_epochs</span><span class=\"p\">):</span>\n  <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">shuffle</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n  <span class=\"k\">for</span> <span class=\"n\">example</span> <span class=\"ow\">in</span> <span class=\"n\">data</span><span class=\"p\">:</span>\n    <span class=\"n\">params_grad</span> <span class=\"o\">=</span> <span class=\"n\">evaluate_gradient</span><span class=\"p\">(</span><span class=\"n\">loss_function</span><span class=\"p\">,</span> <span class=\"n\">example</span><span class=\"p\">,</span> <span class=\"n\">params</span><span class=\"p\">)</span>\n    <span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"n\">params</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">params_grad</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><i><b>mini-batch gradient descent</b></i></p><p>接下来的想法是理所当然的：全量很费劲，一个个的缩减又很慢，那么每次我用一批数据可以吗？可以，于是就有了 <i>mini-batch gradient descent</i></p><p>也就是说，每次更新的时候，我都选择一批数据来计算参数 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 的更新。这样计算量小了，算的又快了，简直美滋滋。其公式如下：<br/> <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta+%3D+%5Ctheta+-+%5Ceta+%5Ccdot+%5Cnabla_%5Ctheta+J%28+%5Ctheta%3B+x%5E%7B%28i%3Ai%2Bn%29%7D%3B+y%5E%7B%28i%3Ai%2Bn%29%7D%29+\" alt=\" \\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta; x^{(i:i+n)}; y^{(i:i+n)}) \" eeimg=\"1\"/> <br/>它带来的好处有：</p><ol><li>由于用了批量的数据来更新导数，所以它的收敛过程就会稳定一些；</li><li>如今很多的深度学习库就是针对的矩阵运算进行的优化，所以算起来效果就会好些。</li></ol><p>下面是它的代码。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">nb_epochs</span><span class=\"p\">):</span>\n  <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">shuffle</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n  <span class=\"k\">for</span> <span class=\"n\">batch</span> <span class=\"ow\">in</span> <span class=\"n\">get_batches</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">):</span>\n    <span class=\"n\">params_grad</span> <span class=\"o\">=</span> <span class=\"n\">evaluate_gradient</span><span class=\"p\">(</span><span class=\"n\">loss_function</span><span class=\"p\">,</span> <span class=\"n\">batch</span><span class=\"p\">,</span> <span class=\"n\">params</span><span class=\"p\">)</span>\n    <span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"n\">params</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">params_grad</span></code></pre></div><p>这种算法也是我们在计算梯度的过程中，主要所使用的方法。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>上面是我们从batch gradient Descent -&gt; <i>Stochastic Gradient Descent  -&gt; mini-batch gradient descent  的一个心路历程。</i>但是它还不够好。主要的问题有三个：</p><ol><li>学习率 <img src=\"https://www.zhihu.com/equation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"/> 需要手工来调；</li><li><img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 中的每个 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_i\" alt=\"\\theta_i\" eeimg=\"1\"/> 都统一由一个学习率 <img src=\"https://www.zhihu.com/equation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"/> 来调整；</li><li>学习率 <img src=\"https://www.zhihu.com/equation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"/> 在整个过程中是保持不变的。</li></ol><p>这三个问题主要是针对 <img src=\"https://www.zhihu.com/equation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"/> 的，实际上对于要前进的方向也是有优化空间的：之前前进的方向都是梯度的负方向，那么是不是还有更好的前进方向呢？</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Momentum算法</b></p><p>我们这样从直觉上考虑这件事情：</p><p>假如我们的球想要往负梯度的方向前进，我们这个前进的方向是不是可以考虑历史的情况呢？如果以前前进的方向也大体是这个方向，说明这个方向还是可以的，我们就前进的步伐大一些；如果跟以前前进的方向是相反的，那么我们就谨慎一点，走的步幅小一点。如果两者之间有个夹角，那就用向量和来中和一下。</p><p>用数学来表达如下：</p><p><img src=\"https://www.zhihu.com/equation?tex=+v_t+%3D+%5Cgamma+v_%7Bt-1%7D+%2B+%5Ceta+%5Cnabla_%5Ctheta+J%28+%5Ctheta%29+%5C%5C+%5Ctheta+%3D+%5Ctheta+-+v_t+\" alt=\" v_t = \\gamma v_{t-1} + \\eta \\nabla_\\theta J( \\theta) \\\\ \\theta = \\theta - v_t \" eeimg=\"1\"/></p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"/> 我们通常可以设置为0.9</p><p>这种算法，就是momentum算法。</p><p><b>Nesterov accelerated gradient (NAG)</b> </p><p>这时候，我们继续挑毛病：我们的球是不是可以智能一点呢？它是不是可以在即将到达谷底的时候减速呢？这样的话，他就不用在谷底的时候可能一不小心加速冲过去了。</p><p><b>Nesterov accelerated gradient (NAG)</b>  就是考虑到这种情况的。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-84948bc59323615ce017903f328a9e55_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"254\" class=\"origin_image zh-lightbox-thumb\" width=\"800\" data-original=\"https://pic2.zhimg.com/v2-84948bc59323615ce017903f328a9e55_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;800&#39; height=&#39;254&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"254\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"800\" data-original=\"https://pic2.zhimg.com/v2-84948bc59323615ce017903f328a9e55_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-84948bc59323615ce017903f328a9e55_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在momentum算法中，我们在前进的时候，直接结合了 <b>历史方向以及当前梯度方向和梯度值</b> ，走到了下个位置；</p><p>而在NAG中，它是先根据历史方向估计的一个值： <img src=\"https://www.zhihu.com/equation?tex=+%5Cgamma+v_%7Bt-1%7D\" alt=\" \\gamma v_{t-1}\" eeimg=\"1\"/> ，走到这个点，然后在新的点求出这个点的负梯度，再向这个新的点的负梯度方向前进。</p><p>这样有什么好处呢？在momentum的情况下，我们是直接相加了之前的方向和当前计算的方向，相对来说比较粗暴；而NAG就会相对精细一些：它先走一小步，也就是之前前进的方向，然后再在新的点上计算梯度，再前进。这样就会更加的谨慎。从而避免前面说的可能冲过谷底的问题。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Adagrad 算法</b></p><p>前面我们根据梯度调整了参数递减的步幅，但是这个步幅不仅仅可以根据梯度来调整。实际上，更直觉的，是根据参数 <img src=\"https://www.zhihu.com/equation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"/> 来进行调整。</p><p>还记得我们之前提出的问题吗？</p><p>首先我们看2，3个问题：是不是可以根据不同的参数设置不同的学习率 <img src=\"https://www.zhihu.com/equation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"/> ，学习率 <img src=\"https://www.zhihu.com/equation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"/> 在整个过程中是不是可以变化呢？</p><p>是的，可以的。接下来我们说一下<b>adagrad算法</b>就是这样的。</p><p>首先，我们对于每个参数都求出它们的梯度：<br/> <img src=\"https://www.zhihu.com/equation?tex=+g_%7Bt%2C+i%7D+%3D+%5CDelta+_%5Ctheta+J%28%5Ctheta+_%7Bt%2C+i+%7D%29+\" alt=\" g_{t, i} = \\Delta _\\theta J(\\theta _{t, i }) \" eeimg=\"1\"/> <br/><i>然后，对于每个参数都做出调整：</i><br/> <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta_%7Bt%2B1%2C+i%7D+%3D+%5Ctheta_%7Bt%2C+i%7D+-+%5Cfrac%7B%5Ceta%7D%7B%5Csqrt%7BG%7Bt%2C+ii%7D+%2B+%5Cepsilon%7D%7D+g%7Bt%2C+i%7D+\" alt=\" \\theta_{t+1, i} = \\theta_{t, i} - \\frac{\\eta}{\\sqrt{G{t, ii} + \\epsilon}} g{t, i} \" eeimg=\"1\"/> <br/><i>其中，</i> <img src=\"https://www.zhihu.com/equation?tex=G_%7Bt%2C+ii%7D\" alt=\"G_{t, ii}\" eeimg=\"1\"/> 是在之前所有t步的平方和。也就是<br/> <img src=\"https://www.zhihu.com/equation?tex=+G_%7Bt%2C+ii%7D+%3D+%5Csum+_k%5E+tg_%7Bk%2C+i%7D%5E2\" alt=\" G_{t, ii} = \\sum _k^ tg_{k, i}^2\" eeimg=\"1\"/> <br/>也就是说，对于那些梯度比较大的参数，他会让它的学习率 <img src=\"https://www.zhihu.com/equation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"/> 小一些，从而下降的慢一些；而对于梯度比较小，或者不太出现的参数，就让它的学习率大一些，从而下降的快一些。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如何从直觉上理解这种情况呢？</p><p>我的理解是这样：我们最终的希望，让所有的参数同时到达谷底，也就是导数为0的点；</p><p>也就是说，对于离谷底越远的点，我们越希望它能下降的快一点；对于离谷底越近的点，我们越希望它下降的慢一点。那么这个远近怎么来衡量呢？</p><p>对于同一个参数来说，远近的衡量是很容易的：梯度越大的，说明离的越远；越小的，说明离的越近。</p><p>但是如果是不同的参数呢？</p><p>我们先想象一个二维的变量空间，他们跟最终的值一起，在三维空间形成一个碗装的图形。这个碗的形状跟什么有关系呢？我们可以发现，越扁的碗，二次导数越小；越尖的碗，二次导数越大。这个尖与不尖的情况，也会跟我们距离碗底的大小有关系：在同样导数的情况下， 尖碗所在的那个点， 比粗碗所在的那个点距离碗底更近些。也就是说：</p><p>点距离谷底的远近，还跟这个点的二次导数成反比。</p><p>而我们用<br/> <img src=\"https://www.zhihu.com/equation?tex=+%5Csqrt+%7BG_%7Bt%2C+ii%7D%7D+\" alt=\" \\sqrt {G_{t, ii}} \" eeimg=\"1\"/> <br/>就是来表征这个二次导数的。</p><p>那么， 为什么它可以表征二次导数？可以见李宏毅老师的课件：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e751758711bc0b55a82755da6ff327f8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"881\" data-rawheight=\"668\" class=\"origin_image zh-lightbox-thumb\" width=\"881\" data-original=\"https://pic1.zhimg.com/v2-e751758711bc0b55a82755da6ff327f8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;881&#39; height=&#39;668&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"881\" data-rawheight=\"668\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"881\" data-original=\"https://pic1.zhimg.com/v2-e751758711bc0b55a82755da6ff327f8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e751758711bc0b55a82755da6ff327f8_b.jpg\"/></figure><p>当抽样足够多的时候，实际上这些点的导数的平方一定程度上反应了二阶导数的特征。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>RMSprop算法</b></p><p>Adagrad 有一个问题，那就是：在距离谷底的时候，由于分母会越来越大，从而使得更新越来越小， 导致参数有可能就不再更新了。</p><p>RMSprop是可以解决adagrad 的问题的。我们看下他的更新公式：<br/> <img src=\"https://www.zhihu.com/equation?tex=+E%28g%5E2%29_t+%3D+0.9+%2A+E%5Bg%5E2+%5D_%7Bt-1%7D%2B+0.1%2A+E%5Bg%5E2%5D_t+%5C%5C+%5Ctheta+_%7Bt%2B1%7D+%3D+%5Ctheta_t+-+%5Cfrac+%7B%5Ceta%7D%7B%5Csqrt%7BE%5Bg%5E2%5D_t%7D+%2B+%5Cepsilon+%7D%2A+g_t+\" alt=\" E(g^2)_t = 0.9 * E[g^2 ]_{t-1}+ 0.1* E[g^2]_t \\\\ \\theta _{t+1} = \\theta_t - \\frac {\\eta}{\\sqrt{E[g^2]_t} + \\epsilon }* g_t \" eeimg=\"1\"/> <br/>也就是说，他会取之前的数据的一部分，跟当前的结合算出一个更新。这样，之前累积的更新就不用一直在后面的更新中施加影响，而是只取出其中的一部分就行了。</p><p>hinton建议 <img src=\"https://www.zhihu.com/equation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"/> 设置为0.9， <img src=\"https://www.zhihu.com/equation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"/> 设置为0.001</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Adam算法</b></p><p>好了，之前说的adagrad， RMSprop针对学习率进行的优化。那么我们可以不可以将之前的,对学习率和方向的优化进行结合呢？</p><p>当然可以，结合起来，就是adam算法了。<br/> <img src=\"https://www.zhihu.com/equation?tex=+m_t+%3D+%5Cbeta_1+m_%7Bt-1%7D+%2B+%281-%5Cbeta1%29g_t+%5C%5C+v_t+%3D+%5Cbeta_2+v_%7Bt-1%7D+%2B+%281-%5Cbeta2%29g_t%5E2+\" alt=\" m_t = \\beta_1 m_{t-1} + (1-\\beta1)g_t \\\\ v_t = \\beta_2 v_{t-1} + (1-\\beta2)g_t^2 \" eeimg=\"1\"/> </p><p>可以看到， 对于 <img src=\"https://www.zhihu.com/equation?tex=v\" alt=\"v\" eeimg=\"1\"/> 的更新其实就是RMSprop算法；而对于m的更新，就是momentum算法。</p><p>同时adam有一个偏差补偿机制：<br/> <img src=\"https://www.zhihu.com/equation?tex=+%5Chat+m_t+%3D+%5Cfrac%7Bm_t%7D%7B1-%5Cbeta_1%5Et%7D+%5C%5C+%5Chat+v_t+%3D+%5Cfrac+%7Bv_t%7D+%7B1-%5Cbeta_2%5Et+%7D+\" alt=\" \\hat m_t = \\frac{m_t}{1-\\beta_1^t} \\\\ \\hat v_t = \\frac {v_t} {1-\\beta_2^t } \" eeimg=\"1\"/> </p><p>于是最终的更新公式为：<br/> <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta_%7Bt%2B1%7D+%3D+%5Ctheta_%7Bt%7D+-+%5Cdfrac%7B%5Ceta%7D%7B%5Csqrt%7B%5Chat%7Bv%7D_t%7D+%2B+%5Cepsilon%7D+%5Chat%7Bm%7D_t+\" alt=\" \\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t \" eeimg=\"1\"/> </p><p>为什么要进行补偿呢？<a href=\"https://www.zhihu.com/question/27012077\" class=\"internal\">这里</a> 有相关论文进行了解释。简单解释的话，就是：我们抽样的点计算出的一阶导数和二阶导数实际是对损失函数实际的一阶矩和二阶矩的有偏估计，因此需要调整。具体的调整推导过程可看链接。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>相关资料</p><p class=\"ztext-empty-paragraph\"><br/></p><a href=\"https://link.zhihu.com/?target=http%3A//cs231n.github.io/neural-networks-3/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-9ccd225e85f97162113a96e05a2e0f40_ipico.jpg\" data-image-width=\"459\" data-image-height=\"414\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CS231n Convolutional Neural Networks for Visual Recognition</a><a href=\"https://link.zhihu.com/?target=http%3A//ruder.io/optimizing-gradient-descent/index.html%23shufflingandcurriculumlearning\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-052e54b96104fea5f3a1604342e9a00c_180x120.jpg\" data-image-width=\"484\" data-image-height=\"380\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">An overview of gradient descent optimization algorithms</a><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "梯度下降", 
                    "tagLink": "https://api.zhihu.com/topics/19650497"
                }, 
                {
                    "tag": "笔记", 
                    "tagLink": "https://api.zhihu.com/topics/19554982"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/kamen"
}
