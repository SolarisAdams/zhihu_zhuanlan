{
    "title": "ML随笔", 
    "description": "实践中的机器学习感悟心得", 
    "followers": [
        "https://www.zhihu.com/people/riodan", 
        "https://www.zhihu.com/people/chuan-ze-62", 
        "https://www.zhihu.com/people/mel-tor", 
        "https://www.zhihu.com/people/xiao-shu-lf", 
        "https://www.zhihu.com/people/david-wei", 
        "https://www.zhihu.com/people/vincent-43-55-66", 
        "https://www.zhihu.com/people/huang-zhi-ming-37", 
        "https://www.zhihu.com/people/zhao-qiang-68-59", 
        "https://www.zhihu.com/people/zheng-jian-yang-56", 
        "https://www.zhihu.com/people/kevin-51-53-25", 
        "https://www.zhihu.com/people/xiao-song-87-39", 
        "https://www.zhihu.com/people/mata-fu", 
        "https://www.zhihu.com/people/wei-song-14", 
        "https://www.zhihu.com/people/sun-yu-chen-46-62", 
        "https://www.zhihu.com/people/xie-jun-jing-85", 
        "https://www.zhihu.com/people/li-de-zhi-33-2", 
        "https://www.zhihu.com/people/zhu-forrest", 
        "https://www.zhihu.com/people/wang-kuan-57-47", 
        "https://www.zhihu.com/people/liu-hui-56-43-81", 
        "https://www.zhihu.com/people/chen-bo-86-42", 
        "https://www.zhihu.com/people/DeepLearnerLT", 
        "https://www.zhihu.com/people/chenwuchen", 
        "https://www.zhihu.com/people/tian-si-16-91", 
        "https://www.zhihu.com/people/li-jia-19-77", 
        "https://www.zhihu.com/people/fiona1997-45", 
        "https://www.zhihu.com/people/chuweiyan-87", 
        "https://www.zhihu.com/people/tai-wu-liao", 
        "https://www.zhihu.com/people/qing-li-bing-xian", 
        "https://www.zhihu.com/people/bright-14", 
        "https://www.zhihu.com/people/BruceWDZ", 
        "https://www.zhihu.com/people/fei-teng-de-rou-wan", 
        "https://www.zhihu.com/people/harderway", 
        "https://www.zhihu.com/people/jesse-wu-82", 
        "https://www.zhihu.com/people/huang-lei-11-16", 
        "https://www.zhihu.com/people/wei-yu-cong-61-9", 
        "https://www.zhihu.com/people/allen-59", 
        "https://www.zhihu.com/people/blueman", 
        "https://www.zhihu.com/people/tgpi", 
        "https://www.zhihu.com/people/learn-furtherly", 
        "https://www.zhihu.com/people/xiao-xin-53-94", 
        "https://www.zhihu.com/people/li-heng-1-66", 
        "https://www.zhihu.com/people/qichao-tang", 
        "https://www.zhihu.com/people/wo-shi-xiao-ming-45", 
        "https://www.zhihu.com/people/gu-ge-ge-62", 
        "https://www.zhihu.com/people/sam-wu-64", 
        "https://www.zhihu.com/people/qiu-jian-zhang-62", 
        "https://www.zhihu.com/people/lan-lan-lan-lan-96-82", 
        "https://www.zhihu.com/people/yildhd-wang", 
        "https://www.zhihu.com/people/li-xian-bo-82", 
        "https://www.zhihu.com/people/ling-yong-3", 
        "https://www.zhihu.com/people/xiao-zhu-43-73-26", 
        "https://www.zhihu.com/people/dong-feng-zao-ji", 
        "https://www.zhihu.com/people/nuo-wei-si-ji-kou-qiao-dan", 
        "https://www.zhihu.com/people/young-mao-72", 
        "https://www.zhihu.com/people/yu-zi-jun-49", 
        "https://www.zhihu.com/people/zhao-zhi-bin-80", 
        "https://www.zhihu.com/people/yi-liang-23-12", 
        "https://www.zhihu.com/people/zhang-chi-40-60", 
        "https://www.zhihu.com/people/hu-jiang-ze", 
        "https://www.zhihu.com/people/huang-huang-bu-bu-bu-zhi-he-chu-qu", 
        "https://www.zhihu.com/people/sun-xiao-kai-49", 
        "https://www.zhihu.com/people/thikingfly", 
        "https://www.zhihu.com/people/brotherchen", 
        "https://www.zhihu.com/people/quxiaofeng", 
        "https://www.zhihu.com/people/hu-gui-feng-45", 
        "https://www.zhihu.com/people/deng-zi-jun-62-67", 
        "https://www.zhihu.com/people/kevin-hill", 
        "https://www.zhihu.com/people/tageee", 
        "https://www.zhihu.com/people/ni-xi-de-yan-yuan", 
        "https://www.zhihu.com/people/moni-gg", 
        "https://www.zhihu.com/people/ke-wu-88", 
        "https://www.zhihu.com/people/jiang-jin-sheng-51", 
        "https://www.zhihu.com/people/meng-shen", 
        "https://www.zhihu.com/people/zhang-zi-yi-31-40", 
        "https://www.zhihu.com/people/WestonLeoHunter", 
        "https://www.zhihu.com/people/lan-nuo-24", 
        "https://www.zhihu.com/people/jiaqi-wang-919", 
        "https://www.zhihu.com/people/liu-xiao-kun-95", 
        "https://www.zhihu.com/people/sudalv0313", 
        "https://www.zhihu.com/people/ying-xiao-tu-59", 
        "https://www.zhihu.com/people/chen-meng-qi-34-61", 
        "https://www.zhihu.com/people/yang-yang-89-95-18", 
        "https://www.zhihu.com/people/gabbar-wang", 
        "https://www.zhihu.com/people/yy-cc-63-54", 
        "https://www.zhihu.com/people/tang-xiao-liang-72", 
        "https://www.zhihu.com/people/li-wa-84-48", 
        "https://www.zhihu.com/people/kang-da-90-63", 
        "https://www.zhihu.com/people/min-rui-zhi-hui-wang", 
        "https://www.zhihu.com/people/owen-92-63-1", 
        "https://www.zhihu.com/people/zhi-bai-41-24", 
        "https://www.zhihu.com/people/cattigers", 
        "https://www.zhihu.com/people/ju-ge-li-zi-57-57", 
        "https://www.zhihu.com/people/homer-wong-33", 
        "https://www.zhihu.com/people/bu-re-chen-ai-57", 
        "https://www.zhihu.com/people/hong-alex", 
        "https://www.zhihu.com/people/crisb_dut", 
        "https://www.zhihu.com/people/xue-hao-40", 
        "https://www.zhihu.com/people/TAT_hanxiao", 
        "https://www.zhihu.com/people/dqpilzg", 
        "https://www.zhihu.com/people/aguaithefreak", 
        "https://www.zhihu.com/people/xin-ba-33-19"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/61901956", 
            "userName": "战歌指挥官", 
            "userLink": "https://www.zhihu.com/people/f4ee3aaa7e722a47be3444599f9bbd5b", 
            "upvote": 4, 
            "title": "远程操纵家（或办公室）中主机——内网渗透", 
            "content": "<h2>1.   背景</h2><p>​   最近组装了一台深度学习服务器，暂放置于家中，使用时需要从公司网络连接（主要是ssh连接）。</p><p>​   目前情况是，公司网络和家庭网络都是局域网，不对外开放（无固定公网ip以及域名），因此需要设法连通。</p><p>​<b>机器类型</b>：</p><ul><li>深度学习服务器：</li><ul><li>ubuntu系统</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>公司机器：</li><ul><li>mac osx系统</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.   具体方案</h2><p>​   调研了一圈，现主要采用以下方案组合。</p><h3>​   a.  teamviewer</h3><p>​       teamviewer支持多平台登录，包括移动端，服务一般也比较稳定。</p><p>​       teamviewer安装较为简单（普通app客户端安装，一步一步按照说明进行即可），这里不再赘述。主要需要注意的是开启开机启动，远程连接可通过添加本机到自己的teamviewer账户中然后在别的机器通过账户登录连接（首次需信任设备），或者开启无人值守模式，通过id和密码登录。</p><p>​       teamviewer只支持界面ui操作，不支持命令行如ssh这种，这里主要在zerotier和frp均失效时作为最后的保险使用（重启等异常）。</p><h3>​   b.  zerotier</h3><p>​       zerotier是一款非常简单易用的内网穿透工具，安装较为简单，其主要是将共同安装客户端的机器虚拟到一个局域网，然后就能按照局域网方式进行访问。</p><p>​       安装步骤大体如下：</p><ol><li>​      注册 ZeroTier，获得 Internal ID</li><li>​      创建私有局域网，获得 Network ID</li><li>​      安装客户端，加入 Network ID（或邀请 Internal ID 加入）</li><li>​      连接</li></ol><p>​      zerotier客户端在每台机器上配置完毕后，去zerotier官网登录帐号，然后在创建的私有局域网（networks）中确认这些新机器（打勾），然后就能用类似<code>ssh root</code>@<code>192.168.196.13</code>等 方式互相访问。</p><p>      为了能够支持重启连接，将客户端程序加入开机自启比较保险。</p><h3>​   c.  frp</h3><blockquote>​   Frp 是一个高性能的反向代理应用，可以帮助您轻松地进行内网穿透，对外网提供服务，支持 tcp, http, https 等协议类型，并且 web 服务支持根据域名进行路由转发。</blockquote><p>​       相比于前面两个，frp的安装要复杂的多，首先需要一台暴露在公网的机器，用于做端口映射。其逻辑是将内网机器A端口映射到公网机器B的一个端口，然后另一个内网机器C（可以与A不在一个局域网）可以通过访问B的映射端口来访问A。</p><p>​       因此在我的需求背景下，需要额外一台公网机器，这里我采用了一台阿里云的机器，在其上边安装了frps（frp的server端）。这里采用了github上的<a href=\"https://link.zhihu.com/?target=https%3A//github.com/MvsCode/frp-onekey\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">frp-onekey</a>的一键安装脚本，根据安装选项进行一步步设置操作即可。</p><p>​       客户端版本下载地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/fatedier/frp/releases\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/fatedier/frp</span><span class=\"invisible\">/releases</span><span class=\"ellipsis\"></span></a></p><p>​       客户端下载完后按照frps的配置选项进行相应设置暴露即可。</p><p>​       同zerotier一样，在完成后，加入开机自启。这里服务器和PC客户端都需要进行。开机自启建议ubuntu下用systemctl，mac下用launchctl。</p><hr/><p>zerotier和frp本身其实功能有些重复，但是在网络条件较差时，frp一般能提供较好的连接体验，因此这里使用了多套方案共同保险。</p>", 
            "topic": [
                {
                    "tag": "内网通讯", 
                    "tagLink": "https://api.zhihu.com/topics/19847883"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/32821110", 
            "userName": "战歌指挥官", 
            "userLink": "https://www.zhihu.com/people/f4ee3aaa7e722a47be3444599f9bbd5b", 
            "upvote": 12, 
            "title": "一维搜索方法总结", 
            "content": "<h2>一维搜索</h2><p>首先说下一维搜索。一维搜索是针对以下场景而生。<img src=\"https://www.zhihu.com/equation?tex=x_%7Bk%2B1%7D%3Dx_k%2B%7B%5Calpha_k%7D%7Bp_k%7D%5C%5C+%5Cvarphi%28%5Calpha%29%3Df%28x_k%2B%7B%5Calpha%7D%7Bp_k%7D%29%5C%5C+%E6%B1%82min%7B%5Cvarphi%28%5Calpha%29%7D\" alt=\"x_{k+1}=x_k+{\\alpha_k}{p_k}\\\\ \\varphi(\\alpha)=f(x_k+{\\alpha}{p_k})\\\\ 求min{\\varphi(\\alpha)}\" eeimg=\"1\"/></p><p>其目的就是找这样的最优步长因子<img src=\"https://www.zhihu.com/equation?tex=%5Calpha_k\" alt=\"\\alpha_k\" eeimg=\"1\"/>。<br/>查找该过程用到一维搜索。</p><p>其一般包含以下结构：</p><ol><li>确定包含问题最优解的搜索区间；</li><li>利用某种分割技术或者插值方法缩小这个区间，进行搜索求解。</li></ol><h2>搜索区间——进退法</h2><p>从一点出发，试图确定出函数值呈现“高-低-高”的三点。一个方向不成功，就退回来，再沿相反方向寻找。</p><p>一般步骤如下：</p><ol><li>选取初始值<img src=\"https://www.zhihu.com/equation?tex=%5Calpha_0\" alt=\"\\alpha_0\" eeimg=\"1\"/>和<img src=\"https://www.zhihu.com/equation?tex=h_0\" alt=\"h_0\" eeimg=\"1\"/>，加倍系数<img src=\"https://www.zhihu.com/equation?tex=t%3E1\" alt=\"t&gt;1\" eeimg=\"1\"/>(一般<img src=\"https://www.zhihu.com/equation?tex=t%3D2\" alt=\"t=2\" eeimg=\"1\"/>)，<img src=\"https://www.zhihu.com/equation?tex=k%3D0\" alt=\"k=0\" eeimg=\"1\"/>;</li><li>如果<img src=\"https://www.zhihu.com/equation?tex=%5Cvarphi%28%5Calpha_k%2Bh_k%29%3C%5Cvarphi%28%5Calpha_k%29\" alt=\"\\varphi(\\alpha_k+h_k)&lt;\\varphi(\\alpha_k)\" eeimg=\"1\"/>，则<img src=\"https://www.zhihu.com/equation?tex=h_%7Bk%2B1%7D%3D%7Bt%7D%7Bh_k%7D%EF%BC%8C%5Calpha_%7Bk%2B1%7D%3D%5Calpha_k%2Bh_%7Bk%2B1%7D%EF%BC%8Ck%2B%2B\" alt=\"h_{k+1}={t}{h_k}，\\alpha_{k+1}=\\alpha_k+h_{k+1}，k++\" eeimg=\"1\"/>，继续进行2，否则继续进行3;</li><li>若k==0，转换搜索方向（即<img src=\"https://www.zhihu.com/equation?tex=h_k%3D-h_k\" alt=\"h_k=-h_k\" eeimg=\"1\"/>,转2）；否则停止迭代，输出<img src=\"https://www.zhihu.com/equation?tex=%5Calpha%3Dmin%5C%7B%5Calpha_0%2C%5Calpha_%7Bk%2B1%7D%5C%7D%2Cb%3Dmax%5C%7B%5Calpha_0%2C+%5Calpha_%7Bk%2B1%7D%5C%7D\" alt=\"\\alpha=min\\{\\alpha_0,\\alpha_{k+1}\\},b=max\\{\\alpha_0, \\alpha_{k+1}\\}\" eeimg=\"1\"/>，<img src=\"https://www.zhihu.com/equation?tex=%5Ba%2Cb%5D\" alt=\"[a,b]\" eeimg=\"1\"/>即为得出的区间。</li></ol><h2>分割方法</h2><ol><li><b>黄金分割法</b>。类似于二分查找的策略，只是每次缩小区间的时候选用的是黄金比例0.618。</li><li><b>Fibonacci</b>。类似黄金分割法，但是在缩减区间时采用的是Fibonacci数。</li></ol><p>此类方法及其适用于针对导数表达式复杂或未知的情况。</p><h2>插值方法</h2><p>针对具备较好解析性质的函数求解时，插值要比分割法好不少。</p><ol><li><b>一点二次插值（牛顿法）</b>。需要利用一点处的函数值、一阶和二阶导数值构造二次插值函数。该方法收敛速度快，具备局部二阶收敛速度。假设已知一个点<img src=\"https://www.zhihu.com/equation?tex=%5Calpha_1\" alt=\"\\alpha_1\" eeimg=\"1\"/>和其对应的一阶导<img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvarphi%7D%27%28%7B%5Calpha_1%7D%29\" alt=\"{\\varphi}&#39;({\\alpha_1})\" eeimg=\"1\"/>和二阶导数<img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvarphi%7D%27%27%28%7B%5Calpha_1%7D%29\" alt=\"{\\varphi}&#39;&#39;({\\alpha_1})\" eeimg=\"1\"/>，那么可推出近似的一维搜索极小点为<img src=\"https://www.zhihu.com/equation?tex=%5Cbar%7B%5Calpha%7D%3D%5Calpha_1-%5Cfrac%7B%7B%5Cvarphi%7D%27%28%7B%5Calpha_1%7D%29%7D%7B%7B%5Cvarphi%7D%27%27%28%7B%5Calpha_1%7D%29%7D\" alt=\"\\bar{\\alpha}=\\alpha_1-\\frac{{\\varphi}&#39;({\\alpha_1})}{{\\varphi}&#39;&#39;({\\alpha_1})}\" eeimg=\"1\"/>。每次更新该点，然后再去迭代查找即可。</li><li><b>二点二次插值法</b>。给出两点的函数值和其中一点的导数值，构造二次插值函数。其求取极小点式子为：<img src=\"https://www.zhihu.com/equation?tex=%5Cbar%7B%5Calpha%7D%3D%5Calpha_1-%5Cfrac%7B%5Calpha_1-%5Calpha_2%7D%7B2%281-%5Cfrac%7B%7B%5Cvarphi%7D%28%7B%5Calpha_1%7D%29-%7B%5Cvarphi%7D%28%7B%5Calpha_2%7D%29%7D%7B%28%5Calpha_1-%5Calpha_2%29%7B%5Cvarphi%7D%27%28%7B%5Calpha_1%7D%29%7D%29%7D\" alt=\"\\bar{\\alpha}=\\alpha_1-\\frac{\\alpha_1-\\alpha_2}{2(1-\\frac{{\\varphi}({\\alpha_1})-{\\varphi}({\\alpha_2})}{(\\alpha_1-\\alpha_2){\\varphi}&#39;({\\alpha_1})})}\" eeimg=\"1\"/>。</li><li><b>三点二次法（抛物线法）</b>。类似，极小点求解公式为：<img src=\"https://www.zhihu.com/equation?tex=%5Cbar%7B%5Calpha%7D%3D%5Cfrac%7B1%7D%7B2%7D%28%5Calpha_1%2B%5Calpha_2%2B%5Cfrac%7B%28%7B%5Cvarphi%7D%28%7B%5Calpha_1%7D%29-%7B%5Cvarphi%7D%28%7B%5Calpha_2%7D%29%29%28%7B%5Cvarphi%7D%28%7B%5Calpha_2%7D%29-%7B%5Cvarphi%7D%28%7B%5Calpha_3%7D%29%29%28%7B%5Cvarphi%7D%28%7B%5Calpha_3%7D%29-%7B%5Cvarphi%7D%28%7B%5Calpha_1%7D%29%29%7D%7B%28%5Calpha_2-%5Calpha_3%29%7B%5Cvarphi%7D%28%7B%5Calpha_1%7D%29%2B%28%5Calpha_3-%5Calpha_1%29%7B%5Cvarphi%7D%28%7B%5Calpha_2%7D%29%2B%28%5Calpha_1-%5Calpha_2%29%7B%5Cvarphi%7D%28%7B%5Calpha_3%7D%29%7D%29\" alt=\"\\bar{\\alpha}=\\frac{1}{2}(\\alpha_1+\\alpha_2+\\frac{({\\varphi}({\\alpha_1})-{\\varphi}({\\alpha_2}))({\\varphi}({\\alpha_2})-{\\varphi}({\\alpha_3}))({\\varphi}({\\alpha_3})-{\\varphi}({\\alpha_1}))}{(\\alpha_2-\\alpha_3){\\varphi}({\\alpha_1})+(\\alpha_3-\\alpha_1){\\varphi}({\\alpha_2})+(\\alpha_1-\\alpha_2){\\varphi}({\\alpha_3})})\" eeimg=\"1\"/>。其中<img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvarphi%7D%28%7B%5Calpha_1%7D%29%3E%7B%5Cvarphi%7D%28%7B%5Calpha_2%7D%29%2C%7B%5Cvarphi%7D%28%7B%5Calpha_3%7D%29%3E%7B%5Cvarphi%7D%28%7B%5Calpha_2%7D%29\" alt=\"{\\varphi}({\\alpha_1})&gt;{\\varphi}({\\alpha_2}),{\\varphi}({\\alpha_3})&gt;{\\varphi}({\\alpha_2})\" eeimg=\"1\"/>。迭代时，从<img src=\"https://www.zhihu.com/equation?tex=%5Calpha_1\" alt=\"\\alpha_1\" eeimg=\"1\"/>，<img src=\"https://www.zhihu.com/equation?tex=%5Calpha_2\" alt=\"\\alpha_2\" eeimg=\"1\"/>，<img src=\"https://www.zhihu.com/equation?tex=%5Calpha_3\" alt=\"\\alpha_3\" eeimg=\"1\"/>和<img src=\"https://www.zhihu.com/equation?tex=%5Cbar%7B%5Calpha%7D\" alt=\"\\bar{\\alpha}\" eeimg=\"1\"/>中挑选最小的点和其左右两点，进行下一步的迭代。</li><li><b>二点三次插值法</b>。用三次多项式来逼近。比二次插值法有较好的收敛效果， 但通常要求计算导数值，且计算量较大。一般当导数易求时，用三次插值法较好，其收敛速度为2阶，一般优于抛物线法。用函数<img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvarphi%7D%28%7B%5Calpha%7D%29\" alt=\"{\\varphi}({\\alpha})\" eeimg=\"1\"/>在<img src=\"https://www.zhihu.com/equation?tex=a\" alt=\"a\" eeimg=\"1\"/>,<img src=\"https://www.zhihu.com/equation?tex=b\" alt=\"b\" eeimg=\"1\"/>两点的函数值<img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvarphi%7D%28%7Ba%7D%29\" alt=\"{\\varphi}({a})\" eeimg=\"1\"/>,<img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvarphi%7D%28%7Bb%7D%29\" alt=\"{\\varphi}({b})\" eeimg=\"1\"/>和导数值<img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvarphi%7D%27%28%7Ba%7D%29\" alt=\"{\\varphi}&#39;({a})\" eeimg=\"1\"/>,<img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvarphi%7D%27%28%7Bb%7D%29\" alt=\"{\\varphi}&#39;({b})\" eeimg=\"1\"/>来构造三次函数，初值条件<img src=\"https://www.zhihu.com/equation?tex=a%3Cb\" alt=\"a&lt;b\" eeimg=\"1\"/>,<img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvarphi%7D%27%28%7Ba%7D%29%3C0\" alt=\"{\\varphi}&#39;({a})&lt;0\" eeimg=\"1\"/>,<img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvarphi%7D%27%28%7Bb%7D%29%3E0\" alt=\"{\\varphi}&#39;({b})&gt;0\" eeimg=\"1\"/>。推导后公式如下:<img src=\"https://www.zhihu.com/equation?tex=%5Cbar%7B%5Calpha%7D%3Da%2B%28b-a%29%5Cfrac%7Bw-%7B%5Cvarphi%7D%27%28%7Ba%7D%29-z%7D%7B%7B%5Cvarphi%7D%27%28%7Bb%7D%29-%7B%5Cvarphi%7D%27%28%7Ba%7D%29%2B2w%7D%5C%5C+%E5%85%B6%E4%B8%ADz%3D3%5Cfrac%7B%7B%5Cvarphi%7D%28%7Bb%7D%29-%7B%5Cvarphi%7D%28%7Ba%7D%29%7D%7Bb-a%7D-%7B%5Cvarphi%7D%27%28%7Ba%7D%29+-+%7B%5Cvarphi%7D%27%28%7Bb%7D%29+%5C%5C+w%5E2%3Dz%5E2-%7B%5Cvarphi%7D%27%28%7Ba%7D%29%7B%5Cvarphi%7D%27%28%7Bb%7D%29\" alt=\"\\bar{\\alpha}=a+(b-a)\\frac{w-{\\varphi}&#39;({a})-z}{{\\varphi}&#39;({b})-{\\varphi}&#39;({a})+2w}\\\\ 其中z=3\\frac{{\\varphi}({b})-{\\varphi}({a})}{b-a}-{\\varphi}&#39;({a}) - {\\varphi}&#39;({b}) \\\\ w^2=z^2-{\\varphi}&#39;({a}){\\varphi}&#39;({b})\" eeimg=\"1\"/> </li></ol><p><b>一般而言，上面插值和分割方法都要结合进退法一起使用，属于精确一维搜索方法。下面讲下不精确一维搜索方法。</b></p><h2>不精确一维搜索</h2><ul><li>精确一维搜索往往需要花费很大的时间。</li><ul><li>当迭代点远离问题的解时，精确求解通常不十分有效。</li><li>很多最优化方法，如牛顿法和拟牛顿法，其收敛速度并不依赖于精确一维搜索过程。</li></ul><li>只要保证目标函数有满意的下降，可大大节省计算量</li></ul><p>一般而言主要有以下两种准则。</p><h2>Armijo-Goldstein准则</h2><p><img src=\"https://www.zhihu.com/equation?tex=f%28x_k%29%2B%281-c%29%5Calpha_k%5Cnabla%7B%7Bf_k%7D%5ET%7Dp_k%5Cleq+f%28x_k%2B%7B%5Calpha_k%7D%7Bp_k%7D%29%5Cleq+f%28x_k%29%2Bc%7B%5Calpha_k%7D%5Cnabla%7B%7Bf_k%7D%5ET%7Dp_k\" alt=\"f(x_k)+(1-c)\\alpha_k\\nabla{{f_k}^T}p_k\\leq f(x_k+{\\alpha_k}{p_k})\\leq f(x_k)+c{\\alpha_k}\\nabla{{f_k}^T}p_k\" eeimg=\"1\"/></p><h2>Wolfe-Powell准则</h2><p>Armijo-Goldstein准则有可能把最优步长因子排除在可接受<br/>区间外，更改其下界约束后，得到以下约束：<img src=\"https://www.zhihu.com/equation?tex=%7B%7Bf_k%7D%5ET%7Dp_k%5Cleq+f%28x_k%2B%7B%5Calpha_k%7D%7Bp_k%7D%29%5Cleq+f%28x_k%29%2B%7Bc_1%7D%7B%5Calpha_k%7D%5Cnabla%7B%7Bf_k%7D%5ET%7Dp_k+%5C%5C+%5Cleft+%7C+%5Cnabla%7B%7Bf%28x_k%2B%7B%5Calpha_k%7D%7Bp_k%7D%29%7D%5ET%7D%7Bp_k%7D+%5Cright+%7C+%3C%3D+c2%5Cleft+%7C+%5Cnabla%7B%7Bf_k%7D%5ET%7D%7Bp_k%7D+%5Cright+%7C\" alt=\"{{f_k}^T}p_k\\leq f(x_k+{\\alpha_k}{p_k})\\leq f(x_k)+{c_1}{\\alpha_k}\\nabla{{f_k}^T}p_k \\\\ \\left | \\nabla{{f(x_k+{\\alpha_k}{p_k})}^T}{p_k} \\right | &lt;= c2\\left | \\nabla{{f_k}^T}{p_k} \\right |\" eeimg=\"1\"/></p><p>这里重点说下这个，毕竟相较于Armijo，本方法为改进型。</p><p>Wolfe-Powell方法相较于精确一维搜索，不再采用进退法去寻找搜索区间。在进退法里面，是通过慢慢扩展生成区间，然后在在区间中查找合适的，而在Wolfe-Powell中我们可以直接定义步长区间的界限，比如[0,10000]，那么其会根据其准则去每次剔除不符合的区间，逐步缩小区间，其能够较为快速的跳过无用的计算，速度要快很多。</p><p>Wolfe-Powell方法计算过程中，也用到了插值方法，用以区间切割剔除。自己在L-BFGS方法上对该准则方法进行了实现，发现这个实现要比其他复杂很多，主要分为区间裁剪和查找求解俩个过程，大致流程如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-91d0f493af94a6869f714313ffcd4888_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1288\" data-rawheight=\"944\" class=\"origin_image zh-lightbox-thumb\" width=\"1288\" data-original=\"https://pic1.zhimg.com/v2-91d0f493af94a6869f714313ffcd4888_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1288&#39; height=&#39;944&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1288\" data-rawheight=\"944\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1288\" data-original=\"https://pic1.zhimg.com/v2-91d0f493af94a6869f714313ffcd4888_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-91d0f493af94a6869f714313ffcd4888_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c1869b0ee3a0c34ace1d8e141451d050_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1206\" data-rawheight=\"948\" class=\"origin_image zh-lightbox-thumb\" width=\"1206\" data-original=\"https://pic1.zhimg.com/v2-c1869b0ee3a0c34ace1d8e141451d050_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1206&#39; height=&#39;948&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1206\" data-rawheight=\"948\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1206\" data-original=\"https://pic1.zhimg.com/v2-c1869b0ee3a0c34ace1d8e141451d050_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c1869b0ee3a0c34ace1d8e141451d050_b.jpg\"/></figure><p>实现代码见<a href=\"https://link.zhihu.com/?target=https%3A//github.com/sloth2012/scalaML/blob/master/src/main/scala/com/lx/algos/ml/optim/newton/LBFGS.scala\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/sloth2012/sc</span><span class=\"invisible\">alaML/blob/master/src/main/scala/com/lx/algos/ml/optim/newton/LBFGS.scala</span><span class=\"ellipsis\"></span></a>，主要是调用fit_Wolfe_Powell函数，zoom即为定义的该函数中的一个函数zoomfunc，测试可见相关的测试代码。</p><h2>参考文献</h2><ol><li><a href=\"https://link.zhihu.com/?target=http%3A//www.cad.zju.edu.cn/home/zhx/csmath/lib/exe/fetch.php%3Fmedia%3D2011%3Aor-2011-3.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">最优化方法</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//www.codelast.com/%25E5%258E%259F%25E5%2588%259B%25E7%2594%25A8%25E4%25BA%25BA%25E8%25AF%259D%25E8%25A7%25A3%25E9%2587%258A%25E4%25B8%258D%25E7%25B2%25BE%25E7%25A1%25AE%25E7%25BA%25BF%25E6%2590%259C%25E7%25B4%25A2%25E4%25B8%25AD%25E7%259A%2584armijo-goldstein%25E5%2587%2586%25E5%2588%2599%25E5%258F%258Awo/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">用“人话”解释不精确线搜索中的Armijo-Goldstein准则及Wolfe-Powell准则</a></li><li><a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/mytestmy/article/details/16903537\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">无约束优化方法读书笔记—入门篇</a></li></ol>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "最优化", 
                    "tagLink": "https://api.zhihu.com/topics/19616819"
                }
            ], 
            "comments": [
                {
                    "userName": "王安", 
                    "userLink": "https://www.zhihu.com/people/105c66b13e1b3953c17736c2d3c4e03f", 
                    "content": "<p>作者是浙大的？</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "知乎用户", 
                            "userLink": "https://www.zhihu.com/people/0", 
                            "content": "并不是…", 
                            "likes": 0, 
                            "replyToAuthor": "王安"
                        }
                    ]
                }, 
                {
                    "userName": "vincent", 
                    "userLink": "https://www.zhihu.com/people/ff54e1b76721de097906366b03e74cb4", 
                    "content": "<p>谢谢分享。请教一个问题，一维搜索中，为啥没有类似梯度下降的算法呢，就是使用导数代替多维函数的梯度，进行迭代呢</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/32488992", 
            "userName": "战歌指挥官", 
            "userLink": "https://www.zhihu.com/people/f4ee3aaa7e722a47be3444599f9bbd5b", 
            "upvote": 1, 
            "title": "L-BFGS算法剖析", 
            "content": "<p>在BFGS算法或者DFP算法中，需要保存一个<img src=\"https://www.zhihu.com/equation?tex=N%5Ctimes+N\" alt=\"N\\times N\" eeimg=\"1\"/>的矩阵，该矩阵用于表示Hessian的逆矩阵的近似。其中<img src=\"https://www.zhihu.com/equation?tex=N\" alt=\"N\" eeimg=\"1\"/>表示特征维度。当特征维度较高时，这个矩阵会超级占用内存。例如在CTR预估等场景中，特征动辄百w到千w，存储这样一个巨大的矩阵，需要的内存往往一般服务器都无法满足。因此研究者们提出了一种近似方法来解决这个问题，这就是L-BFGS，L表示limited-memory或者limited-storage。</p><p>其基本思想是，不再存储Hessian矩阵（其实是逆矩阵），转而存储中间的一些信息，当然中间的信息量可能也很大，那就只保存最新的一些，通过他们推导计算出Hessian矩阵。这样，其与原有的实现逻辑丢失了一些，所以只能是近似算法，但是怎么占用的资源由<img src=\"https://www.zhihu.com/equation?tex=O%28N%5E2%29\" alt=\"O(N^2)\" eeimg=\"1\"/>降到了<img src=\"https://www.zhihu.com/equation?tex=O%28mN%29\" alt=\"O(mN)\" eeimg=\"1\"/>，其中<img src=\"https://www.zhihu.com/equation?tex=m\" alt=\"m\" eeimg=\"1\"/>表示最新的m个中间变量。</p><p>前面说过，BFGS的最后一步更新的公式为：<img src=\"https://www.zhihu.com/equation?tex=H_%7Bk%2B1%7D%3D%28I-%5Cfrac%7B%7Bs_k%7D%7B%7By_k%7D%5ET%7D%7D%7B%7B%7By_k%7D%5ET%7D%7Bs_k%7D%7D%29D_k%28I+-%5Cfrac%7B%7By_k%7D%7B%7Bs_k%7D%5ET%7D%7D%7B%7B%7By_k%7D%5ET%7D%7Bs_k%7D%7D%29%2B%5Cfrac%7B%7Bs_k%7D%7B%7Bs_k%7D%5ET%7D%7D%7B%7B%7By_k%7D%5ET%7D%7Bs_k%7D%7D\" alt=\"H_{k+1}=(I-\\frac{{s_k}{{y_k}^T}}{{{y_k}^T}{s_k}})D_k(I -\\frac{{y_k}{{s_k}^T}}{{{y_k}^T}{s_k}})+\\frac{{s_k}{{s_k}^T}}{{{y_k}^T}{s_k}}\" eeimg=\"1\"/>。</p><p>可以看到，整个过程只与<img src=\"https://www.zhihu.com/equation?tex=y_k\" alt=\"y_k\" eeimg=\"1\"/>和<img src=\"https://www.zhihu.com/equation?tex=s_k\" alt=\"s_k\" eeimg=\"1\"/>有关，每次保存这俩个变量，就能恢复每一轮的<img src=\"https://www.zhihu.com/equation?tex=H_k\" alt=\"H_k\" eeimg=\"1\"/>。<img src=\"https://www.zhihu.com/equation?tex=y_k\" alt=\"y_k\" eeimg=\"1\"/>和<img src=\"https://www.zhihu.com/equation?tex=s_k\" alt=\"s_k\" eeimg=\"1\"/>就是中间保存的变量，其都是<img src=\"https://www.zhihu.com/equation?tex=n%2A1\" alt=\"n*1\" eeimg=\"1\"/>的向量。相比巨大的<img src=\"https://www.zhihu.com/equation?tex=N\" alt=\"N\" eeimg=\"1\"/>，其一般小很多，<strong>一般在实践中<img src=\"https://www.zhihu.com/equation?tex=m\" alt=\"m\" eeimg=\"1\"/>多取3到20之间</strong>。</p><p>保存的逻辑很简单，但是实际操作中，由于最后更新的为特征的权重<img src=\"https://www.zhihu.com/equation?tex=x_k\" alt=\"x_k\" eeimg=\"1\"/>，其一般更新形式为<img src=\"https://www.zhihu.com/equation?tex=x_%7Bk%2B1%7D%3Dx_k-%7B%5Clambda_k%7D%7BH_k%7D%7Bg_k%7D\" alt=\"x_{k+1}=x_k-{\\lambda_k}{H_k}{g_k}\" eeimg=\"1\"/>，其中<img src=\"https://www.zhihu.com/equation?tex=%7B%5Clambda_k%7D\" alt=\"{\\lambda_k}\" eeimg=\"1\"/>为更新的一个步长，一般通过一维搜索或信赖域等算法求得，可参照<a href=\"https://zhuanlan.zhihu.com/p/32094294\" class=\"internal\">拟牛顿法之DFP算法剖析</a>一维搜索部分求得。因此一般，均是采用高效方式求得<img src=\"https://www.zhihu.com/equation?tex=%7BH_k%7D%7Bg_k%7D\" alt=\"{H_k}{g_k}\" eeimg=\"1\"/>。</p><p>在<a href=\"https://link.zhihu.com/?target=https%3A//link.springer.com/book/10.1007/978-0-387-40065-5\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Numerical Optimization</a>的第7章讲述了两步循环求得<img src=\"https://www.zhihu.com/equation?tex=%7BH_k%7D%7Bg_k%7D\" alt=\"{H_k}{g_k}\" eeimg=\"1\"/>的过程，主要是将递推的方式进行了公式推导。其步骤如下：</p><p><figure><noscript><img src=\"https://pic1.zhimg.com/v2-d0d9dac5da4609e506d4c6101e24c718_b.jpg\" alt=\"\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" alt=\"\" class=\"content_image lazy\" data-actualsrc=\"https://pic1.zhimg.com/v2-d0d9dac5da4609e506d4c6101e24c718_b.jpg\"/></figure></p><p>这里的<img src=\"https://www.zhihu.com/equation?tex=%5CLambda+f_k\" alt=\"\\Lambda f_k\" eeimg=\"1\"/>即是我们所的<img src=\"https://www.zhihu.com/equation?tex=g_k\" alt=\"g_k\" eeimg=\"1\"/>，其中<img src=\"https://www.zhihu.com/equation?tex=%5Crho\" alt=\"\\rho\" eeimg=\"1\"/>是前面<img src=\"https://www.zhihu.com/equation?tex=H_%7Bk%2B1%7D%3D%28I-%5Cfrac%7B%7Bs_k%7D%7B%7By_k%7D%5ET%7D%7D%7B%7B%7By_k%7D%5ET%7D%7Bs_k%7D%7D%29D_k%28I+-%5Cfrac%7B%7By_k%7D%7B%7Bs_k%7D%5ET%7D%7D%7B%7B%7By_k%7D%5ET%7D%7Bs_k%7D%7D%29%2B%5Cfrac%7B%7Bs_k%7D%7B%7Bs_k%7D%5ET%7D%7D%7B%7B%7By_k%7D%5ET%7D%7Bs_k%7D%7D\" alt=\"H_{k+1}=(I-\\frac{{s_k}{{y_k}^T}}{{{y_k}^T}{s_k}})D_k(I -\\frac{{y_k}{{s_k}^T}}{{{y_k}^T}{s_k}})+\\frac{{s_k}{{s_k}^T}}{{{y_k}^T}{s_k}}\" eeimg=\"1\"/>中的分母部分，即<img src=\"https://www.zhihu.com/equation?tex=%5Crho_k%3D%5Cfrac%7B1%7D%7B%7B%7By_k%7D%5ET%7D%7Bs_k%7D%7D\" alt=\"\\rho_k=\\frac{1}{{{y_k}^T}{s_k}}\" eeimg=\"1\"/>。 如此更新完后，更新<img src=\"https://www.zhihu.com/equation?tex=x_%7Bk%2B1%7D\" alt=\"x_{k+1}\" eeimg=\"1\"/>，其它就与BFGS一样了。</p><h2>总结</h2><p>自己在实验完后，简单测试几把，发现其实相较于BFGS，其准确度并没降低太多。此外，在特征维度较低时，其实质上每步多了一个恢复<img src=\"https://www.zhihu.com/equation?tex=H_k\" alt=\"H_k\" eeimg=\"1\"/>的操作，因此效率也并不见得比BFGS高效。但是在特征维度较高时，其优势体现就明显很多。 最后，代码在<a href=\"https://link.zhihu.com/?target=https%3A//github.com/sloth2012/scalaML/blob/master/src/main/scala/com/lx/algos/ml/optim/newton/LBFGS.scala\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/sloth2012/sc</span><span class=\"invisible\">alaML/blob/master/src/main/scala/com/lx/algos/ml/optim/newton/LBFGS.scala</span><span class=\"ellipsis\"></span></a>，使用仍在<a href=\"https://link.zhihu.com/?target=https%3A//github.com/sloth2012/scalaML/blob/master/src/test/scala/com/lx/algos/ml/NewtonTest.scala\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/sloth2012/sc</span><span class=\"invisible\">alaML/blob/master/src/test/scala/com/lx/algos/ml/NewtonTest.scala</span><span class=\"ellipsis\"></span></a>。熟悉前面BFGS代码可以发现，仅是在更新<img src=\"https://www.zhihu.com/equation?tex=H_k\" alt=\"H_k\" eeimg=\"1\"/>的部分做了些更改，其它部分完全一致。</p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/32443772", 
            "userName": "战歌指挥官", 
            "userLink": "https://www.zhihu.com/people/f4ee3aaa7e722a47be3444599f9bbd5b", 
            "upvote": 1, 
            "title": "BFGS算法剖析与实现", 
            "content": "<p>之前介绍DFP算法是用以进行Hessian逆矩阵的近似，这里所的BFGS是从逼近Hessian原始矩阵出发，然后进行一系列推导的过程。其最后通过Sherman-Morrison公式将输出进行变换，仍然以更新Hessian逆矩阵来实现的，所不同的是由于其推导的初始目标不同，因此最后的迭代公式与DFP不同。最后一步的更新公式为：<img src=\"https://www.zhihu.com/equation?tex=H_%7Bk%2B1%7D%3D%28I-%5Cfrac%7B%7Bs_k%7D%7B%7By_k%7D%5ET%7D%7D%7B%7B%7By_k%7D%5ET%7D%7Bs_k%7D%7D%29D_k%28I+-%5Cfrac%7B%7By_k%7D%7B%7Bs_k%7D%5ET%7D%7D%7B%7B%7By_k%7D%5ET%7D%7Bs_k%7D%7D%29%2B%5Cfrac%7B%7Bs_k%7D%7B%7Bs_k%7D%5ET%7D%7D%7B%7B%7By_k%7D%5ET%7D%7Bs_k%7D%7D\" alt=\"H_{k+1}=(I-\\frac{{s_k}{{y_k}^T}}{{{y_k}^T}{s_k}})D_k(I -\\frac{{y_k}{{s_k}^T}}{{{y_k}^T}{s_k}})+\\frac{{s_k}{{s_k}^T}}{{{y_k}^T}{s_k}}\" eeimg=\"1\"/>其它步骤可见<a href=\"https://zhuanlan.zhihu.com/p/32094294\" class=\"internal\">拟牛顿法之DFP算法剖析</a>。</p><h2>代码</h2><p>代码位置见：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/sloth2012/scalaML/blob/master/src/main/scala/com/lx/algos/ml/optim/newton/BFGS.scala\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/sloth2012/sc</span><span class=\"invisible\">alaML/blob/master/src/main/scala/com/lx/algos/ml/optim/newton/BFGS.scala</span><span class=\"ellipsis\"></span></a>。<br/>使用见：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/sloth2012/scalaML/blob/master/src/test/scala/com/lx/algos/ml/NewtonTest.scala\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/sloth2012/sc</span><span class=\"invisible\">alaML/blob/master/src/test/scala/com/lx/algos/ml/NewtonTest.scala</span><span class=\"ellipsis\"></span></a>。</p><h2>小收获（bug fix备忘）</h2><p>修正版的DFP或者BFGS，其要求<img src=\"https://www.zhihu.com/equation?tex=%7B%7By_k%7D%5ET%7D%7Bs_k%7D\" alt=\"{{y_k}^T}{s_k}\" eeimg=\"1\"/>正定，其中<img src=\"https://www.zhihu.com/equation?tex=y_k%3Dg_%7Bk%2B1%7D+-+g_k\" alt=\"y_k=g_{k+1} - g_k\" eeimg=\"1\"/>以及<img src=\"https://www.zhihu.com/equation?tex=%7Bs_k%7D%3D%7BX_%7Bk%2B1%7D%7D-%7BX_k%7D\" alt=\"{s_k}={X_{k+1}}-{X_k}\" eeimg=\"1\"/>。这里的<img src=\"https://www.zhihu.com/equation?tex=X\" alt=\"X\" eeimg=\"1\"/>并非是输入，而是权重。计算需要知道的是<img src=\"https://www.zhihu.com/equation?tex=g_k\" alt=\"g_k\" eeimg=\"1\"/>用一阶原始梯度就行，不用加负号，之前加了负号后，导致迭代无法正定约束。因此之前的DFP算法剖析中，那个正定放松约束可去除，即直接<img src=\"https://www.zhihu.com/equation?tex=%7B%7By_k%7D%5ET%7D%7Bs_k%7D%3E0\" alt=\"{{y_k}^T}{s_k}&gt;0\" eeimg=\"1\"/>。</p><p>数值实验指出, BFGS算法是最好的变尺度算法, 当变量个数不超过100时, 通常BFGS法比共轭梯度法效果好.</p><h2>参考文献</h2><ul><li><a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/itplus/article/details/21897443\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">牛顿法与拟牛顿法学习笔记（四）BFGS 算法</a></li></ul>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/32406552", 
            "userName": "战歌指挥官", 
            "userLink": "https://www.zhihu.com/people/f4ee3aaa7e722a47be3444599f9bbd5b", 
            "upvote": 27, 
            "title": "SWATS算法剖析（自动切换adam与sgd）", 
            "content": "<p>SWATS是ICLR在2018的高分论文，提出的一种自动由Adam切换为SGD而实现更好的泛化性能的方法。</p><p>论文名为<strong>Improving Generalization Performance by Switching from Adam to SGD</strong>，下载地址为：<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1712.07628\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/abs/1712.0762</span><span class=\"invisible\">8</span><span class=\"ellipsis\"></span></a>。</p><p>作者指出，基于历史梯度平方的滑动平均值的如adam等算法并不能收敛到最优解，因此在泛化误差上可能要比SGD等方法差，因此提出了一种转换机制，试图让算法自动在经过一定轮次的adam学习后，转而由SGD去执行接下来的操作。</p><p>算法本身思想很简单，就是采用adam这种无需操心learning rate的方法，在开始阶段进行梯度下降，但是在学习到一定阶段后，由SGD接管。这里前面的部分与常规的adam实现区别不大，重要的是在切换到sgd后，这个更新的learning rate如何计算。 整个算法步骤流程如下：</p><p><figure><noscript><img src=\"https://pic4.zhimg.com/v2-958d364456bd8191096078b04f3748bf_b.jpg\" alt=\"\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" alt=\"\" class=\"content_image lazy\" data-actualsrc=\"https://pic4.zhimg.com/v2-958d364456bd8191096078b04f3748bf_b.jpg\"/></figure></p><p>熟悉adam的应该能熟悉蓝色的部分，这个就是adam的原生实现过程。</p><p>作者比较trick的地方就是14行到24行这一部分。这一部分作者做了部分推导，<img src=\"https://www.zhihu.com/equation?tex=%5CLambda%3D%5Clambda_k%2F%281-%7B%5Cbeta_2%7D%5Ek%29\" alt=\"\\Lambda=\\lambda_k/(1-{\\beta_2}^k)\" eeimg=\"1\"/>作为最后的切换learning rate。</p><p>算法的整个实现逻辑并不复杂，这里列出自己实现时遇到的一些问题。</p><h2>填坑 &amp; 问题</h2><ol><li>在上面的算法流程第12行，有个<img src=\"https://www.zhihu.com/equation?tex=%5Calpha_k\" alt=\"\\alpha_k\" eeimg=\"1\"/>，这个在整个流程中未介绍如何实现，本人阅读论文后，发现应该是学习率衰减的设计。一如很多深度学习策略一样，这里可以设置经过若干轮迭代后，学习率降为原来的1/N。在论文中，作者使用了在150轮后，将学习速率减少10倍。即<img src=\"https://www.zhihu.com/equation?tex=%5Calpha_%7Bk%2B1%7D%3D%5Cleft%5C%7B%5Cbegin%7Bmatrix%7D+%7B%5Calpha_k%2F10%7D%26+if%28k%5C%25150%3D%3D0%29%5C%5C+%5Calpha_k+%26+%5Calpha_0%3D%5Calpha+%5Cend%7Bmatrix%7D%5Cright.\" alt=\"\\alpha_{k+1}=\\left\\{\\begin{matrix} {\\alpha_k/10}&amp; if(k\\%150==0)\\\\ \\alpha_k &amp; \\alpha_0=\\alpha \\end{matrix}\\right.\" eeimg=\"1\"/>。</li><li>上面说了<img src=\"https://www.zhihu.com/equation?tex=%5Calpha_k\" alt=\"\\alpha_k\" eeimg=\"1\"/>的更新，我们通过公式推导，其实能发现<img src=\"https://www.zhihu.com/equation?tex=%5Clambda_k\" alt=\"\\lambda_k\" eeimg=\"1\"/>和<img src=\"https://www.zhihu.com/equation?tex=%5Calpha_k\" alt=\"\\alpha_k\" eeimg=\"1\"/>有一定的关系，自己代码实现的版本，发现切换的时机很大程度上和<img src=\"https://www.zhihu.com/equation?tex=%5Calpha_k\" alt=\"\\alpha_k\" eeimg=\"1\"/>有关，因为切换涉及到第17行的一个比较过程，<img src=\"https://www.zhihu.com/equation?tex=%5Clambda_k\" alt=\"\\lambda_k\" eeimg=\"1\"/>和<img src=\"https://www.zhihu.com/equation?tex=%5Cgamma_k\" alt=\"\\gamma_k\" eeimg=\"1\"/>本身都与<img src=\"https://www.zhihu.com/equation?tex=%5Calpha_k\" alt=\"\\alpha_k\" eeimg=\"1\"/>相关，当<img src=\"https://www.zhihu.com/equation?tex=%5Calpha_k\" alt=\"\\alpha_k\" eeimg=\"1\"/>降一个量级时，<img src=\"https://www.zhihu.com/equation?tex=%7C%5Cfrac%7B%5Clambda_k%7D%7B1-%7B%5Cbeta_2%7D%5Ek%7D-%5Cgamma_k\" alt=\"|\\frac{\\lambda_k}{1-{\\beta_2}^k}-\\gamma_k\" eeimg=\"1\"/>|本身也会更接近<img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/>。其有些类似正比关系，因此一般都是在经过一定轮次的衰减后，才能触发SGD切换时机。这一点目前本人实现验证是这样，未深入推理。</li><li>这个<img src=\"https://www.zhihu.com/equation?tex=%5Calpha_k\" alt=\"\\alpha_k\" eeimg=\"1\"/>还有个坑，就是实现该算法，开始不太清楚这个k到底指的是epoch，还是指的经历的batch数量。最后按照常规学习率衰减应该是按照epoch来算的，因此推测其k应该为epoch。</li><li>还有和大坑是<img src=\"https://www.zhihu.com/equation?tex=%5CLambda\" alt=\"\\Lambda\" eeimg=\"1\"/>作为学习率，在切换到SGD后应一直不变，该值为标量，因此应该如常用eta等学习率一样，为正值，因此需要在17行加个约束，即<img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Clambda_k%7D%7B1-%7B%5Cbeta_2%7D%5Ek%7D%3E0\" alt=\"\\frac{\\lambda_k}{1-{\\beta_2}^k}&gt;0\" eeimg=\"1\"/>。（该场景难以复现，之前有次更新发现不设置为正值时，导致切换sgd后准确度大减）</li></ol><h2>总结</h2><p>通过若干的对比，该论文变相增加了一些超参数，所以实际使用有待商榷。自己的数据集上经常就在还未满足切换条件就已经收敛了。 目前已做了相应的实现，放在scalaML中，位置为<a href=\"https://link.zhihu.com/?target=https%3A//github.com/sloth2012/scalaML/blob/master/src/main/scala/com/lx/algos/ml/optim/GradientDescent/SWATS.scala\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/sloth2012/sc</span><span class=\"invisible\">alaML/blob/master/src/main/scala/com/lx/algos/ml/optim/GradientDescent/SWATS.scala</span><span class=\"ellipsis\"></span></a>，使用见<a href=\"https://link.zhihu.com/?target=https%3A//github.com/sloth2012/scalaML/blob/master/src/test/scala/com/lx/algos/ml/GradientDescentTest.scala\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/sloth2012/sc</span><span class=\"invisible\">alaML/blob/master/src/test/scala/com/lx/algos/ml/GradientDescentTest.scala</span><span class=\"ellipsis\"></span></a>。最后想要查看切换过程的话，建议将early_stop设置为false，然后将学习率衰减系数设置低一点。 代码目前仅支持二分类。</p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": [
                {
                    "userName": "夏天", 
                    "userLink": "https://www.zhihu.com/people/04d252382b9ebd81de12ffb5eb63952c", 
                    "content": "<p>博主，我的实验中发现两者的差值具有很大的差异，对于投影计算得到的sgd速率与累积的sgd移动平均值变化不在一个量级上，sgd速率每次衰减后有明显的变化，但移动平均值变化比较小，导致两者差距越来越大，你的实验结果是这样吗</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "胡文博", 
                    "userLink": "https://www.zhihu.com/people/a568096564463b7fa3135e6750809ae6", 
                    "content": "<p>刚看到这篇文章，这就有人开始复现了，赞</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "夏天", 
                    "userLink": "https://www.zhihu.com/people/04d252382b9ebd81de12ffb5eb63952c", 
                    "content": "<p>我也实验了一下，但是切换条件很难达到，博主知道是什么原因吗？还有alphak是adam本身就存在的，所以论文没有介绍.</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "知乎用户", 
                            "userLink": "https://www.zhihu.com/people/0", 
                            "content": "<p>这个我觉得主要是和|有关，我打印过迭代中这个式子的差值，最后发现其基本上在学习率每次衰减后，才会有量级的变化。原文作者打印的那个对比图中也是在其epoch 150轮衰减后有所变化，因此这个应该是与alphak有点类似正比的关系，毕竟和本身都与相关。</p>", 
                            "likes": 0, 
                            "replyToAuthor": "夏天"
                        }, 
                        {
                            "userName": "知乎用户", 
                            "userLink": "https://www.zhihu.com/people/0", 
                            "content": "<p>发现公式打不出来。。。主要是我写的填坑和问题中的第二点说的把</p>", 
                            "likes": 0, 
                            "replyToAuthor": "夏天"
                        }
                    ]
                }, 
                {
                    "userName": "jason", 
                    "userLink": "https://www.zhihu.com/people/04137d41f894ca203f6e1292b3334c87", 
                    "content": "<p>博主，我感觉这个k可能是batch，而不是epoch，因为Adam和SGD更新权重的运算也是以k来计轮次的呢。</p>", 
                    "likes": 1, 
                    "childComments": [
                        {
                            "userName": "知乎用户", 
                            "userLink": "https://www.zhihu.com/people/0", 
                            "content": "<p>嗯 你说的对</p>", 
                            "likes": 0, 
                            "replyToAuthor": "jason"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/32265502", 
            "userName": "战歌指挥官", 
            "userLink": "https://www.zhihu.com/people/f4ee3aaa7e722a47be3444599f9bbd5b", 
            "upvote": 0, 
            "title": "Nagao算法剖析与实现", 
            "content": "<h2>Nagao算法介绍</h2><p>Nagao是一种快速统计文本中所有子串频次的算法。其是由1994年东京大学的长尾真教授提出，核心思想是通过将文本中的射串进行排序。 这里先普及下射串的概念，其是指有起点无终点的字符串。这样介绍有些难以理解，那这里可以以一个例子来进行描述：假设我们有一个长度为5的句子——“北京欢迎你”，那么其应该包含5个射串，分别是“北京欢迎你”、“京欢迎你”、“欢迎你”、“迎你”、“你”。这样一看，其实就是一个有序的序列子串构成了句子，每个这样的子串叫射串。只是叫法而已，不必纠结。</p><p>该算法主要可用于快速计算子串频次，能够在多种基础任务中使用。比如我们做一个无监督的分词任务，其前提假设是构成词的字应该经常在一起；这种指标一般可以通过互信息等熵的指标来表示，但是计算这些指标需要做一些子串的频次计算。</p><h2>Nagao算法流程</h2><p>假设现有语料KB，是个目录文件，目录下每个文件都是一些文本。现做一个统计，主要用于统计这些语料中所有子串的频次。这个结果应该极为稀疏，以中文为例，子串越长，代表文本中所包含的字越多，那么这个子串构成词的可能性越低，因此出现频次也会较少，毕竟文本还是需要以词表义的。针对这种长串情况，频次多数为1，我们应该更多的关注于高频串上，他们很可能就是一个词（这也是为何该算法可以用来做无监督分词）。流程步骤如下：</p><ol><li>读取语料的每个文件，按照句子粒度进行切分，每个句子作为一个原始字符串进行输入；</li><li>针对每个原始字符串按照示例的“北京欢迎你”那种方式切分子串，添加到PTable中（PTable为一个有序列表）；</li><li>全部语料过完后，得到PTable，然后进行构造LTable，构造方式如下： 假设现有P1,P2,L1,L2如下，P1代表PTable第一条子串，L1表示LTable第一个LTable的统计：</li></ol><p><figure><noscript><img src=\"https://pic2.zhimg.com/v2-ec4a412794c0e5656e18a01d0fc79bb5_b.jpg\" alt=\"\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" alt=\"\" class=\"content_image lazy\" data-actualsrc=\"https://pic2.zhimg.com/v2-ec4a412794c0e5656e18a01d0fc79bb5_b.jpg\"/></figure></p><p>这里，P和L计算是有错位的。可以看到，L就是统计相邻俩个串的共同前缀，比如L2就是P1和P2的共同前缀，后续的类似。可以看到其实LTable是比PTable长度少1的，因此一般将第一个L1值赋为0。 如此计算，得到LTable，我们就得可以进行频次计算了。计算逻辑如下：</p><ol><li>按顺序取PTable的元素，假设当前取到元素PTable[i]；</li><li>针对PTable[i]的所有子串进行词频统计，依据是LTable中是否有公共部分，有的话加1，再接着遍历，直到所有PTable的所有<strong>射串</strong>遍历完毕（该部分计算要复杂的多，需要结合LTable辅助计算，详细可见后文代码中<a href=\"https://link.zhihu.com/?target=https%3A//github.com/sloth2012/scalaML/blob/master/src/main/scala/com/lx/algos/nlp/nagao/Nagao.scala%23L83-L112\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/sloth2012/sc</span><span class=\"invisible\">alaML/blob/master/src/main/scala/com/lx/algos/nlp/nagao/Nagao.scala#L83-L112</span><span class=\"ellipsis\"></span></a>）；</li></ol><h2>进阶</h2><p>前面说过该算法能够用来进行无监督分词，其实就是新词发现。具体改进就是，我们设计俩个PTable和LTable，分别是语料从左到右的遍历和从右到左的遍历结果。如此就得到了左子串的一些词频统计和右子串的词频统计，进而就能计算每个串的左熵和右熵，再进行全局计算，得到一个串组成的最佳互信息表示，以作为分词依据。</p><h2>坑+优化</h2><p>实际操作中，这个算法还是有些难度的。首先是，由于PTable相当于整个语料库，保存在内存中，因此这个语料不能过大，当然这也可以通过一些并行手段规避。 其次是若计算所有的子串，计算量还是挺大的，这个可以加一些子串长度进行约束限制，比较太长的串本身也没有意义。</p><h2>代码</h2><p>自己实现了一版用于新词发现的，输出为每个词的词频，左右熵以及互信息。代码放在<a href=\"https://link.zhihu.com/?target=https%3A//github.com/sloth2012/scalaML/tree/master/src/main/scala/com/lx/algos/nlp/nagao\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/sloth2012/sc</span><span class=\"invisible\">alaML/tree/master/src/main/scala/com/lx/algos/nlp/nagao</span><span class=\"ellipsis\"></span></a>目录下，使用见test部分的测试样例。</p><h2>参考文献</h2><ul><li><a href=\"https://link.zhihu.com/?target=http%3A//www.doc88.com/p-664123446503.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Nagao的串频统计方法</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//yq.aliyun.com/ziliao/157494%3Fspm%3D5176.8246799.0.0.TIdBOH\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">java使用Nagao算法实现新词发现、热门词的挖掘_java</a></li></ul>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "Scala", 
                    "tagLink": "https://api.zhihu.com/topics/19566465"
                }
            ], 
            "comments": [
                {
                    "userName": "沙漠之狐", 
                    "userLink": "https://www.zhihu.com/people/c14fe88218dbad38d7cf18a9880c32b9", 
                    "content": "<p>能否解释一下 如何通过P表和L表 来统计词频的，这一点 怎么都看不懂呀</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "知乎用户", 
                            "userLink": "https://www.zhihu.com/people/0", 
                            "content": "本身就是有序的p表，遍历下就能获得。详细看计算逻辑部分。", 
                            "likes": 0, 
                            "replyToAuthor": "沙漠之狐"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/32094294", 
            "userName": "战歌指挥官", 
            "userLink": "https://www.zhihu.com/people/f4ee3aaa7e722a47be3444599f9bbd5b", 
            "upvote": 1, 
            "title": "拟牛顿法之DFP算法剖析", 
            "content": "<p>DFP拟牛顿法也称为DFP校正方法，是第一个拟牛顿法，由Davidon最早提出，后经Fletcher和Powell解释和改进，在命名时以三个人名字的首字母命名。 那拟牛顿法多数时候均为对二阶导hessian矩阵或其逆矩阵的近似逼近，DFP所逼近的就是hessian逆矩阵。</p><p>其算法步骤如下： 假设已知目标函数<img src=\"https://www.zhihu.com/equation?tex=f%28X%29\" alt=\"f(X)\" eeimg=\"1\"/>及梯度<img src=\"https://www.zhihu.com/equation?tex=g%28X%29\" alt=\"g(X)\" eeimg=\"1\"/>,迭代轮数n，终止条件<img src=\"https://www.zhihu.com/equation?tex=%5Cvarepsilon%3E0\" alt=\"\\varepsilon&gt;0\" eeimg=\"1\"/>.</p><ol><li>取初始点<img src=\"https://www.zhihu.com/equation?tex=X_0%5Cin+R%5En\" alt=\"X_0\\in R^n\" eeimg=\"1\"/>，精度<img src=\"https://www.zhihu.com/equation?tex=%5Cvarepsilon%3E0\" alt=\"\\varepsilon&gt;0\" eeimg=\"1\"/>；</li><li>计算<img src=\"https://www.zhihu.com/equation?tex=g_0%3D%5CDelta+f%28X%29\" alt=\"g_0=\\Delta f(X)\" eeimg=\"1\"/>，若<img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7BVmatrix%7Dg_0%5Cend%7BVmatrix%7D%5Cleqslant+%5Cvarepsilon\" alt=\"\\begin{Vmatrix}g_0\\end{Vmatrix}\\leqslant \\varepsilon\" eeimg=\"1\"/>则停止，得到权重<img src=\"https://www.zhihu.com/equation?tex=X%5E%2A%3DX_0\" alt=\"X^*=X_0\" eeimg=\"1\"/>;否则转3；</li><li>令<img src=\"https://www.zhihu.com/equation?tex=k%3D1\" alt=\"k=1\" eeimg=\"1\"/>，<img src=\"https://www.zhihu.com/equation?tex=H_0%3DE\" alt=\"H_0=E\" eeimg=\"1\"/>;</li><li>令<img src=\"https://www.zhihu.com/equation?tex=d_k%3D-%7BH_k%7D%7Bg_k%7D\" alt=\"d_k=-{H_k}{g_k}\" eeimg=\"1\"/>;</li><li>求<img src=\"https://www.zhihu.com/equation?tex=%5Clambda_k%3Amin%7Bf%28X_k%2B%7B%5Clambda%7D%7Bd_k%7D%29%7D%3Df%28X_k%2B%7B%5Clambda_k%7D%7Bd_k%7D%29\" alt=\"\\lambda_k:min{f(X_k+{\\lambda}{d_k})}=f(X_k+{\\lambda_k}{d_k})\" eeimg=\"1\"/>，令<img src=\"https://www.zhihu.com/equation?tex=s_k%3D%7B%5Clambda_k%7D%7Bd_k%7D%2CX_%7Bk%2B1%7D%3DX_k%2Bs_k\" alt=\"s_k={\\lambda_k}{d_k},X_{k+1}=X_k+s_k\" eeimg=\"1\"/>;</li><li>计算<img src=\"https://www.zhihu.com/equation?tex=g_%7Bk%2B1%7D%3D%5CDelta+f%28x_%7Bk%2B1%7D%29\" alt=\"g_{k+1}=\\Delta f(x_{k+1})\" eeimg=\"1\"/>,若<img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7BVmatrix%7Dg_%7Bk%2B1%7D%5Cend%7BVmatrix%7D%5Cleqslant+%5Cvarepsilon\" alt=\"\\begin{Vmatrix}g_{k+1}\\end{Vmatrix}\\leqslant \\varepsilon\" eeimg=\"1\"/>则停止，得到权重<img src=\"https://www.zhihu.com/equation?tex=X%5E%2A%3DX_%7Bk%2B1%7D\" alt=\"X^*=X_{k+1}\" eeimg=\"1\"/>;否则转7；</li><li>计算<img src=\"https://www.zhihu.com/equation?tex=y_k%3Dg_%7Bk%2B1%7D+-+g_k\" alt=\"y_k=g_{k+1} - g_k\" eeimg=\"1\"/>;</li><li>计算<img src=\"https://www.zhihu.com/equation?tex=H_%7Bk%2B1%7D%3DH_k%2B%5Cfrac%7B%7Bs_k%7D%7B%7Bs_k%7D%5ET%7D%7D%7B%7B%7Bs_k%7D%5ET%7D%7By_k%7D%7D-%5Cfrac%7B%7BH_k%7D%7By_k%7D%7B%7By_k%7D%5ET%7D%7BH_k%7D%7D%7B%7B%7By_k%7D%5ET%7D%7BH_k%7D%7By_k%7D%7D\" alt=\"H_{k+1}=H_k+\\frac{{s_k}{{s_k}^T}}{{{s_k}^T}{y_k}}-\\frac{{H_k}{y_k}{{y_k}^T}{H_k}}{{{y_k}^T}{H_k}{y_k}}\" eeimg=\"1\"/>;</li><li>令<img src=\"https://www.zhihu.com/equation?tex=k%3Dk%2B1\" alt=\"k=k+1\" eeimg=\"1\"/>，转4.</li></ol><p>求得的X即为每个特征的权重。一般而言，针对公式<img src=\"https://www.zhihu.com/equation?tex=H_k\" alt=\"H_k\" eeimg=\"1\"/>会补充一个正定的充要条件：<img src=\"https://www.zhihu.com/equation?tex=%7Bs_k%7D%5ET%7By_k%7D%3E0\" alt=\"{s_k}^T{y_k}&gt;0\" eeimg=\"1\"/>。但是在实际实现中，本人发现很多时候，<img src=\"https://www.zhihu.com/equation?tex=%7Bs_k%7D%5ET%7By_k%7D\" alt=\"{s_k}^T{y_k}\" eeimg=\"1\"/>并不一定大于0，但距离0差距很小，一般在0.0001以内，因此可以将该约束进行放松，<img src=\"https://www.zhihu.com/equation?tex=%7Bs_k%7D%5ET%7By_k%7D%3E-0.0001\" alt=\"{s_k}^T{y_k}&gt;-0.0001\" eeimg=\"1\"/>这样。</p><p>相比于之前介绍的基于GD的方法，二阶拟牛顿法要的实现要复杂的多。就以DFP这个算法为例。首先，其每次所求必须过全量样本，这就导致其无法执行如minibatch等类似的方式。当然，这本身是SGD或BGD等的优势所在。 其次，在第5步的求最小步长的过程，该问题本身也是一个优化问题。解决该优化目标的过程叫一维搜索。一维搜索的方法有很多，这里主要介绍一下采用黄金分割法的实现。主要分为俩步：<strong>a. 找出满足条件的大致下降区间。\nb. 采用黄金分隔法找出最合适的极小值点。</strong></p><p>DFP实现时，在步骤a中，每轮迭代中均采用试探法进行尝试（假设当前迭代轮数为k）：</p><ol><li>初始设定<img src=\"https://www.zhihu.com/equation?tex=%5Clambda_1%3D0\" alt=\"\\lambda_1=0\" eeimg=\"1\"/>，前进步长<img src=\"https://www.zhihu.com/equation?tex=h_1+%5Cin+%280%2C1%29\" alt=\"h_1 \\in (0,1)\" eeimg=\"1\"/>，则<img src=\"https://www.zhihu.com/equation?tex=%5Clambda_2%3D%5Clambda_1%2Bh_1\" alt=\"\\lambda_2=\\lambda_1+h_1\" eeimg=\"1\"/>;</li><li>计算<img src=\"https://www.zhihu.com/equation?tex=f1%3D%7Bf%28X_k%2B%7B%5Clambda_1%7D%7Bd_k%7D%29%7D%EF%BC%8Cf2%3D%7Bf%28X_k%2B%7B%5Clambda_2%7D%7Bd_k%7D%29%7D\" alt=\"f1={f(X_k+{\\lambda_1}{d_k})}，f2={f(X_k+{\\lambda_2}{d_k})}\" eeimg=\"1\"/>，比较<img src=\"https://www.zhihu.com/equation?tex=f1\" alt=\"f1\" eeimg=\"1\"/>和<img src=\"https://www.zhihu.com/equation?tex=f2\" alt=\"f2\" eeimg=\"1\"/>，若<img src=\"https://www.zhihu.com/equation?tex=f1%3Ef2\" alt=\"f1&gt;f2\" eeimg=\"1\"/>，说明前进方向正确，应该再次前进，因此可激进一些，让下一次直接前进2倍，即<img src=\"https://www.zhihu.com/equation?tex=h%3D2h\" alt=\"h=2h\" eeimg=\"1\"/>；否则，就后退<img src=\"https://www.zhihu.com/equation?tex=h\" alt=\"h\" eeimg=\"1\"/>，并<img src=\"https://www.zhihu.com/equation?tex=swap%28f1%2Cf2%29\" alt=\"swap(f1,f2)\" eeimg=\"1\"/>和<img src=\"https://www.zhihu.com/equation?tex=swap%28%5Clambda_1%2C%5Clambda_2%29\" alt=\"swap(\\lambda_1,\\lambda_2)\" eeimg=\"1\"/>，以保持下降方向。</li><li>可以再找一个<img src=\"https://www.zhihu.com/equation?tex=%5Clambda_3%3D%5Clambda_2%2Bh\" alt=\"\\lambda_3=\\lambda_2+h\" eeimg=\"1\"/>，<img src=\"https://www.zhihu.com/equation?tex=f3%3D%7Bf%28X_k%2B%7B%5Clambda_3%7D%7Bd_k%7D%29%7D\" alt=\"f3={f(X_k+{\\lambda_3}{d_k})}\" eeimg=\"1\"/>来把控下一次终点是否合理，以用于终止寻找。那通过刚刚的第二步得知，<img src=\"https://www.zhihu.com/equation?tex=f2%3Cf1\" alt=\"f2&lt;f1\" eeimg=\"1\"/>，这里比较下<img src=\"https://www.zhihu.com/equation?tex=f2\" alt=\"f2\" eeimg=\"1\"/>和<img src=\"https://www.zhihu.com/equation?tex=f3\" alt=\"f3\" eeimg=\"1\"/>，看看是否符合下降方向，若符合（<img src=\"https://www.zhihu.com/equation?tex=f2%3Ef3\" alt=\"f2&gt;f3\" eeimg=\"1\"/>），则继续寻找，并更新<img src=\"https://www.zhihu.com/equation?tex=f1%3Df2\" alt=\"f1=f2\" eeimg=\"1\"/>，<img src=\"https://www.zhihu.com/equation?tex=f2%3Df3\" alt=\"f2=f3\" eeimg=\"1\"/>，<img src=\"https://www.zhihu.com/equation?tex=%5Clambda_1%3D%5Clambda_2\" alt=\"\\lambda_1=\\lambda_2\" eeimg=\"1\"/>，<img src=\"https://www.zhihu.com/equation?tex=%5Clambda_2%3D%5Clambda_3\" alt=\"\\lambda_2=\\lambda_3\" eeimg=\"1\"/>，这样的用意是进一步查找到了最小的一个下降区间<img src=\"https://www.zhihu.com/equation?tex=%28%5Clambda_1%2C%5Clambda_2%29\" alt=\"(\\lambda_1,\\lambda_2)\" eeimg=\"1\"/>。这里如果不满足下降条件，即（<img src=\"https://www.zhihu.com/equation?tex=f2%3C%3Df3\" alt=\"f2&lt;=f3\" eeimg=\"1\"/>），即认为寻找的方向已经终止了，那么比较下<img src=\"https://www.zhihu.com/equation?tex=%5Clambda_1\" alt=\"\\lambda_1\" eeimg=\"1\"/>和<img src=\"https://www.zhihu.com/equation?tex=%5Clambda_3\" alt=\"\\lambda_3\" eeimg=\"1\"/>即可找到最小区间，因为<img src=\"https://www.zhihu.com/equation?tex=f2\" alt=\"f2\" eeimg=\"1\"/>是在这两个对应的函数<img src=\"https://www.zhihu.com/equation?tex=f1\" alt=\"f1\" eeimg=\"1\"/>和<img src=\"https://www.zhihu.com/equation?tex=f3\" alt=\"f3\" eeimg=\"1\"/>之间，极小值点应该就存在这样的区间内，如此即可终止，返回区间结果。</li><li>重新转向2，再次进行试探。</li></ol><p>针对步骤b，采用黄金分割法要简单的多。毕竟这是初高中的知识，主要是每次采用黄金分割比例（0.618）不断的缩小区间，然后看区间起始点的f值是否小于最小阈值，若满足，则将其中间的点返回，否则继续查找。整个步骤有点类似二分查找。</p><p>okay，这里主要介绍的是采用黄金分割法的DFP拟牛顿法。本人在之前的scalaML工程中进行了实现，DFP代码为<a href=\"https://link.zhihu.com/?target=https%3A//github.com/sloth2012/scalaML/blob/master/src/main/scala/com/lx/algos/ml/optim/newton/DFP.scala\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/sloth2012/sc</span><span class=\"invisible\">alaML/blob/master/src/main/scala/com/lx/algos/ml/optim/newton/DFP.scala</span><span class=\"ellipsis\"></span></a>，使用样例在<a href=\"https://link.zhihu.com/?target=https%3A//github.com/sloth2012/scalaML/blob/master/src/test/scala/com/lx/algos/ml/NewtonTest.scala\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/sloth2012/sc</span><span class=\"invisible\">alaML/blob/master/src/test/scala/com/lx/algos/ml/NewtonTest.scala</span><span class=\"ellipsis\"></span></a>中。</p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/31767896", 
            "userName": "战歌指挥官", 
            "userLink": "https://www.zhihu.com/people/f4ee3aaa7e722a47be3444599f9bbd5b", 
            "upvote": 0, 
            "title": "高效的关系补全——EMNLP2015 SFE论文研读", 
            "content": "<p>近期做了一些关于知识推理的工作，主要是进行实体关系补全。目前采用了EMNLP2015这篇paper的实现。论文为<b><i>Efficient and Expressive Knowledge Base Completion Using Subgraph Feature Extraction. Matt Gardner and Tom Mitchell. EMNLP 2015</i></b>. (下载点此<a href=\"https://link.zhihu.com/?target=http%3A//rtw.ml.cmu.edu/emnlp2015_sfe\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">website</a>)</p><h2>主要思路</h2><p>传统的pra第一步是找到通路，即节点间是否存在路径，然后再开始随机游走计算。</p><p>EMNLP2015这篇paper认为第二步计算量大，且在之前的工作中表明，随机游走的概率并没卵用，因此仅仅采用了二值特征。</p><p>作者开源了其实现，代码在<a href=\"https://link.zhihu.com/?target=https%3A//github.com/matt-gardner/pra\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">matt-gardner/pra</a>。</p><h2>Subgraph Feature Extraction</h2><p>假设有其是节点source和target，先单独提取能代表source和target的子图特征。具体方法为，找寻以source为起点的n步以内的指定类型的到达节点i（中间节点）的路径。如此将source和target的都找到后，合并其在中间节点i的路径作为特征（合并时，target的路径需要反转）。这其中如果source或target节点找的路径经过对方，就直接选取。</p><p>有多类方式丰富特征：</p><ol><li>PRA-style features.上面的合并特征</li><li>针对1的Bigram类型的特征</li><li>One side features，即不像PRA那样必须由source走到target。可以直接将source或target的子图路径作为特征。思想是，假如想要预测一个城市是否是首都，那么可能根据所有首都城市都拥有较多的运动队即可标识。</li><li>one side features comparisons，具体是指将source和target游走的子图中具备相同类型的边的类型作为特征。比如奥巴马娶了米歇尔，那么“性别”可作为单独的一条边抽出来。</li><li>Vector space similarity features，向量空间，将边的类型用向量来表示，应该是边的embedding。看了下源码，貌似更low，需要自己指定哪些关系是近似的关系。</li><li>Any-Relation features. 是5的更弱的实现，直接认为所有关系是可以近似替换的，那么一些边可以用ANYREL来标识，表示任何类型的边都满足条件。</li></ol><p>源码还提供了更多的特征提取方式，未注释，尚未深究。</p><h2>负样本构造</h2><p>传统方式：采用PRA未在知识库中见到的样例作为负样本。</p><p>作者提出了采用personalized page rank的方式构造，思想是根据source和target的同类型节点计算得到的PPR得分抽样选取构造。</p><p>这俩种方式作者对比了下，发现差距不明显，只是自己选了PPR这种方式。</p><h2>特征描述解释性</h2><p>模型最后抽取的路径特征解释。</p><p>详见&lt;<a href=\"https://link.zhihu.com/?target=https%3A//github.com/matt-gardner/pra/issues/14\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Understand features observations · Issue #14 · matt-gardner/pra</a>&gt;</p><p>一些类似ANYREL之类标识的可参照论文。</p><h2>我的工作</h2><p>因为工作需要，需要复用该算法。然而原开源仅是为了训练和测试，所以较难直接使用（原code 训练和预测是同时进行的，没有剥离开来）。这里自己重构了一些模块，使得每次只需要加载上次训练的目录设置，和本次需要预测的环境目录，即可进行预测输出。</p><p>目前仅对nodepair类型的instance以及模型为LR做了更改，因此只能采用有限方式进行运行。代码放在<a href=\"https://link.zhihu.com/?target=https%3A//github.com/sloth2012/pra\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">sloth2012/pra</a>。预测运行代码见<a href=\"https://link.zhihu.com/?target=https%3A//github.com/sloth2012/pra/blob/master/src/test/scala/edu/cmu/ml/rtw/pra/models/PredictWithTrainedModelSpec.scala\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/sloth2012/pr</span><span class=\"invisible\">a/blob/master/src/test/scala/edu/cmu/ml/rtw/pra/models/PredictWithTrainedModelSpec.scala</span><span class=\"ellipsis\"></span></a>，主要是需要设置上个训练模型的experiment_spec上级目录，以及当前需要预测的数据的experiment_spec上级目录。</p>", 
            "topic": [
                {
                    "tag": "知识图谱", 
                    "tagLink": "https://api.zhihu.com/topics/19838204"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/31373261", 
            "userName": "战歌指挥官", 
            "userLink": "https://www.zhihu.com/people/f4ee3aaa7e722a47be3444599f9bbd5b", 
            "upvote": 1, 
            "title": "常用梯度下降算法（scala实现）", 
            "content": "<p>近期在github上维护了一个库，主要是用于深入学习一些算法的实现。鉴于scala代码撰写起来较为舒爽，所以全部算法都是基于scala来进行实现的。</p><p>该库目前主要的情况如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-438f404d0dcbdf67cdfe1c6b1357236f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1256\" data-rawheight=\"902\" class=\"origin_image zh-lightbox-thumb\" width=\"1256\" data-original=\"https://pic4.zhimg.com/v2-438f404d0dcbdf67cdfe1c6b1357236f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1256&#39; height=&#39;902&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1256\" data-rawheight=\"902\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1256\" data-original=\"https://pic4.zhimg.com/v2-438f404d0dcbdf67cdfe1c6b1357236f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-438f404d0dcbdf67cdfe1c6b1357236f_b.jpg\"/></figure><p>主要是参照很多文献和框架，结合自己的理解，将一些较为经典的梯度下降算法重新撸了一遍。工程名叫scalaML，目标是想用scala重新实现一套sklearn。目前看来遥遥无期，且先进行着吧。</p><p><b>工程链接为<a href=\"https://link.zhihu.com/?target=https%3A//github.com/sloth2012/scalaML\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">sloth2012/scalaML</a>。</b></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>目前的主要参考文献如下：</h2><ul><li><a href=\"https://link.zhihu.com/?target=https%3A//blog.slinuxer.com/2016/09/sgd-comparison\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">SGD算法比较 - Slinuxer</a></li><li><a href=\"https://link.zhihu.com/?target=http%3A//kissg.me/2017/07/23/gradient-descent/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ML之梯度下降算法 - kissg Blog</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//www.aiboy.pub/2017/09/10/A_Brief_Of_Optimization_Algorithms/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">梯度下降优化算法综述与PyTorch实现源码剖析</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/EE-NovRain/p/3810737.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">各大公司广泛使用的在线学习算法FTRL详解 - EE_NovRain - 博客园</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//www.kaggle.com/jiweiliu/ftrl-starter-code\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">FTRL starter code | Kaggle</a></li></ul>", 
            "topic": [
                {
                    "tag": "梯度下降", 
                    "tagLink": "https://api.zhihu.com/topics/19650497"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/29824075", 
            "userName": "战歌指挥官", 
            "userLink": "https://www.zhihu.com/people/f4ee3aaa7e722a47be3444599f9bbd5b", 
            "upvote": 2, 
            "title": "神经网络之Normalization", 
            "content": "<p>最近看了下神经网络的一些Normalization方法，下面针对各个进行描述。</p><h2><b>Batch Normalization：</b></h2><p>BN算法已经可以算是CNN结构网络的标配了，其主要思想其实是将图片预处理的白化思想挪到了隐藏层的数据处理中。我们知道常规的数据Normalization操作其实就是(x-µ)/ø，其中ø为数据的标准差，µ为均值。BN用了一个非常强的假设，认为数据整体上是符合这种标准正态分布的，但是实际上即使这样考虑，整体样本的均值和方差是不确定的（在过完全部样本之前），所以用了每个batch的样本的均值和方差来近似，这样也要求了每个batch的size不能太小。BN算法一般用于卷积层和激活层之间（原paper），但是后来实践中，有人提出质疑，在RELU等激活层之后，dropout之前加上效果比较好（<a href=\"https://link.zhihu.com/?target=https%3A//www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">reddit.com/r/MachineLea</span><span class=\"invisible\">rning/comments/67gonq/d_batch_normalization_before_or_after_relu/</span><span class=\"ellipsis\"></span></a>），具体实验结果可见<a href=\"https://link.zhihu.com/?target=https%3A//github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ducha-aiki/caffenet-benchmark</a>。</p><h2><b>Weight Normalization：</b></h2><p>WN算法是基于BN的一些缺点，如不能直接应用在RNN网络等缺点，从隐藏层参数出发的一种算法。正常隐藏层有y=Wx+b，x为输入。BN算法从x出发，对数据进行Normalization。WN算法将W进行拆解，将这一个参数扩展成俩个——一个标量和一个向量，分别代表权重和方向。WN算法与BN一样是一种参数重写的算法，作者给出了在一些条件下，WN等价于BN。</p><p>WN算法在可以适用于小批量的样本训练过程中，也能较好的支持RNN网络，但是初始参数的设置很敏感，需要玄学基础（微笑脸）。</p><h2><b>Layer Normalization：</b></h2><p>LN算法是基于BN不能处理RNN的时序单元（感觉LN就是专门针对RNN提出的）。LN其实是将BN的一些思想，用在了RNN的同层神经元中，保证同层有相同的均值方差即可。</p><blockquote>LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；而BN中则针对不同神经元输入计算均值和方差，同一个minibatch中的输入拥有相同的均值和方差。(<a href=\"https://zhuanlan.zhihu.com/p/26793298\" class=\"internal\">&lt;优化策略-2&gt;深度学习加速器Layer Normalization-LN</a>)</blockquote><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>总结：</b></h2><p>其实BN算法现在很多人也提出质疑，主要是说其一些假设太理想，现实数据分布并非如此，然而深度学习目前是‘no bb， show me result’状态。好的结果加上一些自圆其说（看不懂的公式）的推导，就能说服别人。（个人玄学基础有待提高(-.-)）</p><p>不瞎扯了，关于这三种方法，也有人总结如下：（<a href=\"https://link.zhihu.com/?target=https%3A//www.slideshare.net/KeigoNishida/layer-normalizationnips\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">slideshare.net/KeigoNis</span><span class=\"invisible\">hida/layer-normalizationnips</span><span class=\"ellipsis\"></span></a>）</p><figure><noscript><img data-rawheight=\"922\" src=\"https://pic1.zhimg.com/v2-4c5051a3f053cc18916435d70904e080_b.jpg\" data-rawwidth=\"872\" class=\"origin_image zh-lightbox-thumb\" width=\"872\" data-original=\"https://pic1.zhimg.com/v2-4c5051a3f053cc18916435d70904e080_r.jpg\"/></noscript><img data-rawheight=\"922\" src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;872&#39; height=&#39;922&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"872\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"872\" data-original=\"https://pic1.zhimg.com/v2-4c5051a3f053cc18916435d70904e080_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-4c5051a3f053cc18916435d70904e080_b.jpg\"/></figure><p>题外话，目前BN算法自提出到现在还是非常火，在各类工程实践中均有使用，而LN和WN并没有想象中的那么work。</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/29786939", 
            "userName": "战歌指挥官", 
            "userLink": "https://www.zhihu.com/people/f4ee3aaa7e722a47be3444599f9bbd5b", 
            "upvote": 5, 
            "title": "Pytorch手撕经典网络之AlexNet", 
            "content": "<p>下图为经典网络AlexNet的结构。</p><figure><noscript><img data-rawheight=\"384\" src=\"https://pic1.zhimg.com/v2-99bdf3d4ddbc1975dff86e1627498ec0_b.jpg\" data-rawwidth=\"740\" class=\"origin_image zh-lightbox-thumb\" width=\"740\" data-original=\"https://pic1.zhimg.com/v2-99bdf3d4ddbc1975dff86e1627498ec0_r.jpg\"/></noscript><img data-rawheight=\"384\" src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;740&#39; height=&#39;384&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"740\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"740\" data-original=\"https://pic1.zhimg.com/v2-99bdf3d4ddbc1975dff86e1627498ec0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-99bdf3d4ddbc1975dff86e1627498ec0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>相比于LeNet，AlexNet有了以下进步：</p><p>1.  Data Augmentation：主要有水平翻转，随机裁剪、平移变换，颜色、关照变化。</p><p>2.  使用了Relu激活函数</p><p>3.  使用了Dropout正则机制</p><p>4.  LRN（临近数据的归一化）</p><p>5.  overlapping Pooling。其实就是带有stride移动（非默认）的pooling</p><p>6.  多GPU（本次实现中未用）</p><p class=\"ztext-empty-paragraph\"><br/></p><p>AlexNet总共有8层网络结构，包含5个卷积和3个全连接。在Pytorch中未实现LRN这个功能，实际上自从后续的VGG和Resnet等提出后，发现LRN本质上也是一种正则化方法，效果并不明显，因此现在很少使用了。</p><p>下面是实现LRN的部分代码：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">import</span> <span class=\"nn\">torch.nn</span> <span class=\"k\">as</span> <span class=\"nn\">nn</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torch.nn</span> <span class=\"k\">import</span> <span class=\"n\">functional</span> <span class=\"k\">as</span> <span class=\"n\">F</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torch.autograd</span> <span class=\"k\">import</span> <span class=\"n\">Variable</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">LRN</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">local_size</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">1.0</span><span class=\"p\">,</span> <span class=\"n\">beta</span><span class=\"o\">=</span><span class=\"mf\">0.75</span><span class=\"p\">,</span> <span class=\"n\">ACROSS_CHANNELS</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">(</span><span class=\"n\">LRN</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">ACROSS_CHANNELS</span> <span class=\"o\">=</span> <span class=\"n\">ACROSS_CHANNELS</span>\n        <span class=\"k\">if</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">ACROSS_CHANNELS</span><span class=\"p\">:</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">average</span><span class=\"o\">=</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">AvgPool3d</span><span class=\"p\">(</span><span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">local_size</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"c1\">#0.2.0_4会报错，需要在最新的分支上AvgPool3d才有padding参数</span>\n                    <span class=\"n\">stride</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span>\n                    <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"nb\">int</span><span class=\"p\">((</span><span class=\"n\">local_size</span><span class=\"o\">-</span><span class=\"mf\">1.0</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span> \n        <span class=\"k\">else</span><span class=\"p\">:</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">average</span><span class=\"o\">=</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">AvgPool2d</span><span class=\"p\">(</span><span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"n\">local_size</span><span class=\"p\">,</span>\n                    <span class=\"n\">stride</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span>\n                    <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"nb\">int</span><span class=\"p\">((</span><span class=\"n\">local_size</span><span class=\"o\">-</span><span class=\"mf\">1.0</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"mi\">2</span><span class=\"p\">))</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">alpha</span> <span class=\"o\">=</span> <span class=\"n\">alpha</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">beta</span> <span class=\"o\">=</span> <span class=\"n\">beta</span>\n    \n    \n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"k\">if</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">ACROSS_CHANNELS</span><span class=\"p\">:</span>\n            <span class=\"n\">div</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"nb\">pow</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">unsqueeze</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n            <span class=\"n\">div</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">average</span><span class=\"p\">(</span><span class=\"n\">div</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n            <span class=\"n\">div</span> <span class=\"o\">=</span> <span class=\"n\">div</span><span class=\"o\">.</span><span class=\"n\">mul</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">alpha</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"mf\">1.0</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"nb\">pow</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">beta</span><span class=\"p\">)</span><span class=\"c1\">#这里的1.0即为bias</span>\n        <span class=\"k\">else</span><span class=\"p\">:</span>\n            <span class=\"n\">div</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"nb\">pow</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n            <span class=\"n\">div</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">average</span><span class=\"p\">(</span><span class=\"n\">div</span><span class=\"p\">)</span>\n            <span class=\"n\">div</span> <span class=\"o\">=</span> <span class=\"n\">div</span><span class=\"o\">.</span><span class=\"n\">mul</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">alpha</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"mf\">1.0</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"nb\">pow</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">beta</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">div</span><span class=\"p\">(</span><span class=\"n\">div</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">x</span></code></pre></div><p>上面代码需要注意的是，本人实验的时候，pytorch的平均池化（AvgPool3d）还未加入pading等参数，这里是在官方github上master上自行build更新完后才能使用（代码均是在python3.5上）。</p><p>接下来是Alexnet的创建过程，主要是将LRN加了进去，创建网络主要是参考了<a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/sunbaigui/article/details/39938097\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">[caffe]深度学习之图像分类模型AlexNet解读 - 孙佰贵的专栏 - CSDN博客</a>。</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">from</span> <span class=\"nn\">torch</span> <span class=\"k\">import</span> <span class=\"n\">nn</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torch.nn</span> <span class=\"k\">import</span> <span class=\"n\">functional</span> <span class=\"k\">as</span> <span class=\"n\">F</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torch.autograd</span> <span class=\"k\">import</span> <span class=\"n\">Variable</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"k\">class</span> <span class=\"nc\">AlexNet</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">num_classes</span> <span class=\"o\">=</span> <span class=\"mi\">1000</span><span class=\"p\">):</span><span class=\"c1\">#imagenet数量</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">layer1</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n            <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"n\">in_channels</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">out_channels</span><span class=\"o\">=</span><span class=\"mi\">96</span><span class=\"p\">,</span> <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">11</span><span class=\"p\">,</span> <span class=\"n\">stride</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n            <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">ReLU</span><span class=\"p\">(</span><span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">),</span>\n            <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">MaxPool2d</span><span class=\"p\">(</span><span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">stride</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n            <span class=\"n\">LRN</span><span class=\"p\">(</span><span class=\"n\">local_size</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">1e-4</span><span class=\"p\">,</span> <span class=\"n\">beta</span><span class=\"o\">=</span><span class=\"mf\">0.75</span><span class=\"p\">,</span> <span class=\"n\">ACROSS_CHANNELS</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n        <span class=\"p\">)</span>\n        \n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">layer2</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n            <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"n\">in_channels</span><span class=\"o\">=</span><span class=\"mi\">96</span><span class=\"p\">,</span> <span class=\"n\">out_channels</span><span class=\"o\">=</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">groups</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n            <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">ReLU</span><span class=\"p\">(</span><span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">),</span>\n            <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">MaxPool2d</span><span class=\"p\">(</span><span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">stride</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n            <span class=\"n\">LRN</span><span class=\"p\">(</span><span class=\"n\">local_size</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">1e-4</span><span class=\"p\">,</span> <span class=\"n\">beta</span><span class=\"o\">=</span><span class=\"mf\">0.75</span><span class=\"p\">,</span> <span class=\"n\">ACROSS_CHANNELS</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n        <span class=\"p\">)</span>\n        \n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">layer3</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n            <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"n\">in_channels</span><span class=\"o\">=</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"n\">out_channels</span><span class=\"o\">=</span><span class=\"mi\">384</span><span class=\"p\">,</span> <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n            <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">ReLU</span><span class=\"p\">(</span><span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n        <span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">layer4</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n            <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"n\">in_channels</span><span class=\"o\">=</span><span class=\"mi\">384</span><span class=\"p\">,</span> <span class=\"n\">out_channels</span><span class=\"o\">=</span><span class=\"mi\">384</span><span class=\"p\">,</span> <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n            <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">ReLU</span><span class=\"p\">(</span><span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n        <span class=\"p\">)</span>\n        \n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">layer5</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n            <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"n\">in_channels</span><span class=\"o\">=</span><span class=\"mi\">384</span><span class=\"p\">,</span> <span class=\"n\">out_channels</span><span class=\"o\">=</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n            <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">ReLU</span><span class=\"p\">(</span><span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">),</span>\n            <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">MaxPool2d</span><span class=\"p\">(</span><span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">stride</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n        <span class=\"p\">)</span>\n        \n         <span class=\"c1\">#需要针对上一层改变view</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">layer6</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n            <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"n\">in_features</span><span class=\"o\">=</span><span class=\"mi\">6</span><span class=\"o\">*</span><span class=\"mi\">6</span><span class=\"o\">*</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"n\">out_features</span><span class=\"o\">=</span><span class=\"mi\">4096</span><span class=\"p\">),</span>\n            <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">ReLU</span><span class=\"p\">(</span><span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">),</span>\n            <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Dropout</span><span class=\"p\">()</span>\n        <span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">layer7</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n            <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"n\">in_features</span><span class=\"o\">=</span><span class=\"mi\">4096</span><span class=\"p\">,</span> <span class=\"n\">out_features</span><span class=\"o\">=</span><span class=\"mi\">4096</span><span class=\"p\">),</span>\n            <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">ReLU</span><span class=\"p\">(</span><span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">),</span>\n            <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Dropout</span><span class=\"p\">()</span>\n        <span class=\"p\">)</span>\n        \n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">layer8</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"n\">in_features</span><span class=\"o\">=</span><span class=\"mi\">4096</span><span class=\"p\">,</span> <span class=\"n\">out_features</span><span class=\"o\">=</span><span class=\"n\">num_classes</span><span class=\"p\">)</span>\n        \n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">layer5</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">layer4</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">layer3</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">layer2</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">layer1</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)))))</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">view</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"o\">*</span><span class=\"mi\">6</span><span class=\"o\">*</span><span class=\"mi\">256</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">layer8</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">layer7</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">layer6</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)))</span>\n        \n        <span class=\"k\">return</span> <span class=\"n\">x</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>最后是使用了Imagenet ILSVRC 2017的目标检测数据，按照图片分类的方式，进行了训练。这一部分由于imagenet提供的数据量太大，鉴于机器配置，就没完全的去运行，感兴趣的可以自己按照之前lenet那篇进行补充验证。这里仅使用了训练集当做图片分类，撰写了训练部分的代码，且没有采用分布式的方式。数据源是从kaggle上下载，地址为<a href=\"https://link.zhihu.com/?target=https%3A//www.kaggle.com/c/imagenet-object-detection-challenge/data\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ImageNet Object Detection Challenge</a>。</p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">torch.utils.data</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torchvision.transforms</span> <span class=\"k\">as</span> <span class=\"nn\">transforms</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torchvision.datasets</span> <span class=\"k\">as</span> <span class=\"nn\">datasets</span>\n\n<span class=\"c1\">#用于不均匀样本抽样的权重计算,在本次imagenet中其实未使用到</span>\n<span class=\"k\">def</span> <span class=\"nf\">make_weights_for_balanced_classes</span><span class=\"p\">(</span><span class=\"n\">images</span><span class=\"p\">,</span> <span class=\"n\">nclasses</span><span class=\"p\">):</span>                        \n    <span class=\"n\">count</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">nclasses</span>                                                      \n    <span class=\"k\">for</span> <span class=\"n\">item</span> <span class=\"ow\">in</span> <span class=\"n\">images</span><span class=\"p\">:</span>                                                         \n        <span class=\"n\">count</span><span class=\"p\">[</span><span class=\"n\">item</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]]</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>                                                     \n    <span class=\"n\">weight_per_class</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mf\">0.</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">nclasses</span>                                      \n    <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">count</span><span class=\"p\">))</span>                                                   \n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">nclasses</span><span class=\"p\">):</span>                                                   \n        <span class=\"n\">weight_per_class</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">N</span><span class=\"o\">/</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">count</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">])</span>                                 \n    <span class=\"n\">weight</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">images</span><span class=\"p\">)</span>                                              \n    <span class=\"k\">for</span> <span class=\"n\">idx</span><span class=\"p\">,</span> <span class=\"n\">val</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">images</span><span class=\"p\">):</span>                                          \n        <span class=\"n\">weight</span><span class=\"p\">[</span><span class=\"n\">idx</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">weight_per_class</span><span class=\"p\">[</span><span class=\"n\">val</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]]</span>                                  \n    <span class=\"k\">return</span> <span class=\"n\">weight</span>\n\n<span class=\"n\">normalize</span> <span class=\"o\">=</span> <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Normalize</span><span class=\"p\">(</span><span class=\"n\">mean</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mf\">0.485</span><span class=\"p\">,</span> <span class=\"mf\">0.456</span><span class=\"p\">,</span> <span class=\"mf\">0.406</span><span class=\"p\">],</span>\n                                     <span class=\"n\">std</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mf\">0.229</span><span class=\"p\">,</span> <span class=\"mf\">0.224</span><span class=\"p\">,</span> <span class=\"mf\">0.225</span><span class=\"p\">])</span>\n<span class=\"n\">train_dataset</span> <span class=\"o\">=</span> <span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">ImageFolder</span><span class=\"p\">(</span><span class=\"s1\">&#39;./data/ILSVRC/Data/CLS-LOC/train/&#39;</span><span class=\"p\">,</span> \n        <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">([</span>\n                <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">RandomSizedCrop</span><span class=\"p\">(</span><span class=\"mi\">227</span><span class=\"p\">),</span> <span class=\"c1\">#AlexNet输入</span>\n                <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">RandomHorizontalFlip</span><span class=\"p\">(),</span>\n                <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">ToTensor</span><span class=\"p\">(),</span>\n                <span class=\"n\">normalize</span>\n        <span class=\"p\">]))</span>\n\n<span class=\"n\">weights</span> <span class=\"o\">=</span> <span class=\"n\">make_weights_for_balanced_classes</span><span class=\"p\">(</span><span class=\"n\">train_dataset</span><span class=\"o\">.</span><span class=\"n\">imgs</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_dataset</span><span class=\"o\">.</span><span class=\"n\">classes</span><span class=\"p\">))</span>\n<span class=\"n\">weights</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">DoubleTensor</span><span class=\"p\">(</span><span class=\"n\">weights</span><span class=\"p\">)</span>\n<span class=\"n\">sampler</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">sampler</span><span class=\"o\">.</span><span class=\"n\">WeightedRandomSampler</span><span class=\"p\">(</span><span class=\"n\">weights</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">weights</span><span class=\"p\">))</span>\n\n<span class=\"n\">batch_size</span> <span class=\"o\">=</span> <span class=\"mi\">256</span>\n<span class=\"n\">train_loader</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"n\">train_dataset</span><span class=\"p\">,</span>\n                                           <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"n\">batch_size</span><span class=\"p\">,</span> \n                                           <span class=\"n\">num_workers</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> \n                                           <span class=\"n\">pin_memory</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> \n                                           <span class=\"n\">sampler</span><span class=\"o\">=</span><span class=\"n\">sampler</span>\n                                          <span class=\"p\">)</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">weight_init</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">):</span>\n<span class=\"c1\"># 使用isinstance来判断m属于什么类型</span>\n    <span class=\"k\">if</span> <span class=\"nb\">isinstance</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">):</span>\n        <span class=\"kn\">import</span> <span class=\"nn\">math</span>\n        <span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">kernel_size</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">kernel_size</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">out_channels</span>\n        <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">weight</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">normal_</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"mf\">2.</span> <span class=\"o\">/</span> <span class=\"n\">n</span><span class=\"p\">))</span>\n    <span class=\"k\">elif</span> <span class=\"nb\">isinstance</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">BatchNorm2d</span><span class=\"p\">):</span>\n<span class=\"c1\"># m中的weight，bias其实都是Variable，为了能学习参数以及后向传播</span>\n        <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">weight</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">fill_</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n        <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">bias</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">zero_</span><span class=\"p\">()</span>\n        \n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">AlexNet</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_dataset</span><span class=\"o\">.</span><span class=\"n\">classes</span><span class=\"p\">))</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">apply</span><span class=\"p\">(</span><span class=\"n\">weight_init</span><span class=\"p\">)</span>\n<span class=\"n\">use_gpu</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">is_available</span><span class=\"p\">()</span>\n<span class=\"k\">if</span> <span class=\"n\">use_gpu</span><span class=\"p\">:</span>\n    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"p\">()</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;USE GPU&#39;</span><span class=\"p\">)</span>\n<span class=\"k\">else</span><span class=\"p\">:</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;USE CPU&#39;</span><span class=\"p\">)</span>\n\n<span class=\"n\">criterion</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">CrossEntropyLoss</span><span class=\"p\">(</span><span class=\"n\">size_average</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"c1\"># optimizer = torch.optim.SGD(model.parameters(), lr=0.001)</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">1e-3</span><span class=\"p\">,</span> <span class=\"n\">betas</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.99</span><span class=\"p\">))</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">train</span><span class=\"p\">(</span><span class=\"n\">epoch</span><span class=\"p\">):</span>\n    <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">()</span>\n    <span class=\"k\">for</span> <span class=\"n\">batch_idx</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">target</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">train_loader</span><span class=\"p\">):</span>\n        <span class=\"k\">if</span> <span class=\"n\">use_gpu</span><span class=\"p\">:</span>\n            <span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">target</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"p\">(),</span> <span class=\"n\">target</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"p\">()</span>\n        <span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">target</span> <span class=\"o\">=</span> <span class=\"n\">Variable</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">),</span> <span class=\"n\">Variable</span><span class=\"p\">(</span><span class=\"n\">target</span><span class=\"p\">)</span>\n        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n        <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n        <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">criterion</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">target</span><span class=\"p\">)</span>\n        <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n        <span class=\"k\">if</span> <span class=\"n\">batch_idx</span> <span class=\"o\">%</span> <span class=\"mi\">100</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;Train Epoch: </span><span class=\"si\">{}</span><span class=\"s1\"> [</span><span class=\"si\">{}</span><span class=\"s1\">/</span><span class=\"si\">{}</span><span class=\"s1\"> (</span><span class=\"si\">{:.0f}</span><span class=\"s1\">%)]</span><span class=\"se\">\\t</span><span class=\"s1\">Loss: </span><span class=\"si\">{:.6f}</span><span class=\"s1\">&#39;</span><span class=\"o\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span>\n                <span class=\"n\">epoch</span><span class=\"p\">,</span> <span class=\"n\">batch_idx</span> <span class=\"o\">*</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">),</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_loader</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"p\">),</span>\n                <span class=\"mf\">100.</span> <span class=\"o\">*</span> <span class=\"n\">batch_idx</span> <span class=\"o\">/</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_loader</span><span class=\"p\">),</span> <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]))</span>\n            \n\n<span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">):</span>\n    <span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">epoch</span><span class=\"p\">)</span>\n    <span class=\"c1\">#未加入测试过程</span>\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>最后完整代码可见<a href=\"https://link.zhihu.com/?target=https%3A//github.com/sloth2012/AlexNet\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">sloth2012/AlexNet</a></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>参考链接</b></h2><ul><li><a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/sunbaigui/article/details/39938097\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">[caffe]深度学习之图像分类模型AlexNet解读 - 孙佰贵的专栏 - CSDN博客</a></li><li><a href=\"https://link.zhihu.com/?target=http%3A//www.voidcn.com/article/p-kvyhsyju-bkg.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【深度学习】使用tensorflow实现AlexNet - 程序园</a></li><li><a href=\"https://link.zhihu.com/?target=http%3A//www.cnblogs.com/52machinelearning/p/5821591.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">#Deep Learning回顾#之LeNet、AlexNet、GoogLeNet、VGG、ResNet - 我爱机器学习 - 博客园</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//github.com/pytorch/pytorch/issues/653\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Feature Request: Local response normalization (LRN) · Issue #653 · pytorch/pytorch</a></li></ul>", 
            "topic": [
                {
                    "tag": "PyTorch", 
                    "tagLink": "https://api.zhihu.com/topics/20075993"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": [
                {
                    "userName": "无名之辈", 
                    "userLink": "https://www.zhihu.com/people/8a11e30978c18d1ab633ee158932beb2", 
                    "content": "学习了", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "Xiao Song", 
                    "userLink": "https://www.zhihu.com/people/3e8bb04c95d58049558e4ed9a4fabe38", 
                    "content": "<p>想问一下楼主，为什么 self.layer4 = nn.Sequential 中没有使用 group = 2 ? 不应该在这一层使用两个并行的计算么？</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/29716516", 
            "userName": "战歌指挥官", 
            "userLink": "https://www.zhihu.com/people/f4ee3aaa7e722a47be3444599f9bbd5b", 
            "upvote": 5, 
            "title": "Pytorch手撕经典网络之LeNet5", 
            "content": "<p>下图为经典网络LeNet5的网络结构。</p><figure><noscript><img src=\"https://pic1.zhimg.com/v2-e144b987285eb9cdba097c9375244074_b.jpg\" data-caption=\"\" data-rawwidth=\"1057\" data-rawheight=\"283\" class=\"origin_image zh-lightbox-thumb\" width=\"1057\" data-original=\"https://pic1.zhimg.com/v2-e144b987285eb9cdba097c9375244074_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1057&#39; height=&#39;283&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-rawwidth=\"1057\" data-rawheight=\"283\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1057\" data-original=\"https://pic1.zhimg.com/v2-e144b987285eb9cdba097c9375244074_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e144b987285eb9cdba097c9375244074_b.jpg\"/></figure><p>具体讲解可见<a href=\"https://link.zhihu.com/?target=http%3A//www.jeyzhang.com/cnn-learning-notes-1.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">卷积神经网络(CNN)学习笔记1：基础入门</a>。</p><p>实现时，主要包括俩个卷积层，俩个pooling层，三个全连接层（严格来说，最后一层的Gaussian connection应有其它的转化方式，这里用全连接）。</p><p>这里面采用了mnist数据集，但为了更深入的学习pytorch，所以这里采用了自定义数据源的方式。</p><p>下面一段代码为加载mnist数据集，主要是解析二进制文件转成numpy格式数据的过程。数据集是从mnist官方下载后的压缩包，放置在工程代码的data目录下。</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">import</span> <span class=\"nn\">gzip</span><span class=\"o\">,</span> <span class=\"nn\">struct</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">_read</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">,</span><span class=\"n\">label</span><span class=\"p\">):</span>\n   <span class=\"n\">minist_dir</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;./data/&#39;</span>\n   <span class=\"k\">with</span> <span class=\"n\">gzip</span><span class=\"o\">.</span><span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">minist_dir</span><span class=\"o\">+</span><span class=\"n\">label</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">flbl</span><span class=\"p\">:</span>\n       <span class=\"n\">magic</span><span class=\"p\">,</span> <span class=\"n\">num</span> <span class=\"o\">=</span> <span class=\"n\">struct</span><span class=\"o\">.</span><span class=\"n\">unpack</span><span class=\"p\">(</span><span class=\"s2\">&#34;&gt;II&#34;</span><span class=\"p\">,</span> <span class=\"n\">flbl</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"p\">(</span><span class=\"mi\">8</span><span class=\"p\">))</span>\n       <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">fromstring</span><span class=\"p\">(</span><span class=\"n\">flbl</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"p\">(),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int8</span><span class=\"p\">)</span>\n   <span class=\"k\">with</span> <span class=\"n\">gzip</span><span class=\"o\">.</span><span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">minist_dir</span><span class=\"o\">+</span><span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"s1\">&#39;rb&#39;</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">fimg</span><span class=\"p\">:</span>\n       <span class=\"n\">magic</span><span class=\"p\">,</span> <span class=\"n\">num</span><span class=\"p\">,</span> <span class=\"n\">rows</span><span class=\"p\">,</span> <span class=\"n\">cols</span> <span class=\"o\">=</span> <span class=\"n\">struct</span><span class=\"o\">.</span><span class=\"n\">unpack</span><span class=\"p\">(</span><span class=\"s2\">&#34;&gt;IIII&#34;</span><span class=\"p\">,</span> <span class=\"n\">fimg</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"p\">))</span>\n       <span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">fromstring</span><span class=\"p\">(</span><span class=\"n\">fimg</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"p\">(),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">uint8</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">label</span><span class=\"p\">),</span> <span class=\"n\">rows</span><span class=\"p\">,</span> <span class=\"n\">cols</span><span class=\"p\">)</span>\n   <span class=\"k\">return</span> <span class=\"n\">image</span><span class=\"p\">,</span><span class=\"n\">label</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">get_data</span><span class=\"p\">():</span>\n   <span class=\"n\">train_img</span><span class=\"p\">,</span><span class=\"n\">train_label</span> <span class=\"o\">=</span> <span class=\"n\">_read</span><span class=\"p\">(</span>\n           <span class=\"s1\">&#39;train-images-idx3-ubyte.gz&#39;</span><span class=\"p\">,</span> \n           <span class=\"s1\">&#39;train-labels-idx1-ubyte.gz&#39;</span><span class=\"p\">)</span>\n   <span class=\"n\">test_img</span><span class=\"p\">,</span><span class=\"n\">test_label</span> <span class=\"o\">=</span> <span class=\"n\">_read</span><span class=\"p\">(</span>\n           <span class=\"s1\">&#39;t10k-images-idx3-ubyte.gz&#39;</span><span class=\"p\">,</span> \n           <span class=\"s1\">&#39;t10k-labels-idx1-ubyte.gz&#39;</span><span class=\"p\">)</span>\n   <span class=\"k\">return</span> <span class=\"p\">[</span><span class=\"n\">train_img</span><span class=\"p\">,</span><span class=\"n\">train_label</span><span class=\"p\">,</span><span class=\"n\">test_img</span><span class=\"p\">,</span><span class=\"n\">test_label</span><span class=\"p\">]</span></code></pre></div><p>为了方便看某些图片，这里简单实现了图片打印的功能：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"nn\">plt</span>\n<span class=\"o\">%</span><span class=\"n\">matplotlib</span> <span class=\"n\">inline</span>\n\n<span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">Xt</span><span class=\"p\">,</span> <span class=\"n\">yt</span> <span class=\"o\">=</span> <span class=\"n\">get_data</span><span class=\"p\">()</span>\n<span class=\"k\">def</span> <span class=\"nf\">imshow</span><span class=\"p\">(</span><span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"p\">):</span>\n   <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">imshow</span><span class=\"p\">(</span><span class=\"n\">img</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">((</span><span class=\"mi\">28</span><span class=\"p\">,</span><span class=\"mi\">28</span><span class=\"p\">)))</span>\n   <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"n\">label</span><span class=\"p\">)</span>\n\n<span class=\"n\">imshow</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">y</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span></code></pre></div><p>接下来是用pytorch实现LeNet的部分。这部分较为简单，对pytorch有了解后，按照LeNet的结构，按照步骤实现即可，需要注意的是由于LeNet处理的默认输入时32*32的图片，这里加padding=2，即上下左右各padding 2个单位像素，扩充到32*32。</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">from</span> <span class=\"nn\">torch</span> <span class=\"k\">import</span> <span class=\"n\">n</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torch.nn</span> <span class=\"k\">import</span> <span class=\"n\">functional</span> <span class=\"k\">as</span> <span class=\"n\">F</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torch.autograd</span> <span class=\"k\">import</span> <span class=\"n\">Variable</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"k\">class</span> <span class=\"nc\">LeNet5</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n   <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n       <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n       <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv1</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n       <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv2</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">)</span>\n       <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">fc1</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"o\">*</span><span class=\"mi\">5</span><span class=\"o\">*</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">120</span><span class=\"p\">)</span>\n       <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">fc2</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">120</span><span class=\"p\">,</span> <span class=\"mi\">84</span><span class=\"p\">)</span>\n       <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">fc3</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">84</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">)</span>\n   <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n       <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">max_pool2d</span><span class=\"p\">(</span><span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv1</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)),</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">))</span>\n       <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">max_pool2d</span><span class=\"p\">(</span><span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv2</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)),</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">))</span>\n       <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">view</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">num_flat_features</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n       <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">fc1</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n       <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">fc2</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n       <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">fc3</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n       <span class=\"k\">return</span> <span class=\"n\">x</span>\n   <span class=\"k\">def</span> <span class=\"nf\">num_flat_features</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n       <span class=\"n\">size</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">()[</span><span class=\"mi\">1</span><span class=\"p\">:]</span>\n       <span class=\"n\">num_features</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n       <span class=\"k\">for</span> <span class=\"n\">s</span> <span class=\"ow\">in</span> <span class=\"n\">size</span><span class=\"p\">:</span>\n           <span class=\"n\">num_features</span> <span class=\"o\">*=</span> <span class=\"n\">s</span>\n       <span class=\"k\">return</span> <span class=\"n\">num_features</span></code></pre></div><p>然后是训练加预测的过程，本人喜欢边训练边测试的过程，所以按照这样的结构参照官方一些例子进行了实现。</p><p>这里custom_normalization是手工实现标准化的过程（貌似不用，效果也没差太多）。最后的准确率在99%以上。</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"c1\">#使用pytorch封装的dataloader进行训练和预测</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torch.utils.data</span> <span class=\"k\">import</span> <span class=\"n\">TensorDataset</span><span class=\"p\">,</span> <span class=\"n\">DataLoader</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torchvision</span> <span class=\"k\">import</span> <span class=\"n\">transforms</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">custom_normalization</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">std</span><span class=\"p\">,</span> <span class=\"n\">mean</span><span class=\"p\">):</span>\n   <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">data</span> <span class=\"o\">-</span> <span class=\"n\">mean</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">std</span>\n\n<span class=\"n\">use_gpu</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">is_available</span><span class=\"p\">()</span>\n<span class=\"n\">batch_size</span> <span class=\"o\">=</span> <span class=\"mi\">256</span>\n<span class=\"n\">kwargs</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">&#39;num_workers&#39;</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s1\">&#39;pin_memory&#39;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">}</span> <span class=\"k\">if</span> <span class=\"n\">use_gpu</span> <span class=\"k\">else</span> <span class=\"p\">{}</span>\n\n<span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">Xt</span><span class=\"p\">,</span> <span class=\"n\">yt</span> <span class=\"o\">=</span> <span class=\"n\">get_data</span><span class=\"p\">()</span>\n<span class=\"c1\">#主要进行标准化处理</span>\n<span class=\"n\">mean</span><span class=\"p\">,</span> <span class=\"n\">std</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(),</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">std</span><span class=\"p\">()</span>\n<span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">custom_normlization</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">mean</span><span class=\"p\">,</span> <span class=\"n\">std</span><span class=\"p\">)</span>\n<span class=\"n\">Xt</span> <span class=\"o\">=</span> <span class=\"n\">custom_normlization</span><span class=\"p\">(</span><span class=\"n\">Xt</span><span class=\"p\">,</span> <span class=\"n\">mean</span><span class=\"p\">,</span> <span class=\"n\">std</span><span class=\"p\">)</span>\n\n<span class=\"n\">train_x</span><span class=\"p\">,</span> <span class=\"n\">train_y</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">from_numpy</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"nb\">float</span><span class=\"p\">(),</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">from_numpy</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"nb\">int</span><span class=\"p\">))</span>\n<span class=\"n\">test_x</span><span class=\"p\">,</span> <span class=\"n\">test_y</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n   <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">from_numpy</span><span class=\"p\">(</span><span class=\"n\">Xt</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"nb\">float</span><span class=\"p\">(),</span>\n   <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">from_numpy</span><span class=\"p\">(</span><span class=\"n\">yt</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"nb\">int</span><span class=\"p\">))</span>\n   <span class=\"p\">]</span>\n\n<span class=\"n\">train_dataset</span> <span class=\"o\">=</span> <span class=\"n\">TensorDataset</span><span class=\"p\">(</span><span class=\"n\">data_tensor</span><span class=\"o\">=</span><span class=\"n\">train_x</span><span class=\"p\">,</span> <span class=\"n\">target_tensor</span><span class=\"o\">=</span><span class=\"n\">train_y</span><span class=\"p\">)</span>\n<span class=\"n\">test_dataset</span> <span class=\"o\">=</span> <span class=\"n\">TensorDataset</span><span class=\"p\">(</span><span class=\"n\">data_tensor</span><span class=\"o\">=</span><span class=\"n\">test_x</span><span class=\"p\">,</span> <span class=\"n\">target_tensor</span><span class=\"o\">=</span><span class=\"n\">test_y</span><span class=\"p\">)</span>\n\n\n<span class=\"n\">train_loader</span> <span class=\"o\">=</span> <span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"n\">dataset</span><span class=\"o\">=</span><span class=\"n\">train_dataset</span><span class=\"p\">,</span> <span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n<span class=\"n\">test_loader</span> <span class=\"o\">=</span> <span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"n\">dataset</span><span class=\"o\">=</span><span class=\"n\">test_dataset</span><span class=\"p\">,</span> <span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">LeNet5</span><span class=\"p\">()</span>\n<span class=\"k\">if</span> <span class=\"n\">use_gpu</span><span class=\"p\">:</span>\n   <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"p\">()</span>\n   <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;USE GPU&#39;</span><span class=\"p\">)</span>\n<span class=\"k\">else</span><span class=\"p\">:</span>\n   <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;USE CPU&#39;</span><span class=\"p\">)</span>\n\n<span class=\"n\">criterion</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">CrossEntropyLoss</span><span class=\"p\">(</span><span class=\"n\">size_average</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">1e-3</span><span class=\"p\">,</span> <span class=\"n\">betas</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.99</span><span class=\"p\">))</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">weight_init</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">):</span>\n<span class=\"c1\"># 使用isinstance来判断m属于什么类型</span>\n   <span class=\"k\">if</span> <span class=\"nb\">isinstance</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">):</span>\n       <span class=\"kn\">import</span> <span class=\"nn\">math</span>\n       <span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">kernel_size</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">kernel_size</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">out_channels</span>\n       <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">weight</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">normal_</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"mf\">2.</span> <span class=\"o\">/</span> <span class=\"n\">n</span><span class=\"p\">))</span>\n   <span class=\"k\">elif</span> <span class=\"nb\">isinstance</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">BatchNorm2d</span><span class=\"p\">):</span>\n<span class=\"c1\"># m中的weight，bias其实都是Variable，为了能学习参数以及后向传播</span>\n       <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">weight</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">fill_</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n       <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">bias</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">zero_</span><span class=\"p\">()</span>\n\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">apply</span><span class=\"p\">(</span><span class=\"n\">weight_init</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">train</span><span class=\"p\">(</span><span class=\"n\">epoch</span><span class=\"p\">):</span>\n   <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">()</span>\n   <span class=\"k\">for</span> <span class=\"n\">batch_idx</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">target</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">train_loader</span><span class=\"p\">):</span>\n       <span class=\"k\">if</span> <span class=\"n\">use_gpu</span><span class=\"p\">:</span>\n           <span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">target</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"p\">(),</span> <span class=\"n\">target</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"p\">()</span>\n       <span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">target</span> <span class=\"o\">=</span> <span class=\"n\">Variable</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">),</span> <span class=\"n\">Variable</span><span class=\"p\">(</span><span class=\"n\">target</span><span class=\"p\">)</span>\n       <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n       <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n       <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">criterion</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">target</span><span class=\"p\">)</span>\n       <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n       <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n       <span class=\"k\">if</span> <span class=\"n\">batch_idx</span> <span class=\"o\">%</span> <span class=\"mi\">100</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n           <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;Train Epoch: </span><span class=\"si\">{}</span><span class=\"s1\"> [</span><span class=\"si\">{}</span><span class=\"s1\">/</span><span class=\"si\">{}</span><span class=\"s1\"> (</span><span class=\"si\">{:.0f}</span><span class=\"s1\">%)]</span><span class=\"se\">\\t</span><span class=\"s1\">Loss: </span><span class=\"si\">{:.6f}</span><span class=\"s1\">&#39;</span><span class=\"o\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span>\n               <span class=\"n\">epoch</span><span class=\"p\">,</span> <span class=\"n\">batch_idx</span> <span class=\"o\">*</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">),</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_loader</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"p\">),</span>\n               <span class=\"mf\">100.</span> <span class=\"o\">*</span> <span class=\"n\">batch_idx</span> <span class=\"o\">/</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_loader</span><span class=\"p\">),</span> <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]))</span>\n<span class=\"k\">def</span> <span class=\"nf\">test</span><span class=\"p\">():</span>\n   <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"nb\">eval</span><span class=\"p\">()</span>\n   <span class=\"n\">test_loss</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n   <span class=\"n\">correct</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n   <span class=\"k\">for</span> <span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">target</span> <span class=\"ow\">in</span> <span class=\"n\">test_loader</span><span class=\"p\">:</span>\n       <span class=\"k\">if</span> <span class=\"n\">use_gpu</span><span class=\"p\">:</span>\n           <span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">target</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"p\">(),</span> <span class=\"n\">target</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"p\">()</span>\n       <span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">target</span> <span class=\"o\">=</span> <span class=\"n\">Variable</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">volatile</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">),</span> <span class=\"n\">Variable</span><span class=\"p\">(</span><span class=\"n\">target</span><span class=\"p\">)</span>\n       <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n       <span class=\"n\">test_loss</span> <span class=\"o\">+=</span> <span class=\"n\">criterion</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">target</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"c1\"># sum up batch loss</span>\n       <span class=\"n\">pred</span> <span class=\"o\">=</span> <span class=\"n\">output</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"nb\">max</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdim</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"c1\"># get the index of the max log-probability</span>\n       <span class=\"n\">correct</span> <span class=\"o\">+=</span> <span class=\"n\">pred</span><span class=\"o\">.</span><span class=\"n\">eq</span><span class=\"p\">(</span><span class=\"n\">target</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">view_as</span><span class=\"p\">(</span><span class=\"n\">pred</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">cpu</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">()</span>\n\n   <span class=\"n\">test_loss</span> <span class=\"o\">/=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">test_loader</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"p\">)</span>\n   <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;</span><span class=\"se\">\\n</span><span class=\"s1\">Test set: Average loss: </span><span class=\"si\">{:.4f}</span><span class=\"s1\">, Accuracy: </span><span class=\"si\">{}</span><span class=\"s1\">/</span><span class=\"si\">{}</span><span class=\"s1\"> (</span><span class=\"si\">{:.2f}</span><span class=\"s1\">%)</span><span class=\"se\">\\n</span><span class=\"s1\">&#39;</span><span class=\"o\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span>\n       <span class=\"n\">test_loss</span><span class=\"p\">,</span> <span class=\"n\">correct</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">test_loader</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"p\">),</span>\n       <span class=\"mf\">100.</span> <span class=\"o\">*</span> <span class=\"n\">correct</span> <span class=\"o\">/</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">test_loader</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"p\">)))</span>\n\n\n\n<span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">501</span><span class=\"p\">):</span>\n   <span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">epoch</span><span class=\"p\">)</span>\n   <span class=\"n\">test</span><span class=\"p\">()</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>完整代码见<a href=\"https://link.zhihu.com/?target=https%3A//github.com/sloth2012/LeNet5\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">sloth2012/LeNet5</a></b></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>本文实现lenet与原paper还是有些不一样，主要体现在评论区说的s2到c3的过程上。对于该问题，本文实现算是一个简化版本。原paper之所以那样实现，也是受限于当时的计算资源。现简化版本也符合pytorch的实现框架。pytorch原生并没有提供s2到c3的计算过程模块，用户可自行实现，详细可借鉴</b><a href=\"https://link.zhihu.com/?target=https%3A//github.com/tiny-dnn/tiny-dnn/tree/master/examples/mnist\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tiny-dnn/tiny-dnn</a>。</p><h2><b>参考链接</b></h2><ul><li><a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/u010165147/article/details/50599490\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">python读取MNIST image数据</a></li><li><a href=\"https://link.zhihu.com/?target=http%3A//www.jeyzhang.com/cnn-learning-notes-1.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">jeyzhang.com/cnn-learni</span><span class=\"invisible\">ng-notes-1.html</span><span class=\"ellipsis\"></span></a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//www.kaggle.com/usingtc/lenet-with-pytorch\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">LeNet with Pytorch | Kaggle</a></li></ul>", 
            "topic": [
                {
                    "tag": "PyTorch", 
                    "tagLink": "https://api.zhihu.com/topics/20075993"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": [
                {
                    "userName": "凡心", 
                    "userLink": "https://www.zhihu.com/people/9f67fc1890367220853da5c0621ddfb9", 
                    "content": "<p>s2到c3之间的6卷积核向16卷积核连接时，那么复杂，这里为什么没有体现出来？现在这样写意味6向16个进行映射，而不是原著中的3-&gt;6, 4-&gt;6, 4-&gt;3, 6-&gt;1的复杂映射关系</p>", 
                    "likes": 1, 
                    "childComments": [
                        {
                            "userName": "知乎用户", 
                            "userLink": "https://www.zhihu.com/people/0", 
                            "content": "嗯 你说的对，这里面我没按原文把其中几个不同卷积核整体和局部的都明确起来，等我抽空修改下", 
                            "likes": 0, 
                            "replyToAuthor": "凡心"
                        }, 
                        {
                            "userName": "张博宁", 
                            "userLink": "https://www.zhihu.com/people/e1038623220b26f74392b63fb5181a73", 
                            "content": "网上实现的几乎都是简化版。很头疼真正实现起来应该怎么写，想找个参考一直没找到。", 
                            "likes": 3, 
                            "replyToAuthor": "凡心"
                        }
                    ]
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>github 上pytorch 的实在太多，挺不住了，开始学pytorch</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_131499746"
}
