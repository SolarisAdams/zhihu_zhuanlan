{
    "title": "船长黑板报", 
    "description": "机器学习，优化等，最新的内容维护在https://github.com/Captain1986/CaptainBlackboard", 
    "followers": [
        "https://www.zhihu.com/people/wo-xiang-xue-java-83", 
        "https://www.zhihu.com/people/xu-peng-36-33", 
        "https://www.zhihu.com/people/chen-chen-chen-86-62", 
        "https://www.zhihu.com/people/david-yue-55", 
        "https://www.zhihu.com/people/liuxingzhuyue", 
        "https://www.zhihu.com/people/liu-dun-qiang-11", 
        "https://www.zhihu.com/people/yafei-qin-54", 
        "https://www.zhihu.com/people/CrazyVertigo", 
        "https://www.zhihu.com/people/aewil-zheng", 
        "https://www.zhihu.com/people/zhan-hai-zhu", 
        "https://www.zhihu.com/people/li-kun-2-96", 
        "https://www.zhihu.com/people/ri-yue-dang-kong-zhao-37", 
        "https://www.zhihu.com/people/xiao-jian-wei-84", 
        "https://www.zhihu.com/people/123456ddd-42", 
        "https://www.zhihu.com/people/feng-jia-zheng", 
        "https://www.zhihu.com/people/la-lian-86", 
        "https://www.zhihu.com/people/jun-liu-98", 
        "https://www.zhihu.com/people/zzqsai", 
        "https://www.zhihu.com/people/liang-yan-kan-shi-jie-37", 
        "https://www.zhihu.com/people/xun-mu-22", 
        "https://www.zhihu.com/people/xdu_zdc_iacas", 
        "https://www.zhihu.com/people/lea-79-45", 
        "https://www.zhihu.com/people/mel-tor", 
        "https://www.zhihu.com/people/xiao-leng-88-5", 
        "https://www.zhihu.com/people/xiao-gu-wei-dao-zhu", 
        "https://www.zhihu.com/people/shang-shi-bo-49", 
        "https://www.zhihu.com/people/raymond-40-48", 
        "https://www.zhihu.com/people/terry-xiao-37", 
        "https://www.zhihu.com/people/juneweng", 
        "https://www.zhihu.com/people/resnet1906", 
        "https://www.zhihu.com/people/15863587027", 
        "https://www.zhihu.com/people/xing-dong-xun-zhang-40", 
        "https://www.zhihu.com/people/000000000-41-96", 
        "https://www.zhihu.com/people/how2", 
        "https://www.zhihu.com/people/chen-zhuo-30-79", 
        "https://www.zhihu.com/people/galiano", 
        "https://www.zhihu.com/people/zhou-yan-zhen-18", 
        "https://www.zhihu.com/people/yixie-ke-zhi-qiu-qiu-fou", 
        "https://www.zhihu.com/people/hqwsky", 
        "https://www.zhihu.com/people/snowolfhawk", 
        "https://www.zhihu.com/people/hun-dan-3-21", 
        "https://www.zhihu.com/people/long-xun-41", 
        "https://www.zhihu.com/people/hu-yun-song-22", 
        "https://www.zhihu.com/people/kai-xu-37-40", 
        "https://www.zhihu.com/people/yuanborong", 
        "https://www.zhihu.com/people/rui-zhu-95", 
        "https://www.zhihu.com/people/tian-zhong-wei-80", 
        "https://www.zhihu.com/people/whjxnyzh", 
        "https://www.zhihu.com/people/zhihuer2018", 
        "https://www.zhihu.com/people/yu-yi-59-26-59", 
        "https://www.zhihu.com/people/wei-lan-74-63", 
        "https://www.zhihu.com/people/stone-82-84", 
        "https://www.zhihu.com/people/zhang-wei-da-93", 
        "https://www.zhihu.com/people/old_wine", 
        "https://www.zhihu.com/people/he-hong-liang-80", 
        "https://www.zhihu.com/people/ling-xi-zhen-quan", 
        "https://www.zhihu.com/people/zhi-mu-8", 
        "https://www.zhihu.com/people/she-liang", 
        "https://www.zhihu.com/people/matrix1984", 
        "https://www.zhihu.com/people/xiao-wa-wa-95-9", 
        "https://www.zhihu.com/people/johnny-63-54", 
        "https://www.zhihu.com/people/ke-xue-jia-6", 
        "https://www.zhihu.com/people/henryyu-23", 
        "https://www.zhihu.com/people/wang-lang-96-64", 
        "https://www.zhihu.com/people/panovr", 
        "https://www.zhihu.com/people/a-lu-lu-lu-lu-64", 
        "https://www.zhihu.com/people/fu-hong-xue-17", 
        "https://www.zhihu.com/people/ccda-shen-12", 
        "https://www.zhihu.com/people/zhang-liang-25-79", 
        "https://www.zhihu.com/people/fei-tian-87-69", 
        "https://www.zhihu.com/people/zhang-xiong-46-98", 
        "https://www.zhihu.com/people/wu-xiao-yu-92-91", 
        "https://www.zhihu.com/people/xiao-xia-mi-65-43", 
        "https://www.zhihu.com/people/lin-bao", 
        "https://www.zhihu.com/people/hu-chuan-rui-88", 
        "https://www.zhihu.com/people/wei-zhen-tian-30-51", 
        "https://www.zhihu.com/people/zou-da-ben-91", 
        "https://www.zhihu.com/people/libin-sui", 
        "https://www.zhihu.com/people/liu-meng-yin-68", 
        "https://www.zhihu.com/people/fenggege", 
        "https://www.zhihu.com/people/zhuan-jiao-92", 
        "https://www.zhihu.com/people/jin-tao-97-50", 
        "https://www.zhihu.com/people/ykp-41", 
        "https://www.zhihu.com/people/miao-jie-97-69", 
        "https://www.zhihu.com/people/cao-ji-49-42", 
        "https://www.zhihu.com/people/a-ai-41-94-79", 
        "https://www.zhihu.com/people/li-qian-yong-60", 
        "https://www.zhihu.com/people/eric-sun-6", 
        "https://www.zhihu.com/people/desperadoola", 
        "https://www.zhihu.com/people/her0kings1ey", 
        "https://www.zhihu.com/people/yu-luo-22-49", 
        "https://www.zhihu.com/people/hao-liu-18-71", 
        "https://www.zhihu.com/people/wang-li-fu-65", 
        "https://www.zhihu.com/people/dgjk1010", 
        "https://www.zhihu.com/people/zhang-mou-61-11", 
        "https://www.zhihu.com/people/oliver-kahn-60", 
        "https://www.zhihu.com/people/zhang-hui-73-87", 
        "https://www.zhihu.com/people/syoalg", 
        "https://www.zhihu.com/people/gocsu", 
        "https://www.zhihu.com/people/qinlibo_nlp", 
        "https://www.zhihu.com/people/yan-hua-chang-an", 
        "https://www.zhihu.com/people/lzb-3", 
        "https://www.zhihu.com/people/969696-67", 
        "https://www.zhihu.com/people/yu-hai-long-22", 
        "https://www.zhihu.com/people/li-dalen", 
        "https://www.zhihu.com/people/xu-kai-71-15", 
        "https://www.zhihu.com/people/zhu-forrest", 
        "https://www.zhihu.com/people/ming-ge-17-88", 
        "https://www.zhihu.com/people/pei-en-5", 
        "https://www.zhihu.com/people/hen-wen-yi-de-ni-cheng", 
        "https://www.zhihu.com/people/zzx-47-7", 
        "https://www.zhihu.com/people/yy-cc-63-54", 
        "https://www.zhihu.com/people/mattzheng7", 
        "https://www.zhihu.com/people/xuan-shao-ye-20", 
        "https://www.zhihu.com/people/wang-peng-cheng-39-36", 
        "https://www.zhihu.com/people/liu-wei-ming-37-41", 
        "https://www.zhihu.com/people/zgd-64", 
        "https://www.zhihu.com/people/yawu-99", 
        "https://www.zhihu.com/people/xia-zheng-79", 
        "https://www.zhihu.com/people/lu-xiao-ze-7", 
        "https://www.zhihu.com/people/caijia", 
        "https://www.zhihu.com/people/Stupid-Z", 
        "https://www.zhihu.com/people/heyang-36", 
        "https://www.zhihu.com/people/archimedes2013", 
        "https://www.zhihu.com/people/liu-fei-94-95", 
        "https://www.zhihu.com/people/ye-li-tiao-deng-kan-jian-99", 
        "https://www.zhihu.com/people/hum-75", 
        "https://www.zhihu.com/people/luan-lin-bao", 
        "https://www.zhihu.com/people/yildhd-wang", 
        "https://www.zhihu.com/people/ning-rain", 
        "https://www.zhihu.com/people/na-bian-e-hao", 
        "https://www.zhihu.com/people/jackal-71"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/88830653", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 0, 
            "title": "Anchor Free第四篇CenterNet: Keypoint Triplets", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文是Anchor Free系列的第四篇，CenterNet（19年出了两篇CenterNet，一个是UT Austin的；还有一个是本文中科院、清华和华为诺亚舟实验室的），性能直接在MS COCO上刷到了47%，应该是现在One-Stage的SOTA了。CenterNet**直接分析并针对CornerNet的不足有针对性地添加了修改**。最大的特点就是在CornerNet的两个Corner关键点的基础上，**添加了一个Center关键点**，并为此提出了**Center Pooling**结构和**Cascade Corner Pooling**结构。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## CenterNet介绍</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### CornerNet的不足之处</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-57e4556f5b37b993b8add70dd8894fc6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"607\" data-rawheight=\"197\" class=\"origin_image zh-lightbox-thumb\" width=\"607\" data-original=\"https://pic3.zhimg.com/v2-57e4556f5b37b993b8add70dd8894fc6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;607&#39; height=&#39;197&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"607\" data-rawheight=\"197\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"607\" data-original=\"https://pic3.zhimg.com/v2-57e4556f5b37b993b8add70dd8894fc6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-57e4556f5b37b993b8add70dd8894fc6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上面表所示，作者在把CornerNet的误检测做过分析后发现，CornerNet的性能，受到错误框的影响很大。平均上CornerNet每生成100个预测框，有37.8个是错的离谱的(和真值的IoU&lt;**0.05**)，这个问题太大了，必须改。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b1e003930e72b779fa7fa558c01f4867_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"596\" data-rawheight=\"624\" class=\"origin_image zh-lightbox-thumb\" width=\"596\" data-original=\"https://pic4.zhimg.com/v2-b1e003930e72b779fa7fa558c01f4867_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;596&#39; height=&#39;624&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"596\" data-rawheight=\"624\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"596\" data-original=\"https://pic4.zhimg.com/v2-b1e003930e72b779fa7fa558c01f4867_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b1e003930e72b779fa7fa558c01f4867_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>怎么改呢？作者认为，光靠左上角和右下角这两个Corner点能得到的信息还不够，**对于和真值IoU很小很小的误检测来说，预测到的目标的中心点一般不在目标身上（如上图所示），如果在训练的时候，加上一个对目标身上中心点的预测，得到中心点的信息辅助判断，对于滤出这些False Discovery的预测框应该会有帮助**。所以作者就在CornerNet的两个Corner点的基础上加上了目标Center点来组合成一个三元组来预测目标框的位置。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>另外，针对CornerNet中Corner Pooling获取到的信息是边缘信息更多一些这个短板，为了加入一些关于物体内部的信息，作者也提出了修改的方案，就是在边缘点先做一次Corner Pooling得到关于目标内部的信息，再串联原来的Corner Pooling得到Cascade Corner Pooling方法。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### CenterNet做法</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8c67beba2021450a4120133262b723fd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1236\" data-rawheight=\"428\" class=\"origin_image zh-lightbox-thumb\" width=\"1236\" data-original=\"https://pic2.zhimg.com/v2-8c67beba2021450a4120133262b723fd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1236&#39; height=&#39;428&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1236\" data-rawheight=\"428\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1236\" data-original=\"https://pic2.zhimg.com/v2-8c67beba2021450a4120133262b723fd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8c67beba2021450a4120133262b723fd_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>CenterNet的主要部分还是和前面有介绍的CornerNet是一致的：先依靠点检测能力很好的Hourglass主干网络提取特征，然后预测Corner Points，Offsets和Embddings，得到一些Bounding box。不同的是，在**得到Bounding box之后，找出中心区域，看Center点Heatmap对应的中心区域有没有同类的Center点**。如果没有，那就是一个False Discovery，需要丢掉；如果有，那就保留这个Bounding box，且用三个点的Score的均值更新Bounding box的Score。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Bounding box的中心区域怎么找</p><p class=\"ztext-empty-paragraph\"><br/></p><p>CornerNet的方法得到Bounding box之后，会按照Bounding box的大小，选择一片Center Area。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-81aad294fb50b9d597448aaccf1d007e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"593\" data-rawheight=\"445\" class=\"origin_image zh-lightbox-thumb\" width=\"593\" data-original=\"https://pic3.zhimg.com/v2-81aad294fb50b9d597448aaccf1d007e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;593&#39; height=&#39;445&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"593\" data-rawheight=\"445\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"593\" data-original=\"https://pic3.zhimg.com/v2-81aad294fb50b9d597448aaccf1d007e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-81aad294fb50b9d597448aaccf1d007e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这很合理，大的目标，中心区域选择大一点，小尺寸的目标，中心区域选择小一点，然后再去Center Point的Heatmap里面去看看对应区域有没有Score足够大的同类别Center点来判断CornerNet的Bounding box是不是False Discovery。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Center Pooling和Cascade Corner Pooling</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e071689ee261d3e7dedfaf438ce0a029_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"591\" data-rawheight=\"321\" class=\"origin_image zh-lightbox-thumb\" width=\"591\" data-original=\"https://pic2.zhimg.com/v2-e071689ee261d3e7dedfaf438ce0a029_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;591&#39; height=&#39;321&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"591\" data-rawheight=\"321\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"591\" data-original=\"https://pic2.zhimg.com/v2-e071689ee261d3e7dedfaf438ce0a029_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e071689ee261d3e7dedfaf438ce0a029_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6eff36adae9581054622118eb43b3d74_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"592\" data-rawheight=\"349\" class=\"origin_image zh-lightbox-thumb\" width=\"592\" data-original=\"https://pic1.zhimg.com/v2-6eff36adae9581054622118eb43b3d74_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;592&#39; height=&#39;349&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"592\" data-rawheight=\"349\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"592\" data-original=\"https://pic1.zhimg.com/v2-6eff36adae9581054622118eb43b3d74_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6eff36adae9581054622118eb43b3d74_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>对于中心点特征的提取，作者类比与CornerNet的Corner Pooling显示地加了一个**Center Pooling**模块，如上上图(a)和上图(a)所示：对于每一个输出特征图中的点，分别做两个横向和竖向**全范围**的Corner Pooling，然后把Pooling的结果相加起来输出。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>对于两个Corner点特征的提取，原来CornerNet的作者没有考虑到物体内部信息，这里CenterNet的作者加了一步做成了**Cascade Corner Pooling**，**对于Corner点也考虑到了物体内部信息**：如上上图(c)和上图(b)所示，对输入的特征图，先做一次竖直（水平）方向的Corner Pooling，再做一次水平（竖直）方向的Corner Pooling。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 网络结构</p><p class=\"ztext-empty-paragraph\"><br/></p><p>和CornerNet一样，CenterNet也是用的关键点检测常用的104-layer **HourglassNet**作为特征提取Backbone网络，但是加上了Center点的预测，加强了Corner Pooling的模块形成了Center Pooling和Cascade Corner Pooling，其余做了很小的相应修改。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 损失函数</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-33c33da6e87cc007dcf5f0bc5da63b26_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"506\" data-rawheight=\"45\" class=\"origin_image zh-lightbox-thumb\" width=\"506\" data-original=\"https://pic3.zhimg.com/v2-33c33da6e87cc007dcf5f0bc5da63b26_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;506&#39; height=&#39;45&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"506\" data-rawheight=\"45\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"506\" data-original=\"https://pic3.zhimg.com/v2-33c33da6e87cc007dcf5f0bc5da63b26_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-33c33da6e87cc007dcf5f0bc5da63b26_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>基本上和CornerNet一样，关键点定位用的是**改造的Focal Loss**，**Push和Pull Loss**来学习Embddings，再加上Offset用的是**L1 Loss**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文是针对**CornerNet的False Discovery做了有针对性的修改**，添加了Center点和原来的Corner点组成三元组做预测，滤掉了很多False Discovery。另外，针对**加强物体中间部分信息的采集**，改造Corner Pooling得到了Center Pooling和Cascade Corner Pooling也是CenterNet的贡献之一。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**八卦**一下，本文介绍的CenterNet和上篇[《D#0048-Anchor-Free第三篇Objects-as-Points》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230048-Anchor-Free%25E7%25AC%25AC%25E4%25B8%2589%25E7%25AF%2587Objects-as-Points/D%25230048.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)介绍的CenterNet都叫CenterNet，而且两者发表时间十分相近，也没有互相引用。窃以为，虽然本文的性能更好，但是Objects as Points的思路更配得上CenterNet这个称呼。深度学习这块发展实在很快，发论文要趁早。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本CenterNet没有提到**ExtremeNet**([这里](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230047-Anchor-Free%25E7%25AC%25AC%25E4%25BA%258C%25E7%25AF%2587CornerNet%25E7%259A%2584%25E5%258F%2598%25E7%25A7%258DExtremeNet/D%25230047.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)有介绍)，感觉和ExtremeNet的关键点选择如果借鉴过来融合一下，效果应该不差。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [CornerNet: Detecting Objects as Paired Keypoints](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1808.01244\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CornerNet: Detecting Objects as Paired Keypoints</a>)</p><p>+ [Bottom-up Object Detection by Grouping Extreme and Center Points](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1901.08043\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Bottom-up Object Detection by Grouping Extreme and Center Points</a>)</p><p>+ [Objects as Points](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1904.07850\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Objects as Points</a>)</p><p>+ [Stacked Hourglass Network for Human Pose Estimation](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1603.06937\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Stacked Hourglass Networks for Human Pose Estimation</a>)</p><p>+ [《D#0045-Stacked-Hourglass-Network-for-Human-Pose-Estimation》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230045-Stacked-Hourglass-Network-for-Human-Pose-Estimation/D%25230045.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p>+ [《D#0046-Anchor-Free第一篇CornerNet-Detecting-Objects-as-Paired-Keypoints》](&lt;<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230046-Anchor-Free%25E7%25AC%25AC%25E4%25B8%2580%25E7%25AF%2587CornerNet-Detecting-Objects-as-Paired-Keypoints/D%25230046.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>&gt;)</p><p>+ [《D#0047-Anchor-Free第二篇CornerNet的变种ExtremeNet》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230047-Anchor-Free%25E7%25AC%25AC%25E4%25BA%258C%25E7%25AF%2587CornerNet%25E7%259A%2584%25E5%258F%2598%25E7%25A7%258DExtremeNet/D%25230047.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p>+ [《D#0048-Anchor-Free第三篇Objects-as-Points》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230048-Anchor-Free%25E7%25AC%25AC%25E4%25B8%2589%25E7%25AF%2587Objects-as-Points/D%25230048.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "目标检测", 
                    "tagLink": "https://api.zhihu.com/topics/19596960"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/88721485", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 1, 
            "title": "Anchor-Free第三篇Objects as Points", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文是Anchor Free系列的第三篇，CenterNet（19年出了两篇CenterNet，一个是本文UT Austin的；还有一个是中科院、清华和华为诺亚舟实验室的），**继续在CornerNet和ExtremeNet的基础之上做了改进**。最大的特点就是把关键点的个数从物体的外围、边缘改成了目标的中心点，然后再从中心点做微调，回归出目标自身的Bounding Box。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## CenterNet的特点</p><p class=\"ztext-empty-paragraph\"><br/></p><p>CenterNet和ExtremeNet，CornerNet和Anchor-based方法相比，有如下几个特点：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-37a71b9e221998cb3eef10b6fb107392_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1238\" data-rawheight=\"351\" class=\"origin_image zh-lightbox-thumb\" width=\"1238\" data-original=\"https://pic3.zhimg.com/v2-37a71b9e221998cb3eef10b6fb107392_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1238&#39; height=&#39;351&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1238\" data-rawheight=\"351\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1238\" data-original=\"https://pic3.zhimg.com/v2-37a71b9e221998cb3eef10b6fb107392_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-37a71b9e221998cb3eef10b6fb107392_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>1. CenterNet的**关键点选择更少也更科学**：如上面两张图片所示，CenterNet也是用关键点检测的Buttom-Up思路去做目标检测，但是只选择了一个中心关键点，比前面CornerNet的两个对角关键点和ExtermeNet的四个极点关键点更少。而且，中心关键点一般都是落在了目标内部，**所提取到的信息直接和目标本身相关性更大**，不像CornerNet那样大部分落在目标外部，ExtermeNet那样落在目标边缘。基本上把基于点做目标检测这个路子的关键点的数量降到了最少了，**如无必要，勿增实体**。</p><p>2. ExtremeNet**不需要NMS，不需要学习Embddings，也不需要几何规则做Grouping**：。像很多Anchor-based检测器，为了提高召回，在同一个目标身上会得出很多有Overlap的框，然后再在后处理时候利用NMS去掉置信度低的框；在CornerNet中，得到的Corner点，需要再后处理中用Embddings组合起来；ExtermeNet也需要在得到Exterme点之后，利用几何规则做后处理将这些零散的Exterme点组合起来形成包围框。CenterNet对**每个目标只会生成一个中心点的预测和回归框，不需要做别的后处理**，一切都追求最简化。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## CenterNet介绍</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### CornerNet和ExtremeNet的启发</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-efdb4798d82db330a31b6c7e77538d37_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1278\" data-rawheight=\"628\" class=\"origin_image zh-lightbox-thumb\" width=\"1278\" data-original=\"https://pic4.zhimg.com/v2-efdb4798d82db330a31b6c7e77538d37_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1278&#39; height=&#39;628&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1278\" data-rawheight=\"628\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1278\" data-original=\"https://pic4.zhimg.com/v2-efdb4798d82db330a31b6c7e77538d37_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-efdb4798d82db330a31b6c7e77538d37_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ecf4d58e5311c81be7aa114967d7cf38_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"588\" data-rawheight=\"505\" class=\"origin_image zh-lightbox-thumb\" width=\"588\" data-original=\"https://pic1.zhimg.com/v2-ecf4d58e5311c81be7aa114967d7cf38_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;588&#39; height=&#39;505&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"588\" data-rawheight=\"505\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"588\" data-original=\"https://pic1.zhimg.com/v2-ecf4d58e5311c81be7aa114967d7cf38_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ecf4d58e5311c81be7aa114967d7cf38_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上面两张图所示，CornerNet把关键点选择在了目标包围框的左上角和右下角，ExtermeNet想着左上角和右下角其实和目标本身相关性不大，就把关键点选择在目标的最顶、最底、最左和最右四个位置，CenterNet觉得Exterme选择的四个点在目标边边上，如果**选择在目标本身中心点，是不是应该和目标本身更相关效果更好**。其次，CornerNet和ExtremeNet得到了一些类别相关的点之后，还需要后处理将这些点Grouping起来，毕竟，一个单独的角点或者边缘点并不能得到一个框，他要和其他点组合起来才行；但是中心点不一样，**一个目标只有中心点，不需要什么关键点的组合步骤了**，只需要和宽高组合起来，就能得到Bounding Box了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### ExtremeNet思路</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0abc2a701bcda4f171cd78ae2d72a511_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"597\" data-rawheight=\"861\" class=\"origin_image zh-lightbox-thumb\" width=\"597\" data-original=\"https://pic2.zhimg.com/v2-0abc2a701bcda4f171cd78ae2d72a511_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;597&#39; height=&#39;861&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"597\" data-rawheight=\"861\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"597\" data-original=\"https://pic2.zhimg.com/v2-0abc2a701bcda4f171cd78ae2d72a511_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0abc2a701bcda4f171cd78ae2d72a511_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图所示（目标检测看第一行三张图）是ExtremeNet大致的思路。CenterNet的网络结构和前面[《Anchor Free第一篇CornerNet: Detecting Objects as Paired Keypoints》](&lt;<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230046-Anchor-Free%25E7%25AC%25AC%25E4%25B8%2580%25E7%25AF%2587CornerNet-Detecting-Objects-as-Paired-Keypoints/D%25230046.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>&gt;)已经介绍过的CornerNet结构类似，这里不再单独介绍了。来看输出：CenterNet对每类目标，生成**1张**表示中心点在哪的Heatmap。另外还有**1x2张**类别无关的offset map特征图，用来微调Center点反算到输入图的坐标。第三个输出是**1x2**张类别无关的宽高预测（以裸像素为单位），一副输入图像，**总共C+4通道特征图**。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7115aaf88cc416d3605226542a6e72b3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"377\" data-rawheight=\"81\" class=\"content_image\" width=\"377\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;377&#39; height=&#39;81&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"377\" data-rawheight=\"81\" class=\"content_image lazy\" width=\"377\" data-actualsrc=\"https://pic4.zhimg.com/v2-7115aaf88cc416d3605226542a6e72b3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>结合中心点预测，Offset预测和宽高预测，由上式就可以得到目标的包围框了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 网络结构和损失函数</p><p class=\"ztext-empty-paragraph\"><br/></p><p>CenterNet源于CornerNet，网络结构和CornerNet很像，损失函数和CornerNet大致一样。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 网络结构</p><p class=\"ztext-empty-paragraph\"><br/></p><p>和CornerNet一样，CenterNet也是用的关键点检测常用的104-layer **HourglassNet**作为特征提取Backbone网络，但是去掉了Corner Pooling模块，加上了宽高预测模块，其余做了很小的相应修改。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 损失函数</p><p class=\"ztext-empty-paragraph\"><br/></p><p>基本上和CornerNet一样，关键点定位用的是**改造的Focal Loss**</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-99fb6b0f5dc03a31e84e35a3d629ef95_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"532\" data-rawheight=\"131\" class=\"origin_image zh-lightbox-thumb\" width=\"532\" data-original=\"https://pic2.zhimg.com/v2-99fb6b0f5dc03a31e84e35a3d629ef95_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;532&#39; height=&#39;131&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"532\" data-rawheight=\"131\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"532\" data-original=\"https://pic2.zhimg.com/v2-99fb6b0f5dc03a31e84e35a3d629ef95_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-99fb6b0f5dc03a31e84e35a3d629ef95_b.jpg\"/></figure><p>，Offset回归用的是**L1 Loss**</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0526a2e57938400cab2e7101dbec1f68_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"334\" data-rawheight=\"81\" class=\"content_image\" width=\"334\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;334&#39; height=&#39;81&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"334\" data-rawheight=\"81\" class=\"content_image lazy\" width=\"334\" data-actualsrc=\"https://pic1.zhimg.com/v2-0526a2e57938400cab2e7101dbec1f68_b.jpg\"/></figure><p>，宽高预测分支用的是**L1 Loss**</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d685bf76d4555d2752c23d7324b03925_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"271\" data-rawheight=\"77\" class=\"content_image\" width=\"271\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;271&#39; height=&#39;77&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"271\" data-rawheight=\"77\" class=\"content_image lazy\" width=\"271\" data-actualsrc=\"https://pic2.zhimg.com/v2-d685bf76d4555d2752c23d7324b03925_b.jpg\"/></figure><p>。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>总的损失函数是以上三者的加权和</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a3f6e8a71a93f88230657c1985bcbd4c_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"379\" data-rawheight=\"56\" class=\"content_image\" width=\"379\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;379&#39; height=&#39;56&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"379\" data-rawheight=\"56\" class=\"content_image lazy\" width=\"379\" data-actualsrc=\"https://pic1.zhimg.com/v2-a3f6e8a71a93f88230657c1985bcbd4c_b.png\"/></figure><p>。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文是以点做目标检测的第三篇论文CenterNet，和CornerNet关键点选取目标包围框的左上角点和右下角点不同，ExtremeNet的关键点选择为目标边缘最高、最低、最左、最右这四个点，CenterNet干脆直接**选择了中心点**这一个点作为关键点。在目标检测领域取得了不错的成绩，而且**去掉了Embddings和几何规则做Grouping这一步**，CenterNet也可以在小改之后应用到人体姿态，3D估计等领域。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [CornerNet: Detecting Objects as Paired Keypoints](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1808.01244\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CornerNet: Detecting Objects as Paired Keypoints</a>)</p><p>+ [Bottom-up Object Detection by Grouping Extreme and Center Points](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1901.08043\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Bottom-up Object Detection by Grouping Extreme and Center Points</a>)</p><p>+ [Objects as Points](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1904.07850\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Objects as Points</a>)</p><p>+ [Stacked Hourglass Network for Human Pose Estimation](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1603.06937\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Stacked Hourglass Networks for Human Pose Estimation</a>)</p><p>+ [《D#0045-Stacked-Hourglass-Network-for-Human-Pose-Estimation》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230045-Stacked-Hourglass-Network-for-Human-Pose-Estimation/D%25230045.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p>+ [《D#0046-Anchor-Free第一篇CornerNet-Detecting-Objects-as-Paired-Keypoints》](&lt;<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230046-Anchor-Free%25E7%25AC%25AC%25E4%25B8%2580%25E7%25AF%2587CornerNet-Detecting-Objects-as-Paired-Keypoints/D%25230046.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>&gt;)</p><p>+ [《D#0047-Anchor-Free第二篇CornerNet的变种ExtremeNet》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230047-Anchor-Free%25E7%25AC%25AC%25E4%25BA%258C%25E7%25AF%2587CornerNet%25E7%259A%2584%25E5%258F%2598%25E7%25A7%258DExtremeNet/D%25230047.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "目标检测", 
                    "tagLink": "https://api.zhihu.com/topics/19596960"
                }
            ], 
            "comments": [
                {
                    "userName": "我想学机器学习", 
                    "userLink": "https://www.zhihu.com/people/bd97104a0f33cc11173e498d1ab5dd52", 
                    "content": "有没有网络结构，在沙漏网络之后进行了什么操作来预测中心点的？", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "船长", 
                            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
                            "content": "<p>生成的是一个热力图，用来预测中心点</p>", 
                            "likes": 0, 
                            "replyToAuthor": "我想学机器学习"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/88595219", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 8, 
            "title": "Anchor-Free第二篇CornerNet的变种ExtremeNet", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>上文介绍的[CornerNet](&lt;<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230046-Anchor-Free%25E7%25AC%25AC%25E4%25B8%2580%25E7%25AF%2587CornerNet-Detecting-Objects-as-Paired-Keypoints/D%25230046.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>&gt;)首次提出将目标检测问题从回归Anchor Box的Top-Down思路转化为检测KeyPoints的Bottom-Up思路，后面就有一些新的基于关键点检测的模型出来，例如ExtremeNet、CenterNet、FCOS等。这里我们先介绍ExtremeNet，它和CornerNet最接近（Backbone都是一样的，Prediction module小改）。ExtremeNet衍生于CornerNet，主要不同点在于(1) **关键点选取不同**：ExtremeNet的关键点选取为Object的最顶点，最底点，最左点和最右点，而不是包围框的左上角点和右下角的点；(2) **Grouping方法不同**：ExtremeNet不需要学习Embddings，直接把所有的候选点集合做暴力穷举，再结合几何规则和中心点Heatmap找出包围框（也可以是多边形）；(3) **ExtremeNet得到的信息更丰富**：ExtremeNet可以得到目标不多边形包围框，比一般的矩形包围框可以提供更多的关于目标的信息。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## ExtremeNet比CornerNet好在哪</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-eec495467878f0538960ffa7fe581152_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1269\" data-rawheight=\"352\" class=\"origin_image zh-lightbox-thumb\" width=\"1269\" data-original=\"https://pic3.zhimg.com/v2-eec495467878f0538960ffa7fe581152_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1269&#39; height=&#39;352&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1269\" data-rawheight=\"352\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1269\" data-original=\"https://pic3.zhimg.com/v2-eec495467878f0538960ffa7fe581152_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-eec495467878f0538960ffa7fe581152_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ecf4d58e5311c81be7aa114967d7cf38_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"588\" data-rawheight=\"505\" class=\"origin_image zh-lightbox-thumb\" width=\"588\" data-original=\"https://pic1.zhimg.com/v2-ecf4d58e5311c81be7aa114967d7cf38_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;588&#39; height=&#39;505&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"588\" data-rawheight=\"505\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"588\" data-original=\"https://pic1.zhimg.com/v2-ecf4d58e5311c81be7aa114967d7cf38_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ecf4d58e5311c81be7aa114967d7cf38_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>1. ExtremeNet的**关键点选择更科学**：如上面两张图片所示，第一张是CornerNet选择的关键点，目标包围框的左上角和右下角两个点，下图是ExtremeNet选择的四个关键点，目标顶点，底点，最左点和最右点。可以看的出来，目标的Cornner点一般离目标很远很少落在目标本身上面，而Extreme点一般都是在目标的边缘上。**离目标本身越远，越难学到很好的和目标位置强相关的特征**，虽然CornerNet提出了Corner Pooling来显示地编码进去一些关于目标边缘的特征，但是肯定还是不如关键点本身就在目标边缘效果好的（**标注四个Extreme点所携带的信息本身也比标注两个Corner点所携带的信息多**）。奥卡姆剃刀告诉我们**如无必要，勿增实体**。</p><p>2. ExtremeNet**不需要学习Embddings**：得到特征点之后，特征点怎么组合起来形成一个框，在CornerNet中单独引出了一个预测分支去学习一组关于点对关系的Embddings特征（同一个目标的点对，Embddings近，不同目标的点对，Embddings远），但是ExtremeNet把Embddings分支去掉了，添加了一些Center Points的Heatmap，再结合几何关系的规则来做Grouping，作者声称效果比Embddings好。</p><p>3. ExtremeNet的**数据标注成本小**：作者引用文献中的结论，认为标注目标的Extreme点比标注目标的Bounding Box要快4倍。鉴于实际项目中，数据标注的成本可能很大，这里也作为一个优势提出来。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## ExtremeNet介绍</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### CornerNet的启发</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-efdb4798d82db330a31b6c7e77538d37_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1278\" data-rawheight=\"628\" class=\"origin_image zh-lightbox-thumb\" width=\"1278\" data-original=\"https://pic4.zhimg.com/v2-efdb4798d82db330a31b6c7e77538d37_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1278&#39; height=&#39;628&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1278\" data-rawheight=\"628\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1278\" data-original=\"https://pic4.zhimg.com/v2-efdb4798d82db330a31b6c7e77538d37_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-efdb4798d82db330a31b6c7e77538d37_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>简单地说，CornerNet的提出给了ExtremeNet很大启发。既然Corner利用两个在目标外的角点可以做定位，那么利用在目标边上的点，应该可以做的更好才是（省去了Corner Pooling，而且四个Extreme点提供了更多的信息）。具体介绍，请见上一节和[《Anchor Free第一篇CornerNet: Detecting Objects as Paired Keypoints》](&lt;<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230046-Anchor-Free%25E7%25AC%25AC%25E4%25B8%2580%25E7%25AF%2587CornerNet-Detecting-Objects-as-Paired-Keypoints/D%25230046.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>&gt;)关于Corner的介绍。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### ExtremeNet思路</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-681d947be793f68b6463f4e8b1d2bddf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1241\" data-rawheight=\"437\" class=\"origin_image zh-lightbox-thumb\" width=\"1241\" data-original=\"https://pic4.zhimg.com/v2-681d947be793f68b6463f4e8b1d2bddf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1241&#39; height=&#39;437&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1241\" data-rawheight=\"437\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1241\" data-original=\"https://pic4.zhimg.com/v2-681d947be793f68b6463f4e8b1d2bddf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-681d947be793f68b6463f4e8b1d2bddf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图所示是ExtremeNet大致的流程图。ExtremeNet的网络结构和前面[《Anchor Free第一篇CornerNet: Detecting Objects as Paired Keypoints》](&lt;<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230046-Anchor-Free%25E7%25AC%25AC%25E4%25B8%2580%25E7%25AF%2587CornerNet-Detecting-Objects-as-Paired-Keypoints/D%25230046.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>&gt;)已经介绍过的CornerNet结构类似，这里不再单独介绍了。来看输出：ExtremeNet对每类目标，生成**5张**Heatmap，其中4张是表示Extreme点在哪里的Confidence，一张是表示Center点(4个Extreme点的</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4cc213e728dcff2d95bf5e998600d5da_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"208\" data-rawheight=\"54\" class=\"content_image\" width=\"208\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;208&#39; height=&#39;54&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"208\" data-rawheight=\"54\" class=\"content_image lazy\" width=\"208\" data-actualsrc=\"https://pic3.zhimg.com/v2-4cc213e728dcff2d95bf5e998600d5da_b.jpg\"/></figure><p>均值)的Confidence。另外还有**4x2张**类别无关的offset map特征图，用来微调Extreme点反算到输入图的坐标。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-cffdc1d6ffea563d3a71bbb95a10a56a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"601\" data-rawheight=\"771\" class=\"origin_image zh-lightbox-thumb\" width=\"601\" data-original=\"https://pic3.zhimg.com/v2-cffdc1d6ffea563d3a71bbb95a10a56a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;601&#39; height=&#39;771&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"601\" data-rawheight=\"771\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"601\" data-original=\"https://pic3.zhimg.com/v2-cffdc1d6ffea563d3a71bbb95a10a56a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-cffdc1d6ffea563d3a71bbb95a10a56a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>得到了5张Heatmap后，先用四张Heatmap找出所有的Extreme点可能出现的位置，再**穷举**这些位置的四元组合，结合Center heatmap和几何规则排除掉False Positive。过滤后剩下的组合就是检测到的目标了。目标的Score就是五个点(4个Extreme点和一个Center点)在五张特征图对应的**Score的平均**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>关于怎么做Grouping的方法，更清晰的可以见下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8bd3cd1d94ee21d939481380ebad6515_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"617\" class=\"origin_image zh-lightbox-thumb\" width=\"594\" data-original=\"https://pic2.zhimg.com/v2-8bd3cd1d94ee21d939481380ebad6515_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;594&#39; height=&#39;617&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"617\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"594\" data-original=\"https://pic2.zhimg.com/v2-8bd3cd1d94ee21d939481380ebad6515_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8bd3cd1d94ee21d939481380ebad6515_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>### 网络结构和损失函数</p><p class=\"ztext-empty-paragraph\"><br/></p><p>ExtremeNet源于CornerNet，网络结构和CornerNet很像，损失函数和CornerNet一样。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 网络结构</p><p class=\"ztext-empty-paragraph\"><br/></p><p>和CornerNet一样，ExtremeNet也是用的关键点检测常用的104-layer **HourglassNet**作为特征提取Backbone网络，但是去掉了Corner Pooling模块，其余做了很小的相应修改。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 损失函数</p><p class=\"ztext-empty-paragraph\"><br/></p><p>基本上和CornerNet一样，关键点定位用的是**改造的Focal Loss**</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ce5d77db5b60ca8567a93c0e24dbe50b_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"535\" data-rawheight=\"85\" class=\"origin_image zh-lightbox-thumb\" width=\"535\" data-original=\"https://pic4.zhimg.com/v2-ce5d77db5b60ca8567a93c0e24dbe50b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;535&#39; height=&#39;85&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"535\" data-rawheight=\"85\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"535\" data-original=\"https://pic4.zhimg.com/v2-ce5d77db5b60ca8567a93c0e24dbe50b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ce5d77db5b60ca8567a93c0e24dbe50b_b.png\"/></figure><p>，Offset回归用的是**Smooth L1 Loss**</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-765e2739bd71c0619146160fd73554b2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"409\" data-rawheight=\"81\" class=\"content_image\" width=\"409\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;409&#39; height=&#39;81&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"409\" data-rawheight=\"81\" class=\"content_image lazy\" width=\"409\" data-actualsrc=\"https://pic3.zhimg.com/v2-765e2739bd71c0619146160fd73554b2_b.jpg\"/></figure><p>。Grouping用的是几何方法，没有用到Embddings，就**没有推拉Loss**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文是以点做目标检测的第二篇论文ExtremeNet，和CornerNet关键点选取目标包围框的左上角点和右下角点不同，ExtremeNet的关键点选择为目标边缘最高、最低、最左、最右这四个点，在目标检测领域取得了不错的成绩，而且**提供的定位信息更多更准确**，也可以作为第一步应用在**两阶段分割任务**中。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [CornerNet: Detecting Objects as Paired Keypoints](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1808.01244\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CornerNet: Detecting Objects as Paired Keypoints</a>)</p><p>+ [Bottom-up Object Detection by Grouping Extreme and Center Points](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1901.08043\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Bottom-up Object Detection by Grouping Extreme and Center Points</a>)</p><p>+ [Stacked Hourglass Network for Human Pose Estimation](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1603.06937\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Stacked Hourglass Networks for Human Pose Estimation</a>)</p><p>+ [《D#0045-Stacked-Hourglass-Network-for-Human-Pose-Estimation》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230045-Stacked-Hourglass-Network-for-Human-Pose-Estimation/D%25230045.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p>+ [《D#0046-Anchor-Free第一篇CornerNet-Detecting-Objects-as-Paired-Keypoints》](&lt;<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230046-Anchor-Free%25E7%25AC%25AC%25E4%25B8%2580%25E7%25AF%2587CornerNet-Detecting-Objects-as-Paired-Keypoints/D%25230046.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>&gt;)</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }, 
                {
                    "tag": "目标检测", 
                    "tagLink": "https://api.zhihu.com/topics/19596960"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/88269398", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 0, 
            "title": "Anchor Free第一篇CornerNet", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p>目标检测这个领域，在14年RCNN系列出现之前，一直是滑动窗口加上图像金字塔的方法主导；在RCNN,SSD和YOLO出现之后，是Proposal和Anchor based方法主导；在18年到现在，新出现了一系列方法，都是Anchor Free的，以**直接检测点**（矩形包围框的角点、中心点等）为思路来做目标检测。这里简单对一些典型的Anchor Free方法做一个系列性的介绍，本文介绍第一篇CornerNet。论文[《CornerNet: Detecting Objects as Paired Keypoints》](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1808.01244\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CornerNet: Detecting Objects as Paired Keypoints</a>)发表于2018年的ECCV上，应该算是以点检测来取代Anchor方法的第一篇，值得一读。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## Anchor方法的缺点</p><p class=\"ztext-empty-paragraph\"><br/></p><p>像Faster RCNN，SSD，YOLO v2/v3这样的比较流行的目标检测框架，都是用到了锚点框的概念。通过在不同特征图上，配置不同的锚点框来通过学习锚点框相对于真值框的偏移和置信度来确定目标的位置。这种检测范式有以下两个缺点：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 正负样本不平衡：在基于Anchor的方法中，需要在不同的特征图上预设很多Anchor，这些Anchor根据和真值框的IoU大小来负责预测是有目标的还是背景图。为了增大召回率，需要很**密集地**布置很多很多的Anchor（这也导致了训练和预测的低效），然而，这么多的Anchor只有很少一部分可以达到和真值框足够的IoU来使其能作为正样本，**绝大多数的Anchor都是负样本**。这导致了训练时**正负样本的不平衡问题**。</p><p>2. 超参数难调：Anchor Box的**数量**、**大小**、**宽高比**，还有**哪个层**的特征图设置多少个Anchor Box，这些都是需要深度学习工程师在训练的时候根据项目的实际情况来设置的。比如，人脸检测中要加大小脸的召回，就在浅层的特征图上多设置一些宽高比为1:1的Anchor Box；在车牌检测中，就不能继续用1:1的Anchor Box了，需要根据车牌的比例（不同国家的车牌，比例还不一样），设置比如5:1，7:1的Anchor Box才合理。但是这些超参数，都是依赖经验的，如果能有方法去掉这些超参数，**全自动地**去从样本中学习目标检测需要的信息，那多好。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## CornerNet介绍</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 前人的启发</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8a82baa18383b37ebc3a1ae9a4083120_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"448\" class=\"origin_image zh-lightbox-thumb\" width=\"594\" data-original=\"https://pic1.zhimg.com/v2-8a82baa18383b37ebc3a1ae9a4083120_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;594&#39; height=&#39;448&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"448\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"594\" data-original=\"https://pic1.zhimg.com/v2-8a82baa18383b37ebc3a1ae9a4083120_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8a82baa18383b37ebc3a1ae9a4083120_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在2017年NIPS上出了一个论文[《Associative Embedding: End-to-End Learning for Joint Detection and Grouping》](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1611.05424\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/abs/1611.0542</span><span class=\"invisible\">4</span><span class=\"ellipsis\"></span></a>)，是做人体姿态估计的。文中提出了一个很有启发性的观点：像人体姿态估计和实例分割这种CV任务来说，底层的逻辑就是要**给点打标签**，然后**将这些点组合(Group)起来**形成上层的语义。受此启发，作者认为对于目标检测来说，我们要检测目标的包围框，其实也是等价于**检测包围框的左上角的点和右下角的点**，然后将它们组合起来。按此思路，作者18年提出了CornerNet这个模型，并发了这篇ECCV的论文。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### CornerNet思路</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-efdb4798d82db330a31b6c7e77538d37_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1278\" data-rawheight=\"628\" class=\"origin_image zh-lightbox-thumb\" width=\"1278\" data-original=\"https://pic4.zhimg.com/v2-efdb4798d82db330a31b6c7e77538d37_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1278&#39; height=&#39;628&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1278\" data-rawheight=\"628\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1278\" data-original=\"https://pic4.zhimg.com/v2-efdb4798d82db330a31b6c7e77538d37_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-efdb4798d82db330a31b6c7e77538d37_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图所示，Cornet将检测目标的包围框这个任务转换成了检测包围框的左上角和右下角角点，然后将其组合起来这个任务，**完全抛弃了Anchor的概念**。首先用一个Backbone网络（文中用的[Hourglass](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230045-Stacked-Hourglass-Network-for-Human-Pose-Estimation/D%25230045.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)，可能是因为作者的思路借鉴与人体姿态估计，而Hourglass也是做人体姿态估计的）提取特征，后面分出了两个单独的检测器分别得到左上角角点和右下角角点的Heatmaps和相应的Embeddings。**通过Heatmaps定位角点的位置集合，再通过Embeddings联系两个集合中的点组成框**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 网络结构和损失函数</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5cf0f229037b35188bb55e67db63bb60_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1273\" data-rawheight=\"389\" class=\"origin_image zh-lightbox-thumb\" width=\"1273\" data-original=\"https://pic1.zhimg.com/v2-5cf0f229037b35188bb55e67db63bb60_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1273&#39; height=&#39;389&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1273\" data-rawheight=\"389\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1273\" data-original=\"https://pic1.zhimg.com/v2-5cf0f229037b35188bb55e67db63bb60_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-5cf0f229037b35188bb55e67db63bb60_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上面的图看的更清楚了，CornerNet先用一个Hourglass提取特征，后面接两个Predition Module分别负责左上角点和右下角点的定位。每一个Prediction Module内含先走一个Corner Pooling模块，再牵出三个分支分别预测Heatmaps，Embeddings和Offsets。Heatmaps和Offsets一起定位角点，Embeddings将一些零散的角点组合起来形成预测框。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**Backbone Hourglass**在[《D#0045-Stacked-Hourglass-Network-for-Human-Pose-Estimation》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230045-Stacked-Hourglass-Network-for-Human-Pose-Estimation/D%25230045.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)里面已经做了比较详细的介绍，这里不多说。下面来看Prediction Module：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-445f1fc00891208e2632cf81b0cf3e85_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1265\" data-rawheight=\"468\" class=\"origin_image zh-lightbox-thumb\" width=\"1265\" data-original=\"https://pic2.zhimg.com/v2-445f1fc00891208e2632cf81b0cf3e85_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1265&#39; height=&#39;468&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1265\" data-rawheight=\"468\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1265\" data-original=\"https://pic2.zhimg.com/v2-445f1fc00891208e2632cf81b0cf3e85_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-445f1fc00891208e2632cf81b0cf3e85_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>**Prediction Module**开始是一个残差块的变种，在残差块的主路径中，作者发明了一个**Corner Pooling**的块（Corner Pooling也是论文中的创新点之一）**利用一些先验知识抽取和Corner强相关的特征**，这些特征经过一次ReLU和一次3x3 Conv-BN-ReLU的操作后分别开三个分支，去做3x3 Conv-ReLU和1x1 Conv去得到负责的那类点（左上或者右下）的Heatmaps、Embeddings和Offsets信息。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**Heatmaps**的形状为CxHxW，C是通道数，等于需要检测的类别数量，这里**不需要负责背景的heatmap通道**（如果某个点是背景，那么所有的heatmap在那个位置都是0就行了）。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6c1a5d67fd93836b8a378cde5e79a894_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"619\" data-rawheight=\"580\" class=\"origin_image zh-lightbox-thumb\" width=\"619\" data-original=\"https://pic1.zhimg.com/v2-6c1a5d67fd93836b8a378cde5e79a894_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;619&#39; height=&#39;580&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"619\" data-rawheight=\"580\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"619\" data-original=\"https://pic1.zhimg.com/v2-6c1a5d67fd93836b8a378cde5e79a894_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6c1a5d67fd93836b8a378cde5e79a894_b.jpg\"/></figure><p>在训练的Prediction Module时候，会对真值做一些处理。如上图，红框是真值框，但是绿框也不是完全不对，他和红框的IoU依然很大。对此，作者对真值点的标签以真值点为中心，以一定的半径（这个半径和真值框的大小有关）做了一个Gaussian加权处理，这也是很合理的。最终Prediction Module部分的损失函数如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-184d74596aa2c9fea7630442326066c5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"636\" data-rawheight=\"446\" class=\"origin_image zh-lightbox-thumb\" width=\"636\" data-original=\"https://pic2.zhimg.com/v2-184d74596aa2c9fea7630442326066c5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;636&#39; height=&#39;446&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"636\" data-rawheight=\"446\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"636\" data-original=\"https://pic2.zhimg.com/v2-184d74596aa2c9fea7630442326066c5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-184d74596aa2c9fea7630442326066c5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>是一个修改了的Focal Loss。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>由于网络中Pooling和Stride的存在，前面的Heatmap只能比较粗略地从HxW的图上反推出原图的坐标，实际误差是</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cfb219f9bc0c00fe9a522fdc9410fa64_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"348\" data-rawheight=\"67\" class=\"content_image\" width=\"348\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;348&#39; height=&#39;67&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"348\" data-rawheight=\"67\" class=\"content_image lazy\" width=\"348\" data-actualsrc=\"https://pic1.zhimg.com/v2-cfb219f9bc0c00fe9a522fdc9410fa64_b.jpg\"/></figure><p>，如果知道了这个误差值，就可以在反算时做一个弥补，使得到的坐标值更精确一些。**Offsets**模块就是干这个事情的，它使用</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3993bf59ed6f28243f8a74df56a6b4a7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"408\" data-rawheight=\"85\" class=\"content_image\" width=\"408\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;408&#39; height=&#39;85&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"408\" data-rawheight=\"85\" class=\"content_image lazy\" width=\"408\" data-actualsrc=\"https://pic4.zhimg.com/v2-3993bf59ed6f28243f8a74df56a6b4a7_b.jpg\"/></figure><p>学习误差值得到Offsets，利用Offset和Heatmaps的信息共同就精确定位了角点坐标。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>只是得到了左上角的点的坐标集合和右下角点的坐标集合，还没办法将两个单独集合里面的点组合起来成为一个框。组合的信息就是通过**Embddings**来学习得到的。Embddings的思路很简单，也很巧妙。它为每个角点都设置一个值（这个值就叫Embdding值），训练的时候，设计目标函数，让同一个目标的两个角点的Embdding距离相近（将同一个目标的两个角点对应的Embdding**拉**到一起），而同时让不同目标的两个角点的Embdding的均值的距离很远（将不同目标的两个角点对应的Embdding的均值**推**开）。用公式表达如下：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b35b182e212f55fcae2aaf77bcbc4903_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"612\" data-rawheight=\"249\" class=\"origin_image zh-lightbox-thumb\" width=\"612\" data-original=\"https://pic4.zhimg.com/v2-b35b182e212f55fcae2aaf77bcbc4903_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;612&#39; height=&#39;249&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"612\" data-rawheight=\"249\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"612\" data-original=\"https://pic4.zhimg.com/v2-b35b182e212f55fcae2aaf77bcbc4903_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b35b182e212f55fcae2aaf77bcbc4903_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>最终整个CornerNet网络的训练损失函数就是上面介绍的Heatmaps的损失，Offsets的损失和Embddings的损失的加权调和：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-06b5c80cae862027cdb0373354cb7258_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"405\" data-rawheight=\"53\" class=\"content_image\" width=\"405\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;405&#39; height=&#39;53&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"405\" data-rawheight=\"53\" class=\"content_image lazy\" width=\"405\" data-actualsrc=\"https://pic1.zhimg.com/v2-06b5c80cae862027cdb0373354cb7258_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>### Corner Pooling</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Corner Pooling是论文中比较大的创新点，它是紧跟在Backbone模块后的第一步，对整个网络的性能影响也很大，这里值得单独细讲一下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-eec495467878f0538960ffa7fe581152_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1269\" data-rawheight=\"352\" class=\"origin_image zh-lightbox-thumb\" width=\"1269\" data-original=\"https://pic3.zhimg.com/v2-eec495467878f0538960ffa7fe581152_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1269&#39; height=&#39;352&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1269\" data-rawheight=\"352\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1269\" data-original=\"https://pic3.zhimg.com/v2-eec495467878f0538960ffa7fe581152_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-eec495467878f0538960ffa7fe581152_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们可以想象一下，如上图所示，以定位某个目标包围框的左上角关键点为例。**人眼在定位左上角关键点的时候，需要分别像右和向下扫描，找到目标的最顶和最左的边缘**。Corner Pooling就是**显示地将这个假设体现在网络中**，特意去编码那些对向右和向下寻找边缘这个任务有利的信息，来帮助定位角点。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-62fe5ac232bd7af027b8ca0bbb98dbd0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1266\" data-rawheight=\"519\" class=\"origin_image zh-lightbox-thumb\" width=\"1266\" data-original=\"https://pic1.zhimg.com/v2-62fe5ac232bd7af027b8ca0bbb98dbd0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1266&#39; height=&#39;519&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1266\" data-rawheight=\"519\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1266\" data-original=\"https://pic1.zhimg.com/v2-62fe5ac232bd7af027b8ca0bbb98dbd0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-62fe5ac232bd7af027b8ca0bbb98dbd0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>Corner Pooling是怎么编码目标的最顶上和最左边的边缘的信息呢？其实很简单，如上图所示，Corner Pooling模块输入两张特征图，分别扫描特征图上每个像素，替换成这个点及其右边所有点和这个点及其下面所有点的集合的最大值，然后相加起来得到输出的特征图。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-bfeff9ef24be1544fcd592091725dd15_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1281\" data-rawheight=\"397\" class=\"origin_image zh-lightbox-thumb\" width=\"1281\" data-original=\"https://pic2.zhimg.com/v2-bfeff9ef24be1544fcd592091725dd15_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1281&#39; height=&#39;397&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1281\" data-rawheight=\"397\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1281\" data-original=\"https://pic2.zhimg.com/v2-bfeff9ef24be1544fcd592091725dd15_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-bfeff9ef24be1544fcd592091725dd15_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>Corner Pooling的实现也特别简单，并不需要自己再在现有的训练框架上加特别的层写一些前向和反向的代码，只需要利用常见的Max Pooling层就行的（有点像快速计算积分图）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文介绍了今年流行的以点检测代替Anchor做目标检测的论文CornerNet，点检测的方法借鉴了人体姿态估计和实例分割里面的思想，避免了Anchor方法中正负样本不均衡和需要手动调的那些超参数那些弊端。CornerNet出来后，CenterNet，FCOS，ExtremeNet等等方法都沿着这个Anchor Free的思路继续深入下去，先挖个坑，以后有机会再讲。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [Associative embedding: End-to-end learning for joint detection and grouping](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1611.05424\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/abs/1611.0542</span><span class=\"invisible\">4</span><span class=\"ellipsis\"></span></a>)</p><p>+ [CornerNet: Detecting Objects as Paired Keypoints](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1808.01244\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CornerNet: Detecting Objects as Paired Keypoints</a>)</p><p>+ [Stacked Hourglass Network for Human Pose Estimation](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1603.06937\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Stacked Hourglass Networks for Human Pose Estimation</a>)</p><p>+ [《D#0045-Stacked-Hourglass-Network-for-Human-Pose-Estimation》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230045-Stacked-Hourglass-Network-for-Human-Pose-Estimation/D%25230045.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p>", 
            "topic": [
                {
                    "tag": "目标检测", 
                    "tagLink": "https://api.zhihu.com/topics/19596960"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/88080620", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 2, 
            "title": "Stacked Hourglass Network", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p>本文介绍一篇2016 ECCV上的老文章[《Stacked Hourglass Network for Human Pose Estimation》](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1603.06937\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Stacked Hourglass Networks for Human Pose Estimation</a>)，文章提出了一种用于做人体姿态估计的Hourglass网络，形状和前面介绍的用于分割的[U-Net](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230040-%25E7%2594%25A8U-Net%25E5%2581%259A%25E5%2588%2586%25E5%2589%25B2/D%25230040.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)很像，设计思路也很像，是人体姿态检测领域一篇比较重要的文章。而且，Hourglass的结构也作为Backbone网络应用于除人体姿态估计之外的领域中，比如今年大热的目标检测Anchor-Free的[CornetNet](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1808.01244\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CornerNet: Detecting Objects as Paired Keypoints</a>)中。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 网络结构</p><p class=\"ztext-empty-paragraph\"><br/></p><p>对于人体姿态估计这样的高级别的视觉任务来说，既需要模型能捕获低级别小尺度的信息（比如定位大概某个点是一个手腕），有需要模型能捕获高级别大尺度的信息（比如判断这个点是左手的手腕还是右手的手腕）。对不多尺度信息的捕获，有许多方法，比如简单的多分辨率预测，不同深度的层做特征融合，或者复杂一点的特征金字塔网络FPN，和前面介绍过的用于做语义分割的U-Net那样的。**这里说的Hourglass就属于和U-Net比较像的思路，先不断地缩小特征图空间分辨率，再不断提升和回复原来输入的分辨率，然后再在前后加一些Skip Layer，从而得到涵盖了不同尺度信息的输出特征图**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Hourglass模块</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 大致结构</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-90b1a9b835ad0f23eca26dadafddd0d6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1281\" data-rawheight=\"551\" class=\"origin_image zh-lightbox-thumb\" width=\"1281\" data-original=\"https://pic3.zhimg.com/v2-90b1a9b835ad0f23eca26dadafddd0d6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1281&#39; height=&#39;551&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1281\" data-rawheight=\"551\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1281\" data-original=\"https://pic3.zhimg.com/v2-90b1a9b835ad0f23eca26dadafddd0d6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-90b1a9b835ad0f23eca26dadafddd0d6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图就是一个Hourglass模块，可以直观地看到分辨率是先降低再升高，两头大中间细，很像一个沙漏形状，所以取名Hourglass。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在Hourglass模块中，使用max pooling来降低特征图分辨率，在每次**max pooling**之后，牵出来一个Skip Layer来处理原来分辨率的信息（后面跟升起来分辨率的相对应的特征图做融合）。在模块达到最低分辨率的时候，后面又用**最近邻插值**的方法（不是unpooling或者deconv）进行上采样得到捕获全局信息的不同分辨率的特征图，和前面Skip Connection对应的特征图进行**Element-wise相加**做融合。上图中所有的max pooling操作都对应着一个最近邻插值操作，降低分辨率的过程和提升分辨率的过程是**完全对称**的结构。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-fa37ccb8bb6ddf3bc34842ecb8856563_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1275\" data-rawheight=\"403\" class=\"origin_image zh-lightbox-thumb\" width=\"1275\" data-original=\"https://pic4.zhimg.com/v2-fa37ccb8bb6ddf3bc34842ecb8856563_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1275&#39; height=&#39;403&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1275\" data-rawheight=\"403\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1275\" data-original=\"https://pic4.zhimg.com/v2-fa37ccb8bb6ddf3bc34842ecb8856563_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-fa37ccb8bb6ddf3bc34842ecb8856563_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在输出的时候，通过1x1的卷积调整得到一个heatmap，heatmap的通道个数等于人体Pose的关键点类别数。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### Residual Module</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e70dec41e3c5846866823a2e13b51c42_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"526\" data-rawheight=\"256\" class=\"origin_image zh-lightbox-thumb\" width=\"526\" data-original=\"https://pic3.zhimg.com/v2-e70dec41e3c5846866823a2e13b51c42_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;526&#39; height=&#39;256&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"526\" data-rawheight=\"256\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"526\" data-original=\"https://pic3.zhimg.com/v2-e70dec41e3c5846866823a2e13b51c42_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e70dec41e3c5846866823a2e13b51c42_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图是一个Hourglass模块中使用的残差模块的示意图（在图3中就是一个小方框），这个没什么说的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Stacked Hourglass网络</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-161b611598148a7581b769afbbb390f4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1283\" data-rawheight=\"465\" class=\"origin_image zh-lightbox-thumb\" width=\"1283\" data-original=\"https://pic1.zhimg.com/v2-161b611598148a7581b769afbbb390f4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1283&#39; height=&#39;465&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1283\" data-rawheight=\"465\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1283\" data-original=\"https://pic1.zhimg.com/v2-161b611598148a7581b769afbbb390f4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-161b611598148a7581b769afbbb390f4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>把多个（论文中是8个）Hourglass模块堆叠(Stacked)起来就组成了Stack Hourglass模型。需要注意的是，输入模型的图片分辨率是256x256，为了节约内存，一开始就用大卷积和一系列的max pooling操作把分辨率降到了64x64（这也是网络中Hourglass模块的输入和输出分辨率）。在网络中间各个Hourglass模块串联的时候，添加了一些卷积层。整个网络输出的时候，又通过1x1的卷积调整得到各个关节点的heatmap，heatmap通过最大值激活可以得到准确的关节点坐标，通道对应着关节点的种类。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 训练方法</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Intermediate Supervision</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-883235f19b44d2757f5ee6a78494fba8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"706\" data-rawheight=\"206\" class=\"origin_image zh-lightbox-thumb\" width=\"706\" data-original=\"https://pic1.zhimg.com/v2-883235f19b44d2757f5ee6a78494fba8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;706&#39; height=&#39;206&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"706\" data-rawheight=\"206\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"706\" data-original=\"https://pic1.zhimg.com/v2-883235f19b44d2757f5ee6a78494fba8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-883235f19b44d2757f5ee6a78494fba8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>由于Stacked Hourglass网络使用的是多个Hourglass模块堆叠而成，每个Hourglass模块都应该捕获了全局的和局部的信息，所以从这些信息中生成GT信息也应该是合理的。而且这个堆叠的过程，作者假设它是一个不断bottom-up，top-down的信息处理过程，在这个过程中输出的都是关于人体Pose的整体和局部信息，只不过后面的实在前面的基础上做further evaluate和reassess。基于这个假设，作者在每个Hourglass的输出上加了生成GT的模块，用GT进行监督训练来降低网络的训练难度。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 训练细节</p><p class=\"ztext-empty-paragraph\"><br/></p><p>训练数据集采用FLIC和MPII，首先通过标注信息扣出人体，再resize到256x256的分辨率，做了一些旋转(正负30度)和缩放的数据增强(x0.75~x1.25)。优化方法使用rmsprop，初始学习率2.5e-4（后面每到达平台期就除以5），使用Titan X训练了3天。单次预测耗时75ms。在测试的时候，做两次预测，一次原图，一次翻转后的图，把两次的heatmap平均一下可以得到大约1%的性能提升。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>损失函数用的是MSE，真值用2D正态分布做了软化处理。为了提升定位精度，在从heatmap坐标反推到原图坐标的时候，把heatmap坐标向第二高的激活值方向移动1/4个像素（其实就是heatmap的最大值和第二大值进行了一个调和）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>对于严重遮挡和扭曲的关节点，在把真值heatmap上都设置为0。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验结果</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 数据集上测试结果</p><p class=\"ztext-empty-paragraph\"><br/></p><p>作者使用关键点正确分类的百分比(PCK)指标和一些在人体姿态估计上流行的方法做了比较，比较结果如下：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>FLIC的结果</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2c8f68df22f05fd756f655211e686bbe_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1165\" data-rawheight=\"382\" class=\"origin_image zh-lightbox-thumb\" width=\"1165\" data-original=\"https://pic3.zhimg.com/v2-2c8f68df22f05fd756f655211e686bbe_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1165&#39; height=&#39;382&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1165\" data-rawheight=\"382\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1165\" data-original=\"https://pic3.zhimg.com/v2-2c8f68df22f05fd756f655211e686bbe_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2c8f68df22f05fd756f655211e686bbe_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>MPII的结果</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3130087cd4e85d722e92cd61b038ff60_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1266\" data-rawheight=\"803\" class=\"origin_image zh-lightbox-thumb\" width=\"1266\" data-original=\"https://pic1.zhimg.com/v2-3130087cd4e85d722e92cd61b038ff60_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1266&#39; height=&#39;803&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1266\" data-rawheight=\"803\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1266\" data-original=\"https://pic1.zhimg.com/v2-3130087cd4e85d722e92cd61b038ff60_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-3130087cd4e85d722e92cd61b038ff60_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>### 剪枝实验</p><p class=\"ztext-empty-paragraph\"><br/></p><p>另外，作者为了证明Intermediate Supervision的效果确实对性能提升帮助很大，性能提升不是由于网络参数增多达到的，做了如下的剪枝实验。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3e360f4e0b392f3ea824eab7bd9dd4e1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1273\" data-rawheight=\"620\" class=\"origin_image zh-lightbox-thumb\" width=\"1273\" data-original=\"https://pic2.zhimg.com/v2-3e360f4e0b392f3ea824eab7bd9dd4e1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1273&#39; height=&#39;620&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1273\" data-rawheight=\"620\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1273\" data-original=\"https://pic2.zhimg.com/v2-3e360f4e0b392f3ea824eab7bd9dd4e1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3e360f4e0b392f3ea824eab7bd9dd4e1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Hourglass网络在人体姿态领域算是比较早也比较经典的一个模型，其结构和U-Net类似，都是分辨率先下降再提升的过程。Hourglass不但应用于人体姿态检测，也作为Backbone被应用于目标检测模型CornerNet中。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [D#0040-用U-Net做分割](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230040-%25E7%2594%25A8U-Net%25E5%2581%259A%25E5%2588%2586%25E5%2589%25B2/D%25230040.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p>+ [Stacked Hourglass Network for Human Pose Estimation](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1603.06937\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Stacked Hourglass Networks for Human Pose Estimation</a>)</p><p>+ [CornerNet: Detecting Objects as Paired Keypoints](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1808.01244\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CornerNet: Detecting Objects as Paired Keypoints</a>)</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "模型", 
                    "tagLink": "https://api.zhihu.com/topics/19579715"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/86275649", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 2, 
            "title": "用Network Slimming做模型加速和压缩", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p>对于做好一个产品来说，算法工程师输出的模型光性能达标是不够的，模型**文件的体积**，**运行时速度**和**资源占用大小**也是不可忽视的因素，特别是对于高吞吐的业务量和受限的边缘计算场景来说。对于这些方面的优化都可以称之为模型的加速和压缩。这里以2017年ICCV上的一篇[《Learning Efficient Convolutional Networks through **Network Slimming**》](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1708.06519\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Learning Efficient Convolutional Networks through Network Slimming</a>)为例，介绍一下做模型通道剪枝来加速和压缩CNN模型的方法。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 模型加速和压缩的几个大的思路</p><p class=\"ztext-empty-paragraph\"><br/></p><p>对于模型的前向推理速度，要么是**1. 加快运算速度**（例如上更好的CPU，GPU，DSP，更大内存，更快的主板这样的硬件升级），或者是**2. 进行针对部署平台的深度优化**（比如移动端arm上的neno优化，桌面CPU上的SIMD优化，或者GPU上的cuda优化等），要么是**3. 减少计算量**（模型的乘加次数）。硬件升级简单粗暴见效快，不过成本也高；针对平台的深度优化，开发周期长，可移植性差；减少模型计算量的优化，主要是思路和调参的事情，相对而言比较容易。而且，1，2和3的几个优化方法也不是矛盾的，可以根据项目的具体情况来组合实施。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 加快运算速度</p><p class=\"ztext-empty-paragraph\"><br/></p><p>升级更好的更贵的硬件，更好的主频，更多的计算单元，更大的内存，来提升前向推理速度，这个手段只要有点计算机基础知识的人都知道。但是并不代表这个方法不可以用，更不代表这个方法很容易用。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>首先，硬件升级**见效快**。现在AI领域很多To B的客户，定制化需求都很强，各种想法，各种细节，工程师和PM不可能在做研发的时候都能考虑到，但是限于要快速出demo或者提供POC的要求，慢工出细活地针对平台做优化或者对模型进行剪枝压缩都来不及，这时是可以考虑升级一下硬件的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>其次，硬件升级被很多人忽视，但是其实升级也不是真的简单粗暴地瞎升级的。要对现有模型的速度和资源瓶颈做分析后，得到分析数据后才能有针对性地去升级硬件。比如，模型写成Demo后跑一下看一看，是主要卡在CPU的主频低了，还是CPU的核心数少了，还是CPU和内存的cache命中率太低，或者是内存的大小，频率，还有主板的总线带宽太垃圾等，不一样的问题有不一样的解决方案，**要有数据的支撑后选择有针对性的硬件升级方案**可能更好。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 前向框架加速</p><p class=\"ztext-empty-paragraph\"><br/></p><p>现在深度学习在不同的设备上都有落地开花，同一个模型可能部署在不同的硬件设备上。针对不同的设备，选择一些成熟的前向框架无疑是应该的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### ARM</p><p class=\"ztext-empty-paragraph\"><br/></p><p>小米的mace，腾讯的[ncnn](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Tencent/ncnn\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Tencent/ncnn</a>)和阿里的[MNN](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/alibaba/MNN\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">alibaba/MNN</a>)这些大公司开源的框架，专注于前向优化，在基于arm的移动端上做了很深的优化，如果用这些计算框架大概率比用tensorflow，caffe或者pytorch这种做训练的框架要快。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### GPU</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在GPU上，NVIDIA也推出了TensorRT这样的前向框架，如果是用他家的GPU做服务器部署，可以考虑一下。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 其它</p><p class=\"ztext-empty-paragraph\"><br/></p><p>也有像[TVM](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/dmlc/tvm\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">dmlc/tvm</a>)这种，针对不同的硬件平台都做了tuning的，可以考虑一下。另外值得一提的是OpenCV在桌面CPU上跑的速度也不赖，可以作为备选方案。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>当然，**最好的东西，构成一些商业壁垒的产品，都是不会开源的**，一些实力更强的大公司，例如华为，海康和vivo这些，都有自己的团队，针对自己的情况做更深度的前向框架加速和优化，这里不做太多介绍。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>上面讲的都是从非模型本身角度来做加速和优化，下面介绍一下从模型本身角度做优化的常见方法。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 低秩分解</p><p class=\"ztext-empty-paragraph\"><br/></p><p>全连接层的权重参数，就是一个巨大的二维矩阵，对这个巨大的二维矩阵做SVD这样的分解，提取特征向量来表达这个矩阵，可以达到**压缩模型体积**的作用，因为大多数的CNN模型参数都是全连接层贡献的。但是低秩分解**不能取得计算量的压缩**，因为CNN的计算量大都是前面的卷积层贡献的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 权值量化</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在模型训练和存储，还有进行前向的过程中，权重一般的都是默认为fp32的浮点值存储。但是有研究表明，CNN模型是一类过参数化的模型，参数的具体值对于结果来说不是很重要。所以有些库就把模型默认的浮点值用离散的整型值来量化，这样起到了减小模型体积(例如FP16比FP32节约一半的模型体积)和加速的作用(很多平台上整数值的计算比浮点值快)。但是这种权值量化的方法，**会对模型的性能产生一定的伤害**，就要看能不能接收了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 权值剪枝和稀疏化</p><p class=\"ztext-empty-paragraph\"><br/></p><p>对模型训练后出现的绝对值较小的接近于0的权值和连接，把他们强制置零。这样整个模型的权重大多数都是0了，用稀疏矩阵的特殊存储方式存储，可以减小模型文件的体积，节约不少存储空间。然而，如果要获得运行时速度的加速，**需要配合专门针对稀疏矩阵运算做优化的库或者硬件**；而且，运行时内存的消耗主要不是在权重参数的存储上，而是在中间生成的特征图的存储上（这些特征图还是稠密的），所以**对运行时内存的压缩也不是很多**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 结构化剪枝和稀疏化</p><p class=\"ztext-empty-paragraph\"><br/></p><p>结构化剪枝指的是整个卷积filter，整个feature map的channel或者整个网络的layer整个去掉的加速压缩方法。一般都是按照某些猜想，在网络训练的时候改造目标函数，然后在训练的结果中按照一定的规则进行剪枝达到加速和压缩的目的。本文在后面要介绍的Network Slimming方法就是属于结构化剪枝的思路。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 网络结构学习</p><p class=\"ztext-empty-paragraph\"><br/></p><p>现在主流的CNN模型设计思路还是用一些专门的假设和前提设计好网络的结构，然后用梯度下降方法去训练优化出这个结构的权值使经验损失最小化。在这个方法中，模型结构是固定的。如果模型结构放得松弛一些，在优化的目标中也加上对模型结构的优化（比如让模型参数尽量小，层数尽量少，特征图尽量窄等），让算法在一个很大的模型结构的假设空间内去**自动搜索学习一个最好的结构**，那么这种方法就是网络结构学习（Neural Architecture Learning）。这种方式由于模型结构的假设空间巨大，可能性太多，需要训练成百个的模型来验证，所以**对计算资源的要求是很高的**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>另外提一句，**Network Slimming也可以当成一种特殊的网络结构的自动学习方法，只不过它的学习只限于在优化特征图宽度的这个维度**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## Network Slimming</p><p class=\"ztext-empty-paragraph\"><br/></p><p>上面的都是模型加速和压缩这一领域的背景和铺垫，下面内容主要是根据论文[《Learning Efficient Convolutional Networks through **Network Slimming**》](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1708.06519\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Learning Efficient Convolutional Networks through Network Slimming</a>)来介绍。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 通道剪枝的优点</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Network Slimming实在通道层面来进行剪枝的，但是剪枝的方式不仅仅在通道层面，也有人在权值层面，卷积核层面和层的层面来做剪枝。在权值层面做**细粒度**的剪枝，灵活性最大，不过需要配合专门的做稀疏矩阵运算的软件硬件才能达到效果；做整层整层的剪枝，方式最为**粗糙**，不需要专门软件硬件的配合，但是灵活性比较差，一剪枝就是一整层，而且，如果原来网络的层数不多，这个方法效果也不好。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Network Slimming做的是在通道层面的剪枝，也就是在NCHW的Channel维度做裁剪，粒度比层这个层面做裁剪要精细，而且也**不需要专门的软硬件的配合，在粗粒度和细粒度的剪枝优缺点上做了很好的平衡。而且它既适用于卷积层也可用于全连接层的剪枝。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 用[BN层](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230020-Batch-Normalization%25E5%25B1%2582%25E5%258E%259F%25E7%2590%2586%25E4%25B8%258E%25E5%2588%2586%25E6%259E%2590/D%25230020.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)的缩放因子(Scale Factor)做剪枝</p><p class=\"ztext-empty-paragraph\"><br/></p><p>既然要在通道维度做剪枝，那么怎么样挑选出哪些通道可剪枝，哪些通道不可剪枝呢？</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-454aed984bb8fa3540f9830c967050b6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"369\" data-rawheight=\"67\" class=\"content_image\" width=\"369\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;369&#39; height=&#39;67&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"369\" data-rawheight=\"67\" class=\"content_image lazy\" width=\"369\" data-actualsrc=\"https://pic3.zhimg.com/v2-454aed984bb8fa3540f9830c967050b6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>作者这里简洁地像给每个通道分配一个可训练的权重因子gamma，每个通道的输出**乘以**这个权重因子当做这个通道的最终输出。然后在训练的时候，对这些权重因子做L1惩罚（如上图式子所示），强迫学习到稀疏的gamma，最后认为这些gamma的绝对值大小就代表了相应通道是不是重要，做一个全局排序，按比例去一个阈值，把不重要的通道剪枝。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>整体思路是这样的，那么这个gamma就真的是在卷积层后面加一个缩放层吗？不是的，因为卷积层和缩放层都是线性操作，两个是可以互补的，缩放层的大小并不能代表层的重要性；那么在卷积层的后面，BN层的前面加一个缩放层呢？也不行，这个缩放层的效果会被BN层的Normalization操作抵消掉；还有一种是把缩放层放在BN层的后面，下一层卷积层的前面，那样和第一种是一样的，连续两个线性变换有可能是相互互补的，并不能用缩放层系数的大小代表该层的重要性。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-56fc5c99252308e646e0c4d9682844f7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"322\" data-rawheight=\"77\" class=\"content_image\" width=\"322\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;322&#39; height=&#39;77&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"322\" data-rawheight=\"77\" class=\"content_image lazy\" width=\"322\" data-actualsrc=\"https://pic4.zhimg.com/v2-56fc5c99252308e646e0c4d9682844f7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这里作者没有特别地加其它层来做，直接用**的BN层的gamma缩放系数来表示对应Channel的重要性**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 迭代剪枝策略</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ad33e468f3e410bff8aaec1ebb28611d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb\" width=\"594\" data-original=\"https://pic2.zhimg.com/v2-ad33e468f3e410bff8aaec1ebb28611d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;594&#39; height=&#39;192&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"594\" data-original=\"https://pic2.zhimg.com/v2-ad33e468f3e410bff8aaec1ebb28611d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ad33e468f3e410bff8aaec1ebb28611d_b.jpg\"/></figure><p>有时候剪枝一次还不能满足压缩和加速的要求，有时候剪枝后会带来性能的退化，这时候可以把上面介绍的剪枝过程迭代几次，直到满足压缩加速的要求或者性能退化到已经很难用fine-tune再拉回来的时候，示意图如上图所示。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验结果</p><p class=\"ztext-empty-paragraph\"><br/></p><p>作者在ImageNet，MNIST，CIFAR-10/100和SVHN，用VGGNet，ResNet和DenseNet做了很全面的实验，这里简单挑选几个来展示Network Slimming的效果。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 加速压缩效果</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-cda918404ea67008bb919ec9518c7c49_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1139\" data-rawheight=\"919\" class=\"origin_image zh-lightbox-thumb\" width=\"1139\" data-original=\"https://pic2.zhimg.com/v2-cda918404ea67008bb919ec9518c7c49_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1139&#39; height=&#39;919&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1139\" data-rawheight=\"919\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1139\" data-original=\"https://pic2.zhimg.com/v2-cda918404ea67008bb919ec9518c7c49_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-cda918404ea67008bb919ec9518c7c49_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>从上图可以看到，在剪枝40%~70%的情况下，都可以取得比原来模型更好的性能，在大多数情况下，剪枝40%可以获得最佳的性能。从Parameters和FLOPs两列也可以看出来，模型参数数量和计算量都有很大的压缩（具体压缩多少和模型结构有关的，不一定是前面剪枝的比例那个数值）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>另外，可以看到，对模型进行剪枝，不但没有使模型性能下降，反而让模型性能提升了一些，这应该是因为Network Slimming剪枝的方式，对于本来就是过参数化的模型，基本没有增加网络的容量，反而相当于对训练过程加了一个L1正则化的作用，还提升了模型的泛化能力。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 关键参数分析</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Network Slimming中有两个比较重要的参数，一个是剪枝的百分比，一个是正则化强度系数，这里说一下他们对最终剪枝效果的的影响。具体实施时也要根据具体情况来试。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 剪枝百分比</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-691239dd8a7e3de13bb4938a859c0944_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"590\" data-rawheight=\"425\" class=\"origin_image zh-lightbox-thumb\" width=\"590\" data-original=\"https://pic1.zhimg.com/v2-691239dd8a7e3de13bb4938a859c0944_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;590&#39; height=&#39;425&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"590\" data-rawheight=\"425\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"590\" data-original=\"https://pic1.zhimg.com/v2-691239dd8a7e3de13bb4938a859c0944_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-691239dd8a7e3de13bb4938a859c0944_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>从上图可以看到，如果使用剪枝后再Fine-tune的策略，剪枝百分比在一定阈值（这里是80%）以下会是比较安全的（Fine-tune之后得到的性能和不做剪枝的性能差不多甚至更好），当超过这个阈值，Fine-tune弥补不了性能的下降。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>另外，从上图也可以看到，用L1稀疏化惩罚做训练的绿色虚线结果会比Baseline的紫色虚线方法好，这也说明了前面说的Network Slimming方法有正则化的作用。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 正则化强度</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0f4a6cb271a200bab27045c04c1a376c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1238\" data-rawheight=\"323\" class=\"origin_image zh-lightbox-thumb\" width=\"1238\" data-original=\"https://pic1.zhimg.com/v2-0f4a6cb271a200bab27045c04c1a376c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1238&#39; height=&#39;323&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1238\" data-rawheight=\"323\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1238\" data-original=\"https://pic1.zhimg.com/v2-0f4a6cb271a200bab27045c04c1a376c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0f4a6cb271a200bab27045c04c1a376c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>从上面的图可以看出，正则化强度系数gamma越大，最后得到的缩放系数的结果越稀疏，越多的系数往0靠拢。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1cea8c86504990c6442321814ec85b6c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"604\" data-rawheight=\"504\" class=\"origin_image zh-lightbox-thumb\" width=\"604\" data-original=\"https://pic1.zhimg.com/v2-1cea8c86504990c6442321814ec85b6c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;604&#39; height=&#39;504&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"604\" data-rawheight=\"504\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"604\" data-original=\"https://pic1.zhimg.com/v2-1cea8c86504990c6442321814ec85b6c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1cea8c86504990c6442321814ec85b6c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图是说在gamma强度一定的情况下，随着训练轮次Epoch的增加，越来越多的缩放系数往0靠拢，最终只剩下少部分的缩放系数比较大，它们对应的通道就是要保留下来的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文先介绍了模型加速的几个大的思路，然后根据ICCV2017的Network Slimming的论文介绍了一下通道剪枝压缩加速的方法。Network Slimming的方法实现简单，压缩加速效果好，不牺牲性能，通用性好，而且不需要专用的软硬件的支持，可以在计算资源有限或者实时性要求高的场合使用。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [Learning Efficient Convolutional Networks through Network Slimming](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1708.06519\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Learning Efficient Convolutional Networks through Network Slimming</a>)</p><p>+ [ncnn GitHub](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Tencent/ncnn\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Tencent/ncnn</a>)</p><p>+ [MNN GitHub](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/alibaba/MNN\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">alibaba/MNN</a>)</p><p>+ [tvm GitHub](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/dmlc/tvm\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">dmlc/tvm</a>)</p><p>+ [TensorRT](<a href=\"https://link.zhihu.com/?target=https%3A//developer.nvidia.com/tensorrt\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">NVIDIA TensorRT</a>)</p><p>+ [D#0020-Batch-Normalization层原理与分析](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230020-Batch-Normalization%25E5%25B1%2582%25E5%258E%259F%25E7%2590%2586%25E4%25B8%258E%25E5%2588%2586%25E6%259E%2590/D%25230020.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p>", 
            "topic": [
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "优化", 
                    "tagLink": "https://api.zhihu.com/topics/19570512"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/85646778", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 11, 
            "title": "用HRNet做分割", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p>这里继续介绍第五篇著名的图像分割模型，HRNet[v2]。最开始的HRNet的论文发表于2019年的CVPR上，是做Pose检测的，而HRNetv2是在原来HRNet的基础上把它稍作改造使其成为用于分割的网络。不过，由于HRNet提取的特征丰富，各种分辨率的都有，而且在网络一路都保持着高分辨率特征，所以也很容易类似于改造VGG，GoogLeNet和ResNet那样**根据需要将其改造别的任务**（比如图像识别，目标检测，人脸特征点检测，语义分割的等）的主干网络。本文仅以HRNetv2做分割为例，来介绍HRNet。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## HRNet思路来源</p><p class=\"ztext-empty-paragraph\"><br/></p><p>做语义分割，现在主流的网络设计可以按照最终特征图的生成方式分为两大流派：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 以FCN，U-Net的代表的特征图先缩小后恢复的方法</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-09ece6f61c5aa63ec9653c7f7309deb0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"632\" data-rawheight=\"383\" class=\"origin_image zh-lightbox-thumb\" width=\"632\" data-original=\"https://pic1.zhimg.com/v2-09ece6f61c5aa63ec9653c7f7309deb0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;632&#39; height=&#39;383&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"632\" data-rawheight=\"383\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"632\" data-original=\"https://pic1.zhimg.com/v2-09ece6f61c5aa63ec9653c7f7309deb0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-09ece6f61c5aa63ec9653c7f7309deb0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>像以前介绍过的[FCN](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230039-%25E7%2594%25A8FCN%25E5%2581%259A%25E5%2588%2586%25E5%2589%25B2/D%25230039.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)和[U-Net](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230040-%25E7%2594%25A8U-Net%25E5%2581%259A%25E5%2588%2586%25E5%2589%25B2/D%25230040.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">https://github.com/Captain1986/CaptainBlackboard/blob/master/D%230040-用U-Net做分割/D%230040.md</a>)的网络，如上图所示，在网络的传播过程中，**逐步减小特征图的空间分辨率**，可以视为一个Encoder的过程，然后再加上一个尾巴Decoder把特征图编码了的信息解码出来，一次性或者**逐步地放大特征图分辨率**直到原始输入分辨率，得到Segmentation map。这其中可能也穿插着不同分辨率信息的融合。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 以DeepLab为代表的一路保持较大分辨率特征图的方法</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f1dc1a594cfec88df50de5b30cde8020_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1397\" data-rawheight=\"521\" class=\"origin_image zh-lightbox-thumb\" width=\"1397\" data-original=\"https://pic1.zhimg.com/v2-f1dc1a594cfec88df50de5b30cde8020_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1397&#39; height=&#39;521&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1397\" data-rawheight=\"521\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1397\" data-original=\"https://pic1.zhimg.com/v2-f1dc1a594cfec88df50de5b30cde8020_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f1dc1a594cfec88df50de5b30cde8020_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上面的FCN和U-Net那样的思路，会在特征图分辨率缩小的过程中有**信息的流失**，就算加上了跳跃连接和多分辨率融合也不一定能很好的补偿这种信息流失。所以就有另外一种思路，在**特征提取的过程中一路保持较大的分辨率**，如上图b，在最后的大分辨率特征图上预测Segmentation map。这种思路比较有代表性的就是DeepLab系列的空洞卷积了，[以前也有介绍](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230042-%25E7%2594%25A8DeepLabv3%252B%25E7%259A%2584Encoder-Decoder%25E5%2581%259A%25E5%2588%2586%25E5%2589%25B2/D%25230042.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## HRNet网络结构</p><p class=\"ztext-empty-paragraph\"><br/></p><p>HRNet的设计思路延续了一路保持较大分辨率特征图的方法，在网络前进的过程中，都**保持较大的特征图**，但是在网路前进过程中，也会**平行地**做一些下采样缩小特征图，如此**迭代**下去。最后生成**多组有不同分辨率的特征图**，**再融合**这些特征图做Segmentation map的预测。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 主干网络结构</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1d6e351cfcc9f68c163a53d80790e846_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1227\" data-rawheight=\"261\" class=\"origin_image zh-lightbox-thumb\" width=\"1227\" data-original=\"https://pic3.zhimg.com/v2-1d6e351cfcc9f68c163a53d80790e846_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1227&#39; height=&#39;261&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1227\" data-rawheight=\"261\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1227\" data-original=\"https://pic3.zhimg.com/v2-1d6e351cfcc9f68c163a53d80790e846_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1d6e351cfcc9f68c163a53d80790e846_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图是HRNet简单地示意图，生成多种不同分辨率的特征。这里需要注意的细节是，它在网络的前，中，后三段都做了特征融合，而不是仅仅在最后的特征图上做融合。别的好像也没什么了，结构和思路都比较简单，没有[前面的RefineNet](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230041-%25E7%2594%25A8RefineNet%25E5%2581%259A%25E5%2588%2586%25E5%2589%25B2/D%25230041.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">https://github.com/Captain1986/CaptainBlackboard/blob/master/D%230041-用RefineNet做分割/D%230041.md</a>)那么复杂，就不多做介绍了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 多分辨率融合Multi-resolution Fusion</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4e34fa5460b9d73e3abc8d710c988182_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1229\" data-rawheight=\"369\" class=\"origin_image zh-lightbox-thumb\" width=\"1229\" data-original=\"https://pic3.zhimg.com/v2-4e34fa5460b9d73e3abc8d710c988182_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1229&#39; height=&#39;369&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1229\" data-rawheight=\"369\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1229\" data-original=\"https://pic3.zhimg.com/v2-4e34fa5460b9d73e3abc8d710c988182_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4e34fa5460b9d73e3abc8d710c988182_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>HRNet作为主干网络提取了特征，这些特征有不同的分辨率，需要根据不同的任务来选择融合的方式。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在HRNet的最初CVPR做人体姿态检测的版本中，用的是上图a的融合方式，也就是丢掉低分辨率的特征，只用最大分辨率的特征。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如果做语义分割或者人脸特征点定位，那么就是如上图b中所示，把不同分辨率的特征通过upsample操作后得到一致的大分辨率特征图，然后concate起来做融合。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如果做目标检测，那么如上图c所示，在b的基础上构造一个多分辨率的特征金字塔。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 计算量</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-68d081451498a502b257b410ded535e2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"235\" class=\"origin_image zh-lightbox-thumb\" width=\"594\" data-original=\"https://pic3.zhimg.com/v2-68d081451498a502b257b410ded535e2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;594&#39; height=&#39;235&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"235\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"594\" data-original=\"https://pic3.zhimg.com/v2-68d081451498a502b257b410ded535e2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-68d081451498a502b257b410ded535e2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>HRNet虽然有许多分辨率，而且一路都有保持大分辨率，但是根据论文的Table 1来看，貌似参数数量和计算量与前面的UNet和DeepLab相比并没有增加多少，特别是与DeepLab相比，计算量少了一大半。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>HRNet构造思路很简单，一路保持较大的分辨率，而且并行地下采样、融合，最终生成多个分辨率的特征图，可以根据不同任务的具体需要进行选择性地融合使用。HRNet能在图像分类，目标检测，语义分割，人脸特征点定位等应用上取得不错的效果，有望像VGG，GoogLeNet和ResNet那样，成为各个任务主干网络新的选择。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [Deep High-Resolution Representation Learning for Human Pose Estimation](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1902.09212\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deep High-Resolution Representation Learning for Human Pose Estimation</a>)</p><p>+ [High-Resolution Representations for Labeling Pixels and Regions](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1904.04514\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">High-Resolution Representations for Labeling Pixels and Regions</a>)</p><p>+ [D#0039-用FCN做分割](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230039-%25E7%2594%25A8FCN%25E5%2581%259A%25E5%2588%2586%25E5%2589%25B2/D%25230039.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p>+ [D#0040-用U-Net做分割](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230040-%25E7%2594%25A8U-Net%25E5%2581%259A%25E5%2588%2586%25E5%2589%25B2/D%25230040.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">https://github.com/Captain1986/CaptainBlackboard/blob/master/D%230040-用U-Net做分割/D%230040.md</a>)</p><p>+ [RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1611.06612\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation</a>)</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "图像分割", 
                    "tagLink": "https://api.zhihu.com/topics/20137632"
                }, 
                {
                    "tag": "目标检测", 
                    "tagLink": "https://api.zhihu.com/topics/19596960"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/85423896", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 2, 
            "title": "用DeepLabv3+的Encoder-Decoder做分割", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>这里继续介绍第四篇著名的图像分割模型，DeepLabv3+。论文发表于2018年的ECCV上。DeepLabv3+在DepLabv3的基础上加了一个精细的Decoder模块得到了一个**Encoder-Decoder**的分割模型，以快速的**Xception为主干网络**，还吸收了MobileNet的**深度可分离卷积**进一步加速，最终在PASCAL VOC 2012和Cityspace两个benchmark上分别得到了89.0%和82.1%的mIOU成绩。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 分割的常用特征提取思路</p><p class=\"ztext-empty-paragraph\"><br/></p><p>图像分割，其实就是在整图大小的分辨率上做每个像素的分类，实际上属于一个**稠密分类问题**，分类的是每一个像素。既然要做到每个像素这么精细的级别，那么就需要很精细的浅层特征，既然要做分类，那么就要有抽象的上层特征。所以，基本的分割方法都是在考虑**怎么样提取浅层和深层的特征**，和**怎么样把这两种特征联合利用**。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f26da9fd1172b8eb33dc2748e4d6137f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1042\" data-rawheight=\"584\" class=\"origin_image zh-lightbox-thumb\" width=\"1042\" data-original=\"https://pic4.zhimg.com/v2-f26da9fd1172b8eb33dc2748e4d6137f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1042&#39; height=&#39;584&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1042\" data-rawheight=\"584\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1042\" data-original=\"https://pic4.zhimg.com/v2-f26da9fd1172b8eb33dc2748e4d6137f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f26da9fd1172b8eb33dc2748e4d6137f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>### 用SPP来提取特征</p><p class=\"ztext-empty-paragraph\"><br/></p><p>一般利用不同大小的卷积核或者Pooling在主干网的最后一层来得到不同分辨率的特征图，形成一个空间特征金字塔SPP，也就是如上图所示的a，然后再这个SPP上恢复出每个像素的label的预测值。但是这个思路有个明显的弊病，他是在主干网的最后一层来做SPP操作的，而**主干网最后一层虽然有很丰富的语义信息，但是由于一路上的卷积和Pooling操作，分辨率不断压缩，许多对分割细节至关重要的浅层细节信息还是流失了**。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-fbd0e177e5988895f849b2b5de2debc9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1242\" data-rawheight=\"408\" class=\"origin_image zh-lightbox-thumb\" width=\"1242\" data-original=\"https://pic2.zhimg.com/v2-fbd0e177e5988895f849b2b5de2debc9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1242&#39; height=&#39;408&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1242\" data-rawheight=\"408\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1242\" data-original=\"https://pic2.zhimg.com/v2-fbd0e177e5988895f849b2b5de2debc9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-fbd0e177e5988895f849b2b5de2debc9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>本文介绍的DeepLabv3+前的DeepLabv3（也叫ASPP）就是这么一个思路，如上图所示，只不过在做SPP这一步的时候把普通卷积推广到了空洞卷积。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 用空洞卷积来提取特征</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-573eac81afc193e1c5bca448c1c6bb4a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"457\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb\" width=\"457\" data-original=\"https://pic3.zhimg.com/v2-573eac81afc193e1c5bca448c1c6bb4a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;457&#39; height=&#39;124&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"457\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"457\" data-original=\"https://pic3.zhimg.com/v2-573eac81afc193e1c5bca448c1c6bb4a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-573eac81afc193e1c5bca448c1c6bb4a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>尽管还可以用如上图所示的空洞卷积来做，这样就可以一路保持比较大的特征图的分辨率，保持丰富的细节信息，但是这么做，又太消耗GPU显存资源了，也不太好。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 用Encoder的方式来提取特征，Decoder再来解码</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-09ece6f61c5aa63ec9653c7f7309deb0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"632\" data-rawheight=\"383\" class=\"origin_image zh-lightbox-thumb\" width=\"632\" data-original=\"https://pic1.zhimg.com/v2-09ece6f61c5aa63ec9653c7f7309deb0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;632&#39; height=&#39;383&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"632\" data-rawheight=\"383\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"632\" data-original=\"https://pic1.zhimg.com/v2-09ece6f61c5aa63ec9653c7f7309deb0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-09ece6f61c5aa63ec9653c7f7309deb0_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8693d3ff9e26b6b49353eba09f66283a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1038\" data-rawheight=\"838\" class=\"origin_image zh-lightbox-thumb\" width=\"1038\" data-original=\"https://pic3.zhimg.com/v2-8693d3ff9e26b6b49353eba09f66283a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1038&#39; height=&#39;838&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1038\" data-rawheight=\"838\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1038\" data-original=\"https://pic3.zhimg.com/v2-8693d3ff9e26b6b49353eba09f66283a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8693d3ff9e26b6b49353eba09f66283a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>还有第三种方式，就是本文第一幅图中的b，用一个Encoder模块抽取特征，网络的特征图是逐渐变小的，这样就节约了内存，而在网络的不同深度有不同抽象程度的特征，然后再用一个Decoder模块来利用这些不同层的特征去解码出一个全图大小的pixel-label map。以前介绍的[FCN](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230039-%25E7%2594%25A8FCN%25E5%2581%259A%25E5%2588%2586%25E5%2589%25B2/D%25230039.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)和[U-Net](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230040-%25E7%2594%25A8U-Net%25E5%2581%259A%25E5%2588%2586%25E5%2589%25B2/D%25230040.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)基本上就是属于这么一个路子。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## DeepLabv3+的Encoder-Decoder网络结构</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 融合SPP，空洞卷积和Encoder-Decoder结构来得到DeepLabv3+</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-23137a4c6d63ee04a08ddd4f0a7e9d2d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1039\" data-rawheight=\"682\" class=\"origin_image zh-lightbox-thumb\" width=\"1039\" data-original=\"https://pic2.zhimg.com/v2-23137a4c6d63ee04a08ddd4f0a7e9d2d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1039&#39; height=&#39;682&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1039\" data-rawheight=\"682\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1039\" data-original=\"https://pic2.zhimg.com/v2-23137a4c6d63ee04a08ddd4f0a7e9d2d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-23137a4c6d63ee04a08ddd4f0a7e9d2d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>本文提出的DeepLabv3+，综合吸收了上面几种思路的有点，主体设计如第一幅图所示，细节设计如上图所示，它主体上是一个**Encoder-Decoder结构**：Encoder部分，用主干网DCNN（可以使Xception，也可以是VGG或者ResNet等）提取基本特征，再用**空洞卷积**提取不同感受野的特征图，最后用1x1的卷积混合它们。Decoder部分，抽取主干网前面的特征（**这里是细节信息**），然后对Encoder混合出来的小分辨率特征（**这里是抽象特征**）进行上采样，Concate的方式混合两者，再经过3x3的卷积和上采样之后回复出输入图分辨率的pixel-wise预测结果。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这么一路分析下来，思路就很明显了，**就是在DeepLabv3（ASPP结合了空洞卷积和SPP）的基础上连一个比较复杂的Decoder模块改造成一个Encoder-Decoder结构**，用主干网中间的细节信息，和更多的非线性来解码出来预测图。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文比较简介的介绍常用于分割网络设计的特征提取思路，包括，多分辨率特征，SPP，空洞卷积和Encoder-Decoder结构，基本上所有的分割网络、甚至目标检测和别的任务的网络，都大量的采用了这些设计思路。DeepLabv3+是结合了多种设计思路的模型，也取得了不错的结果，值得我们去学习和借鉴。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [《D#0039-用FCN做分割》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230039-%25E7%2594%25A8FCN%25E5%2581%259A%25E5%2588%2586%25E5%2589%25B2/D%25230039.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [《D#0040-用U-Net做分割》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230040-%25E7%2594%25A8U-Net%25E5%2581%259A%25E5%2588%2586%25E5%2589%25B2/D%25230040.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">https://github.com/Captain1986/CaptainBlackboard/blob/master/D%230040-用U-Net做分割/D%230040.md</a>)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [《D#0041-用RefineNet做分割》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230041-%25E7%2594%25A8RefineNet%25E5%2581%259A%25E5%2588%2586%25E5%2589%25B2/D%25230041.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">https://github.com/Captain1986/CaptainBlackboard/blob/master/D%230041-用RefineNet做分割/D%230041.md</a>)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1802.02611\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</a>)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1611.06612\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation</a>)</p>", 
            "topic": [
                {
                    "tag": "图像分割", 
                    "tagLink": "https://api.zhihu.com/topics/20137632"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "图像识别", 
                    "tagLink": "https://api.zhihu.com/topics/19588774"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/84584467", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 0, 
            "title": "用RefineNet做分割", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p>这里继续介绍第三篇著名的图像分割模型，RefineNet。论文发表于2017年的CVPR上。RefineNet提出了一种通用的多路级联精修结构的网络，与FCN相比，它更充分利用了主干网不同层次分辨率的特征图里面的信息，与Deeplab的Dilated Convolution方法相比，它要求更少的内存。最重要的是，它的效果很好，在七个benchmark上都做出了更好的效果，属于当年的SOTA。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## RefineNet网络结构</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-39dcce06091f999b1ebeba39ebc7265b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1485\" data-rawheight=\"536\" class=\"origin_image zh-lightbox-thumb\" width=\"1485\" data-original=\"https://pic4.zhimg.com/v2-39dcce06091f999b1ebeba39ebc7265b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1485&#39; height=&#39;536&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1485\" data-rawheight=\"536\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1485\" data-original=\"https://pic4.zhimg.com/v2-39dcce06091f999b1ebeba39ebc7265b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-39dcce06091f999b1ebeba39ebc7265b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图基本上表示了RefineNet的思路起源和它比之前方法的优点。如左上角的a所示是一个基于ResNet的全卷积分割模型，但是它只是利用了最后一层的低分辨率的特征图，很多空间信息都在不断地卷积stride和pooling层中丢失了，很难恢复出高精度的Segmentation map；左下的b模型，是Deeplab利用Dilated Convolution的方法来做特征提取，Dilated Convolution优点在于可以在不增大计算量和参数量的情况下保持比较大的感受野，也可以保证网络中每层特征图的分辨率不至于太小，但是缺点也是很显然的，由于中间结果都是分辨率很大的特征图，那么训练和推理过程中都对内存/显存提出了很大的要求。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>而右边表示的是RefineNet的示意图，**主干网络是ResNet**，但是在ResNet的4个不同分辨率阶段都会抽出来给一个RefineNet块做处理，而且也有identity mapping的连接，即丰富了不同分辨率的特征混合（和U-Net类似），也让大网络的训练更加容易。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>另外值得注意的是，这个网络的**Cascade Multi-Path Refinement**。从ResNet出来的低分辨率特征，不断的结合上一个Stage的较高分辨率的特征，进行混合；而且这是一个级联(Cascade)的过程，从而不断不断地提升分割精度。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## RefineNet块结构</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c86cd32388ca1cddea03e05526de7015_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1135\" data-rawheight=\"750\" class=\"origin_image zh-lightbox-thumb\" width=\"1135\" data-original=\"https://pic2.zhimg.com/v2-c86cd32388ca1cddea03e05526de7015_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1135&#39; height=&#39;750&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1135\" data-rawheight=\"750\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1135\" data-original=\"https://pic2.zhimg.com/v2-c86cd32388ca1cddea03e05526de7015_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c86cd32388ca1cddea03e05526de7015_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图所示就是RefineNet网络中基础的RefineNet块的结构，RefineNet结构是输入一个或者多个不同分辨率的特征图，进行混合和提升，输出一个较大特征图的块。它由三个级联的子块组成，下面分别介绍。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 残差卷积单元Residual Conv Unit</p><p class=\"ztext-empty-paragraph\"><br/></p><p>每个input path的后面都会接两个串联的RCU。这个RCU块的作用就是**fine-tune主干网ResNet的输出**使他更适应分割这个任务。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 多分辨率融合Multi-resolution Fusion</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在RCU输出的特征图每个path的空间分辨率是不一样的，这个多分辨率融合块的作用就是把输入的各种**不同分辨率的特征提升并对齐**到最大的输入path的分辨率，然后将它们通过Sum操作融合起来。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 链式残差池化Chained Residual Pooling</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这个层的作用是通过pooling操作让不同的特征图有不同的感受野以便于**提取不同尺度的背景上下文**的信息。用不同的残差连接一个作用是便于训练，第二个作用是混合复用不同分辨率的特征。每个pooling层后面添加的卷积层的作用相当于在sum操作前学习一个**自适应的权重**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 输出卷积层Output Conv</p><p class=\"ztext-empty-paragraph\"><br/></p><p>RefineNet块最后的输出层其实就是一个前面介绍的Residual Conv Unit（这样每个RefineNet有三个RCN，两个在最前面，一个在最后，RCU进，RCU出）。这一层的作用就是给前面层输出的特征**增加一些非线性**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## RefineNet网络中的恒等映射Identity Mapping</p><p class=\"ztext-empty-paragraph\"><br/></p><p>受到ResNet的启发，在RefineNet的设计中，作者大量使用了Identity Maping这种结构。ResNet的shortcut连接形成一条干净的通道使信息的流通更加顺畅，而在主路上，添加了非线性来学习有效特征。这种结构使很深的网络也可以很好的训练出来。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在RefineNet中，有两种Identity Mapping，Long-term的和Short-term的。在RefineNet块的RCU和CRP里面的是Short-term的，在各个RefineNet和主干网ResNet各个Stage输出之间的是Long-term的Identity Mapping。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验结果</p><p class=\"ztext-empty-paragraph\"><br/></p><p>RefineNet不但可以用在语义分割任务上，也可以用在Object Parsing的任务上，而且都取得了不错的效果。下面的三幅图分别是在Person-Part 数据集上做Object Parsing和在VOC2012，Cityspace数据集上做语义分割的效果。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-efc9675bdae999fcd145ace818ac3677_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"489\" data-rawheight=\"859\" class=\"origin_image zh-lightbox-thumb\" width=\"489\" data-original=\"https://pic4.zhimg.com/v2-efc9675bdae999fcd145ace818ac3677_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;489&#39; height=&#39;859&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"489\" data-rawheight=\"859\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"489\" data-original=\"https://pic4.zhimg.com/v2-efc9675bdae999fcd145ace818ac3677_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-efc9675bdae999fcd145ace818ac3677_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-43ab8429d288afa38f7329a2fffeec93_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"509\" data-rawheight=\"901\" class=\"origin_image zh-lightbox-thumb\" width=\"509\" data-original=\"https://pic4.zhimg.com/v2-43ab8429d288afa38f7329a2fffeec93_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;509&#39; height=&#39;901&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"509\" data-rawheight=\"901\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"509\" data-original=\"https://pic4.zhimg.com/v2-43ab8429d288afa38f7329a2fffeec93_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-43ab8429d288afa38f7329a2fffeec93_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-07c40b85fe371760e5f7dce8d7cd9329_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"536\" data-rawheight=\"635\" class=\"origin_image zh-lightbox-thumb\" width=\"536\" data-original=\"https://pic2.zhimg.com/v2-07c40b85fe371760e5f7dce8d7cd9329_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;536&#39; height=&#39;635&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"536\" data-rawheight=\"635\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"536\" data-original=\"https://pic2.zhimg.com/v2-07c40b85fe371760e5f7dce8d7cd9329_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-07c40b85fe371760e5f7dce8d7cd9329_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>## Cascade RefineNet网络的变种</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d86b17776bc7675f825994b4c7633dc5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1144\" data-rawheight=\"895\" class=\"origin_image zh-lightbox-thumb\" width=\"1144\" data-original=\"https://pic2.zhimg.com/v2-d86b17776bc7675f825994b4c7633dc5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1144&#39; height=&#39;895&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1144\" data-rawheight=\"895\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1144\" data-original=\"https://pic2.zhimg.com/v2-d86b17776bc7675f825994b4c7633dc5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d86b17776bc7675f825994b4c7633dc5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>RefineNet网络做很少的修改就可以变化到不同的结构（论文主要介绍的是4个Stage的RefineNet），比如如上图，把网络中的RefineNet块的个数修改一下就得到了变种a和b，把输入图片的个数和分辨率修改一下就可以得到变种c。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>RefineNet采用多路，多分辨率，Cascade Refine和广泛使用残差结构的网络做语义分割任务，取得了很好的效果。其提出的RefineNet块，也可以以一个基础块的方式嵌入到别的网络中去。另外，RefineNet这个网络还可以做很多不同的泛化和拓展。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [《D#0025-CNN中使用卷积代替全连接》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230025-CNN%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8%25E5%258D%25B7%25E7%25A7%25AF%25E4%25BB%25A3%25E6%259B%25BF%25E5%2585%25A8%25E8%25BF%259E%25E6%258E%25A5/D%25230025.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p>+ [RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1611.06612\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation</a>)</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "图像分割", 
                    "tagLink": "https://api.zhihu.com/topics/20137632"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/84541921", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 2, 
            "title": "用U-Net做分割", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>这里继续介绍第二篇著名的图像分割模型，U-Net。论文由德国弗莱堡大学的研究人员发表于15年MICCAI，初始是应用于医学图像分割上的。论文思路清晰，和[《D#0039-用FCN做分割》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230039-%25E7%2594%25A8FCN%25E5%2581%259A%25E5%2588%2586%25E5%2589%25B2/D%25230039.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)一样也是一个**全卷积网络**，网络结构简单，而且U-Net具有速度快，需要的**训练样本少**的优势。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 网络结构</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8693d3ff9e26b6b49353eba09f66283a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1038\" data-rawheight=\"838\" class=\"origin_image zh-lightbox-thumb\" width=\"1038\" data-original=\"https://pic3.zhimg.com/v2-8693d3ff9e26b6b49353eba09f66283a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1038&#39; height=&#39;838&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1038\" data-rawheight=\"838\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1038\" data-original=\"https://pic3.zhimg.com/v2-8693d3ff9e26b6b49353eba09f66283a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8693d3ff9e26b6b49353eba09f66283a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图所示，U-Net的结构非常清晰和**U形对称**，整体呈一种U型结构。在左半边，是它的编码结构，论文中成为**收缩路径**(contracting path)，分辨率逐渐由输入的572x572减小到最小的28x28；右边是**对称的扩张路径**(expanding path)，分辨率由28x28逐渐扩张到半个输入分辨率的大小388x388。只做灰度图上的细胞轮廓分割，所以输入是1通道，输出是2通道的，由1x1的卷积产生。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>另外还可以看到，在扩张路径的过程中，也crop了前面收缩路径产生的特征图进行融合，形成了一种不同感受野特征的融合，也是希望结合局部的精细的信息和整体的分类的信息以求更好的分割效果，比较常见的手段。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 外围边界处理策略</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1deaf20569be942cdeff27163fe2ab3b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1052\" data-rawheight=\"516\" class=\"origin_image zh-lightbox-thumb\" width=\"1052\" data-original=\"https://pic4.zhimg.com/v2-1deaf20569be942cdeff27163fe2ab3b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1052&#39; height=&#39;516&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1052\" data-rawheight=\"516\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1052\" data-original=\"https://pic4.zhimg.com/v2-1deaf20569be942cdeff27163fe2ab3b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1deaf20569be942cdeff27163fe2ab3b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>因为是全卷积模型，所以理论上可以输入任意分辨率的图片。但是，在边缘处的像素，由于缺少部分（至少缺少上、下、左、右其中之一）上下文信息，得到的Segmentation map会不准确。为了解决这个问题，如上图所示，作者简单地对边缘处的像素做了个镜像弥补边缘的上下文信息，以让输出的Segmentation map的每个像素反算到输入都有有效像素点（而不是null）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 训练方法</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 损失函数</p><p class=\"ztext-empty-paragraph\"><br/></p><p>作者使用的是pixel-wise Softmax</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f6bc3b305069cba4fdefba49266fd793_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"553\" data-rawheight=\"56\" class=\"origin_image zh-lightbox-thumb\" width=\"553\" data-original=\"https://pic4.zhimg.com/v2-f6bc3b305069cba4fdefba49266fd793_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;553&#39; height=&#39;56&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"553\" data-rawheight=\"56\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"553\" data-original=\"https://pic4.zhimg.com/v2-f6bc3b305069cba4fdefba49266fd793_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f6bc3b305069cba4fdefba49266fd793_b.png\"/></figure><p>输出最后的Segmentation map。损失函数用的是交叉熵</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b7cf6cc3be4a327071ea1ca2990181cd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"354\" data-rawheight=\"83\" class=\"content_image\" width=\"354\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;354&#39; height=&#39;83&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"354\" data-rawheight=\"83\" class=\"content_image lazy\" width=\"354\" data-actualsrc=\"https://pic2.zhimg.com/v2-b7cf6cc3be4a327071ea1ca2990181cd_b.jpg\"/></figure><p>。不过，为了让各个相接触的细胞与细胞之间的小边缘得到更好的分割效果，在预先算出了一个weight map</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-bd9791937b7c23a16df0ad22acce6fd1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"619\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb\" width=\"619\" data-original=\"https://pic2.zhimg.com/v2-bd9791937b7c23a16df0ad22acce6fd1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;619&#39; height=&#39;108&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"619\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"619\" data-original=\"https://pic2.zhimg.com/v2-bd9791937b7c23a16df0ad22acce6fd1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-bd9791937b7c23a16df0ad22acce6fd1_b.jpg\"/></figure><p>对每个pixel的损失进行加权。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 初始化</p><p class=\"ztext-empty-paragraph\"><br/></p><p>对于有许多卷积层而且不同path的模型，好的参数初始化策略对于模型的优化是不可轻视的。在最佳情况下，每个层的参数应该初始化到输出的特征图分布拥有**单位方差**最好。对于U-Net这样的只有卷积层和ReLU激活层的模型，用**He初始化**根据输入节点的个数做初始化可以达到上面最佳情况的要求。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 数据增强</p><p class=\"ztext-empty-paragraph\"><br/></p><p>医疗数据的图像很少，作者用的原始带标签的训练数据集分别只有20张，30张和35张。为了生成更多的训练数据，作者使用随机弹性形变Random elastic deformations进行了数据增强。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验结果</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5f8f4753e371e5b231408a43f461b8a0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1040\" data-rawheight=\"390\" class=\"origin_image zh-lightbox-thumb\" width=\"1040\" data-original=\"https://pic1.zhimg.com/v2-5f8f4753e371e5b231408a43f461b8a0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1040&#39; height=&#39;390&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1040\" data-rawheight=\"390\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1040\" data-original=\"https://pic1.zhimg.com/v2-5f8f4753e371e5b231408a43f461b8a0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-5f8f4753e371e5b231408a43f461b8a0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图是部分实验结果的展示，带颜色的部分是Segmentation map，黄色细线是真值，可以看出来效果还是不错的，有些很困难的微弱的边缘，都切割得八九不离十。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文介绍的是分割早期比较有名的U-Net模型，模型结构简单，思路清晰，**编码解码结构**划分，加上不同层相似分辨率的特征图融合，都提升了分割的性能。虽说论文中主要介绍了医学图像的分割，但是U-Net也很容易迁移到别的任务的分割上。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [《D#0025-CNN中使用卷积代替全连接》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230025-CNN%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8%25E5%258D%25B7%25E7%25A7%25AF%25E4%25BB%25A3%25E6%259B%25BF%25E5%2585%25A8%25E8%25BF%259E%25E6%258E%25A5/D%25230025.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p>+ [《D#0039-用FCN做分割》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230039-%25E7%2594%25A8FCN%25E5%2581%259A%25E5%2588%2586%25E5%2589%25B2/D%25230039.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p>+ [Fully Convolutional Networks for Semantic Segmentation](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1411.4038.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1411.4038</span><span class=\"invisible\">.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [U-Net: Convolutional Networks for Biomedical Image Segmentation](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1505.04597\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">U-Net: Convolutional Networks for Biomedical Image Segmentation</a>)</p><p>+ [U-Net Implementation](<a href=\"https://link.zhihu.com/?target=https%3A//lmb.informatik.uni-freiburg.de/people/ronneber/u-net/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">lmb.informatik.uni-freiburg.de</span><span class=\"invisible\">/people/ronneber/u-net/</span><span class=\"ellipsis\"></span></a>)</p><p>+ [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1502.01852\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a>)</p>", 
            "topic": [
                {
                    "tag": "图像分割", 
                    "tagLink": "https://api.zhihu.com/topics/20137632"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/84256121", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 0, 
            "title": "用FCN做分割", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p>图像分割是计算机视觉中比较常见的技术，广泛应用于智能交通、自动驾驶等领域。恰好船长最近正在做图像分割的项目，也调研了一些图像分割的经典方法，准备把用CNN做图像分割的方法都做个记录，这里是第一篇FCN，原始论文发表于15年的CVPR，属于用深度学习做图像分割的挖坑之作（褒义）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 语义分割：从整体图片分类到像素级分类</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在FCN提出之前，CNN（AlexNet，VGG，GoogLeNet，RCNN和SPPNet）都已经在图像分类和目标检测领域攻城拔寨，既然CNN可以在整图分类和部分图片分类上取得成功，那么把CNN来做pixel-to-pixel的分类进而解决图像分割问题也就是可以想象的了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>以AlexNet为例，它的开始的几层是卷积层，最后卷积层后面连接全连接Softmax层输出一个长度为1000的向量代表分类的类别概率。它的最后输出是针对整个图的分类结果，此时空间信息消失了。如果在去掉全连接层，换上卷积层，那么一样也可以输出分类的结果，还保存了空间维度上的信息，示意图如下图所示：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c70d2176d085f674f80ae3a88e790c8a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"631\" data-rawheight=\"459\" class=\"origin_image zh-lightbox-thumb\" width=\"631\" data-original=\"https://pic3.zhimg.com/v2-c70d2176d085f674f80ae3a88e790c8a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;631&#39; height=&#39;459&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"631\" data-rawheight=\"459\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"631\" data-original=\"https://pic3.zhimg.com/v2-c70d2176d085f674f80ae3a88e790c8a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c70d2176d085f674f80ae3a88e790c8a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这里就不再赘述，在[《D#0025-CNN中使用卷积代替全连接》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230025-CNN%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8%25E5%258D%25B7%25E7%25A7%25AF%25E4%25BB%25A3%25E6%259B%25BF%25E5%2585%25A8%25E8%25BF%259E%25E6%258E%25A5/D%25230025.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)已有比较详细的介绍，感兴趣的朋友可以去看。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>然后，既然FCN可以生成整张图大小的热力图，那么如果这个图的大小和输入图像大小一致，且有了输入图中每个像素的真值标签(Ground Truth)，那么就可以用这个标签来监督FCN的训练，让热力图和标签趋向一致，原理如下图所示。有了每个像素的分类信息，那么做分割就是个太简单的事情了。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-09ece6f61c5aa63ec9653c7f7309deb0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"632\" data-rawheight=\"383\" class=\"origin_image zh-lightbox-thumb\" width=\"632\" data-original=\"https://pic1.zhimg.com/v2-09ece6f61c5aa63ec9653c7f7309deb0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;632&#39; height=&#39;383&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"632\" data-rawheight=\"383\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"632\" data-original=\"https://pic1.zhimg.com/v2-09ece6f61c5aa63ec9653c7f7309deb0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-09ece6f61c5aa63ec9653c7f7309deb0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>## FCN做分割的网络架构</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 基础网络</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-643dd87445c9b43c4bc89538aa315d79_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"707\" data-rawheight=\"576\" class=\"origin_image zh-lightbox-thumb\" width=\"707\" data-original=\"https://pic2.zhimg.com/v2-643dd87445c9b43c4bc89538aa315d79_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;707&#39; height=&#39;576&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"707\" data-rawheight=\"576\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"707\" data-original=\"https://pic2.zhimg.com/v2-643dd87445c9b43c4bc89538aa315d79_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-643dd87445c9b43c4bc89538aa315d79_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>作者试用了AlexNet，VGG-16，VGG-19和GoogLeNet等在分类任务上表现较好的网络，首先把它们的Softmax砍掉，第二步把全连接层改成卷积层，第三步附加一个1x1的卷积层生成21个PASCAL分类的热力图，最后添加一个双线性采样的upsample层作为Deconvolution层来把热力图扩充到输入图像的分辨率。在做了这些改造之后，如上图所示，实验发现复用了VGG-16的卷积部分的效果最好，VGG-16可以作为特征提取的主干网络。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Skip连接Combining what and where</p><p class=\"ztext-empty-paragraph\"><br/></p><p>像VGG这样的网络是一层一层的层次性结构，不同的层感受野不同，特征图所能表达的含义也不同。浅层的特征感受野小，可以表达一些精细的特征，能回答**在哪里**的问题；而深层的特征感受野大，适合表达一些整体的语义，适合回答**是什么**的问题。如果把不同层次的特征图融合起来，整个网络形成一个有向无环图DAG，那么应该适合于分割这类既要回答是什么，又要（精确到像素级地）回答在哪里的问题。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3c20c03e58816bd6d3fb2393386bf3b3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1484\" data-rawheight=\"542\" class=\"origin_image zh-lightbox-thumb\" width=\"1484\" data-original=\"https://pic4.zhimg.com/v2-3c20c03e58816bd6d3fb2393386bf3b3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1484&#39; height=&#39;542&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1484\" data-rawheight=\"542\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1484\" data-original=\"https://pic4.zhimg.com/v2-3c20c03e58816bd6d3fb2393386bf3b3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3c20c03e58816bd6d3fb2393386bf3b3_b.jpg\"/></figure><p>上图是FCN的网络结构图，VGG不同层的特征在upsample之后会进行(sum或者concate)融合，最终融合到pool3这一层，得出来一个和原图大小一样的图。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f9d1c35b544a6c4725f84b1721285b0f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"737\" data-rawheight=\"449\" class=\"origin_image zh-lightbox-thumb\" width=\"737\" data-original=\"https://pic4.zhimg.com/v2-f9d1c35b544a6c4725f84b1721285b0f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;737&#39; height=&#39;449&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"737\" data-rawheight=\"449\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"737\" data-original=\"https://pic4.zhimg.com/v2-f9d1c35b544a6c4725f84b1721285b0f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f9d1c35b544a6c4725f84b1721285b0f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图可以看出，不做不同层特征图的融合会怎么样。可以看到，**融合的浅层特征越多，分割得越精细**。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-57c41a6b37d3fc50a2d8af6a0d3afcdd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"715\" data-rawheight=\"358\" class=\"origin_image zh-lightbox-thumb\" width=\"715\" data-original=\"https://pic2.zhimg.com/v2-57c41a6b37d3fc50a2d8af6a0d3afcdd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;715&#39; height=&#39;358&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"715\" data-rawheight=\"358\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"715\" data-original=\"https://pic2.zhimg.com/v2-57c41a6b37d3fc50a2d8af6a0d3afcdd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-57c41a6b37d3fc50a2d8af6a0d3afcdd_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上表是作者给出的实验数据，具体地证明了上上图的结论。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### UpSample</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在类似VGG这样的分类网络中，因为卷积Stride和Pooling层的作用，特征图的空间分辨率是随着层数越来越深递进地越来越小的，但是分割要做pixel-to-pixel的分类任务，真值标签是原图分辨率地像素级的图，那么分割网络的输出也要是同样的分辨率，即输入图片大小的分辨率。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>FCN作者这里为了放大特征图分辨率采用了很简单地x2 Upsample层，具体来说就是添加了一个**类似于双线性插值**的层，来根据位置坐标计算输出图的像素级标签。当然，这里说是类似于双线性插值的层，是因为这一层它的插值系数是可以学习的，不是定死了的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文介绍了一种早期较为经典的分割网络FCN，可端对端地进行全图训练。该网络特征提取部分还是基于VGG-16，改造成全卷积网络。添加了Skip连接融合浅层细节和深层语义特征，采用了可学习的UpSample层使网络输出分辨率和输入一致。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [《D#0025-CNN中使用卷积代替全连接》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230025-CNN%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8%25E5%258D%25B7%25E7%25A7%25AF%25E4%25BB%25A3%25E6%259B%25BF%25E5%2585%25A8%25E8%25BF%259E%25E6%258E%25A5/D%25230025.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p>+ [Fully Convolutional Networks for Semantic Segmentation](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1411.4038.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1411.4038</span><span class=\"invisible\">.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [Fully Convolutional Networks for Semantic Segmentation](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1605.06211.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1605.0621</span><span class=\"invisible\">1.pdf</span><span class=\"ellipsis\"></span></a>)</p>", 
            "topic": [
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }, 
                {
                    "tag": "图像分割", 
                    "tagLink": "https://api.zhihu.com/topics/20137632"
                }, 
                {
                    "tag": "卷积", 
                    "tagLink": "https://api.zhihu.com/topics/19678959"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/84063499", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 2, 
            "title": "Multi-View Active Learning做视频推荐", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>今天介绍的是阿里优酷Cognitive and Intelligent Lab做的用Multi-View Active Learning做视频推荐的方法，[论文](<a href=\"https://link.zhihu.com/?target=https%3A//www.ijcai.org/proceedings/2019/0284.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">ijcai.org/proceedings/2</span><span class=\"invisible\">019/0284.pdf</span><span class=\"ellipsis\"></span></a>)发表于2019年的IJCAI上。与[D#0035](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230035-2KW%25E7%259F%25AD%25E8%25A7%2586%25E9%25A2%2591%25E6%2589%2593%25E6%25A0%2587%25E9%2597%25AE%25E9%25A2%2598%25E4%25B9%258BActivate-Learning/D%25230035.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)和[D#0036](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230036-2KW%25E7%259F%25AD%25E8%25A7%2586%25E9%25A2%2591%25E6%2589%2593%25E6%25A0%2587%25E9%2597%25AE%25E9%25A2%2598%25E4%25B9%258BMulti-Modal-Machine-Learning/D%25230036.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)做海量视频打标不一样，这里是把主动学习和多模态(转换)的方法运用到了**视频推荐**这个问题上。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 问题提出和整体思路</p><p class=\"ztext-empty-paragraph\"><br/></p><p>视频推荐系统要做的事情就是把你喜欢（或者可能喜欢）的视频喂给你。推荐规则的学习方法分成两种：1. 是从你以往视频浏览痕迹或者是2. 和你类似的人的浏览痕迹来学习到的。后者就是有名的[协同过滤](<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Collaborative_filtering\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">en.wikipedia.org/wiki/C</span><span class=\"invisible\">ollaborative_filtering</span><span class=\"ellipsis\"></span></a>)方法，前者是基于内容的方法。协同过滤方法在早期推荐系统中运用比较多，但是面临着**冷启动**问题，后面更多流行的是基于内容的推荐方法或者两者的混合方法了。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6ccee578ea61e019767aaa3611034913_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"728\" data-rawheight=\"489\" class=\"origin_image zh-lightbox-thumb\" width=\"728\" data-original=\"https://pic4.zhimg.com/v2-6ccee578ea61e019767aaa3611034913_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;728&#39; height=&#39;489&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"728\" data-rawheight=\"489\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"728\" data-original=\"https://pic4.zhimg.com/v2-6ccee578ea61e019767aaa3611034913_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6ccee578ea61e019767aaa3611034913_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>但是，基于内容的方法得要有丰富的内容才行。而带标签的数据都是很珍贵的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>比如，现在主流做基于内容的推荐方法，都是用和视频相伴文本信息（靠视频的信息会有semantic gap的问题，不如文本信息可靠）来做预测，预测用户喜欢还是不喜欢这个视频。但是这些文本信息不一定每个视频都有，特别是UGC视频。不过，视频信息是每个视频都有的，我们可以用深度学习方法理解这个视频，为这个视频生成相应的文本信息。有了这些视频生成的文本信息后，再用分类器去分类用户是不是喜欢，根据预测的结果和真值比对，根据比对的结果来挑选一些信息量大的未标记样本主动送给专家去做文本标记，如此迭代来提升推荐系统的性能。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>根据上面的描述，下面分别介绍这两个比较关键的点，一个是由视频生成文本信息怎么生成（即Video to Text映射），第二个是在学习到Video to Text映射之后，根据映射的结果怎么去主动选择一些未标记（文本信息）的样本去做手工标记。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## Video to Text映射</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cba8d84d0bab7f2ac3045d1bd5bc0fcc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1483\" data-rawheight=\"504\" class=\"origin_image zh-lightbox-thumb\" width=\"1483\" data-original=\"https://pic1.zhimg.com/v2-cba8d84d0bab7f2ac3045d1bd5bc0fcc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1483&#39; height=&#39;504&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1483\" data-rawheight=\"504\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1483\" data-original=\"https://pic1.zhimg.com/v2-cba8d84d0bab7f2ac3045d1bd5bc0fcc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-cba8d84d0bab7f2ac3045d1bd5bc0fcc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图是学习V2T映射的大框架，看起来有点复杂，其实思路很通顺，让我们来一个个拆解。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>首先我们要学习一个从视频到文本的映射，而有的样本已经有已知的文本信息了，那么最直观的想法就是**看V2T这个映射能不能把视频信息映射到已知的文本信息，或者它映射的差距有多大**。换句话说就是以文本信息监督V2T函数的学习，损失函数如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f53cabd03c1503254c7890c121859759_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"524\" data-rawheight=\"84\" class=\"origin_image zh-lightbox-thumb\" width=\"524\" data-original=\"https://pic2.zhimg.com/v2-f53cabd03c1503254c7890c121859759_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;524&#39; height=&#39;84&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"524\" data-rawheight=\"84\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"524\" data-original=\"https://pic2.zhimg.com/v2-f53cabd03c1503254c7890c121859759_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f53cabd03c1503254c7890c121859759_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>式子中e就是V2T映射函数，A是带标签的样本集合。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>另外，我们有用户行为的记录y（用户点了视频还是没有点视频），这个记录的信息每个样本都有。由视频文本信息v和用户自身信息u到用户点没点视频y之间的关系可以用函数f建模（就是图2中的classifier），假设f是可靠的，那么可以根据V2T映射出来的文本信息输入到f中，看f的预测，是不是和y的真值一致的。也就是，**加上y的信息来监督V2T函数的学习**。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-dc51be91977522d8c9ad3292f6b5e506_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"627\" data-rawheight=\"177\" class=\"origin_image zh-lightbox-thumb\" width=\"627\" data-original=\"https://pic3.zhimg.com/v2-dc51be91977522d8c9ad3292f6b5e506_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;627&#39; height=&#39;177&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"627\" data-rawheight=\"177\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"627\" data-original=\"https://pic3.zhimg.com/v2-dc51be91977522d8c9ad3292f6b5e506_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-dc51be91977522d8c9ad3292f6b5e506_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>式子(2)的前部分是在带完整文本信息的数据集上学习f，后部分是在不带文本信息的数据集上学习V2T，这是一个**联合学习**的过程。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>最后，综合式子(1)和(2)，得到最终联合学习e和f的损失函数</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3a9ba6f6f4906848eceeaf9e67213d29_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"650\" data-rawheight=\"283\" class=\"origin_image zh-lightbox-thumb\" width=\"650\" data-original=\"https://pic2.zhimg.com/v2-3a9ba6f6f4906848eceeaf9e67213d29_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;650&#39; height=&#39;283&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"650\" data-rawheight=\"283\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"650\" data-original=\"https://pic2.zhimg.com/v2-3a9ba6f6f4906848eceeaf9e67213d29_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3a9ba6f6f4906848eceeaf9e67213d29_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在作者的实现中，把V2T和f的学习都融合到一个神经网络中去优化，损失都用的是均方差。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 未标记样本的主动选择</p><p class=\"ztext-empty-paragraph\"><br/></p><p>由上面学习到的V2T和f，是不是事情就这么完了呢？不是的，学习到的V2T和f，毕竟是从很少的信息里面学到的，不一定可靠，为了增加V2T和f的性能，需要更多的带真值文本信息的样本，那么就涉及到**主动学习**，怎么挑选样本去给人工做标注了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>首先容易想到的是对在未标记集合U中的每个视频样本v，用V2T处理得到它的文本信息，再集合用户信息用f得到点击没点击的预测值，根据预测值和真值的偏离程度来选择是不是把v送去人工标注其文本信息，选择标准如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ba70b431ad66ecb47ae0dc26aca46720_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"611\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb\" width=\"611\" data-original=\"https://pic1.zhimg.com/v2-ba70b431ad66ecb47ae0dc26aca46720_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;611&#39; height=&#39;92&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"611\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"611\" data-original=\"https://pic1.zhimg.com/v2-ba70b431ad66ecb47ae0dc26aca46720_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ba70b431ad66ecb47ae0dc26aca46720_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>ni是在未标记集合中视频i出现的次数。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>再进一步，如果视频在总的记录中出现的次数多，送入做人工标记肯定对模型的性能帮助更大，那么，用它在总记录中出现的次数来做一个加权，也就是顺理成章的事情了：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-dd29d1b556cff5563f7b12ced6611d09_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"495\" data-rawheight=\"54\" class=\"origin_image zh-lightbox-thumb\" width=\"495\" data-original=\"https://pic2.zhimg.com/v2-dd29d1b556cff5563f7b12ced6611d09_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;495&#39; height=&#39;54&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"495\" data-rawheight=\"54\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"495\" data-original=\"https://pic2.zhimg.com/v2-dd29d1b556cff5563f7b12ced6611d09_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-dd29d1b556cff5563f7b12ced6611d09_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>再再进一步，搞得更复杂一点，考虑到不同长短的视频标记成本是不一样的，为了平衡标记成本和对模型性能帮助大小这两个因素，可以把视频长短当做一个因子去除Si得到考虑到标记成本的新的Si。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>最后，综合上面&#34;Video to Text映射&#34;和&#34;未标记样本的主动选择&#34;两节介绍的内容，Multi-View Active Learning的算法流程图如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-181500a6bf728634a1c3ccc5a8f50f5d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"741\" data-rawheight=\"689\" class=\"origin_image zh-lightbox-thumb\" width=\"741\" data-original=\"https://pic2.zhimg.com/v2-181500a6bf728634a1c3ccc5a8f50f5d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;741&#39; height=&#39;689&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"741\" data-rawheight=\"689\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"741\" data-original=\"https://pic2.zhimg.com/v2-181500a6bf728634a1c3ccc5a8f50f5d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-181500a6bf728634a1c3ccc5a8f50f5d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文介绍了一种综合利用了多目标学习，多模态学习和主动学习等技术来做视频推荐系统的一种方法。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [Multi-View Active Learning for Video Recommendation](<a href=\"https://link.zhihu.com/?target=https%3A//www.ijcai.org/proceedings/2019/0284.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">ijcai.org/proceedings/2</span><span class=\"invisible\">019/0284.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [D#0035-2KW短视频打标问题之Activate-Learning](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230035-2KW%25E7%259F%25AD%25E8%25A7%2586%25E9%25A2%2591%25E6%2589%2593%25E6%25A0%2587%25E9%2597%25AE%25E9%25A2%2598%25E4%25B9%258BActivate-Learning/D%25230035.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p>+ [D#0036-2KW短视频打标问题之Multi-Modal-Machine-Learning](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230036-2KW%25E7%259F%25AD%25E8%25A7%2586%25E9%25A2%2591%25E6%2589%2593%25E6%25A0%2587%25E9%2597%25AE%25E9%25A2%2598%25E4%25B9%258BMulti-Modal-Machine-Learning/D%25230036.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p>", 
            "topic": [
                {
                    "tag": "多模态学习", 
                    "tagLink": "https://api.zhihu.com/topics/20688199"
                }, 
                {
                    "tag": "视频推荐", 
                    "tagLink": "https://api.zhihu.com/topics/19607780"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/84063165", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 4, 
            "title": "CentralNet做多模态融合", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在[《D#0036》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230036-2KW%25E7%259F%25AD%25E8%25A7%2586%25E9%25A2%2591%25E6%2589%2593%25E6%25A0%2587%25E9%2597%25AE%25E9%25A2%2598%25E4%25B9%258BMulti-Modal-Machine-Learning/D%25230036.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)里面简单介绍了一下多模态机器学习的各个研究点，本文继续了解一下多模态融合的问题。[《D#0036》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230036-2KW%25E7%259F%25AD%25E8%25A7%2586%25E9%25A2%2591%25E6%2589%2593%25E6%25A0%2587%25E9%2597%25AE%25E9%25A2%2598%25E4%25B9%258BMulti-Modal-Machine-Learning/D%25230036.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)说了，多种模态的信息是研究怎么集成到一起来帮助我们做预测，而且有早期融合，晚期融合和混合融合几种大类。本文要介绍的是2018年ECCV上发表的一篇对CNN模型做混合融合的方法，[CentralNet](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1808.07275\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/abs/1808.0727</span><span class=\"invisible\">5</span><span class=\"ellipsis\"></span></a>)（注意不要和做检测的CenterNet混淆）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 最基础的多模态融合Concatenate</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ac5ee89bdedf1f28f5a206a29d50efc3_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"681\" data-rawheight=\"113\" class=\"origin_image zh-lightbox-thumb\" width=\"681\" data-original=\"https://pic4.zhimg.com/v2-ac5ee89bdedf1f28f5a206a29d50efc3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;681&#39; height=&#39;113&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"681\" data-rawheight=\"113\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"681\" data-original=\"https://pic4.zhimg.com/v2-ac5ee89bdedf1f28f5a206a29d50efc3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ac5ee89bdedf1f28f5a206a29d50efc3_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>顾名思义，Concate的方式就是将每种模态抽取出来的表达连接到一起后（连接全连接）学习整体的表达，这种方式简单，并且能够提供一个不错的基线。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## CentralNet相比于Concatenate的创新点</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Concate的方法相当于在各自模态的特征分别独立抽取之后做融合，但是不干预特征抽取的过程。这显然会漏掉一些不同模态之间的相关性的信息，比如，一个人尖叫的视频片段，里面有人的嘴巴长得很大的这种视觉特征，也会有尖叫声这种声音特征，这两种特征出现的时间都是相关连的。如果在抽取特征的过程中，**相互有一个监督或者借鉴的关系**，那样应该会更好，这是CentralNet提出的动机。CentralNet把两个可以独立在各自模态做出判断的网络的中间层联系起来，利用联合多任务训练来优化，最后也得到了不错的结果。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## CentralNet细节</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 网络结构</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1debee429a4b410dbafe33f7af93947a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"903\" data-rawheight=\"881\" class=\"origin_image zh-lightbox-thumb\" width=\"903\" data-original=\"https://pic3.zhimg.com/v2-1debee429a4b410dbafe33f7af93947a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;903&#39; height=&#39;881&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"903\" data-rawheight=\"881\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"903\" data-original=\"https://pic3.zhimg.com/v2-1debee429a4b410dbafe33f7af93947a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1debee429a4b410dbafe33f7af93947a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图中的a是基本的多模态融合方法，首先独立的模型M1,M2提取出来各自模态的特征，然后一起输入到融合Fusion模块进行融合，融合得到的特征送入分类器去分类。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2d59de024b119543796ac66d04061bcb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"351\" data-rawheight=\"75\" class=\"content_image\" width=\"351\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;351&#39; height=&#39;75&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"351\" data-rawheight=\"75\" class=\"content_image lazy\" width=\"351\" data-actualsrc=\"https://pic4.zhimg.com/v2-2d59de024b119543796ac66d04061bcb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>下图中的b是CentralNet的示意图，它各个模态模型M1,M2中间的特征也拿到，用上图公式加权送入下一层。如此一层一层地做，传到最后一层得到一个融合特征hcx，利用这个融合特征送入分类器分类。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>CentralNet思路简单，架构清晰，也没有难理解的地方。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 训练方法</p><p class=\"ztext-empty-paragraph\"><br/></p><p>CentralNet模型借助多任务对每个模态的表达进行约束，以期Fusion后的表达能够获取更好的泛化能力。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>训练的损失函数是各个模态损失和融合后特征送入融合分类器后得到的损失的一个加权和：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-cf1473d4568b7686d2ce14c4809a1979_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"320\" data-rawheight=\"68\" class=\"content_image\" width=\"320\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;320&#39; height=&#39;68&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"320\" data-rawheight=\"68\" class=\"content_image lazy\" width=\"320\" data-actualsrc=\"https://pic2.zhimg.com/v2-cf1473d4568b7686d2ce14c4809a1979_b.jpg\"/></figure><p>。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 实验结果</p><p class=\"ztext-empty-paragraph\"><br/></p><p>作者在Multimodal MNIST和Audiovisual MNIST，Montalbano和MM-IMDb数据集上都做了对比试验。这里简单介绍前两个数据集上的结果和结论。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### Multimodal MNIST</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Multimodal MNIST是把原始Mnist数据集用PCA方法做分解，用分解出来的特征向量重构出来的数据集。 energy和share ratio分别表示各自重构用的特征向量的个数和两个模态公用的特征向量的比例。下面是两个生成数据集里面样本的示意图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-85f96b14b6bdb58b1ffd436fa290eaf4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"346\" data-rawheight=\"611\" class=\"content_image\" width=\"346\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;346&#39; height=&#39;611&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"346\" data-rawheight=\"611\" class=\"content_image lazy\" width=\"346\" data-actualsrc=\"https://pic1.zhimg.com/v2-85f96b14b6bdb58b1ffd436fa290eaf4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>*重构的两个数据集，虽然都是图像信息，不能算严格的多模态，但是这里只是一个玩具性质的实验，而且用PCA做了分解和各自重构，勉强算模拟了多模态吧。而且做PCA分解再合成的方法比另外有人做的直接切1/4图片的方法相比有可以控制各个模态信息量和各自模态信息之间依赖性的好处。*</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-35d38040af2a7d84d37eb9a58d58cd32_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"284\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https://pic3.zhimg.com/v2-35d38040af2a7d84d37eb9a58d58cd32_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;900&#39; height=&#39;284&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"284\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https://pic3.zhimg.com/v2-35d38040af2a7d84d37eb9a58d58cd32_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-35d38040af2a7d84d37eb9a58d58cd32_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>作者用来实验的各自模态的网络就是简单的LeNet5，中间做融合的网络也是用的同样结构的LeNet5结构。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-da7fa416be7a3431664e91a9183dade6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"898\" data-rawheight=\"504\" class=\"origin_image zh-lightbox-thumb\" width=\"898\" data-original=\"https://pic3.zhimg.com/v2-da7fa416be7a3431664e91a9183dade6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;898&#39; height=&#39;504&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"898\" data-rawheight=\"504\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"898\" data-original=\"https://pic3.zhimg.com/v2-da7fa416be7a3431664e91a9183dade6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-da7fa416be7a3431664e91a9183dade6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图是控制重构特征向量的多少和控制两个模态共享特征向量的多少的实验结果。左边没什么说的，重构用的特征向量越多，效果越好，符合我们的预期。需要注意的是，右边显示，**两个模态共享的信息太多或者太小，对所有的方法都效果不好**。从左右两个图中都可以看出来，简单三个LeNet做Ensemble的效果都比CentralNet方法效果差，这也证明了CentralNet的性能提升**不是单单依靠更多的参数达到的**。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-98200bfd388ea8e14c8f010c4622d24a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"502\" data-rawheight=\"577\" class=\"origin_image zh-lightbox-thumb\" width=\"502\" data-original=\"https://pic3.zhimg.com/v2-98200bfd388ea8e14c8f010c4622d24a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;502&#39; height=&#39;577&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"502\" data-rawheight=\"577\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"502\" data-original=\"https://pic3.zhimg.com/v2-98200bfd388ea8e14c8f010c4622d24a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-98200bfd388ea8e14c8f010c4622d24a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>从上图可以看出来，简单Fusion和Ensemble联合起来的方法要好于单独做简单Fusion的方法，说明他们有互补性。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c5a5689a83851c3f766fc10e04e551a6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"424\" data-rawheight=\"312\" class=\"origin_image zh-lightbox-thumb\" width=\"424\" data-original=\"https://pic3.zhimg.com/v2-c5a5689a83851c3f766fc10e04e551a6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;424&#39; height=&#39;312&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"424\" data-rawheight=\"312\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"424\" data-original=\"https://pic3.zhimg.com/v2-c5a5689a83851c3f766fc10e04e551a6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c5a5689a83851c3f766fc10e04e551a6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上面的结果是比较有意思的，显示的是CentralNet中三个LeNet5子网络各自的贡献大小。可以看到Central的网络，随着网络加深，贡献越来越大。到第三层和第四层，ImageB对应的网络贡献很小，似乎可以考虑做剪枝。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### Audiovisual MNIST</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Audiovisual MNIST也是作者自己建的一个测试集。图像部分是把原始28x28的Mnist数据集做PCA，采用25%能量的特征向量重构（之所以这么低质量的重构，是为了更好的验证多模态融合方法好不好，能不能得到互补的更好的特征）。声音样本是读数字，然后加上一些噪音合成出来的。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6df7a13bfb61bb4721f43dc67352a6ac_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"812\" data-rawheight=\"290\" class=\"origin_image zh-lightbox-thumb\" width=\"812\" data-original=\"https://pic1.zhimg.com/v2-6df7a13bfb61bb4721f43dc67352a6ac_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;812&#39; height=&#39;290&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"812\" data-rawheight=\"290\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"812\" data-original=\"https://pic1.zhimg.com/v2-6df7a13bfb61bb4721f43dc67352a6ac_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6df7a13bfb61bb4721f43dc67352a6ac_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>图像部分继续用LeNet5进行实验，声音部分用自己设计的一个简单的6层CNN网络，具体实现结构如上。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c65ce49f6e91d710f317d5bc001b22d1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"574\" data-rawheight=\"314\" class=\"origin_image zh-lightbox-thumb\" width=\"574\" data-original=\"https://pic2.zhimg.com/v2-c65ce49f6e91d710f317d5bc001b22d1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;574&#39; height=&#39;314&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"574\" data-rawheight=\"314\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"574\" data-original=\"https://pic2.zhimg.com/v2-c65ce49f6e91d710f317d5bc001b22d1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c65ce49f6e91d710f317d5bc001b22d1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图显示了实验结果，结果表明，**各自模态单独做预测的结果都不好，不管用哪种融合的方法，所得到的性能都比单独做好**，而且CentralNet能做到最好。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e50b20c22d10a883bffead17e1d9f465_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"421\" data-rawheight=\"329\" class=\"origin_image zh-lightbox-thumb\" width=\"421\" data-original=\"https://pic2.zhimg.com/v2-e50b20c22d10a883bffead17e1d9f465_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;421&#39; height=&#39;329&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"421\" data-rawheight=\"329\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"421\" data-original=\"https://pic2.zhimg.com/v2-e50b20c22d10a883bffead17e1d9f465_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e50b20c22d10a883bffead17e1d9f465_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图是分析每层各自哪个模态权重较大，可以看到，每个模块都对最终的性能发挥了作用。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>人有各种感觉器官收集信息去做判断，如果我们在做短视频打标，能把图像，声音，字符等信息综合起来去做判断，无疑是更科学的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [CentralNet: a Multilayer Approach for Multimodal Fusion](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1808.07275\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CentralNet: a Multilayer Approach for Multimodal Fusion</a>)</p><p>+ [爱奇艺短视频分类技术解析](<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/t801Q3OO_DBrgI60fKSJxQ\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">干货|爱奇艺短视频分类技术解析</a>)</p><p>+ [PRCV2018 美图短视频实时分类挑战赛第一名解决方案介绍](<a href=\"https://link.zhihu.com/?target=https%3A//www.leiphone.com/news/201811/yhkoD7Ty8WRaCBqe.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">PRCV2018 美图短视频实时分类挑战赛第一名解决方案介绍</a>)</p>", 
            "topic": [
                {
                    "tag": "多模态学习", 
                    "tagLink": "https://api.zhihu.com/topics/20688199"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": [
                {
                    "userName": "毛豆大白", 
                    "userLink": "https://www.zhihu.com/people/dedb4b4be9fb137282ee80b21ea9cb11", 
                    "content": "<p>船长您好，请问这篇文章您那有源码吗？</p><p><br></p><a class=\"comment_sticker\" href=\"https://pic4.zhimg.com/v2-b789d8e597d920061dcd4efb585cd343.gif\" data-width=\"\" data-height=\"\">[思考]</a>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/83570146", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 13, 
            "title": "海量短视频打标问题之Multi-Modal-Machine-Learning", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>接着[《D#0035》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230035-2KW%25E7%259F%25AD%25E8%25A7%2586%25E9%25A2%2591%25E6%2589%2593%25E6%25A0%2587%25E9%2597%25AE%25E9%25A2%2598%25E4%25B9%258BActivate-Learning/D%25230035.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)讲，既然我们是给视频打标签，那么肯定就不能只局限于图像上做文章。视频文件包含的信息很多，一个短视频除了有一帧一帧的图像，还有声音信息，甚至还有字幕或者用户打的标签和文字评论之类的这些信息，那么怎么把这些不同类别的信息抽取并利用起来，就是一个很关键的问题了。研究这类多种不同信息源做机器学习问题的领域就是今天要聊的“多模态机器学习”(Multi-Modal Machine Learning)。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 什么是多模态机器学习</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这里不想去扣多模态严格的学术定义，只需要把多模态理解成不同来源或不同形式的信息就可以了，比如视频里面的声音和图像就是多模态。对图像用CNN抽取特征，对声音用CNN进行特征抽取，这叫多模态的特征**学习**；将汉语用算法转换成英语，这叫多模态的**转换**；对抽取出来的图像和声音特征在时间维度进行对齐，这叫多模态特征的**对齐**；人身上有眼耳口鼻等不同的器官，根据这些器官检测到周围各种信息，比如图像、声音、味道、温度等，再根据这些不同来源的信息综合起来做出判断，这叫多模态特征的**融合**；物理学家利用他丰富的物理学知识去教一个人学化学原理，这叫多模态**学习**。而**多模态机器学习**就是研究这些的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 多模态机器学习的研究点</p><p class=\"ztext-empty-paragraph\"><br/></p><p>多模态机器学习广泛应用于视频理解（比如UGC短视频打标），机器翻译，情感分析，图片搜索和内容推荐等领域，而这些领域所侧重的研究点又各有不同，这里一一做个简单介绍。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 多模态特征学习Representation</p><p class=\"ztext-empty-paragraph\"><br/></p><p>普通做图像深度学习，只考虑单个图像的特征，只需要学习图像中例如纹理，形状这种特征，只需要考虑学习到的这类特征的表达能力好就可以了。而多模态的深度学习需要考虑到不同modal信息的特征一起学习出来为任务目标服务，他们是要考虑相互的影响的，要学习到的各模态特征之间有更多互补性，剔除模态间的冗余性，要处理不同modal中带的不同大小的噪音，缺失值怎么处理。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>多模态特征学习是多模态机器学习的基础问题。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>多模态特征学习主要的研究方向有**联合表示（Joint Representations）**和**协同表示（Coordinated Representations）**。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8179331379f5216c78999d81226f44ab_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1332\" data-rawheight=\"421\" class=\"origin_image zh-lightbox-thumb\" width=\"1332\" data-original=\"https://pic4.zhimg.com/v2-8179331379f5216c78999d81226f44ab_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1332&#39; height=&#39;421&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1332\" data-rawheight=\"421\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1332\" data-original=\"https://pic4.zhimg.com/v2-8179331379f5216c78999d81226f44ab_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8179331379f5216c78999d81226f44ab_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图所示，联合表示的方法（上图左边），是把多个独立modal得到的特征映射到**一个统一的特征空间**；而协同表示的方法是各个modal各自映射自己的特征到**两个相关的特征空间**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 多模态的转换Translation</p><p class=\"ztext-empty-paragraph\"><br/></p><p>把一种模态的信息映射为另一种模态，这叫模态的转换。比如，机器翻译中把汉语映射为英语，图片描述（Image Captioning) 或者视频描述（Video Captioning)把图片和视频信息生成一段描述语句。对于海量短视频打标签这种问题来说，会遇到一个问题就是评价的**主观性**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>不同模态转换的方法大致可以分为两类，**基于样本（Example based）**的和**生成式（Generative）**的。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1ab0b3cff7bc500d703653ca7c473a50_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1331\" data-rawheight=\"424\" class=\"origin_image zh-lightbox-thumb\" width=\"1331\" data-original=\"https://pic1.zhimg.com/v2-1ab0b3cff7bc500d703653ca7c473a50_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1331&#39; height=&#39;424&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1331\" data-rawheight=\"424\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1331\" data-original=\"https://pic1.zhimg.com/v2-1ab0b3cff7bc500d703653ca7c473a50_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1ab0b3cff7bc500d703653ca7c473a50_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图所示，左边是基于样本的，首先用许多样本构建一个从Source modal到Target modal的**转移字典**，然后在转换的时候，根据这个事先构建的字典做模态转换。而右边的是生成式的转移方法，它利用转移字典训练一个转移模型，然后用这个转移模型去做Source modal到Target modal的状态转移。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 多模态特征的对齐Alignment</p><p class=\"ztext-empty-paragraph\"><br/></p><p>不同模态特征之间可能有某些对应关系，比如视频的某一帧和它的声音的切片之间的对应关系，在做多模态机器学习的时候，一个很重要的步骤是将不同模态特征做对齐（当然，有的时候也不需要对齐或者不同模态特征已经对齐了）。多模态对齐方法主要分为显示对齐和隐式对齐两种，具体分类可见下表：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d9298f057a078defdc6239f7b64e3c45_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"653\" data-rawheight=\"369\" class=\"origin_image zh-lightbox-thumb\" width=\"653\" data-original=\"https://pic2.zhimg.com/v2-d9298f057a078defdc6239f7b64e3c45_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;653&#39; height=&#39;369&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"653\" data-rawheight=\"369\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"653\" data-original=\"https://pic2.zhimg.com/v2-d9298f057a078defdc6239f7b64e3c45_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d9298f057a078defdc6239f7b64e3c45_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>### 多模态特征的融合Fusion</p><p class=\"ztext-empty-paragraph\"><br/></p><p>多模态特征融合是多模态机器学习最初开始研究的主题，现在也是研究的主要热点。它主要是**集成**多模态的特征，来输出一个预测结果（这个结果在分类问题中是类别，在回归问题中是一个连续值）。多模态特征融合按照融合发生的阶段是离原始信息近还是离多模态特征近可以分为早期融合（Early Fusion），晚期融合（Late Fusion）和混合式融合（Hybrid Fusion）。现在主要是做混合式的了，后面抽空我会再写一篇CV领域做混合式融合的方法CentralNet（示例如下图），这个文章比较有代表性。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1debee429a4b410dbafe33f7af93947a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"903\" data-rawheight=\"881\" class=\"origin_image zh-lightbox-thumb\" width=\"903\" data-original=\"https://pic3.zhimg.com/v2-1debee429a4b410dbafe33f7af93947a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;903&#39; height=&#39;881&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"903\" data-rawheight=\"881\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"903\" data-original=\"https://pic3.zhimg.com/v2-1debee429a4b410dbafe33f7af93947a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1debee429a4b410dbafe33f7af93947a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>### 协同学习Co-learning</p><p class=\"ztext-empty-paragraph\"><br/></p><p>协同学习是最后一个要介绍的模块。协同学习就是用一个资源丰富模态下的模型去帮助资源不丰富模态下模型的学习。这样说可能比较抽象，但是多模态协同学习还是经常用到的，比如CV训练模型很多都是把ImageNet这个资源丰富的模态下训练的模型的权值拿到新的模型上复用，然后用资源不丰富的样本做fine-tune，这种**迁移学习**（Transfer Learning），也属于协同学习的一类。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-be0b871e858d8a9afa4f67260d4da345_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1050\" data-rawheight=\"733\" class=\"origin_image zh-lightbox-thumb\" width=\"1050\" data-original=\"https://pic2.zhimg.com/v2-be0b871e858d8a9afa4f67260d4da345_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1050&#39; height=&#39;733&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1050\" data-rawheight=\"733\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1050\" data-original=\"https://pic2.zhimg.com/v2-be0b871e858d8a9afa4f67260d4da345_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-be0b871e858d8a9afa4f67260d4da345_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>另外，在多模态问题中，以有限带标签样本来生成更多带标签训练样本的**协同训练**（Co-training）问题，也是协同学习的一类。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-40c5ca6d1e4a16aaa721c362571a9ef0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"857\" data-rawheight=\"416\" class=\"origin_image zh-lightbox-thumb\" width=\"857\" data-original=\"https://pic1.zhimg.com/v2-40c5ca6d1e4a16aaa721c362571a9ef0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;857&#39; height=&#39;416&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"857\" data-rawheight=\"416\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"857\" data-original=\"https://pic1.zhimg.com/v2-40c5ca6d1e4a16aaa721c362571a9ef0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-40c5ca6d1e4a16aaa721c362571a9ef0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图是协同训练比较经典的文章《Combining labeled and unlabeled data with co-training》（T. Mitchell也是机器学习的泰斗）。这里简单介绍一下协同训练：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>比如我们要做图片猫狗分类任务，已经有一批已经标注了的猫狗图片集L，和一批未标注的图片集U。我们可以训练两个分类器，比如SVM和Baysian分类器，分别命名为h1和h2（这里取SVM和Baysian分类器是因为这两个分类器原理很不一样，希望可以抓住图片的**不同方面**的特征，后面还要投票）。分别用h1，h2为U集合打标签，然后分析他们各自打的标签，如果对U里面同一个样本u打了一样的标签，那么可以把这个标签和u样本绑定，加入到L集合中，如果不一样，那么还是放回到U集合中；然后L扩大了，进行下一轮，再训练两个分类器h1,h2，再对U里面的样本进行打标，如此迭代，直到U为空。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>多模态机器学习肯定是未来发展的方向，为了追求更高的预测性能，单单依赖于一种模态提供的信息来做决策肯定是不可靠的，多模态机器学习，CV、Speech和NLP的融合肯定会有很大的未来。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [Multimodal Machine Learning: A Survey and Taxonomy](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1705.09406\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Multimodal Machine Learning: A Survey and Taxonomy</a>)</p><p>+ [Multimodal_learning wiki](<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Multimodal_learning\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">en.wikipedia.org/wiki/M</span><span class=\"invisible\">ultimodal_learning</span><span class=\"ellipsis\"></span></a>)</p><p>+ [Multimodal Deep Learning](<a href=\"https://link.zhihu.com/?target=https%3A//towardsdatascience.com/multimodal-deep-learning-ce7d1d994f4\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">towardsdatascience.com/</span><span class=\"invisible\">multimodal-deep-learning-ce7d1d994f4</span><span class=\"ellipsis\"></span></a>)</p><p>+ [Combining labeled and unlabeled data with co-training](<a href=\"https://link.zhihu.com/?target=http%3A//cms.brookes.ac.uk/research/visiongroup/talks/rg_july_09_ss_learning/p92-blum.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">cms.brookes.ac.uk/resea</span><span class=\"invisible\">rch/visiongroup/talks/rg_july_09_ss_learning/p92-blum.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [爱奇艺短视频分类技术解析](<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/t801Q3OO_DBrgI60fKSJxQ\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">干货|爱奇艺短视频分类技术解析</a>)</p><p>+ [PRCV2018 美图短视频实时分类挑战赛第一名解决方案介绍](<a href=\"https://link.zhihu.com/?target=https%3A//www.leiphone.com/news/201811/yhkoD7Ty8WRaCBqe.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">PRCV2018 美图短视频实时分类挑战赛第一名解决方案介绍</a>)</p>", 
            "topic": [
                {
                    "tag": "短视频", 
                    "tagLink": "https://api.zhihu.com/topics/19871674"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "小样本", 
                    "tagLink": "https://api.zhihu.com/topics/20223432"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/83569964", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 4, 
            "title": "海量短视频打标问题之Active-Learning", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在网络中，每时每刻都会产生很多**无标签**数据信息，比如最近很火的一些短视频APP，每天都有很多用户发布自己生产的短视频(UGC)内容，这些内容一般是部分打了标签或者标签中有很多噪音的，为了进行做推荐或者做分类、识别的训练，需要给这些短视频**自动生成高质量的标签**；而另外，随着人工智能的发展，许多以前积累的数据，需要自动做标注，比如很多医疗领域的核磁共振片子。这只是CV领域(许多数据集已经打标或者很容易打标)，在NLP和推荐领域，这样的问题更加重要和常见。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这些给**海量视频数据打标签**的问题是很常见也很重要的问题，而且这些问题牵扯到机器学习中的“主动学习”(Active Learning)，“多模态机器学习”(MultiModal Machine Learning)，“多标签(Multi-label)分类”，“增量学习”(Incremental Learning)，“在线学习”(Online Learning)，“少样本学习”(Few Shot/Zero Shot Learning)等等领域。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>为此，船长打算以海量短视频打标这个具体的问题写几篇(具体写几篇，要看我有多忙lan)文章专门捋一捋这方面的常用算法，这个系列的文章将是第一篇，专注于主动学习领域。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 主动学习基本思路</p><p class=\"ztext-empty-paragraph\"><br/></p><p>假设现在有两千万短视频池，需要给他们打上一万个标签。如果手工一个个检查一个个打标签，成本太高，肯定是不现实的。但是我们可以少量打一些标签，比如为1000个视频打标签，这个成本还是可以接收的。然后用这1000个视频和**手动打的标签**去训练一个自动打标签的模型alpha。用模型alpha去给视频池**剩余的样本去预测标签**，根据预测出的标签的信息，根据**一定规则**挑选出某些更有意义的样本，比如2000个，把这些挑选出来的2000个样本**再找人工打标签**（因为经过挑选的，所以数量可以控制少一点）。把这些打标签的数据和训练模型alpha的数据合在一起，形成一个3000个样本的**更大的带标签数据集**训练一个**更好的模型beta**，再用模型beta重复由模型alpha得到模型beta的过程，可以继续生成更更好的模型gamma，把这个过程可以迭代下去，就可以在有限的标注成本下，得到不错的自动打标模型了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 主动学习为什么有用？</p><p class=\"ztext-empty-paragraph\"><br/></p><p>我们知道，一般而言，样本量越多，训练的模型越好。但是不是所有的样本对最终模型性能的贡献都是一样的（比如我们要描述一个正方体，并不需要穷举列出正方体内所有的点，只需要列举这个正方体的某些定点上的点就行了），如果能找出一些**关键样本**，这些样本对模型训练更加重要，那么自然我们可以只标准哪些关键样本来训练模型啦。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>那么，很显然，关键样本的**挑选规则是主动学习成功的关键**，这也是主动学习领域研究比较多的一个问题，后文会有简单介绍两种比较经典的样本挑选规则。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 主动学习实施</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 算法流程</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在前面一节，已经大概介绍了主动学习的一般思路和过程，这里写一下流程吧：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 将两千万短视频初始化未标注样本池P；</p><p>2. 在样本池中随机选出1000个样本，对齐人工做标注，形成训练集合T；</p><p>3. 用训练集合T训练模型M；</p><p>4. 用上一步训练出来的模型M预测样本池P中不属于T的样本，得到预测信息Pred；</p><p>5. 根据Pred用**挑选规则挑选**出一些样本，给**人工**进行标注，并把新标注的样本和原来训练集合T合并成新的训练集合T；</p><p>6. 如果模型M满足性能要求，则终止，否则转到步骤3；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 挑选规则</p><p class=\"ztext-empty-paragraph\"><br/></p><p>主动学习中样本挑选规则是很重要的，主流有如下集中方法：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 基于不确定度缩减的方法。比如分类问题中，一般会出现一个概率向量，这个概率向量的信息熵可以认为是模型对分类的不确定度；挑选信息熵最大的那些样本送去给人工进行标注。从几何角度看，这种方法优先选择靠近分类边界的样例。</p><p>2. 基于最大两个类别概率差距最小的方法。和上面差不多，只不过选择的标注不是概率向量的信息熵，而是选择那些top1和top2分量差距最小的概率向量所对应的未标注样本送去给人工标注。很好理解，如果模型预测出某个样本有很高的概率属于1类，也有很高的概率属于2类，那么就说明模型对这个样本不是很确定，就需要人工标注给他更多的信息去学习。</p><p>3. 预先聚类的方法：预先运行聚类算法预处理，选择样例时优先选择最靠近分类边界的样例和最能代表聚类的样例（即聚类中心）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 主动学习和难例挖掘的比较</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在CV中常见的提升性能的方法有难例挖掘（在线，离线），也是挑选一些少量关键样本来提升性能，这里做个简单的比较。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 相同点</p><p class=\"ztext-empty-paragraph\"><br/></p><p>主动学习和难例挖掘(Hard Example Mining)很像，都是用训练了的模型去做预测，找出少量可能对模型性能改进有帮助的关键样本，然后用关键样本去帮助模型改进。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 不同点</p><p class=\"ztext-empty-paragraph\"><br/></p><p>但是不同的地方也很明显，主动学习需要人工参与标注，样本挑选规则挑选出来的样本，要送到人那里去做手动标注；而难例挖掘是在所有样本都有标签的前提下，找出那些特别难的样本。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文以为海量短视频打标签为例子，简单介绍了一下主动学习这种实用的学习方法。但是单单靠这一种方法来做海量短视频打标还是**远远不够**的，后续我会再介绍这个问题上用得上的其他技术点。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [Active Learning Tutorial](<a href=\"https://link.zhihu.com/?target=https%3A//towardsdatascience.com/active-learning-tutorial-57c3398e34d\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">towardsdatascience.com/</span><span class=\"invisible\">active-learning-tutorial-57c3398e34d</span><span class=\"ellipsis\"></span></a>)</p><p>+ [Active Learning wiki](<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Active_learning_%28machine_learning%29\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">en.wikipedia.org/wiki/A</span><span class=\"invisible\">ctive_learning_(machine_learning)</span><span class=\"ellipsis\"></span></a>)</p><p>+ [爱奇艺短视频分类技术解析](<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/t801Q3OO_DBrgI60fKSJxQ\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">干货|爱奇艺短视频分类技术解析</a>)</p><p>+ [PRCV2018 美图短视频实时分类挑战赛第一名解决方案介绍](<a href=\"https://link.zhihu.com/?target=https%3A//www.leiphone.com/news/201811/yhkoD7Ty8WRaCBqe.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">PRCV2018 美图短视频实时分类挑战赛第一名解决方案介绍</a>)</p>", 
            "topic": [
                {
                    "tag": "短视频", 
                    "tagLink": "https://api.zhihu.com/topics/19871674"
                }, 
                {
                    "tag": "主动学习", 
                    "tagLink": "https://api.zhihu.com/topics/20091839"
                }, 
                {
                    "tag": "标签（Tag）", 
                    "tagLink": "https://api.zhihu.com/topics/19557891"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/83457292", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 2, 
            "title": "火箭发射：阿里巴巴的轻量网络训练方法", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>一般而言，简单的网络速度快，但性能不如复杂的网络好；负责的网络性能好，但是计算量大，也快不了。如何能结合小网络的速度和大网络的性能是一个很难也很重要的问题。主流的思路有这么几种：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>从模型上来讲，</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 模型的**剪枝和压缩**，先训练一个性能好的大的模型，然后进行裁剪减小计算量；</p><p>2. **轻量级网络**，比如MobileNet, ShuffeNet等；</p><p>3. [**知识蒸馏**](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1503.02531\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Distilling the Knowledge in a Neural Network</a>)，用性能好的大模型作为“教师”指导小模型训练，让小模型模仿大模型的泛化行为（[《**D#0031-知识蒸馏Knowledge-Distillation**》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230031-%25E7%259F%25A5%25E8%25AF%2586%25E8%2592%25B8%25E9%25A6%258FKnowledge-Distillation/D%25230031.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)已有介绍）；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>从工程上讲，</p><p class=\"ztext-empty-paragraph\"><br/></p><p>4. 模型**量化**，例如在某些平台上，用int8代替float32来做预测；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>5. **算子融合**，比如BN融合（[《**D#0020-Batch-Normalization层原理与分析**》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230020-Batch-Normalization%25E5%25B1%2582%25E5%258E%259F%25E7%2590%2586%25E4%25B8%258E%25E5%2588%2586%25E6%259E%2590/D%25230020.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)中也有介绍）, Scale融合等；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这些只是简单列举了几种，而且他们也并不是互斥的，比如用了剪枝和压缩之后也可以继续做算子融合。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这里介绍一种**训练方法**，出自于**阿里巴巴**在2018 AAAI上的论文《Rocket Launching: A Universal and Efficient　Framework for Training Well-performing Light Net》，思路上像知识蒸馏，实际上是借鉴了知识蒸馏思路的一种通用的高效训练方法。但与传统的知识蒸馏相比，它的创新点和优势在于：能同时训练复杂和简单两个网络，而不是先训练一个复杂网络，然后用其训练一个简单网络，**缩短了总的训练时间**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 火箭发射思路简介</p><p class=\"ztext-empty-paragraph\"><br/></p><p>火箭发射升空过程分为多个阶段，最开始的一个阶段，多级助推器朝着一个目标共同飞行，在后期稳定阶段，第一级助推器脱离，只用第二级助推器飞行。在“火箭发射”训练方法中，训练时，同时训练**共享底层特征**的复杂和简单两个网络，让他们朝着同样的目标优化的同时，用复杂网络**实时**的输出结果作为hint去指导简单网络来提升小网络的性能；在预测时，**砍掉**复杂网络，只用简单网络做预测来减小预测时间。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 火箭发射训练细节</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-00cc765405d95ef9c905eaf5f4f3ed6a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"597\" data-rawheight=\"678\" class=\"origin_image zh-lightbox-thumb\" width=\"597\" data-original=\"https://pic3.zhimg.com/v2-00cc765405d95ef9c905eaf5f4f3ed6a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;597&#39; height=&#39;678&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"597\" data-rawheight=\"678\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"597\" data-original=\"https://pic3.zhimg.com/v2-00cc765405d95ef9c905eaf5f4f3ed6a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-00cc765405d95ef9c905eaf5f4f3ed6a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图所示为训练时候大的框架：粉色虚线框住的是复杂网络，蓝色虚线框住的是简单网络；底下黄色的层是复杂网络和简单网络共享的层，粉色的那些层是复杂网络独有的层，蓝色的层是简单网络独有的层；z(x)是复杂网络训练时候输出的logits score，q(x)是z(x)通过Softmax激活得到的概率，l(x)和p(x)同理。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9329c006f434fb4922ab7eb027539df3_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"519\" data-rawheight=\"82\" class=\"origin_image zh-lightbox-thumb\" width=\"519\" data-original=\"https://pic4.zhimg.com/v2-9329c006f434fb4922ab7eb027539df3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;519&#39; height=&#39;82&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"519\" data-rawheight=\"82\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"519\" data-original=\"https://pic4.zhimg.com/v2-9329c006f434fb4922ab7eb027539df3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-9329c006f434fb4922ab7eb027539df3_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图是训练时候的前向预测的Loss，前两两项很好理解，是分类常用的**多分类交叉熵**，最后黄色那一项也很好理解，是复杂网络的score和简单网络的**score的均方差**，称之为**Hint Loss**，这两项的差应该尽量小，代表复杂网络在教简单网络。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>值得注意的几点：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. **参数共享**：在网络的底层，黄色的那几层是大网络和小网络共享的。很好理解，浅层的特征表达的都是一些很基本的信息，比如线和边，这些信息是后面层做更高层抽象表达的原材料，如果小网络共用了大网络的这些层，无疑是**复用了大网络学习到的优良的原材料**，是有好处的；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2. **同步训练**：在最初的知识蒸馏中，是先有了大的复杂网络，然后用复杂网络的输出去监督小的简单网络训练。而这除了训练过程时间变长（训练复杂网络的时间加训练简单网络的时间大于同步两者一起训练的时间）的问题之外，还有一个潜在的缺点，那就是大网络学习的**过程中的信息**丢掉了，如果同时训练，用大网络每一个mini-batch的结果去监督小网络，那么小网络学习的知识可能更多；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3. **Hint Loss**：在最初的知识蒸馏中，大网络给小网络的监督信号是q(x)，然后加入温度超参数T来调节，取交叉熵作为loss，而这里**训练用的监督信号是z(x)，取MSE作为loss**；</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0eb07d55cfeeef2cc12911a43facd8ab_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"597\" data-rawheight=\"612\" class=\"origin_image zh-lightbox-thumb\" width=\"597\" data-original=\"https://pic4.zhimg.com/v2-0eb07d55cfeeef2cc12911a43facd8ab_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;597&#39; height=&#39;612&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"597\" data-rawheight=\"612\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"597\" data-original=\"https://pic4.zhimg.com/v2-0eb07d55cfeeef2cc12911a43facd8ab_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0eb07d55cfeeef2cc12911a43facd8ab_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>4. **梯度拦截Gradient Block**：大网络有更强的表达能力，要给大网络更大的自由去学习，**不能让小网络的结果去干扰了大网络的训练**，所以，在参数更新，损失梯度回传的时候更新Wb的信号应该**只来自于大网络的输出和真值的交叉熵**，小网络更新Wl的时候，损失应该来自于他自己输出和真值的交叉熵还有他的输出l(x)和大网络的z(x)之间的Hint Loss。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>火箭网络的训练框架思路清晰，实现也比较简单，从论文中给出的效果来看，比传统的知识蒸馏效果要好一点，可以作为在网络提速项目中知识蒸馏方向的一个有益的思路拓展和另外一个选择，此记。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [Distilling the Knowledge in a Neural Network](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1503.02531\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Distilling the Knowledge in a Neural Network</a>)</p><p>+ [Rocket Launching: A Universal and Efficient Framework for Training Well-performing Light Net](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1708.04106\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Rocket Launching: A Universal and Efficient Framework for Training Well-performing Light Net</a>)</p><p>+ [《D#0020-Batch-Normalization层原理与分析》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230020-Batch-Normalization%25E5%25B1%2582%25E5%258E%259F%25E7%2590%2586%25E4%25B8%258E%25E5%2588%2586%25E6%259E%2590/D%25230020.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p>+ [《D#0031-知识蒸馏Knowledge-Distillation》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230031-%25E7%259F%25A5%25E8%25AF%2586%25E8%2592%25B8%25E9%25A6%258FKnowledge-Distillation/D%25230031.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p>", 
            "topic": [
                {
                    "tag": "阿里巴巴集团", 
                    "tagLink": "https://api.zhihu.com/topics/19551577"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "训练", 
                    "tagLink": "https://api.zhihu.com/topics/19554288"
                }
            ], 
            "comments": [
                {
                    "userName": "SomeoneLikeYou", 
                    "userLink": "https://www.zhihu.com/people/5e65c9c488c5b695ec74187450fdb87c", 
                    "content": "<p>写的很详细, 赞一个!</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "船长", 
                            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
                            "content": "<p>哈，谢谢</p>", 
                            "likes": 0, 
                            "replyToAuthor": "SomeoneLikeYou"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/83456995", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 1, 
            "title": "一些分类网络的训练技巧", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>最近再看李沐老师的论文《Bag of Tricks for Image Classification with Convolutional Neural Networks》，里面总结了一些分类网络的训练技巧，这里简单做个记录。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 训练加速手段</p><p class=\"ztext-empty-paragraph\"><br/></p><p>通常我们训练模型都是使用fp32浮点精度来表示图像、中间结果、保存权重和进行优化，但是fp32需要占用4字节内存，但是有研究表示，用低精度的浮点数，比如fp16来训练，最终性能也不会降低太多，而且可以增大batch size。建议用**低精度浮点数和适当大点的batch size来训练**，这样能加速训练过程，而且不会牺牲模型性能，甚至在某些时候还会提升。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 大的batch size来训练</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a44d619908d03cf9f8c8fafc9d97abf1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"601\" data-rawheight=\"411\" class=\"origin_image zh-lightbox-thumb\" width=\"601\" data-original=\"https://pic2.zhimg.com/v2-a44d619908d03cf9f8c8fafc9d97abf1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;601&#39; height=&#39;411&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"601\" data-rawheight=\"411\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"601\" data-original=\"https://pic2.zhimg.com/v2-a44d619908d03cf9f8c8fafc9d97abf1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a44d619908d03cf9f8c8fafc9d97abf1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图所示的为Mini-batch SGD的训练流程。每个epoch，会把所有训练样本划分为b个batch，每次参数更新会用一个batch里面的所有样本综合的结果来算loss和梯度，再更新权重。Mini-batch SGD之所以用多个图片组成batch来训练，为的是提升计算的并行性和减少数据通信带来的overhead。但是，太大的batch size也不一定好，因为对于凸优化问题，优化过程收敛的速率（而不是收敛的结果！）会随着batch size的增大而降低。换句话说，同样的数量的epoch前提下，大batch size训练的模型验证机精度要比小batch size训练的模型差。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>为了用大的batch size提升模型性能，且让训练过程速度提升，这里有四个Trick：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. **初始学习率线性缩放**：随着batch size的增大，batch样本总的variance变小，梯度中的噪音会变小，所以我们可以适当增大学习率来加速训练。我们可以像Goyal建议的那样，**随着batch size的增大，线性增大学习率**。例如，何恺明在训练ResNet的时候，batch size设置为256，相应初始学习率设置为0.1，如果我们设置batch size为b，那我们的初始学习率可以设置为0.1 x b/256。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2. **学习率预热到初始学习率**：在网络开始训练的时候，给它的初始化权重一般都是随机赋值的，和最终要学习到的结果相去甚远。所以开始学习的时候，权重更新的梯度很大，如果用一开始就用很大的学习率，很可能造成训练中数值不稳定。所以，最开始训练的时候，应该用比较小的学习率，然后训练过程稳定之后切回到最初的初始学习率。我们可以像Goval建议的那样做一个学习率预热，最开始用0学习率，然后随着训练的过程，把学习率线性增加到初始学习率。比如，假设我们用头m个batch做这个预热的过程，**初始学习率设置为η，那么在batch i(1 ≤ i ≤ m)的时候，学习率设置为iη/m**，让学习率逐步增大到初始学习率η；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3. BN层γ用零初始化：ResNet的残差块中，非等量映射的那一支的最后一层可能是BN层。在常规初始化策略中，这个BN层的γ和β一般分别初始化为1和0，但是如果把γ初始化为0的话，那样**残差块就相当于没有了，整个网络的层数也相当于减少了**，这样在训练的最初阶段更好训练一点。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>4. 不要bias decay：**只在卷积层和全连接层做weight decay(L2正则化)**，其他参数，例如bias和BN层的γ和β不要做正则。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 低精度训练</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f67857b53375f7fa76c42eed07464948_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1239\" data-rawheight=\"276\" class=\"origin_image zh-lightbox-thumb\" width=\"1239\" data-original=\"https://pic1.zhimg.com/v2-f67857b53375f7fa76c42eed07464948_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1239&#39; height=&#39;276&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1239\" data-rawheight=\"276\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1239\" data-original=\"https://pic1.zhimg.com/v2-f67857b53375f7fa76c42eed07464948_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f67857b53375f7fa76c42eed07464948_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>一般都用fp32的浮点数来训练模型，但是在某些GPU上有增强的算数逻辑单元可以处理低精度数据类型。例如Nvidia</p><p>V100在fp32运算时提供了14TFLOPS的算力，在fp16运算时算力可以达到100TFLOPS(提升了6+倍)。如上图所示，在V100低精度配置下，就算用了更大的batch size，训练速度也有2~3倍的提升。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在低精度训练时也要注意，不要产生数值溢出问题干扰了训练过程。Micikevicius等建议**用fp16表示权重和激活值，梯度计算也用fp16，同时，在参数更新的时候，所有的参数转换为fp32来表示**。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-87d8f2d151db8ca28fded5f4b601de4f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"595\" data-rawheight=\"331\" class=\"origin_image zh-lightbox-thumb\" width=\"595\" data-original=\"https://pic4.zhimg.com/v2-87d8f2d151db8ca28fded5f4b601de4f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;595&#39; height=&#39;331&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"595\" data-rawheight=\"331\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"595\" data-original=\"https://pic4.zhimg.com/v2-87d8f2d151db8ca28fded5f4b601de4f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-87d8f2d151db8ca28fded5f4b601de4f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>把上面说的那些训练手段做消融实验的结果如上图，可以看到，**那些Trick对模型的性能最终的好坏其实没有多大影响，最主要的还是帮助用大bacth size加低精度的fp16做训练这一点，可以让训练速度加快，更快拿到实验结果**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 训练细节微调</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 余弦学习率</p><p class=\"ztext-empty-paragraph\"><br/></p><p>训练中学习率的调整策略是至关重要的。按照一定比例在一定数量epoch后缩小学习率的step decay这种策略最为常见。这里也可以用按照余弦函数来将学习率从初始学习率降为0的策略。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如果一轮epoch中有T个mini batch，那么在第t个mini batch时的学习率ηt可以表示为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6cb83ed8639842d7dacb63eee032ff52_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"278\" data-rawheight=\"77\" class=\"content_image\" width=\"278\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;278&#39; height=&#39;77&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"278\" data-rawheight=\"77\" class=\"content_image lazy\" width=\"278\" data-actualsrc=\"https://pic3.zhimg.com/v2-6cb83ed8639842d7dacb63eee032ff52_b.jpg\"/></figure><p>。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0ba6c3d85d62372d085d64e895ab7561_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"592\" data-rawheight=\"568\" class=\"origin_image zh-lightbox-thumb\" width=\"592\" data-original=\"https://pic2.zhimg.com/v2-0ba6c3d85d62372d085d64e895ab7561_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;592&#39; height=&#39;568&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"592\" data-rawheight=\"568\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"592\" data-original=\"https://pic2.zhimg.com/v2-0ba6c3d85d62372d085d64e895ab7561_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0ba6c3d85d62372d085d64e895ab7561_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>用余弦学习率和step decay学习率相比较的训练如上图所示(都有几轮的学习率预热过程)。可以看到，step decay对于精度的提升更快，但是如果训练充分的话，**余弦学习率和step decay最终达到的精度都差不多的**。在李沐老师这个文章中，并没有看出来余弦学习率的优势所在（相反还有点劣势），原文也是说“Compared to the step decay, the cosine decay starts to decay the learning since the beginning but remains large until step decay reduces the learning rate by 10x, which **potentially** improves the training progress.”，可能在别的模型上效果更明显吧。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 标签平滑</p><p class=\"ztext-empty-paragraph\"><br/></p><p>假设Softmax层输入叫score，输出叫probability。如果标签是采用one-hot形式的，那么为了达到损失函数最小，必然会促使probability_i = 1,其他的都是0。等于说**促使score_i等于无穷大**，其余的score分量为0。这样很大值的值出现很可能会导致网络的过拟合。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ad404b055224cfa1d84194bccb299cb8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"335\" data-rawheight=\"93\" class=\"content_image\" width=\"335\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;335&#39; height=&#39;93&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"335\" data-rawheight=\"93\" class=\"content_image lazy\" width=\"335\" data-actualsrc=\"https://pic1.zhimg.com/v2-ad404b055224cfa1d84194bccb299cb8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>解决这一问题的办法是将one-hot的标签做一下如上图所示的处理，使其由hard-target变为soft-target。这样，最优解的socre就变成了</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c239d3a0be996484496b07ecc1922d67_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"505\" data-rawheight=\"97\" class=\"origin_image zh-lightbox-thumb\" width=\"505\" data-original=\"https://pic4.zhimg.com/v2-c239d3a0be996484496b07ecc1922d67_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;505&#39; height=&#39;97&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"505\" data-rawheight=\"97\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"505\" data-original=\"https://pic4.zhimg.com/v2-c239d3a0be996484496b07ecc1922d67_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c239d3a0be996484496b07ecc1922d67_b.jpg\"/></figure><p>。不同score分量的最大gap = log((K − 1)(1 − eps)/eps) ，当eps变大，gap就会变小。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 知识蒸馏</p><p class=\"ztext-empty-paragraph\"><br/></p><p>用知识蒸馏来提升网络性能，这一点在[《D#0031-知识蒸馏Knowledge-Distillation》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230031-%25E7%259F%25A5%25E8%25AF%2586%25E8%2592%25B8%25E9%25A6%258FKnowledge-Distillation/D%25230031.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)已有详细介绍，本文不再解释。不过，做知识蒸馏的时候，最好用结构相似(**same family**)的大小网络来做，效果会更有保证一点。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Mixup数据增强训练</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7b2408887a8a2c1ff797e861b5a0f2fd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"251\" data-rawheight=\"67\" class=\"content_image\" width=\"251\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;251&#39; height=&#39;67&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"251\" data-rawheight=\"67\" class=\"content_image lazy\" width=\"251\" data-actualsrc=\"https://pic2.zhimg.com/v2-7b2408887a8a2c1ff797e861b5a0f2fd_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>除了传统的数据增强手段，Mixup数据增强用上图所示的公式同时加权求和训练样本**和其对应的标签**得到新的训练数据。并（丢弃原来的训练数据）只用新的训练数据做训练。Mixup得到的新样本比原始单独的样本更难一点，所以训练的过程也会长一点。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文简单地总结了李沐老师在论文《Bag of Tricks for Image Classification with Convolutional Neural Networks》总介绍的一些训练经验和技巧，有的技巧可能在某些情况下不是很通用，可以在自己的项目里有筛选的借鉴。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [Bag of Tricks for Image Classification with Convolutional Neural Networks](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1812.01187\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Bag of Tricks for Image Classification with Convolutional Neural Networks</a>)</p><p>+ [《D#0031-知识蒸馏Knowledge-Distillation》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230031-%25E7%259F%25A5%25E8%25AF%2586%25E8%2592%25B8%25E9%25A6%258FKnowledge-Distillation/D%25230031.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p>+ [Highly scalable deep learning training system with mixed-precision: Training imagenet in four minutes](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1807.11205.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1807.1120</span><span class=\"invisible\">5.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1706.02677\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</a>)</p><p>+ [Training and investigating residual net](<a href=\"https://link.zhihu.com/?target=http%3A//torch.ch/blog/2016/02/04/resnets.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Torch | Training and investigating Residual Nets</a>)</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "人工智能", 
                    "tagLink": "https://api.zhihu.com/topics/19551275"
                }, 
                {
                    "tag": "训练", 
                    "tagLink": "https://api.zhihu.com/topics/19554288"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/83456743", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 25, 
            "title": "CNN可视化之类激活热力图Grad-CAM", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>卷积神经网络因为其在很多任务上效果很好但是其学到的内容和规则很难用人类理解的方式来呈现（相对于传统机器学习算法，例如决策树或者逻辑回归等），所以被很多人认为是“黑盒”。如果我们可以可视化</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 网络模型里面的**中间层的激活**结果；</p><p>2. 或者网络学到的**过滤器**是提取什么类型特征的；</p><p>3. 或者是图像中哪些位置的像素对输出有着强烈的影响，换句话说，**输出对哪些位置的像素值比较敏感**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>那么无疑对我们理解模型，或者分析模型为什么犯错有着很实际的用处。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>所幸的是，2013年以来，随着深度学习的发展，也有一些很好的论文研究深度学习中模型可视化这一领域并取得了一些成果。例如，2014年ECCV上Zeiler的《Visualizing and Understanding Convolutional Networks》研究了第一个问题，此文也是可视化这一领域的开山之作；2012年NIPS上Alex Krizhevsky发表的AlexNet应该是最早可视化过滤器的文章；而针对第三个问题，2016年的**CAM** 《Learning deep features for discriminative localization》，2017年的**Grad-CAM** 《Grad-CAM:Visual Explanations from Deep Networks via Gradient-based Localization》和紧接着2018年的**Grad-CAM++** 《Grad-CAM++: Generalized Gradient-based Visual Explanations for Deep Convolutional Networks》这一系列的文章都对**类激活图**(**C**lass **A**ctivation **M**ap)这种可视化技术做了比较充分的研究。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文结合论文和有关资料，详细描述一些Grad-CAM这种可视化的方法，它除了原理简单，效果好之外，还有个很大的优点就是**不需要重新训练模型也不需要改造模型结构**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## Grad CAM的基本思路推导</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 先说什么是CAM</p><p class=\"ztext-empty-paragraph\"><br/></p><p>CAM是**C**lass **A**ctivation **M**ap类激活热力图的意思，比如说，一个分类网络AlexNet，输入一个既包含着一只狗，又包含着一只猫的猫狗合影图片，它会输出一个1000维度的概率向量，其中有两个分量分别对应着图片分类为猫和图片分类为狗的概率。那么这两个概率，与图片中的哪些部分的关系更大，那些部分的关系更小呢？换句话讲，输出的概率的大小与输入图片中哪些区域的像素值更**敏感**呢？如果找出这个敏感度的一个热力图，越敏感的地方，温度越高，越不敏感的地方，温度月底，那就是类激活热力图，如下图中c和i所示就分别时预测猫类别的CAM和预测狗类别的CAM，它很直观地告诉了我们模型是“看到了”猫的身体所以才认为图中有猫，“看到了”狗的脸才认为图中有狗。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-574f14990bd0100fc7b76ed280aa9a21_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1236\" data-rawheight=\"549\" class=\"origin_image zh-lightbox-thumb\" width=\"1236\" data-original=\"https://pic2.zhimg.com/v2-574f14990bd0100fc7b76ed280aa9a21_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1236&#39; height=&#39;549&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1236\" data-rawheight=\"549\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1236\" data-original=\"https://pic2.zhimg.com/v2-574f14990bd0100fc7b76ed280aa9a21_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-574f14990bd0100fc7b76ed280aa9a21_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>### 再说怎么计算CAM</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-967c149fd46524c360935113beb71baf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1230\" data-rawheight=\"550\" class=\"origin_image zh-lightbox-thumb\" width=\"1230\" data-original=\"https://pic4.zhimg.com/v2-967c149fd46524c360935113beb71baf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1230&#39; height=&#39;550&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1230\" data-rawheight=\"550\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1230\" data-original=\"https://pic4.zhimg.com/v2-967c149fd46524c360935113beb71baf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-967c149fd46524c360935113beb71baf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图所示，计算CAM的过程很简单，这里以计算上上图中猫类的CAM为例说明：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>首先，计算最后一层Softmax输出中猫类概率yc对最后一层特征图所有像素Aij的偏导数，即</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c19b5625e1696d23d60d3b320724d138_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"55\" data-rawheight=\"72\" class=\"content_image\" width=\"55\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;55&#39; height=&#39;72&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"55\" data-rawheight=\"72\" class=\"content_image lazy\" width=\"55\" data-actualsrc=\"https://pic1.zhimg.com/v2-c19b5625e1696d23d60d3b320724d138_b.png\"/></figure><p>，其中y是Softmax输出的概率向量，c是猫那一类的序号，A是最后一层卷积层输出的特征图，k是特征图的通道维度的序号，i和j分别时宽高维度的序号。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>然后，把yc对特征图每个像素的偏导数求出来之后，取一次宽高维度上的全局平均</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b2602bcf0ec4daf8a4fb864beb183cfc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"345\" data-rawheight=\"146\" class=\"content_image\" width=\"345\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;345&#39; height=&#39;146&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"345\" data-rawheight=\"146\" class=\"content_image lazy\" width=\"345\" data-actualsrc=\"https://pic1.zhimg.com/v2-b2602bcf0ec4daf8a4fb864beb183cfc_b.jpg\"/></figure><p>。这一步得到的akc就是c类相对于最后一层卷积层输出的特征图的第k个通道的敏感程度。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>第三步，把这个akc当作权重将最后一层特征图加权，线性组合起来。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>第四步，把这个组合起来的值（其实是个二维map），送入ReLU激活函数处理一下输出。第三步和第四步写在一起就是</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-63db30e09980ff3ee90c989c02fede71_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"344\" data-rawheight=\"119\" class=\"content_image\" width=\"344\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;344&#39; height=&#39;119&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"344\" data-rawheight=\"119\" class=\"content_image lazy\" width=\"344\" data-actualsrc=\"https://pic2.zhimg.com/v2-63db30e09980ff3ee90c989c02fede71_b.jpg\"/></figure><p>。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这个L是一个二维map，它就是我们要的类别激活热力图。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这里有几个需要说明的地方，</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 原始论文《Grad-CAM:Visual Explanations from Deep Networks via Gradient-based Localization》中说yc应该是“the score for class c”，“before the softmax”，但是一些实现代码里面，直接用的是Softmax层的输出，我觉得也是可以的，这里是忠于了文章的思想但是没有完全终于原文（私以为原文是为了让它的Grad-CAM方法适用于更多的问题领域），尽信书不如无书；</p><p>2. 计算akc时用到的global average pooling这个方法比较粗糙，在后面2018年出来的Grad-CAM++论文中有进一步的细化和推广；</p><p>3. 为什么第四步里面要加一个ReLU？因为我们需要得到的是和c类概率得分**正相关**的热力图，如果特征图A上某个位置通过a加权得到的值为负数，那么它更可能属于不是c类的别的类。另外，基于这个前提，我们是不是也可以去取一个f(x) = min(0,x)这样，来看看图片中哪些位置最**抑制**预测c类的分数的提高呢？挺有意思的。</p><p>4. 因为最后一层特征图的分辨率不一定和输入图片一致，为了得到和输入图片一样大小的热力图，最后还有个resize操作，不要忘记了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## Grad CAM的代码实现过程</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Chollet的《Python深度学习》这本书5.4节有比较**简洁的实现**(也有GitHub上的Jupyter notebook)，在GitHub上jacobgil分别用Keras和PyTorch也实现了Grad-CAM。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文简介了类激活可视化方法Grad-CAM来可视化深度学习模型是怎么做预测的，它对我们理解模型原理，做错误Case分析都有很大的帮助。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [Visualizing and Understanding Convolutional Networks](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1311.2901\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Visualizing and Understanding Convolutional Networks</a>)</p><p>+ [Understanding Neural Networks Through Deep Visualization](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1506.06579\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Understanding Neural Networks Through Deep Visualization</a>)</p><p>+ [Learning deep features for discriminative localization](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1512.04150.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1512.0415</span><span class=\"invisible\">0.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [Grad-CAM:Visual Explanations from Deep Networks via Gradient-based Localization](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1610.02391\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization</a>)</p><p>+ [Grad-CAM++: Generalized Gradient-based Visual Explanations for Deep Convolutional Networks](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1710.11063v1\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Grad-CAM++: Generalized Gradient-based Visual Explanations for Deep Convolutional Networks</a>)</p><p>+ [《Python深度学习》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/30293801/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Python深度学习 (豆瓣)</a>)</p><p>+ [pytorch-grad-cam](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/jacobgil/pytorch-grad-cam\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">jacobgil/pytorch-grad-cam</a>)</p><p>+ [keras-grad-cam](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/jacobgil/keras-grad-cam\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">jacobgil/keras-grad-cam</a>)</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "可视化", 
                    "tagLink": "https://api.zhihu.com/topics/19635471"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/83456418", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 4, 
            "title": "知识蒸馏Knowledge Distillation", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>作为模型压缩和加速的经典方法之一，知识蒸馏(Knowledge Distillation)是不得不了解的一个领域。14年NIPS上由Google 的Hinton发表的《[Distilling the Knowledge in a Neural Network](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1503.02531\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Distilling the Knowledge in a Neural Network</a>)》是首次提出知识蒸馏这个概念的文章，本文对这个文章做一个初步理解。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 知识蒸馏的基本思路</p><p class=\"ztext-empty-paragraph\"><br/></p><p>一般来讲，越是复杂的网络，参数越多，计算量越大，其性能越好；越是小的网络，越难训练到大网络那么好的性能。或者用一些大的模型来做集成学习（例如最简单地投票）来提升整体性能。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 提升性能和落地部署不要用相同的模型</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在工作中，一般常见的开发范式是，以一个大模型结构，在很大的数据上，训练出一个性能很好的模型，然后在部署阶段，也部署这个模型以预测实际情况下的数据。作者这里认为，部署用的模型和训练提高性能用的模型，其实应用场合不一样，**用一样的模型是不对的**！应该用不一样的模型（训练用复杂大模型，目标为提高性能；而部署用小模型，目标是为了速度和节约资源），就好像很多昆虫有专门为了从环境中提取能量和营养的幼体形态和专门为了迁徙和繁殖的成虫形态一样，我们也应该用大模型来学习抽取大规模，强冗余的训练集信息，然后利用新的训练方法（这里称之为**蒸馏**）把大模型学到的知识转移到一个更适合部署的小模型上。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 大模型的Softmax输出概率里面富含知识</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这里有一个在概念上的认识障碍，就是我们通常认为模型学到的知识是体现在它学习到的具体的参数中的，而这些参数我们又很难轻易精简。我们可以从更上一层的概念想一下，其实我们学到的知识，**是一个函数，它把样本的输入向量映射到输出向量**。例如，我们的VGG模型，就是把输入的224x224x3的图片向量映射成1000维的输出的每类概率向量。而在训练VGG的过程中，我们会最大化真值那一类的概率。但是，我们模型输出的结果，还有个另外的作用，就是为不是真值的那些类也赋予了一定的概率，虽然这些概率一般比较小，但是也有些是比其他的大一些的，**这相对大小里面也包含了很多大模型学习到的信息**！比如，输入给训练好的VGG模型一个宝马轿车图片，虽然只有很小的概率会被分类为垃圾车，但是这个概率怎么来说也会比VGG将其分类成一个胡萝卜的概率大许多倍吧，这里这些错误分类的概率里面，其实也包含了大模型VGG是怎么分类的大量的有用信息的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 我们要学的不是真值标签！要学的其实是泛化能力！</p><p class=\"ztext-empty-paragraph\"><br/></p><p>我们训练模型的目标函数越能反应真实场景下用户所要达到的目标越好。用户的目标是模型的泛化能力好，对未见过的样本要正确分类。然而由于条件所限，我们一般把提升模型的泛化能力这个目标简化为训练模型在训练集上对真值标签的预测能力，我们也认为，训练得到的模型对真值标签的预测能力越强，它的泛化能力也应该越强，这也是很合理的。但是，**如果我们已经有了一个泛化能力很强的模型，那么为什么不去用一个小模型直接学习这个很强的模型的泛化能力呢**？那不是舍本逐末吗？</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 怎么学习泛化能力呢？知识蒸馏！</p><p class=\"ztext-empty-paragraph\"><br/></p><p>那么问题来了。如果我们有了一个大模型，泛化能力很好，我们怎么才能让一个小模型去学习它的泛化能力呢？换句话说，怎么让大模型的泛化能力转移到小模型身上去呢？这里就用到了论文里面介绍的方法：知识蒸馏。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>简单地说，知识蒸馏就是把大模型对样本输出的概率向量作为软目标“soft targets”，去让小模型的输出尽量去和这个软目标靠（原来是和One-hot编码上靠）。知识蒸馏过程所用的训练样本可以和训练大模型用的训练样本一样，或者另找一个独立的Transfer集也行。**因为“soft targets”比One-hot编码所携带的信息更多**，所以我们在**训练小模型时可以用比训练大模型时更少的训练集和更大的学习率**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 当“soft targets”携带信息太少怎么办？用高温T煮出来！</p><p class=\"ztext-empty-paragraph\"><br/></p><p>想象有一种情况，比如对于类似于MNIST这样的简单任务来说，大模型表现地很好，它输出的soft targets几乎和真值的One-hot编码一样（真值那一位有一个巨接近1.0的概率，其余各位概率都很小），大多数的关于大模型泛化能力的信息都集中在那些值很小的概率上。比如，某个2的测试图像，输出它是3的概率为10^-6，是7的概率为10^-9，而另外一个2的测试图像，输出它是3的概率为10^-9而是7的概率为10^-6。很显然，这些很小的概率（10^-6和10^-9）体现了两个测试图上2的写法和3更像还是和7更像，它们是携带了很重要的信息的。但是在Transfer阶段计算交叉熵的时候，由于它们的值很小，他们对于Transfer阶段的目标函数的影响很小，这是不应该的！怎么样解决这个问题呢？怎么样放大这些小概率值里面所携带的信息呢？[Caruana](<a href=\"https://link.zhihu.com/?target=https%3A//www.cs.cornell.edu/~caruana/compression.kdd06.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">cs.cornell.edu/~caruana</span><span class=\"invisible\">/compression.kdd06.pdf</span><span class=\"ellipsis\"></span></a>)所采取的方法是用logits（Softmax的输入）作为学习的“Soft targets”（而不是前面Hinton说的Softmax的输出），然后计算大模型的logits和小模型的logits的均方差来优化小模型。而我们本文说的Hinton，用了一种更通用的称之为蒸馏的办法，**引入温度参数T去放大（蒸馏出来）这些小概率值所携带的信息**。后文我们会证明，Caruana教授的方法只是Hinton大牛方法的一个特例。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 关于训练集</p><p class=\"ztext-empty-paragraph\"><br/></p><p>用来做知识蒸馏训练小模型的训练集合可以是无标签的数据，也可以是最初用来训练大模型的数据（反正我们要学的是模型的泛化能力，知道模型的输入输出就行了）。不过，在实际做知识蒸馏的时候，如果**有最初训练大模型的带标签的训练集训练效果会更好**，这时我们可以在训练小模型的目标函数里面加入让小模型也预测出训练样本标签的目标函数（原始目标函数是让小模型预测出大模型的输出Soft target）。而且，有时候小模型直接去学习输出Soft target有难度，加上让它去学习样本真值的损失，去诱导小模型的输出往样本真值上靠也会对学习Soft target有帮助作用。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 蒸馏过程</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 训练和预测</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0191aaee35986af34e09adecb5ed1807_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1249\" data-rawheight=\"564\" class=\"origin_image zh-lightbox-thumb\" width=\"1249\" data-original=\"https://pic4.zhimg.com/v2-0191aaee35986af34e09adecb5ed1807_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1249&#39; height=&#39;564&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1249\" data-rawheight=\"564\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1249\" data-original=\"https://pic4.zhimg.com/v2-0191aaee35986af34e09adecb5ed1807_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0191aaee35986af34e09adecb5ed1807_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图所示为怎么做知识蒸馏的示意图。左边和中间的图为训练过程，右边的图为预测过程。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>首先第一步，肯定是训练一个大的模型，让它在训练集上性能良好（有一个好的大模型是知识蒸馏的前提）；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>然后是用这个大模型来训练小模型。在训练时，对样本xl，大模型的倒数第二层先除以一个温度T，然后通过Softmax预测一个软目标Soft target，小模型也一样，倒数第二层除以同样的温度T，然后通过Softmax预测一个结果，再把这个结果和软目标的交叉熵作为训练的total loss的一部分。训练的total loss的另一部分是正常的输出和真值标签(hard target)的交叉熵。Total loss把这两个损失**加权**合起来做为训练小模型的最终的loss。在小模型训练好了，预测的时候，就不需要再有温度T了，直接按照常规的Softmax输出就可以了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 再来说说T的取值</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这里的温度T是干什么的，前面已经提到了，就是引入温度T去放大小概率所携带的信息。可以用下面这个图来解释：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-61827e54f6a0259539952c57d7b904ff_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"629\" data-rawheight=\"379\" class=\"origin_image zh-lightbox-thumb\" width=\"629\" data-original=\"https://pic4.zhimg.com/v2-61827e54f6a0259539952c57d7b904ff_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;629&#39; height=&#39;379&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"629\" data-rawheight=\"379\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"629\" data-original=\"https://pic4.zhimg.com/v2-61827e54f6a0259539952c57d7b904ff_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-61827e54f6a0259539952c57d7b904ff_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-eda69ac444515579e2f7a54273b881d7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"216\" data-rawheight=\"79\" class=\"content_image\" width=\"216\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;216&#39; height=&#39;79&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"216\" data-rawheight=\"79\" class=\"content_image lazy\" width=\"216\" data-actualsrc=\"https://pic4.zhimg.com/v2-eda69ac444515579e2f7a54273b881d7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上面第二张图中的公式是怎么由倒数第二层的zi产生软目标qi，上面第一张图是在不同的温度T的取值下，各个横轴的zi得到的纵轴的qi的变化规律。可以看出有连个值得关注的点：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 当温度T越高的时候，软目标越平滑，信息不会集中在少数分量上，这样增大温度参数T相当于放大（蒸馏出来）这些小概率值分量所携带的信息；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2. 第二点，不管温度T怎么取值，Soft target都有忽略小的zi携带的信息的倾向（产生的Prob小）</p><p class=\"ztext-empty-paragraph\"><br/></p><p>至于T在实际训练时候的取值，这还比较玄学，T取值小，那就忽略了小的zi所携带的信息；T取值大一点，那就放大了小的zi所携带的信息（但这些信息，也可能是噪音）。 Which of these effects dominates is an **empirical question**。但是从经验上来说，当小模型相对于大模型和任务来说非常非常小的时候，取一个适中（偏小）的温度T效果最好。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Hinton首次提出知识蒸馏这个概念并且引入了温度系数来进行操作。知识蒸馏可以作为模型加速和压缩的方法用。本文重点结合原始论文讲了知识蒸馏的来龙去脉和提出思路，后面简要介绍了知识蒸馏的大致过程。具体在操作中还有一些细节（例如温度的选择，数据集的选择）没有涉及很深入，需要在具体应用时再去深入看。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [Distilling the Knowledge in a Neural Network](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1503.02531\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Distilling the Knowledge in a Neural Network</a>)</p><p>+ [ModelCompression](<a href=\"https://link.zhihu.com/?target=https%3A//www.cs.cornell.edu/~caruana/compression.kdd06.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">cs.cornell.edu/~caruana</span><span class=\"invisible\">/compression.kdd06.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [知识蒸馏、在线蒸馏](<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/xbinworld/article/details/83063726\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/xbinworld</span><span class=\"invisible\">/article/details/83063726</span><span class=\"ellipsis\"></span></a>)</p><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "加速", 
                    "tagLink": "https://api.zhihu.com/topics/19609182"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/83221858", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 0, 
            "title": "类MTCNN的360RIP人脸检测器PCN", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>最近在看一些RIP 360度人脸检测方法时看到了一篇中科院山世光老师的《Real-Time Rotation-Invariant Face Detection with　**P**rogressive **C**alibration **N**etworks》，简称PCN，发表于2018年的CVPR。顾名思义，这个人脸检测器有两个显著特点：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. **速度快**，可以在CPU和GPU平台做到实时性；</p><p>2. **旋转不变形**，可以检测RIP内360度旋转的人脸；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>看了内容之后，发现其思路和训练方式都和以前做过的MTCNN思路很像，都是三个小网络做Cascade，分类和回归多任务训练，其和MTCNN最大的不同是：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. FCN为了做到360度人脸都检测，**去掉了MTCNN预测5点landmark的任务，换成了子网络加上了额外的预测人脸角度的任务，逐步旋转图像做角度校正**；</p><p>2. FCN第一个网络采用的是滑窗加多层金字塔的形式穷举人脸可能的位置，不如MTCNN生成热力图的方式做的聪明；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 检测旋转人脸常用的三个招式</p><p class=\"ztext-empty-paragraph\"><br/></p><p>人脸检测是一个基础而且通用的功能，学界工业界都对其做了很多的研究，其中Rotation-In-Plane的人脸检测，是一个比正脸人脸检测更具有挑战性的任务。为了做RIP360度的人脸检测，通常会从三个方面着手：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e481ba7465d721ea35b7dbcf8cab2ffe_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"525\" data-rawheight=\"254\" class=\"origin_image zh-lightbox-thumb\" width=\"525\" data-original=\"https://pic3.zhimg.com/v2-e481ba7465d721ea35b7dbcf8cab2ffe_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;525&#39; height=&#39;254&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"525\" data-rawheight=\"254\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"525\" data-original=\"https://pic3.zhimg.com/v2-e481ba7465d721ea35b7dbcf8cab2ffe_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e481ba7465d721ea35b7dbcf8cab2ffe_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>1. **数据增强Data Augmentation**：数据增强来做旋转人脸检测，就是把各种各样的人脸样本在图像平面内各个方向上做旋转，生成许多的旋转人脸加入训练集中去训练网络，企图使**一个网络**(如上图)自动学习通用于各种旋转人脸的特征来检测旋转的人脸。这种方法的优点是，可以直接套用到现有的人脸检测网络中去直接训一把看看效果，缺点也很明显，它网络要学习到更加通用更加对旋转不变的特征，学习任务变难了，势必要增加网络的容量，也势必计算量大网络大，耗时很多。总之，这种大力出奇迹简单粗暴的方法做旋转人脸检测，在实践中不多见；</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7369a6689245c5906cb26fc394bf6fe2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"575\" data-rawheight=\"235\" class=\"origin_image zh-lightbox-thumb\" width=\"575\" data-original=\"https://pic3.zhimg.com/v2-7369a6689245c5906cb26fc394bf6fe2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;575&#39; height=&#39;235&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"575\" data-rawheight=\"235\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"575\" data-original=\"https://pic3.zhimg.com/v2-7369a6689245c5906cb26fc394bf6fe2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7369a6689245c5906cb26fc394bf6fe2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>2. **分治法Divide-and-Conquer**：这个思路也很清晰，既然一个大的网络做360度人脸检测太吃力了，那么能不能用几个小的人脸检测网络，各自负责检测一定角度范围内的人脸，然后把各自检测出来的人脸再合并一下不就是所有角度的人脸了吗？例如上图，用四个小检测器检测上下左右四个方位的人脸，然后合并。这样做理论上也是可以的，不过需要四个网络，需要检测四次，总的时间怕也不会少。在CNN发展到现在这么强的时候去这样做，也显得不够聪明；</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-837ac0d06291fb28074e96d27128dd88_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"888\" data-rawheight=\"365\" class=\"origin_image zh-lightbox-thumb\" width=\"888\" data-original=\"https://pic1.zhimg.com/v2-837ac0d06291fb28074e96d27128dd88_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;888&#39; height=&#39;365&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"888\" data-rawheight=\"365\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"888\" data-original=\"https://pic1.zhimg.com/v2-837ac0d06291fb28074e96d27128dd88_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-837ac0d06291fb28074e96d27128dd88_b.jpg\"/></figure><p>3. **旋转路由Rotation Router**：既然旋转人脸检测与普通向上的人脸检测难点主要来自于人脸是旋转的，那么很自然的，如果能先预测出人脸的旋转角度，然后把它转成向上的人脸，再用普通的正脸人脸检测器去做检测(分类和回归)不就可以了吗？示意图如上，使用滑窗法和图像金字塔穷举所有可能出现人脸的位置，送入**一个**Router网络，预测出旋转角度，使用这个角度旋转人脸，再送入正脸检测器得到最终结果。这种方式，要求Router网络能精确预测旋转角度，这是一个困难的问题，必然会使网络增大，而预测不了精确的角度，又会使最终召回率降低。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## PCN的思路</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### A Big Picture</p><p class=\"ztext-empty-paragraph\"><br/></p><p>为了降低角度预测的难度，PCN使用三个Cascade网络递进预测人脸角度，递进地旋转人脸，使得每个网络不需要预测得很准，任务难度降低，更快更好；再配合MTCNN的人脸分类和框回归任务来做人脸检测，网络的大致示意图如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7937e96c6d06ef18898a79a7741e1d54_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1491\" data-rawheight=\"722\" class=\"origin_image zh-lightbox-thumb\" width=\"1491\" data-original=\"https://pic1.zhimg.com/v2-7937e96c6d06ef18898a79a7741e1d54_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1491&#39; height=&#39;722&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1491\" data-rawheight=\"722\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1491\" data-original=\"https://pic1.zhimg.com/v2-7937e96c6d06ef18898a79a7741e1d54_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7937e96c6d06ef18898a79a7741e1d54_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>PCN网络分为三个Cascade子网络，每个子网络分别负责一个阶段。首先给定一个输入图片，依然通过滑窗和金字塔穷举所有可能的人脸位置，送入第一阶段PCN-1。第一阶段负责预测一个角度信息打分，告诉这个人脸是向下的还是向上的，如果向下的，则旋转180度，如果向上的，则不变。第一阶段保证送入第二个阶段PCN-2的人脸都是向上的(向左向右也算向上)。然后第二阶段PCN-2拿到向上的图后，继续预测这个人脸是正上，还是向左、向右，根据判断结果，继续调整人脸到正向上。PCN-3拿到正方向的人脸后，预测一个更精细的旋转角度打分，并回归出人脸框位置。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>上面没有仔细介绍的是，每个网络都也会做人脸/非人脸的分类和框回归任务，可以在中途过滤掉一些非人脸；每个网络都会做角度预测，但是只有前两个预测会去旋转人脸，第三个预测不用再去旋转人脸了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>三个阶段由粗到细地分类人脸，回归人脸框，预测角度，最后总结并输出人脸的角度和框。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 细节分析</p><p class=\"ztext-empty-paragraph\"><br/></p><p>上面的大致介整个PCN网络的处理流水线，下面我们仔细看看组成它的三个子网络，PCN-1，PCN-2和PCN-3。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-837090a95b37de251e43708140fb6f6a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"691\" data-rawheight=\"157\" class=\"origin_image zh-lightbox-thumb\" width=\"691\" data-original=\"https://pic3.zhimg.com/v2-837090a95b37de251e43708140fb6f6a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;691&#39; height=&#39;157&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"691\" data-rawheight=\"157\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"691\" data-original=\"https://pic3.zhimg.com/v2-837090a95b37de251e43708140fb6f6a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-837090a95b37de251e43708140fb6f6a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上面是PCN-1的结构图，PCN-1的任务主要有三个：1. 人脸/非人脸分类；2. 人脸框回归；3. 预测人脸角度。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这个网络的主要任务还是人脸分类，对于人脸分类，使用常用的交叉熵损失函数</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4350a68f0184b0f7d09ddac304c9498e_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"421\" data-rawheight=\"51\" class=\"origin_image zh-lightbox-thumb\" width=\"421\" data-original=\"https://pic3.zhimg.com/v2-4350a68f0184b0f7d09ddac304c9498e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;421&#39; height=&#39;51&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"421\" data-rawheight=\"51\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"421\" data-original=\"https://pic3.zhimg.com/v2-4350a68f0184b0f7d09ddac304c9498e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4350a68f0184b0f7d09ddac304c9498e_b.png\"/></figure><p>优化。对于第二个任务，人脸框回归，使用Smooth L1损失函数来</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-57e7da9479efccdc4d188126f15f8bc6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"284\" data-rawheight=\"51\" class=\"content_image\" width=\"284\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;284&#39; height=&#39;51&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"284\" data-rawheight=\"51\" class=\"content_image lazy\" width=\"284\" data-actualsrc=\"https://pic3.zhimg.com/v2-57e7da9479efccdc4d188126f15f8bc6_b.jpg\"/></figure><p>优化，其中S是Smooth L1损失函数，t是</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5284fbfbc94eb93950443829ff4d8a3d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"426\" data-rawheight=\"123\" class=\"origin_image zh-lightbox-thumb\" width=\"426\" data-original=\"https://pic2.zhimg.com/v2-5284fbfbc94eb93950443829ff4d8a3d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;426&#39; height=&#39;123&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"426\" data-rawheight=\"123\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"426\" data-original=\"https://pic2.zhimg.com/v2-5284fbfbc94eb93950443829ff4d8a3d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5284fbfbc94eb93950443829ff4d8a3d_b.jpg\"/></figure><p>代表宽高和中心点的偏移，t\\*是对应的要预测是真值。对于第三个任务，预测人脸是向上还是向下，是一个二分类任务，还是使用二分类交叉熵损失函数</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ad79c657dabff2378b641242e00419e6_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"412\" data-rawheight=\"51\" class=\"content_image\" width=\"412\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;412&#39; height=&#39;51&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"412\" data-rawheight=\"51\" class=\"content_image lazy\" width=\"412\" data-actualsrc=\"https://pic3.zhimg.com/v2-ad79c657dabff2378b641242e00419e6_b.png\"/></figure><p>。最后，综合这三个任务的损失韩式，并加上调整因子，就得到了训练PCN-1的目标函数了</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5389f4394bc99ee814ba0051af5e5644_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"497\" data-rawheight=\"61\" class=\"origin_image zh-lightbox-thumb\" width=\"497\" data-original=\"https://pic1.zhimg.com/v2-5389f4394bc99ee814ba0051af5e5644_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;497&#39; height=&#39;61&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"497\" data-rawheight=\"61\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"497\" data-original=\"https://pic1.zhimg.com/v2-5389f4394bc99ee814ba0051af5e5644_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-5389f4394bc99ee814ba0051af5e5644_b.png\"/></figure><p>。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在训练好PCN-1网络后，可以用这个网络的人脸非人脸分类能力先滤掉大量的候选框。对于剩下的它认为是人脸的候选框，利用输出的角度预测值，用</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ae7acdc66b9fa12ec85185c2eadd8d4d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"290\" data-rawheight=\"96\" class=\"content_image\" width=\"290\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;290&#39; height=&#39;96&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"290\" data-rawheight=\"96\" class=\"content_image lazy\" width=\"290\" data-actualsrc=\"https://pic2.zhimg.com/v2-ae7acdc66b9fa12ec85185c2eadd8d4d_b.jpg\"/></figure><p>判断人脸朝向并旋转人脸。这里需要注意一点，网络输出的是一个打分g，代表它认为输入是向上的人脸(g&gt;0.5)还是向下的人脸(g&lt;0.5)，实际得到的是一个**二值化的判断**，要么是0度旋转要么是180度旋转，而不是根据g打分再算一个别的什么值。这样做的原因前面已经讲过了，控制任务的难度，控制网路复杂度。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7e733189e041c60be6c8fff2d79218c0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"689\" data-rawheight=\"162\" class=\"origin_image zh-lightbox-thumb\" width=\"689\" data-original=\"https://pic1.zhimg.com/v2-7e733189e041c60be6c8fff2d79218c0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;689&#39; height=&#39;162&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"689\" data-rawheight=\"162\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"689\" data-original=\"https://pic1.zhimg.com/v2-7e733189e041c60be6c8fff2d79218c0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7e733189e041c60be6c8fff2d79218c0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>讲完了PCN-1，继续来看PCN-2。上面是PCN-2的结构图，PCN-2的任务同样有三个：1. 人脸/非人脸分类；2. 人脸框回归；3. 预测人脸角度。前面两个功能和PCN-1的大差不离，主要是第三个预测人脸角度有点不一样。这里PCN-2输出三个打分来预测人脸角度(PCN-1是2个打分)，预测方式如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-22df83ad62f124f09ea2ac99cdc3a2d2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"306\" data-rawheight=\"201\" class=\"content_image\" width=\"306\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;306&#39; height=&#39;201&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"306\" data-rawheight=\"201\" class=\"content_image lazy\" width=\"306\" data-actualsrc=\"https://pic3.zhimg.com/v2-22df83ad62f124f09ea2ac99cdc3a2d2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>它预测人脸是向左偏、正中间还是向右偏。然后再根据预测的结果调整图片给下一个网络PCN-3。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d27153c655c0f46959df458d3424f8bc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"158\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pic1.zhimg.com/v2-d27153c655c0f46959df458d3424f8bc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;690&#39; height=&#39;158&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"158\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"690\" data-original=\"https://pic1.zhimg.com/v2-d27153c655c0f46959df458d3424f8bc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d27153c655c0f46959df458d3424f8bc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>继续看PCN-3，如上图。同样滴，也做三个事情，同样的，主要的不同也在角度预测上。这里PCN-3网络预测的结果不是打分了，而**直接是偏转的角度**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这样，PCN-1输出的角度判断，加上PCN-2输出的角度判断，再加上PCN-3输出的角度预测值，就是原始图像上的人脸偏转角度了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 数据分析</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 网络性能</p><p class=\"ztext-empty-paragraph\"><br/></p><p>作者将数据增强策略用在了Faster R-CNN，RFCN和SSD500；实现了一个Cascade CNN，通过旋转图片的形式跑四次；第三，用PCN-1作为Router网络接一个1998年的人脸检测器[Rotation invariant neural network-based face detection](<a href=\"https://link.zhihu.com/?target=https%3A//www.researchgate.net/publication/3758608_Rotation_Invariant_Neural_Network-Based_Face_Detection\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">researchgate.net/public</span><span class=\"invisible\">ation/3758608_Rotation_Invariant_Neural_Network-Based_Face_Detection</span><span class=\"ellipsis\"></span></a>)。并和以上几个对比方法得出的结果如下</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-67b0289dfe8bc6cc1cca11ebe1f62be1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1479\" data-rawheight=\"914\" class=\"origin_image zh-lightbox-thumb\" width=\"1479\" data-original=\"https://pic2.zhimg.com/v2-67b0289dfe8bc6cc1cca11ebe1f62be1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1479&#39; height=&#39;914&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1479\" data-rawheight=\"914\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1479\" data-original=\"https://pic2.zhimg.com/v2-67b0289dfe8bc6cc1cca11ebe1f62be1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-67b0289dfe8bc6cc1cca11ebe1f62be1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>可以看到，以FDDB为测试集，在误报数量为100的条件下，FCN是召回率最高的方法。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-9461b3e573bb9bfb86f39bd109012955_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"617\" data-rawheight=\"432\" class=\"origin_image zh-lightbox-thumb\" width=\"617\" data-original=\"https://pic2.zhimg.com/v2-9461b3e573bb9bfb86f39bd109012955_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;617&#39; height=&#39;432&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"617\" data-rawheight=\"432\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"617\" data-original=\"https://pic2.zhimg.com/v2-9461b3e573bb9bfb86f39bd109012955_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-9461b3e573bb9bfb86f39bd109012955_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在WIDER FACE上，当误报数量为50的条件下，PCN也是召回率最高的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 网络速度</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在FDDB上，误报数量设置为100个，输入640x480的VGA分辨率图片，最小人脸设置为40x40时，速度测量结果如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6437b0da5509c6e2875a4fe1cd15cde6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1475\" data-rawheight=\"457\" class=\"origin_image zh-lightbox-thumb\" width=\"1475\" data-original=\"https://pic3.zhimg.com/v2-6437b0da5509c6e2875a4fe1cd15cde6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1475&#39; height=&#39;457&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1475\" data-rawheight=\"457\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1475\" data-original=\"https://pic3.zhimg.com/v2-6437b0da5509c6e2875a4fe1cd15cde6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6437b0da5509c6e2875a4fe1cd15cde6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>可以看到，PCN速度和Cascade CNN差不多，比Faster R-CNN(VGG16)，SSD500(VGG16)和R-FCN(ResNet-50)快不少。模型文件的size也比较小。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文介绍的PCN网络是一个RIP360度检测的网络，它通过在MTCNN的思路上去掉五点预测加上角度预测，来一步步地校正图像来实现RIP360度的人脸检测，从数据上来看，**性能不是最好的**(不说比最新的RetinaFace，连FaceBoxes都比不上)，**速度也不是最快的**，但是**优势在旋转不变**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [Real-Time Rotation-Invariant Face Detection withProgressive Calibration Networks](<a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/papers/Shi_Real-Time_Rotation-Invariant_Face_CVPR_2018_paper.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">openaccess.thecvf.com/c</span><span class=\"invisible\">ontent_cvpr_2018/papers/Shi_Real-Time_Rotation-Invariant_Face_CVPR_2018_paper.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [《D#0017-MTCNN和FaceBoxes》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230017-MTCNN%25E5%2592%258CFaceBoxes/D%25230017.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "人脸识别", 
                    "tagLink": "https://api.zhihu.com/topics/19559196"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/83221558", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 36, 
            "title": "CV中的注意力机制", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>注意力机制最早在自然语言处理和机器翻译对齐文本中提出并使用，并取得了不错的效果。在计算机视觉领域，也有一些学者探索了在视觉和卷积神经网络中使用注意力机制来提升网络性能的方法。注意力机制的基本原理很简单：它认为，网络中每层不同的（可以是不同通道的，也可以是不同位置的，都可以）特征的重要性不同，后面的层应该更注重其中重要的信息，抑制不重要的信息。比如，性别分类中，应该更注意人的头发长短、胸部隆起情况这些和性别关系大的特征的抽取和判断，而不是去注意人体和性别关系不大的，像腰部粗细、身高头部比例，这些特征。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文以2018年在ECCV发表的论文《CBAM: Convolutional Block Attention Module》为具体例子介绍计算机视觉中注意力机制的应用。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 网络设计考虑的三个点：深度、宽度和感受野</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在工程师和研究者设计新的网络的时候，为了提升网络的容量和性能，通常会从三个方面着手：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 增加网络深度：例如，从开始的LeNet-5，到VGG-16，到ResNet-101，等等。网络的设计越来越深，性能也越来越好。网路深了，简单地讲，下一层的输出是上一层的线性组合加激活，可以复合出更多更灵活的特征（其实就是更复杂的复合函数嘛）；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2. 增加网络宽度：这里的网络宽度即特征图的通道数。这里的典型例子可以从LeNet-5，到VGG-16，Wider ResNet。增加网络宽度，特征图通道数多起来了，更多的卷积核可以得到更多更丰富的特征，网络的表达能力自然就强了；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3. 丰富网络感受野：这里可以参考Inception中的不同分辨率的卷积核组成的小网络，还有FPN，SSD里面的各种变种，都有增加感受野的多样性来提升性能的路子。感受野不同，对不同大小的目标的特征提取和表达能力就不同，一般是特征图上感受野大的像素更能代表大的目标上面提取的信息，感受野小的像素，只能看到大目标的一部分；反过来想，感受野大的像素，看到的视野太大，对于小目标就掺杂了太多的冗余信息和噪音。所以，使用不同分辨率的感受野，丰富网络的感受野来提升网络，让大感受野处理大目标，小感受野处理小目标，各司其职，来提升网络性能也就成了很多网络设计的自然手段了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>当然，很多网络设计也是同时考虑组合这三者进行更加精细的设计，思想本质还是一样的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>CBAM和上面提的三种思路都不同，它不增加网络深度，不增加网络的宽度，也不增加不同Kernel size的卷积核；它在现有的特征图上做有选择地更精细地调整（标定？）来提升网络的性能。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## CBAM模块介绍</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### A Big Picture</p><p class=\"ztext-empty-paragraph\"><br/></p><p>CBAM模块的构造很也简单，如下图所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f3d41d27e69e5e5ba09defd2f3605021_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1018\" data-rawheight=\"440\" class=\"origin_image zh-lightbox-thumb\" width=\"1018\" data-original=\"https://pic2.zhimg.com/v2-f3d41d27e69e5e5ba09defd2f3605021_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1018&#39; height=&#39;440&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1018\" data-rawheight=\"440\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1018\" data-original=\"https://pic2.zhimg.com/v2-f3d41d27e69e5e5ba09defd2f3605021_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f3d41d27e69e5e5ba09defd2f3605021_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>首先给定一个输入特征图Input Feature，首先通过输入特征图，计算出一个通道注意力模块Channel Attention Module，然后按照通道顺序乘以这个模块；然后继续计算出空间注意力模块Spatial Attention Module，继续按照宽高维度乘以这个模块，最后输出经过调整的特征图Refined Feature。在这里注意，并不是谁规定了Channel Attention Module一定要在Spatial Attention Module前面，而是作者**通过实验**得出在前面会更好些的这个结论。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 细节分析</p><p class=\"ztext-empty-paragraph\"><br/></p><p>上面的大致介绍了CBAM的通道注意力机制和空间注意力机制是怎么样串联起来一个流水线的，这里深入它的两个注意力模块介绍一下。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5801f5c6f7d6f2aaee1a6c5b4264e31d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1457\" data-rawheight=\"353\" class=\"origin_image zh-lightbox-thumb\" width=\"1457\" data-original=\"https://pic2.zhimg.com/v2-5801f5c6f7d6f2aaee1a6c5b4264e31d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1457&#39; height=&#39;353&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1457\" data-rawheight=\"353\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1457\" data-original=\"https://pic2.zhimg.com/v2-5801f5c6f7d6f2aaee1a6c5b4264e31d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5801f5c6f7d6f2aaee1a6c5b4264e31d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>1. **Channel Attention Module**: 首先来看通道注意力模块，首先，输入特征图F按照取每个通道的Max Pooling和Global Avg Pooling的结果，然后**分别**送入一个三层感知机MLP，输出结果直接加起来，然后送入ReLU激活函数，得到通道注意力模块的特征图Mc。Mc怎么使用在Input Featre列成公式这个过程就是：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9e9c7ce5e94f858a87823ac4b64095b6_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"732\" data-rawheight=\"103\" class=\"origin_image zh-lightbox-thumb\" width=\"732\" data-original=\"https://pic3.zhimg.com/v2-9e9c7ce5e94f858a87823ac4b64095b6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;732&#39; height=&#39;103&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"732\" data-rawheight=\"103\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"732\" data-original=\"https://pic3.zhimg.com/v2-9e9c7ce5e94f858a87823ac4b64095b6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9e9c7ce5e94f858a87823ac4b64095b6_b.png\"/></figure><p>，因为前面的讲解很清晰，而且过程也并不复杂，这里就不一一介绍各个符号的意义了；</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5b9c1fbd36fec04956908d6100edbe2f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1136\" data-rawheight=\"393\" class=\"origin_image zh-lightbox-thumb\" width=\"1136\" data-original=\"https://pic4.zhimg.com/v2-5b9c1fbd36fec04956908d6100edbe2f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1136&#39; height=&#39;393&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1136\" data-rawheight=\"393\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1136\" data-original=\"https://pic4.zhimg.com/v2-5b9c1fbd36fec04956908d6100edbe2f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5b9c1fbd36fec04956908d6100edbe2f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>2. **Spatial Attention Module**: 在通道注意力模块执行完之后，中间结果特征图会被送入空间注意力模块继续处理，得到运用了空间注意力机制的最终的特征图。如上图所示，由通道特征图机制处理后的特征图F&#39;(其实就是前面的Mc(F))，首先在**通道维度(扫描宽高)**上计算出Global Max Pooling和Global Average Pooling的两张特征图，将其Concate起来，然后用一个传统卷积处理一次，最后使用Sigmoid激活就得到了空间注意力模块用到的特征图Ms，Ms怎么用到F&#39;上可以参考</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-922a00c2722852aed7ed33a8db3e336f_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"616\" data-rawheight=\"97\" class=\"origin_image zh-lightbox-thumb\" width=\"616\" data-original=\"https://pic4.zhimg.com/v2-922a00c2722852aed7ed33a8db3e336f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;616&#39; height=&#39;97&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"616\" data-rawheight=\"97\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"616\" data-original=\"https://pic4.zhimg.com/v2-922a00c2722852aed7ed33a8db3e336f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-922a00c2722852aed7ed33a8db3e336f_b.png\"/></figure><p>。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 在ResNet结构应用CBAM</p><p class=\"ztext-empty-paragraph\"><br/></p><p>CBAM的结构小巧灵活，在AlexNet或者VGG中怎么用是显而易见的，这里介绍SENet在更新的ResNet结构中的用法，其余结构也是类似的。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-467780293340b8793419f4218498d448_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1527\" data-rawheight=\"533\" class=\"origin_image zh-lightbox-thumb\" width=\"1527\" data-original=\"https://pic1.zhimg.com/v2-467780293340b8793419f4218498d448_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1527&#39; height=&#39;533&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1527\" data-rawheight=\"533\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1527\" data-original=\"https://pic1.zhimg.com/v2-467780293340b8793419f4218498d448_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-467780293340b8793419f4218498d448_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>对于ResNet，CBAM模块嵌入到残差结构中的残差学习分支中(直接嵌入在identity map中，那还叫ResNet吗？嘿嘿)。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 进一步思考：发(水)论文的正确姿势</p><p class=\"ztext-empty-paragraph\"><br/></p><p>瞎扯时间到，最近连续写了三篇文章，[《D#0027-聊聊2017 ImageNet夺冠的SENet》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230027-%25E8%2581%258A%25E8%2581%258A2017%2520ImageNet%25E5%25A4%25BA%25E5%2586%25A0%25E7%259A%2584SENet/D%25230027.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)，[《D#0028-再聊SENet的孪生兄弟SKNet》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230028-%25E5%2586%258D%25E8%2581%258ASENet%25E7%259A%2584%25E5%25AD%25AA%25E7%2594%259F%25E5%2585%2584%25E5%25BC%259FSKNet/D%25230028.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)和本篇[《D#0029-CV中的注意力机制》](#CV中的注意力机制)。两篇CVPR和一篇ECCV，在CV界都是很好的会议。三个论文的着眼点不一样，但是思路其实是一样的，SENet把注意力机制用到了通道维度，对特征图通道做调整；SKNet把注意力机制用到了Kernel Size的选择上，对不同Kernel Size出来的特征图做加权；而本文中介绍的CBAM则把注意力机制分别用到了通道维度和分辨率维度，再将其组合起来了。Taste不是很高，Insight也不是很深刻，但也算是做了前人没有做的工作。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>所以，多看Paper，就可以看到很多论文的方法就是在各种排列组合和更灵活的扩展泛化，然后做实验证明这种排列组合有效。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文通过CBAM这个具体的例子，介绍了注意力机制及其在计算机视觉领域的应用。最后还对于计算机视觉领域的科研创新做出了进一步的思(tu)考(cao)。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [CBAM: Convolutional Block Attention Module](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1807.06521\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CBAM: Convolutional Block Attention Module</a>)</p><p>+ [《D#0027-聊聊2017 ImageNet夺冠的SENet》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230027-%25E8%2581%258A%25E8%2581%258A2017%2520ImageNet%25E5%25A4%25BA%25E5%2586%25A0%25E7%259A%2584SENet/D%25230027.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p>+ [《D#0028-再聊SENet的孪生兄弟SKNet》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230028-%25E5%2586%258D%25E8%2581%258ASENet%25E7%259A%2584%25E5%25AD%25AA%25E7%2594%259F%25E5%2585%2584%25E5%25BC%259FSKNet/D%25230028.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "注意力机制", 
                    "tagLink": "https://api.zhihu.com/topics/20682987"
                }
            ], 
            "comments": [
                {
                    "userName": "燃烧的括约肌", 
                    "userLink": "https://www.zhihu.com/people/3144d558815a84e6b7df837fc1f91943", 
                    "content": "<p>请教一下答主，注意力分支和某特征图两者在resnet的相加短接有什么含义？为什么不是单纯获取注意力乘到原特征图上</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "船长", 
                            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
                            "content": "<p>你问的是『在ResNet结构应用CBAM』这节的最后一幅图那个加号吗？</p>", 
                            "likes": 0, 
                            "replyToAuthor": "燃烧的括约肌"
                        }, 
                        {
                            "userName": "燃烧的括约肌", 
                            "userLink": "https://www.zhihu.com/people/3144d558815a84e6b7df837fc1f91943", 
                            "content": "<p>是的。。。其实类似的还有nonlocal，senet等一系列注意力模型的相加操作，还是resnet的思路吗</p>", 
                            "likes": 0, 
                            "replyToAuthor": "船长"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/83221337", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 0, 
            "title": "再聊SENet的孪生兄弟SKNet", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>上节[《聊聊2017 ImageNet夺冠的SENet》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230027-%25E8%2581%258A%25E8%2581%258A2017%2520ImageNet%25E5%25A4%25BA%25E5%2586%25A0%25E7%259A%2584SENet/D%25230027.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)对SENet这种在通道维度(channel-wise)进行显示学习来做信息调整的结构做了介绍。SENet的思想(显示学习不同通道特征图的重要性来标定不同通道)如果用在Kernal Size这个维度会怎样呢？不同尺寸的Kernal学习到的特征图，能不能用一种显示的方式来学习他们的输出权重进行标定融合呢？发表在2019 CVPR上的论文[Selective Kernel Networks](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1903.06586\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Selective Kernel Networks</a>)对此做了一些有益的分析，此文也是Momenta出的，可谓和SENet思路一脉相承，就像孪生兄弟一下，这里也简单介绍一下。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 从Inception到SKNet</p><p class=\"ztext-empty-paragraph\"><br/></p><p>SKNet和SENet的渊源关系在前面引言部分已经交代清楚了，这里讲讲SKNet和经典的Inception结构之间的关系。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-23b4302da66b5745380857c48503754c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"700\" data-rawheight=\"378\" class=\"origin_image zh-lightbox-thumb\" width=\"700\" data-original=\"https://pic1.zhimg.com/v2-23b4302da66b5745380857c48503754c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;700&#39; height=&#39;378&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"700\" data-rawheight=\"378\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"700\" data-original=\"https://pic1.zhimg.com/v2-23b4302da66b5745380857c48503754c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-23b4302da66b5745380857c48503754c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图所示的是基本的Inception结构，该结构在上一层的输入上用多个不同Kernal size的卷积核进行卷积加上max polling操作得到输出的各个特征图，然后各个特征图Concate起来组合成输出的特征图。不同Kernal size的特征图上的像素点具有不同大小的感受野，所表达的信息在空间上大小不同，这样就丰富了所提取的特征，加强了信息的丰富程度和特征的表达能力，进而提升了网络的性能。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>但是，这样的Concate还是觉得有点粗暴，能不能用类似于SENet的显示学习的方法，根据特征图的内容，去学习一套更精细的组合规则(系数)呢？而且，根据神经学的研究也发现，单个神经元的感受野的大小也不是固定不变的，而是根据输入的刺激的大小来做相应调整的。这就是SKNet要做的事情。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## SKNet模块介绍</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### A Big Picture</p><p class=\"ztext-empty-paragraph\"><br/></p><p>SKNet模块的构造很简单，以最简单的两个不同尺寸卷积核为例，如下图所示：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e3c40dd3a25a6adc507cb5b5e19f376a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1477\" data-rawheight=\"418\" class=\"origin_image zh-lightbox-thumb\" width=\"1477\" data-original=\"https://pic3.zhimg.com/v2-e3c40dd3a25a6adc507cb5b5e19f376a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1477&#39; height=&#39;418&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1477\" data-rawheight=\"418\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1477\" data-original=\"https://pic3.zhimg.com/v2-e3c40dd3a25a6adc507cb5b5e19f376a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e3c40dd3a25a6adc507cb5b5e19f376a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>首先给定一个输入特征图X，用两个不同尺寸的卷积核对其进行卷积得到黄色和绿色特征图，然后把这两个特征图加起来得到特征图U，对这个特征图U进行Global average polling得到特征图S，进行全连接得到Z，然后Softmax Attention得到两组权重向量a,b，用这两组权重向量对前面最先得到的黄色绿色特征图进行加权求和得到SKNet模块的输出V。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 细节分析</p><p class=\"ztext-empty-paragraph\"><br/></p><p>上面的讨论还是比较抽象，这里分成三个阶段Split, Fuse和Select来一个个分析SKNet结构的实现细节。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. **Split**: 首先是Split操作，Split操作用两个不同尺寸的卷积核对同一个输入X分别做卷积，得到两个特征图（上图中3x3卷积核生成黄色特征图，5x5卷积核生成绿色特征图）。在这里，为了减少计算量，卷积可以采用分组卷积的方式，后面加上BN层和ReLU激活函数；更进一步，可以采用dilation = 2的3x3空洞卷积作为这里的5x5卷积在不增加计算量的前提下得到更大的感受野；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2. **Fuse**: 再看中间，是Fuse操作。这里先把前面卷积出来的蓝色和绿色特征图加起来，然后和SENet类似地，用一个Global average pooling操作在空间维度压缩特征图S，获取全局感受野，然后对S采用全连接</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b1d9f4ed12d988156811534aafb448ec_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"315\" data-rawheight=\"53\" class=\"content_image\" width=\"315\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;315&#39; height=&#39;53&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"315\" data-rawheight=\"53\" class=\"content_image lazy\" width=\"315\" data-actualsrc=\"https://pic1.zhimg.com/v2-b1d9f4ed12d988156811534aafb448ec_b.jpg\"/></figure><p>(先做全连接，再做BN，最后ReLU)的方式得到Z，当然，这里Z的长度也是可以用超参数</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cab4673d8ed4e5007560c7223e6c2028_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"220\" data-rawheight=\"44\" class=\"content_image\" width=\"220\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;220&#39; height=&#39;44&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"220\" data-rawheight=\"44\" class=\"content_image lazy\" width=\"220\" data-actualsrc=\"https://pic1.zhimg.com/v2-cab4673d8ed4e5007560c7223e6c2028_b.jpg\"/></figure><p>控制的；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3. **Select**: 前得到的Z只是得到了一个包含不同感受野特征图和空间上全局信息的编码，接下来需要另外一种变换来抓取各个卷积核的特征图之间的关系。这里作者用了Softmax Attention机制：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7a53c194ca824c2ee2159d1983ff5989_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"467\" data-rawheight=\"86\" class=\"origin_image zh-lightbox-thumb\" width=\"467\" data-original=\"https://pic2.zhimg.com/v2-7a53c194ca824c2ee2159d1983ff5989_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;467&#39; height=&#39;86&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"467\" data-rawheight=\"86\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"467\" data-original=\"https://pic2.zhimg.com/v2-7a53c194ca824c2ee2159d1983ff5989_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7a53c194ca824c2ee2159d1983ff5989_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>   这里ac和bc是互补关系，如果想在更多的核做Select，那么就可以扩展到更多的Softmax输出。这里得到的ac和bc作为权重，对开始左边的黄色特征图和绿色特征图做加权求和，可以得到最终SKNet模块的输出特征图V。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 我的一点困惑</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在前面的SENet的分析中，对于SENet的Excitation步骤，用的是**Sigmoid**来把1x1xC的特征转换成1x1xC的权重，论文给的理由是各个Channel的特征不一定要互斥，这样允许有更大的灵活性(例如可以允许多个通道同时得到比较大的权值)，原文如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-13bfb37faac71bbfb6010f8a7b2c17fd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"598\" data-rawheight=\"261\" class=\"origin_image zh-lightbox-thumb\" width=\"598\" data-original=\"https://pic2.zhimg.com/v2-13bfb37faac71bbfb6010f8a7b2c17fd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;598&#39; height=&#39;261&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"598\" data-rawheight=\"261\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"598\" data-original=\"https://pic2.zhimg.com/v2-13bfb37faac71bbfb6010f8a7b2c17fd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-13bfb37faac71bbfb6010f8a7b2c17fd_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>但是，在SKNet里面，为什么就要用Softmax这种one-hot前提的形式的函数来转换呢？难道作者认为不同Kernal Size得到的特征图是mutually-exclusive的吗？这一点我觉得作者没解释得很清楚，也可能是我理解错了，如果哪位读者有好的理解，欢迎探讨。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文介绍的SKNet模块通过显示地学习一组权重，可以在一定程度上对不同的输入自适应地融合不同尺寸的卷积核得到的不同感受野的特征图的信息。思路是和SENet差不多，只不过SENet是显示地学习Channel-wise的权重来重标定通道特征图，它把SENet的思想迁移到了Inception上来显示地学习不同Kernal Size的特征图的权重。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [Squeeze-and-Excitation Networks](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1709.01507\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Squeeze-and-Excitation Networks</a>)</p><p>+ [SENet GitHub](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/hujie-frank/SENet\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">hujie-frank/SENet</a>)</p><p>+ [Going Deeper with Convolutions](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1409.4842\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Going Deeper with Convolutions</a>)</p><p>+ [Selective Kernel Networks](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1903.06586\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Selective Kernel Networks</a>)</p><p>+ [depthwise separable convolutions in mobilenet](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230004-depthwise_separable_convolutions_in_mobilenet/D%25230004.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p>+ [《聊聊2017 ImageNet夺冠的SENet》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230027-%25E8%2581%258A%25E8%2581%258A2017%2520ImageNet%25E5%25A4%25BA%25E5%2586%25A0%25E7%259A%2584SENet/D%25230027.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "模型", 
                    "tagLink": "https://api.zhihu.com/topics/19579715"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/83220942", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 3, 
            "title": "聊聊2017 ImageNet夺冠的SENet", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>SENet是由Momenta出的一种在通道维度(channel-wise)进行信息调整的结构，在2017年的ImageNet分类任务中夺得了第一名的成绩(SENet-154 top-5 error为2.251%)。SENet网络结构思路清晰，实现简单，适用范围广(可以很方便地将其植入ResNet, MobileNet或者Inception中)，它不但对于提高分类性能有作用，而且也可以把使用了SENet结构的分类网络作为backbone来提升网络的特征提取能力，对提升分割和检测任务的性能也有帮助。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 传统卷积结构的局限性</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8c91a1a9490149ee6a5c0ff491a71b33_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"340\" data-rawheight=\"175\" class=\"content_image\" width=\"340\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;340&#39; height=&#39;175&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"340\" data-rawheight=\"175\" class=\"content_image lazy\" width=\"340\" data-actualsrc=\"https://pic4.zhimg.com/v2-8c91a1a9490149ee6a5c0ff491a71b33_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图所示的是传统卷积结构，传统卷积利用一组3D立体filter在w, h空间维度和c的通道维度提取聚合上一层特征图的信息组成新的特征图。卷积神经网络则是由一系列串联的(当然，局部会有并联)卷积层、非线性激活层和下采样层等构成，这样它们能够从全局感受野上去捕获图像的特征来进行图像的描述。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>但是由于优化算法的能力有限或者是数据有限等等原因，要直接从这样的网络(搜索空间太巨大)中学习出强劲的参数是非常困难的。于是，类似于Inception结构，膨胀卷积(Dilated Conv)等等这样利用丰富感受野、扩大感受野这样的结构被提出来，也取得了明显的效果。但是这些手段只是着眼于在空间维度(W和H维)对网络结构进行了改造，而且取得了不错的效果；而像分组卷积(Group Conv)和[深度卷积(Depth-wise Conv)](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230004-depthwise_separable_convolutions_in_mobilenet/D%25230004.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)这样的卷积，确实在通道维度上做了改造，但是主要也不是提升性能而是提升速度的，而且分组的手段略显粗暴；很自然的就会想，能不能在通道维度(C维)来改造网络，自动学习一组参数对上层通道输出进行调整来提升网络性能呢？有，这就是SENet。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## SENet模块介绍</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### A Big Picture</p><p class=\"ztext-empty-paragraph\"><br/></p><p>SENet模块的构造很简单，如下图所示：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e4e0ff31648a5d6e02b76e8491402446_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"359\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-e4e0ff31648a5d6e02b76e8491402446_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;359&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"359\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-e4e0ff31648a5d6e02b76e8491402446_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e4e0ff31648a5d6e02b76e8491402446_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>首先给定一个输入特征图x，其特征通道数为c1，通过一系列传统卷积Ftr等一般变换后得到一个特征通道数为c2的特征图。与传统的CNN不一样的是，接下来通过三个操作在通道维度上来**重标定**前面得到的特征图。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>看上面的支路，首先是Squeeze操作Fsq，顺着空间维度(W,H维度)来进行特征压缩，将每个二维的特征通道变成一个实数，这个实数某种程度上具有全局的感受野，并且输出的维度和输入的特征通道数相匹配。它表征着在特征通道上响应的全局分布，而且使得靠近输入的层也可以获得全局的感受野，这一点在很多任务中都是非常有用的。紧接着是Excitation操作Fex，它通过一组显示学习到的参数来将Squeeze的输出转换为一组标定用的权值。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>最后是一个Reweighting的操作Fscale，就是一个在**通道维度的缩放操作**，将Excitation的输出的权重看做是进过特征选择后的每个特征通道的重要性，然后通过乘法**逐通道加权**到先前的特征上，完成在通道维度上的对原始特征的重标定。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 细节分析</p><p class=\"ztext-empty-paragraph\"><br/></p><p>上面的讨论还是比较抽象，这里来一个个分析SENet结构的实现细节。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. Ftr: 首先是Ftr操作，这是一个传统卷积操作，输出c2xhxw形状的特征图，由于传统卷积对各个channel的卷积结果做了sum，所以channel特征关系与卷积核学习到的空间关系混合在一起。**而SE模块就是为了抽离这种混合，使得模型直接学习到channel特征关系**；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2. Squeeze: 然后是Fsq操作，上面一步的卷积Ftr只是在一个局部空间内进行操作，很难获得足够的信息来提取channel之间的关系(上面解释了，由于sum操作混合了空间维度信息和通道维度信息)，为此，SENet提出Squeeze操作，将一个channel上整个空间特征编码为一个全局特征，论文中是采用global average pooling来实现，但原则上也可以采用更复杂的编码策略；这一层的输出是c2x1x1的特征图；</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8f35eb5e56f04b1b06f1625707c144ea_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"416\" data-rawheight=\"85\" class=\"content_image\" width=\"416\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;416&#39; height=&#39;85&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"416\" data-rawheight=\"85\" class=\"content_image lazy\" width=\"416\" data-actualsrc=\"https://pic3.zhimg.com/v2-8f35eb5e56f04b1b06f1625707c144ea_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>3. Excitation: 前看的Squeeze操作只是得到了一个包含空间上全局信息的编码，接下来需要另外一种变换来抓取各个channel之间的关系。这个变换需要满足两个准则：首先要灵活，它要可以学习到各个channel之间的非线性关系；第二点是学习的关系不是互斥的，因为这里允许多channel特征，而不是one-hot形式(所以不能用Softmax，Softmax的假设输入的各个类别应该都是互斥关系)。基于此，这里采用Sigmoid形式的gating机制：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9b33b12257ca7467e3b52b13443b9362_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"506\" data-rawheight=\"64\" class=\"origin_image zh-lightbox-thumb\" width=\"506\" data-original=\"https://pic3.zhimg.com/v2-9b33b12257ca7467e3b52b13443b9362_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;506&#39; height=&#39;64&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"506\" data-rawheight=\"64\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"506\" data-original=\"https://pic3.zhimg.com/v2-9b33b12257ca7467e3b52b13443b9362_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9b33b12257ca7467e3b52b13443b9362_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>   其中W1是C通道输入，C/r通道输出的全连接层的权重，W2是C/r通道输入，C通道输出的全连接的权重，r是控制模型复杂度的超参数。内层激活函数是ReLU，外层激活函数是Sigmoid(为了输出范围在0~1的权重)。因为通道缩放系数r大于0，所以这个Excitation操作可以看成是两个全连接层组成的bootneck结构。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>4. Fscale: 在学习到了通道权重后，用Fscale操作来把原来特征图的权重做一次**缩放(而不是个通道再做融合)**，也可以认为是重标定操作。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f1b7563522ab2d852f4586d828301070_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"304\" data-rawheight=\"49\" class=\"content_image\" width=\"304\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;304&#39; height=&#39;49&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"304\" data-rawheight=\"49\" class=\"content_image lazy\" width=\"304\" data-actualsrc=\"https://pic1.zhimg.com/v2-f1b7563522ab2d852f4586d828301070_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>## 在Inception和ResNet结构应用SENet</p><p class=\"ztext-empty-paragraph\"><br/></p><p>SENet的结构小巧灵活，在AlexNet或者VGG中怎么用是显而易见的，这里介绍SENet在更新的Inception和ResNet结构中的用法，其余结构也是类似的。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b82212100c40389001c9bb9f814e7308_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"942\" class=\"origin_image zh-lightbox-thumb\" width=\"594\" data-original=\"https://pic1.zhimg.com/v2-b82212100c40389001c9bb9f814e7308_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;594&#39; height=&#39;942&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"942\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"594\" data-original=\"https://pic1.zhimg.com/v2-b82212100c40389001c9bb9f814e7308_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b82212100c40389001c9bb9f814e7308_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>对于Inception就是直接嵌入在Inception module之后就行了，好像没什么说的。对于ResNet，SENet模块嵌入到残差结构中的残差学习分支中(直接嵌入在identity map中，那还叫ResNet吗？嘿嘿)。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>同样地，SE模块也可以应用在其它网络结构，如ResNetXt，Inception-ResNet，MobileNet和ShuffleNet中。这里给出SE-ResNet-50和SE-ResNetXt-50的具体结构，如下表所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-44ea529efd2338c4dac59faad3232fbb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1232\" data-rawheight=\"614\" class=\"origin_image zh-lightbox-thumb\" width=\"1232\" data-original=\"https://pic4.zhimg.com/v2-44ea529efd2338c4dac59faad3232fbb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1232&#39; height=&#39;614&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1232\" data-rawheight=\"614\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1232\" data-original=\"https://pic4.zhimg.com/v2-44ea529efd2338c4dac59faad3232fbb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-44ea529efd2338c4dac59faad3232fbb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参数数量和速度分析</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 参数数量分析</p><p class=\"ztext-empty-paragraph\"><br/></p><p>对于一个采用了经典结构的SENet：Global Pooling没有参数；对于第一个FC层，输入C输出C/r，它的参数数量是(C^2)/r；对于ReLU激活层，没有参数；对于第二个FC层，输入C/r通道输出C通道，他的参数数量也是(C^2)/r；最后Sigmoid激活，也没参数。所以，对一个SENet，增加参数数量为2*(C^2)/r个，对一个有S个stage(每个stage又repeat应用了N次SENet结构)都采用了SENet结构的网络，总共增加的参数数量就为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-552ed54a02287a2aab415b8852c8d0ea_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"159\" data-rawheight=\"79\" class=\"content_image\" width=\"159\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;159&#39; height=&#39;79&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"159\" data-rawheight=\"79\" class=\"content_image lazy\" width=\"159\" data-actualsrc=\"https://pic3.zhimg.com/v2-552ed54a02287a2aab415b8852c8d0ea_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>### 速度分析</p><p class=\"ztext-empty-paragraph\"><br/></p><p>对于速度，因为和优化能力有关系，这里只给个论文中对计算量GFLOPS的大概估计：当r=16时，SE-ResNet-50只增加了不到10%的参数量，且计算量（GFLOPS）却增加不到1%。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-9c7a3020a9427e24859a1c272d3b8895_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"622\" data-rawheight=\"159\" class=\"origin_image zh-lightbox-thumb\" width=\"622\" data-original=\"https://pic2.zhimg.com/v2-9c7a3020a9427e24859a1c272d3b8895_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;622&#39; height=&#39;159&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"622\" data-rawheight=\"159\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"622\" data-original=\"https://pic2.zhimg.com/v2-9c7a3020a9427e24859a1c272d3b8895_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-9c7a3020a9427e24859a1c272d3b8895_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>## 模型效果</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如下图所示是作者把SENet结构嵌入到ResNet，ResNeXt和Inception这种流行基础网络后的性能结果，可以看到性能指标都用一定的提升：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e81f7766c6d07b1277da545ef335515a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1237\" data-rawheight=\"473\" class=\"origin_image zh-lightbox-thumb\" width=\"1237\" data-original=\"https://pic3.zhimg.com/v2-e81f7766c6d07b1277da545ef335515a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1237&#39; height=&#39;473&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1237\" data-rawheight=\"473\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1237\" data-original=\"https://pic3.zhimg.com/v2-e81f7766c6d07b1277da545ef335515a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e81f7766c6d07b1277da545ef335515a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在轻量级网络MobileNet和ShuffleNet中，性能也有提升：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0204eb690729cad9de00156007498aa5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1231\" data-rawheight=\"225\" class=\"origin_image zh-lightbox-thumb\" width=\"1231\" data-original=\"https://pic2.zhimg.com/v2-0204eb690729cad9de00156007498aa5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1231&#39; height=&#39;225&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1231\" data-rawheight=\"225\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1231\" data-original=\"https://pic2.zhimg.com/v2-0204eb690729cad9de00156007498aa5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0204eb690729cad9de00156007498aa5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>## 进一步思考和1x1卷积的联系和区别</p><p class=\"ztext-empty-paragraph\"><br/></p><p>继续瞎扯两句。大家都知道1x1卷积也可以做到通道之间特征的融合，那么这里提到的SENet结构和1x1卷积有哪些联系和区别呢？</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. SENet的结构，要做的事情在某种意义上讲它其实是1x1卷积的一个特例：对于C个输入，C个输出的block，理想情况下可以用C个Cx1x1形状的卷积去学，如果学出来一个CxC的对角阵出来了，那其实就是这里SENet要做的事情，对通道进行缩放(或者说重标定)。但是，这是理想情况下，现实中，数据量和优化算法(别忘了这样用1x1卷积你的搜索空间会增加多大)的要求也就更高了。我们设计新的网络，或者结构的时候，很多时候是要根据原来的经验和insight去一点点加上去，一点点试出来的，设计一个超牛逼超复杂的涵盖一切的网络，往往很难优化。要不然根据万能近似定理，三层神经网络就可以拟合一切函数了，为什么不用？</p><p>2. 这里来讲不同。如果不考虑上面特殊的1x1卷积核构成对角矩阵的情况，那么1x1卷积做的是通道之间的融合，而不是SENet做的，通道本身自己的缩放；</p><p>3. 第二个不同，感受野不同：1x1卷积在不同通道之间是按照像素点来逐点加权求和的，融合到的只是局部的信息；而SENet里面第一步加上了全局polling操作，具有全局的感受野，考虑到了全局的信息。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文介绍的SENet模块主要为了**提升模型对channel特征的敏感性**，该模块是轻量级的，而且可以很方便地应用在现有的网络结构中，只需要增加较少的计算量就可以带来性能的提升，在2017年ImageNet的比赛中也取得了冠军名次。另外，在CVPR 2019中Momenta还有一篇与SENet非常相似的网络[Selective Kernel Networks](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1903.06586\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Selective Kernel Networks</a>)，SKNet主要是**提升模型对感受野的自适应能力**，后面有空再专门介绍。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>想到在channel维度做更精细化的操作，想到进行按通道的缩放都很容易，但这只是想到；难的是去用自己积累的知识和从前人那里学到的思路，去构造出一个结构来做缩放而且去验证这个结构很有效果，这需要积累，需要insight，需要不断多次细心尝试，甚至还需要一点运气。这也是一些做出成绩的人和一般人的区别吧，自勉。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [Squeeze-and-Excitation Networks](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1709.01507\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Squeeze-and-Excitation Networks</a>)</p><p>+ [SENet GitHub](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/hujie-frank/SENet\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">hujie-frank/SENet</a>)</p><p>+ [depthwise separable convolutions in mobilenet](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230004-depthwise_separable_convolutions_in_mobilenet/D%25230004.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p>+ [如何评价Momenta ImageNet 2017夺冠架构SENet?](<a href=\"https://www.zhihu.com/question/63460684\" class=\"internal\">如何评价Momenta ImageNet 2017夺冠架构SENet?</a>)</p><p></p>", 
            "topic": [
                {
                    "tag": "ImageNet", 
                    "tagLink": "https://api.zhihu.com/topics/20044591"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/83220498", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 90, 
            "title": "深度学习检测小目标常用方法", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在深度学习目标检测中，特别是人脸检测中，小目标、小人脸的检测由于**分辨率低，图片模糊，信息少，噪音多**，所以一直是一个实际且常见的困难问题。不过在这几年的发展中，也涌现了一些提高小目标检测性能的解决手段，本文对这些手段做一个分析、整理和总结。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 传统的图像金字塔和多尺度滑动窗口检测</p><p class=\"ztext-empty-paragraph\"><br/></p><p>最开始在深度学习方法流行之前，对于不同尺度的目标，大家普遍使用将原图build出**不同分辨率的图像金字塔**，再对每层金字塔用固定输入分辨率的分类器在该层滑动来检测目标，以求在金字塔底部检测出小目标；或者只用一个原图，在原图上，用**不同分辨率的分类器**来检测目标，以求在比较小的窗口分类器中检测到小目标。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在著名的人脸检测器[MTCNN](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1604.02878\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks</a>)中，就使用了图像金字塔的方法来检测不同分辨率的人脸目标。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-19342542a727133b4df2c90214f54378_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"497\" data-rawheight=\"175\" class=\"origin_image zh-lightbox-thumb\" width=\"497\" data-original=\"https://pic1.zhimg.com/v2-19342542a727133b4df2c90214f54378_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;497&#39; height=&#39;175&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"497\" data-rawheight=\"175\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"497\" data-original=\"https://pic1.zhimg.com/v2-19342542a727133b4df2c90214f54378_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-19342542a727133b4df2c90214f54378_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>不过这种方式速度慢（虽然通常build图像金字塔可以使用卷积核分离加速或者直接简单粗暴地resize，但是还是需要做多次的特征提取呀），后面有人借鉴它的思想搞出了特征金字塔网络FPN，它在不同层取特征进行融合，只需要一次前向计算，不需要缩放图片，也在小目标检测中得到了应用，在本文后面会讲到。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 简单粗暴又可靠的Data Augmentation</p><p class=\"ztext-empty-paragraph\"><br/></p><p>深度学习的效果在某种意义上是靠大量数据喂出来的，小目标检测的性能同样也可以通过增加训练集中小目标样本的种类和数量来提升。在[《深度学习中不平衡样本的处理》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230016-%25E6%25B7%25B1%25E5%25BA%25A6%25E5%25AD%25A6%25E4%25B9%25A0%25E4%25B8%25AD%25E4%25B8%258D%25E5%25B9%25B3%25E8%25A1%25A1%25E6%25A0%25B7%25E6%259C%25AC%25E7%259A%2584%25E5%25A4%2584%25E7%2590%2586/D%25230016.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)一文中已经介绍了许多数据增强的方案，这些方案虽然主要是解决不同类别样本之间数量不均衡的问题的，但是有时候小目标检测之难其中也有数据集中小样本相对于大样本来说数量很少的因素，所以其中很多方案都可以用在小样本数据的增强上，这里不赘述。另外，在19年的论文[Augmentation for small object detection](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1902.07296.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1902.0729</span><span class=\"invisible\">6.pdf</span><span class=\"ellipsis\"></span></a>)中，也提出了两个简单粗暴的方法：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 针对COCO数据集中包含小目标的图片数量少的问题，使用过采样OverSampling策略；</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-060742e905acf4f65e01e098c7384393_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"838\" data-rawheight=\"319\" class=\"origin_image zh-lightbox-thumb\" width=\"838\" data-original=\"https://pic4.zhimg.com/v2-060742e905acf4f65e01e098c7384393_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;838&#39; height=&#39;319&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"838\" data-rawheight=\"319\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"838\" data-original=\"https://pic4.zhimg.com/v2-060742e905acf4f65e01e098c7384393_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-060742e905acf4f65e01e098c7384393_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>2. 针对同一张图片里面包含小目标数量少的问题，在图片内用分割的Mask抠出小目标图片再使用复制粘贴的方法（当然，也加上了一些旋转和缩放，另外要注意不要遮挡到别的目标）。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4c460b27f648baff5bf41544f8d1aebd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"835\" data-rawheight=\"387\" class=\"origin_image zh-lightbox-thumb\" width=\"835\" data-original=\"https://pic2.zhimg.com/v2-4c460b27f648baff5bf41544f8d1aebd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;835&#39; height=&#39;387&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"835\" data-rawheight=\"387\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"835\" data-original=\"https://pic2.zhimg.com/v2-4c460b27f648baff5bf41544f8d1aebd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-4c460b27f648baff5bf41544f8d1aebd_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>在同一张图中有更多的小目标，在Anchor策略的方法中就会匹配出更多的正样本。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c1737fe94464472c3b3cf09061731953_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"839\" data-rawheight=\"908\" class=\"origin_image zh-lightbox-thumb\" width=\"839\" data-original=\"https://pic4.zhimg.com/v2-c1737fe94464472c3b3cf09061731953_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;839&#39; height=&#39;908&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"839\" data-rawheight=\"908\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"839\" data-original=\"https://pic4.zhimg.com/v2-c1737fe94464472c3b3cf09061731953_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c1737fe94464472c3b3cf09061731953_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>## 特征融合的FPN</p><p class=\"ztext-empty-paragraph\"><br/></p><p>不同阶段的特征图对应的感受野不同，它们表达的信息抽象程度也不一样。**浅层的特征图感受野小，比较适合检测小目标（要检测大目标，则其只“看”到了大目标的一部分，有效信息不够）；深层的特征图感受野大，适合检测大目标（要检测小目标，则其”看“到了太多的背景噪音，冗余噪音太多）**。所以，有人就提出了将不同阶段的特征图，都融合起来，来提升目标检测的性能，这就是特征金字塔网络[FPN](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1612.03144\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Feature Pyramid Networks for Object Detection</a>)。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-344ef96c9648f7e3dd801dace80bca77_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"717\" data-rawheight=\"712\" class=\"origin_image zh-lightbox-thumb\" width=\"717\" data-original=\"https://pic4.zhimg.com/v2-344ef96c9648f7e3dd801dace80bca77_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;717&#39; height=&#39;712&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"717\" data-rawheight=\"712\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"717\" data-original=\"https://pic4.zhimg.com/v2-344ef96c9648f7e3dd801dace80bca77_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-344ef96c9648f7e3dd801dace80bca77_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在人脸领域，基本上性能好一点的方法都是用了FPN的思想，其中比较有代表性的有[RetinaFace: Single-stage Dense Face Localisation in the Wild](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1905.00641.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1905.0064</span><span class=\"invisible\">1.pdf</span><span class=\"ellipsis\"></span></a>)</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b55df2b30e842c4864244cbc7cd6dcbc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1489\" data-rawheight=\"404\" class=\"origin_image zh-lightbox-thumb\" width=\"1489\" data-original=\"https://pic1.zhimg.com/v2-b55df2b30e842c4864244cbc7cd6dcbc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1489&#39; height=&#39;404&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1489\" data-rawheight=\"404\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1489\" data-original=\"https://pic1.zhimg.com/v2-b55df2b30e842c4864244cbc7cd6dcbc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b55df2b30e842c4864244cbc7cd6dcbc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>**另外一个思路**：既然可以在不同分辨率特征图做融合来提升特征的丰富度和信息含量来检测不同大小的目标，那么自然也有人会进一步地猜想，如果只用高分辨率的特征图（浅层特征）去检测小脸；用中间分辨率的特征图（中层特征）去检测大脸；最后用地分辨率的特征图（深层特征）去检测小脸。比如人脸检测中的[SSH](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1708.03979.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1708.0397</span><span class=\"invisible\">9.pdf</span><span class=\"ellipsis\"></span></a>)。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d5d3c36beb4ed4968ffe94ca0282a414_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"951\" data-rawheight=\"442\" class=\"origin_image zh-lightbox-thumb\" width=\"951\" data-original=\"https://pic1.zhimg.com/v2-d5d3c36beb4ed4968ffe94ca0282a414_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;951&#39; height=&#39;442&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"951\" data-rawheight=\"442\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"951\" data-original=\"https://pic1.zhimg.com/v2-d5d3c36beb4ed4968ffe94ca0282a414_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d5d3c36beb4ed4968ffe94ca0282a414_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>## 合适的训练方法SNIP,SNIPER,SAN</p><p class=\"ztext-empty-paragraph\"><br/></p><p>机器学习里面有个重要的观点，**模型预训练的分布要尽可能地接近测试输入的分布**。所以，在大分辨率（比如常见的224 x 224）下训练出来的模型，不适合检测本身是小分辨率再经放大送入模型的图片。如果是小分辨率的图片做输入，应该在小分辨率的图片上训练模型；再不行，应该用大分辨率的图片训练的模型上用小分辨率的图片来微调fine-tune；最差的就是直接用大分辨率的图片来预测小分辨率的图（通过上采样放大）。但是这是在理想的情况下的（训练样本数量、丰富程度都一样的前提下，但实际上，很多数据集都是小样本严重缺乏的），所以**放大输入图像+使用高分率图像预训练再在小图上微调，在实践中要优于专门针对小目标训练一个分类器**。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-44c87a58ce77619942ab224f1ae759a1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"503\" class=\"origin_image zh-lightbox-thumb\" width=\"594\" data-original=\"https://pic2.zhimg.com/v2-44c87a58ce77619942ab224f1ae759a1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;594&#39; height=&#39;503&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"503\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"594\" data-original=\"https://pic2.zhimg.com/v2-44c87a58ce77619942ab224f1ae759a1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-44c87a58ce77619942ab224f1ae759a1_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-87857af48b295d7246a042619f810ab3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1242\" data-rawheight=\"330\" class=\"origin_image zh-lightbox-thumb\" width=\"1242\" data-original=\"https://pic4.zhimg.com/v2-87857af48b295d7246a042619f810ab3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1242&#39; height=&#39;330&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1242\" data-rawheight=\"330\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1242\" data-original=\"https://pic4.zhimg.com/v2-87857af48b295d7246a042619f810ab3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-87857af48b295d7246a042619f810ab3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在下图中示意的是SNIP训练方法，**训练时只训练合适尺寸的目标样本，只有真值的尺度和Anchor的尺度接近时来用来训练检测器，太小太大的都不要，预测时输入图像多尺度，总有一个尺寸的Anchor是合适的，选择那个最合适的尺度来预测**。对[R-FCN](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1605.06409\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">R-FCN: Object Detection via Region-based Fully Convolutional Networks</a>)提出的改进主要有两个地方，一是多尺寸图像输入，针对不同大小的输入，在经过RPN网络时需要判断valid GT和invalid GT，以及valid anchor和invalid anchor，通过这一分类，使得得到的预选框更加的准确；二是在RCN阶段，根据预选框的大小，只选取在一定范围内的预选框，最后使用NMS来得到最终结果。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-512f718d2d9f94cfa72105529a34e0dc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1258\" data-rawheight=\"564\" class=\"origin_image zh-lightbox-thumb\" width=\"1258\" data-original=\"https://pic1.zhimg.com/v2-512f718d2d9f94cfa72105529a34e0dc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1258&#39; height=&#39;564&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1258\" data-rawheight=\"564\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1258\" data-original=\"https://pic1.zhimg.com/v2-512f718d2d9f94cfa72105529a34e0dc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-512f718d2d9f94cfa72105529a34e0dc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>SNIPER是SNIP的实用升级版本，这里不做详细介绍了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 更稠密的Anchor采样和匹配策略S3FD,FaceBoxes</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在前面Data Augmentation部分已经讲了，复制小目标到一张图的多个地方可以增加小目标匹配的Anchor框的个数，增加小目标的训练权重，减少网络对大目标的bias。同样，反过来想，如果在数据集已经确定的情况下，我们也可以增加负责小目标的Anchor的设置策略来让训练时对小目标的学习更加充分。例如人脸检测中的[FaceBoxes](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1708.05234\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">FaceBoxes: A CPU Real-time Face Detector with High Accuracy</a>)其中一个Contribution就是Anchor densification strategy，Inception3的anchors有三个scales(32,64,128)，而32 scales是稀疏的，所以需要密集化4倍，而64 scales则需要密集化2倍。在[S3FD](<a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_S3FD_Single_Shot_ICCV_2017_paper.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">openaccess.thecvf.com/c</span><span class=\"invisible\">ontent_ICCV_2017/papers/Zhang_S3FD_Single_Shot_ICCV_2017_paper.pdf</span><span class=\"ellipsis\"></span></a>)人脸检测方法中，则用了Equal-proportion interval principle来保证不同大小的Anchor在图中的密度大致相等，这样大脸和小脸匹配到的Anchor的数量也大致相等了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>另外，对小目标的Anchor使用比较宽松的匹配策略（比如IoU &gt; 0.4）也是一个比较常用的手段。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8f91783597d53f1a4f62931aac2e2ecf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"583\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"583\" data-original=\"https://pic4.zhimg.com/v2-8f91783597d53f1a4f62931aac2e2ecf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;583&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"583\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"583\" data-original=\"https://pic4.zhimg.com/v2-8f91783597d53f1a4f62931aac2e2ecf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8f91783597d53f1a4f62931aac2e2ecf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>## 先生成放大特征再检测的GAN</p><p class=\"ztext-empty-paragraph\"><br/></p><p>[Perceptual GAN]()使用了GAN对小目标生成一个和大目标很相似的Super-resolved Feature（如下图所示），然后把这个Super-resolved Feature叠加在原来的小目标的特征图（如下下图所示）上，以此增强对小目标特征表达来提升小目标（在论文中是指交通灯）的检测性能。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3419f989b1fc4280a1c0487d29c62ad7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"602\" data-rawheight=\"446\" class=\"origin_image zh-lightbox-thumb\" width=\"602\" data-original=\"https://pic4.zhimg.com/v2-3419f989b1fc4280a1c0487d29c62ad7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;602&#39; height=&#39;446&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"602\" data-rawheight=\"446\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"602\" data-original=\"https://pic4.zhimg.com/v2-3419f989b1fc4280a1c0487d29c62ad7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3419f989b1fc4280a1c0487d29c62ad7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ca645230d0956150b255f04ff5746fa0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1263\" data-rawheight=\"697\" class=\"origin_image zh-lightbox-thumb\" width=\"1263\" data-original=\"https://pic1.zhimg.com/v2-ca645230d0956150b255f04ff5746fa0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1263&#39; height=&#39;697&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1263\" data-rawheight=\"697\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1263\" data-original=\"https://pic1.zhimg.com/v2-ca645230d0956150b255f04ff5746fa0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ca645230d0956150b255f04ff5746fa0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>## 利用Context信息的Relation Network和PyramidBox</p><p class=\"ztext-empty-paragraph\"><br/></p><p>小目标，特别是像人脸这样的目标，不会单独地出现在图片中（想想单独一个脸出现在图片中，而没有头、肩膀和身体也是很恐怖的）。像[PyramidBox](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1803.07737\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">PyramidBox: A Context-assisted Single Shot Face Detector</a>)方法，加上一些头、肩膀这样的上下文Context信息，那么目标就相当于变大了一些，上下文信息加上检测也就更容易了。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ed80b3d04202a1df9643e2e807881cef_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"886\" data-rawheight=\"559\" class=\"origin_image zh-lightbox-thumb\" width=\"886\" data-original=\"https://pic4.zhimg.com/v2-ed80b3d04202a1df9643e2e807881cef_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;886&#39; height=&#39;559&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"886\" data-rawheight=\"559\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"886\" data-original=\"https://pic4.zhimg.com/v2-ed80b3d04202a1df9643e2e807881cef_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ed80b3d04202a1df9643e2e807881cef_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这里顺便再提一下通用目标检测中另外一种加入Context信息的思路，[Relation Networks](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1711.11575\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Relation Networks for Object Detection</a>)虽然主要是解决提升识别性能和过滤重复检测而不是专门针对小目标检测的，但是也和上面的PyramidBox思想很像的，都是利用上下文信息来提升检测性能，可以归类为Context一类。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-329278812ee21a8d1b04239654eac702_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"599\" data-rawheight=\"393\" class=\"origin_image zh-lightbox-thumb\" width=\"599\" data-original=\"https://pic3.zhimg.com/v2-329278812ee21a8d1b04239654eac702_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;599&#39; height=&#39;393&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"599\" data-rawheight=\"393\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"599\" data-original=\"https://pic3.zhimg.com/v2-329278812ee21a8d1b04239654eac702_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-329278812ee21a8d1b04239654eac702_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文比较详细地总结了一些在通用目标检测和专门人脸检测领域常见的小目标检测的解决方案，后面有时间会再写一些专门在人脸领域的困难点（比如ROP的侧脸，RIP的360度人脸）及现在学术界的解决方案。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1604.02878\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks</a>)</p><p>+ [《深度学习中不平衡样本的处理》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230016-%25E6%25B7%25B1%25E5%25BA%25A6%25E5%25AD%25A6%25E4%25B9%25A0%25E4%25B8%25AD%25E4%25B8%258D%25E5%25B9%25B3%25E8%25A1%25A1%25E6%25A0%25B7%25E6%259C%25AC%25E7%259A%2584%25E5%25A4%2584%25E7%2590%2586/D%25230016.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p>+ [Augmentation for small object detection](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1902.07296.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1902.0729</span><span class=\"invisible\">6.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [Feature Pyramid Networks for Object Detection](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1612.03144\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Feature Pyramid Networks for Object Detection</a>)</p><p>+ [RetinaFace: Single-stage Dense Face Localisation in the Wild](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1905.00641.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1905.0064</span><span class=\"invisible\">1.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [SSH: Single Stage Headless Face Detector](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1708.03979.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1708.0397</span><span class=\"invisible\">9.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [An Analysis of Scale Invariance in Object Detection - SNIP](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1711.08189\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">An Analysis of Scale Invariance in Object Detection - SNIP</a>)</p><p>+ [R-FCN: Object Detection via Region-based Fully Convolutional Networks](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1605.06409\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">R-FCN: Object Detection via Region-based Fully Convolutional Networks</a>)</p><p>+ [SNIPER: Efficient Multi-Scale Training](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1805.09300.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1805.0930</span><span class=\"invisible\">0.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [SAN: Learning Relationship between Convolutional Features for Multi-Scale Object Detection](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1808.04974.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1808.0497</span><span class=\"invisible\">4.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [ScratchDet: Training Single-Shot Object Detectors from Scratch](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1810.08425.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1810.0842</span><span class=\"invisible\">5.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [FaceBoxes: A CPU Real-time Face Detector with High Accuracy](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1708.05234\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">FaceBoxes: A CPU Real-time Face Detector with High Accuracy</a>)</p><p>+ [S3FD: Single Shot Scale-Invariant Face Detector](<a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_S3FD_Single_Shot_ICCV_2017_paper.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">openaccess.thecvf.com/c</span><span class=\"invisible\">ontent_ICCV_2017/papers/Zhang_S3FD_Single_Shot_ICCV_2017_paper.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [Perceptual Generative Adversarial Networks for Small Object Detection](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1706.05274\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Perceptual Generative Adversarial Networks for Small Object Detection</a>)</p><p>+ [PyramidBox: A Context-assisted Single Shot Face Detector](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1803.07737\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">PyramidBox: A Context-assisted Single Shot Face Detector</a>)</p><p>+ [Relation Networks for Object Detection](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1711.11575\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Relation Networks for Object Detection</a>)</p><p></p>", 
            "topic": [
                {
                    "tag": "目标检测", 
                    "tagLink": "https://api.zhihu.com/topics/19596960"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "训练", 
                    "tagLink": "https://api.zhihu.com/topics/19554288"
                }
            ], 
            "comments": [
                {
                    "userName": "H丶w", 
                    "userLink": "https://www.zhihu.com/people/48c6229002a097880cf28d11a45c1830", 
                    "content": "真心可以", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "stephen", 
                    "userLink": "https://www.zhihu.com/people/554fa4391fc219d9c0484c5f204f94c8", 
                    "content": "总结的不错", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "船长", 
                            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
                            "content": "<p>谢谢鼓励</p>", 
                            "likes": 0, 
                            "replyToAuthor": "stephen"
                        }
                    ]
                }, 
                {
                    "userName": "H丶w", 
                    "userLink": "https://www.zhihu.com/people/48c6229002a097880cf28d11a45c1830", 
                    "content": "真心可以", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "船长", 
                            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
                            "content": "<p>哈哈，喜欢的话，欢迎Star</p>", 
                            "likes": 0, 
                            "replyToAuthor": "H丶w"
                        }
                    ]
                }, 
                {
                    "userName": "H丶w", 
                    "userLink": "https://www.zhihu.com/people/48c6229002a097880cf28d11a45c1830", 
                    "content": "通用小目标检测还有很多可以做和改进的地方", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "船长", 
                            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
                            "content": "<p>嗯，小目标是个常见问题，也有很多人提出各种方案，这里只是部分做法</p>", 
                            "likes": 0, 
                            "replyToAuthor": "H丶w"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/82739829", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 1, 
            "title": "CNN中使用卷积代替全连接", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在经典分类网络，比如LeNet、AlexNet中，在前面的卷积层提取特征之后都串联全连接层来做分类。但是近些年来，越来越多的网络，比如SSD，FasterRCNN的RPN，MTCNN中的PNet，都使用卷积层来代替全连接，也一样可以做到目标分类的效果，而且</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 更灵活，不需要限定输入图像的分辨率；</p><p>2. 更高效，只需要做一次前向计算。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文首先对全连接层和卷积层关系做分析，然后比较全连接层和卷积层的优缺点，自然也就搞清楚了为什么用卷积层替代全连接层这个问题了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 全连接和卷积的关系</p><p class=\"ztext-empty-paragraph\"><br/></p><p>全连接层和卷积层只要设置好了对应的参数，可以在达到相同输入输出的效果，在这个意义上，在数学上可以认为它们是可以相互替换的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>下面我们以输入10 x 10 x 3的特征图，输出10 x 10 x  1的特征图来证明。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 全连接层是一种核很大的卷积层</p><p class=\"ztext-empty-paragraph\"><br/></p><p>全连接层怎么做？全连接层输入10 x 10 x 3的特征图，首先将其reshape成一维的300个输入神经元，然后每个输出神经元的值都是这300个输入神经元的线性组合，最后将100个输出reshape成10 x 10 x 1的形状。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>卷积层怎么做？卷积层可以直接用100个10 x 10 x 3的滤波器，分别直接贴着输入的10 x 10 x 3的输入特征图做滤波，得到100个一维的输出，然后把这个100个一维输出reshape成10 x 10 x 1的形状。这100个输出，每个也都是由输入特征图上300个像素点线性组合而来，**在数学上和上面的全连接层理论上可以达到一样的效果**。这里用到的卷积和通常我们看到的卷积唯一有点特殊的是，为了达到全连接（采集到所有输入特征图像素的信息）的效果，它的分辨率和输入特征图分辨率一样。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>所以，由上面的讨论可知，全连接成可以用（**分辨率和输入特征图相同的**）卷积代替。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 卷积层是一种稀疏的全连接层</p><p class=\"ztext-empty-paragraph\"><br/></p><p>卷积层怎么做？假设卷积核大小为3 x 3的方形核，stride = 1， 为了输出分辨率不变padding用SAME方式。那么卷积核的形状是3 x 3 x 3，输出通道数为1，只用一个这样的卷积核按照常规卷积来做就行，输出10 x 10 x 1的特征图很容易。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>全连接层怎么做？全连接层输入10 x 10 x 3 = 300个神经元，卷积的时候每次卷积只是连接了其中的一个3 x 3 x 3 = 27的子集，那么可以在做全连接的时候，除了这27个神经元设置连接关系，其余的 300 - 27 = 273个**连接系数直接设置为0**就可以了。做 10 x 10 x 1 = 100次这样的全连接，就可以得到100个输出神经元，再reshape成10 x 10 x 1的形状就可以了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>所以，由上面的讨论可以得到，卷积层只是全连接层的一个子集，把**全连接的某些连接系数设置为0**，就可以达到和卷积相同的效果。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 卷积替代全连接的优点</p><p class=\"ztext-empty-paragraph\"><br/></p><p>由上一节的讨论可以知道，其实卷积层和全连接层在理论上是可以相互替代的，那么，为什么我们看到的趋势是全连接被卷积层替代，而不是卷积层被全连接层替代呢？这要从卷积层相比于全连接层两个工程上的优点来讲：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 对输入分辨率的限制</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如果网络后面有全连接层，而全连接层的输入神经元个数就是固定的，那么反推上层卷积层的输出是固定的，继续反推可知输入网络的图片的分辨率是固定的。例如，LetNet由于由全连接层，输入就只能是28 x 28的。如果网络中的全连接层都用卷积层替代，网络中只有卷积层，那么网络的输出分辨率是随着输入图片的分辨率而来的，输出图中每一个像素点都对应着输入图片的一个区域（可以用stride,pooling来反算）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 计算效率比较</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c70d2176d085f674f80ae3a88e790c8a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"631\" data-rawheight=\"459\" class=\"origin_image zh-lightbox-thumb\" width=\"631\" data-original=\"https://pic3.zhimg.com/v2-c70d2176d085f674f80ae3a88e790c8a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;631&#39; height=&#39;459&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"631\" data-rawheight=\"459\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"631\" data-original=\"https://pic3.zhimg.com/v2-c70d2176d085f674f80ae3a88e790c8a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c70d2176d085f674f80ae3a88e790c8a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>同样以LeNet来做例子，如果一个图片是280 x 280的分辨率，为了识别图片中所有的数字（为了简单，假设每个数字都是在这个大图划分为10 x 10的网格中），那么为了识别这100个位置数字，那么至少需要做100次前向；而全卷积网络的特点就在于输入和输出都是二维的图像，并且**输入和输出具有相对应的空间结构**，我们可以将网络的输出看作是一张**heat-map**，用热度来代表待检测的原图位置出现目标的概率，只做一次前向就可以得到所有位置的分类概率。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文首先在理论上论证了卷积层和全连接层的可互换性质，然后详细分析了在实践中用卷积层代替全连接层的两个好处，第一个是去掉全连接层对网络输入图像分辨率的限制；第二个好处是全卷积网络只需要做一次前向运算就可以获得一张目标所在位置的heat-map，节约了计算。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [CNN中使用卷积层替代全连接层训练](<a href=\"https://zhuanlan.zhihu.com/p/65150848\" class=\"internal\">永远在你身后：CNN中使用卷积层替代全连接层训练</a>)</p><p>+ [Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks](<a href=\"https://link.zhihu.com/?target=https%3A//kpzhang93.github.io/MTCNN_face_detection_alignment/paper/spl.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">kpzhang93.github.io/MTC</span><span class=\"invisible\">NN_face_detection_alignment/paper/spl.pdf</span><span class=\"ellipsis\"></span></a>)</p>", 
            "topic": [
                {
                    "tag": "卷积", 
                    "tagLink": "https://api.zhihu.com/topics/19678959"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/82739709", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 1, 
            "title": "CNN模型内存访问估计", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在上一篇[《CNN模型计算量估计》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230023-CNN%25E6%25A8%25A1%25E5%259E%258B%25E8%25AE%25A1%25E7%25AE%2597%25E9%2587%258F%25E4%25BC%25B0%25E8%25AE%25A1/D%25230023.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)一文中，我们对常见层的计算量(MACC，FLOPS)做了分析和估算，但这只是模型性能估计这整个故事的一部分。**内存带宽**(bandwidth)是另一部分，大部分情况下，它比计算次数更重要！本文继续对CNN模型的内存访问量做介绍和分析。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 内存访问</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>在当前的计算机架构中，内存的访问比CPU中执行单个计算要慢得多（需要更多的时钟周期）—— 大约100或更多倍！上一篇[《CNN模型计算量估计》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230023-CNN%25E6%25A8%25A1%25E5%259E%258B%25E8%25AE%25A1%25E7%25AE%2597%25E9%2587%258F%25E4%25BC%25B0%25E8%25AE%25A1/D%25230023.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)刚刚看到这些神经网络进行了大量计算，但它们执行了多少次内存访问？</p><p class=\"ztext-empty-paragraph\"><br/></p><p>```</p><p>关于内存访问速度和CPU运算速度的分析，可以参考《深入理解计算机系统》、《计算机体系结构量化分析方法》</p><p>```</p><p class=\"ztext-empty-paragraph\"><br/></p><p>对于网络中的每个层，CPU需要：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 首先，从主存储器读取输入向量或特征图；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2. 然后，计算点积——这也涉及从主存中读取层的权重；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3. 最后，将计算出的结果作为新的矢量或特征图写回主存储器。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这涉及大量的内存访问。由于内存非常慢（相对于CPU计算速度而言），因此该层执行的内存读/写操作量也会对其速度产生很大影响——可能比计算次数更大。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 卷积层和全连接层：读取权重带来的内存访问</p><p class=\"ztext-empty-paragraph\"><br/></p><p>网络每层学习的参数或权重存储在主存储器中。通常，模型的权重越少，运行的速度就越快。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 将权重读入</p><p class=\"ztext-empty-paragraph\"><br/></p><p>正如我们在《CNN模型计算量估计》所讨论的，**全连接层**将其权重保持在大小I × J矩阵中，其中I是输入神经元的数量和J是输出的数量。它还有一个大小J的偏置量。所以这一层的权重总共有(I + 1) × J。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>今天使用的大多数**卷积层**都有正方形内核，因此对于具有内核大小K和Cin输入通道的卷积层，每个滤波器都有权重K × K × Cin。该层将具有Cout滤波器/输出通道，因此权重总数K × K × Cin × Cout加上额外的Cout个偏置值。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>通常，**卷积层的权重数量小于全连接层**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>例如：具有4096个输入和4096个输出的全连接层具有(4096+1) × 4096 = 16.8M权重。具有3×3内核的卷积层和48个滤波器在64 × 64具有32通道的输入图像上工作，权重大小为3 × 3 × 32 × 48 + 48 = 13,872。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>请注意，此示例中卷积层的输入实际上比完全连接层的输入大32倍，输出大48倍。因此，Conv层可以处理更多数据，但权重却减少了1000倍。很明显，**全连接层是内存权重访问的负担**！</p><p class=\"ztext-empty-paragraph\"><br/></p><p>```</p><p>有用的结论：由于权值共享，卷积层一般占网络更少的权重参数数量，但是更多的计算量。</p><p>```</p><p class=\"ztext-empty-paragraph\"><br/></p><p>```</p><p>注意：全连接和卷积层实际上非常相似（可以参考本黑板报文章《D#0025-CNN中使用卷积代替全连接》）。</p><p>我们可以使用全连接层实现卷积层，反之亦然。卷积可以看成是一个全连接层，绝大多数连接设置为0——每个输出仅连接到K × K输入</p><p>而不是所有输出，并且所有输出对这些连接使用相同的值。这就是卷积层对内存更有效的原因，因为它们不存储未使用的连接的权重。</p><p>```</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 用精度更低的权值</p><p class=\"ztext-empty-paragraph\"><br/></p><p>每个权重的字节长度也很重要。桌面级计算机使用32位浮点数，每个浮点数占4个字节。在iOS上，使用16位浮点数（“半精度”）更为常见，每次只占用2个字节。它们的精度要低得多，但从好的方面来说它们更快，特别是因为iPhone和iPad GPU只有16位ALU。但是使用8位权重甚至1位权重都可能比这更低。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>区分权重的**存储格式与用于计算的格式**也很重要。如果您将权重存储为8位量化值，GPU内核将首先将它们转换回浮点数，然后使用浮点值进行计算。（尽管有些工具包具有卷积层，可以直接使用量化数字。）</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在计算点积期间发生的**累加精度**也很重要。即使使用16位浮点数，使用32位浮点数执行点积也是有意义的，然后将结果转换回16位。这样，在对数字做加法时不会丢失任何精度。但它也比使用16位数字进行累加要慢。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>读取内存很慢，因此权重较少的层将比具有更多权重的同一层更快。不仅因为它具有更少的MACC，而且因为它访问主存储器来读取权重的次数更少。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>对于具有相同数量的权重的两个层，但是一个使用float32而另一个使用float16，具有较小权重的那个将更快，但是以准确性为代价。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>实际上，已有证据证明，在部署时16位浮点数（甚至8位）足以用于卷积神经网络。你会失去一点精度，但平均而言，这些精度误差会被抵消，模型仍会给出正确的结果。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 卷积层：读取特征图、权重参数和写回中间结果带来的内存访问</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 通用分析</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在文献中，经常会看到模型的复杂性，其中列出了MACC（或FLOPS）的数量和训练参数的数量。但是，这忽略了一个重要的指标：层的输入读取的内存量，以及写入该层输出执行的内存访问次数。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>我们将在这里假设读取单个输入值计为“一次内存访问”，写入单个输出值也算作一次内存访问。这在实践中不一定正确：在很多支持SIMD指令的CPU中，一次可以读入多个字节的数据，但这不应该影响本节的计算。这里只想提出一些描述给定模型的内存访问量的数值的估算方式，这样我们就可以比较两个模型的内存访问量。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>同样，这些数字只是近似值，因为我们并不确切知道CPU/GPU内核是如何工作的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>```</p><p>注意：与CPU一样，GPU也可以进行缓存以加速内存读写。GPU内核还可以将少量内存读入本地或“线程组”存储，以便更快地访问。</p><p>一般来讲，CPU/GPU内核已经过优化，可以尽可能高效地读写内存，因此这里给出的数字是理论上限，而不是精确数字。</p><p>```</p><p class=\"ztext-empty-paragraph\"><br/></p><p>假设卷积层的输入形状是Hin x Win x Cin图像，输出特征图形状Hout x Wout x Cout那么，对于每个输出特征图的像素来说，需要访问输入特征图次数为每个卷积核的参数的个数：K x K x Cin。所以，此卷积层需要访问内存（读取输入特征）的次数为(K × K × Cin) x (Hout x Wout x Cout)。（当然，一个聪明的GPU内核程序员将有办法优化这一点。每个GPU线程可以计算多个输出像素而不是一个，允许它多次重复使用一些输入值，总体上需要更少的内存读取，所有这些优化都将平等地应用于所有模型。因此，即使我的公式不是100％正确，它们的误差是常数级的，因此仍然可用于比较模型。）</p><p class=\"ztext-empty-paragraph\"><br/></p><p>对于计算得到的特征图的输出，如果此特定卷积层的步幅为2，滤波器为32个，则它会写入具有112×112×32个值的输出特征图。那么需要112 x 112 x 32 = 401,408次内存访问。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>对于本层卷积的参数从内存中读取，因为参数数量很少，可以直接认为只读取一次，存储在缓存中。这里读取次数为K x K x Cin x Cout + Cout。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>总结下来，**每个层将进行以下总内存访问**：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>```</p><p>input = (K × K × Cin) x (Hout x Wout x Cout)</p><p>output = Hout × Wout × Cout</p><p>weights = K × K × Cin × Cout + Cout</p><p>```</p><p class=\"ztext-empty-paragraph\"><br/></p><p>具体举例来说，如果是一副输入224 x 224 x 3的图片，经过stride = 2，K = 3的卷积，输出112 x 112 x 32的特征图，那么有：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>```</p><p>input = 3 × 3 × 3 × 112 × 112 × 32 = 10,838,016(96.42%)</p><p>output = 112 × 112 × 32 = 401,408(3.57%)</p><p>weights = 3 × 3 × 3 × 32 + 32 = 896(0.01%)</p><p>total = 11,240,320</p><p>```</p><p class=\"ztext-empty-paragraph\"><br/></p><p>有这个例子我们可以看到，**卷积层主要的内存访问发生在把输入特征图反复搬运到CPU参与计算，把计算得到的输出特征图写入内存和权重的读取带来的内存访问，可以忽略不计**。顺便说一句，我们这里假设了**权重只被读取一次并缓存在本地CPU/GPU内存中**，因此它们可以在CPU/GPU线程之间共享，并将重新用于每个输出像素。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>对于网络中较深的层，具有28 x 28 x 256个输入和28 x 28 x 512个输出，K = 3，stride = 1，那么：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>```</p><p>input = 3 × 3 × 256 × 28 × 28 × 512 = 924,844,032(99.83%)</p><p>output = 28 × 28 × 512 = 401,408(0.04%)</p><p>weights = 3 × 3 × 256 × 512 + 512 = 1,180,160(0.13%)</p><p>total = 926,425,600</p><p>```</p><p class=\"ztext-empty-paragraph\"><br/></p><p>即使特征图的宽度和高度现在较小，它们也会有更多的通道。这就是为什么权重的计算更多，因为由于通道数量的增加，权重会越来越多。但是主要的内存访问依然是把输入特征图反复搬运到CPU参与计算。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 深度可分离卷积分析</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如果使用深度可分离卷积呢？使用跟前面相同的输入和输出大小，计算3×3深度卷积层和1×1逐点层的内存访问次数：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>```</p><p>DepthWise layer</p><p>input = 3 × 3 × 1 x 28 × 28 × 256 = 1,806,336</p><p>output = 28 × 28 × 256 = 200,704</p><p>weights = 3 × 3 × 1 x 256 + 256 = 2,560</p><p>total = 2,009,600(1.91%)</p><p>PointWise layer</p><p>input = 1 × 1 × 256 × 28 × 28 × 512 = 102,760,448</p><p>output = 28 × 28 × 512 = 401,408</p><p>weights = 1 × 1 × 256 × 512 + 512 = 131,584</p><p>total = 103,293,440(98.09%)</p><p>total of both layers = 105,303,040</p><p>```</p><p class=\"ztext-empty-paragraph\"><br/></p><p>可以看到深度可分离卷积它的内存访问量减少到大约原来的926425600 / 105303040 = 8.80倍（几乎是K × K倍）,这就是使用深度可分层的好处。还可以看到Depth-Wise层的内存访问成本非常便宜，几乎可以忽略不计。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 激活层和BN层：融合</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在PyTorch和大多数训练框架中，经常会看到Conv2D层后面跟着一个应用ReLU的激活层。这对训练框架来说很好，提供了灵活性，但是让ReLU成为一个单独的层是浪费的，特别是因为这个函数非常简单。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>示例：对28 × 28 × 512卷积层的输出应用ReLU ：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>```</p><p>input = 28 × 28 × 512 = 401,408</p><p>output = 28 × 28 × 512 = 401,408</p><p>weights = 0</p><p>total = 802,816</p><p>```</p><p class=\"ztext-empty-paragraph\"><br/></p><p>首先，它需要从卷积层读取特征图每个像素，然后对其应用ReLU，最后将结果写回内存。当然，这非常快，因为它几乎与将数据从一个内存位置复制到另一个内存位置相同，但这样的操作有些浪费。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>因此，激活函数通常与卷积层融合。这意味着卷积层在计算出点积之后直接应用ReLU，然后才能写出最终结果。这节省了一次读取和一次写入存储器的昂贵时钟开销。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>同理，对于BN层来说，将BN层融合进卷积层也是一种在实践中经常用到的策略。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>|                |                         内存访问次数                         | 备注                         |</p><p>| -------------- | :----------------------------------------------------------: | ---------------------------- |</p><p>| 全连接层       |          I次读输入，(I + 1) × J次读权重，J次写结果           | 访问次数不多                 |</p><p>| 激活函数(ReLU) |                       Hin x Win x Cin                        | 融合，且比例很小，通常不关心 |</p><p>| 卷积层         | input = (K × K × Cin) x (Hout x Wout x Cout)　　　　　output = Hout × Wout × Cout</p><p>weights = K × K × Cin × Cout + Cout | 大多数在input的消耗上        |</p><p>| BN层           |                             N/A                              | BN融合，通常不关心           |</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如上表，本文讨论了在CNN网络中常见的层的内存读取次数，没有涉及到的层也可以参考本文的分析方法来算。后面在工作中遇到新的模型的时候，不需要实际实现它就可以按照本文的讨论粗略地估计模型在内存访问上的可行性；更重要的是，以上分析在自己设计、改造网络的时候也有很强的指导性作用。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>最后需要强调的一点是，关于模型计算量和内存访问次数两者是怎么对最终模型的速度产生影响是一个比较复杂的问题。有些人认为参数数量少或者计算量少的模型，就比计算量大的模型快，这是不对的。有时候（如果不是很多时候的话），内存速度和Cache才是制约因素。如果在模型性能、计算量、内存访问量之间做取舍和平衡，还需要做很多工作。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [《深入理解计算机系统》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/5333562/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深入理解计算机系统（原书第2版） (豆瓣)</a>)</p><p>+ [《计算机体系结构量化分析方法》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/7006537/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">book.douban.com/subject</span><span class=\"invisible\">/7006537/</span><span class=\"ellipsis\"></span></a>)</p><p>+ [How fast is my model?](&lt;<a href=\"https://link.zhihu.com/?target=http%3A//machinethink.net/blog/how-fast-is-my-model/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">How fast is my model?</a>&gt;)</p><p>+ [depthwise separable convolutions in mobilenet](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230004-depthwise_separable_convolutions_in_mobilenet/D%25230004.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p>+ [Batch-Normalization层原理与分析](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230020-Batch-Normalization%25E5%25B1%2582%25E5%258E%259F%25E7%2590%2586%25E4%25B8%258E%25E5%2588%2586%25E6%259E%2590/D%25230020.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p>+ [《CNN模型计算量估计》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230023-CNN%25E6%25A8%25A1%25E5%259E%258B%25E8%25AE%25A1%25E7%25AE%2597%25E9%2587%258F%25E4%25BC%25B0%25E8%25AE%25A1/D%25230023.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p>+ [《CNN中使用卷积代替全连接》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230025-CNN%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8%25E5%258D%25B7%25E7%25A7%25AF%25E4%25BB%25A3%25E6%259B%25BF%25E5%2585%25A8%25E8%25BF%259E%25E6%258E%25A5/D%25230025.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "高性能计算", 
                    "tagLink": "https://api.zhihu.com/topics/19608622"
                }, 
                {
                    "tag": "优化", 
                    "tagLink": "https://api.zhihu.com/topics/19570512"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/82739507", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 8, 
            "title": "CNN模型计算量估计", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在我们训练的深度学习模型在资源受限的嵌入式设备上落地时，**精度不是我们唯一的考量因素**，我们还需要考虑</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. **安装包的大小**，如果你的模型文件打包进app一起让客户下载安装，那么动辄数百MB的模型会伤害用户的积极性；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2. 模型速度，或者说**计算量的大小**。现在手机设备上的图片和视频的分辨率越来越大，数据量越来越多；对于视频或者游戏，FPS也越来越高，这都要求我们的模型在计算时，速度越快越好，计算量越小越好；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3. 运行时**内存占用大小**，内存一直都是嵌入式设备上的珍贵资源，占用内存小的模型对硬件的要求低，可以部署在更广泛的设备上，降低我们**算法落地的成本**；况且，一些手机操作系统也不会分配过多的内存给单一一个app，当app占用内存过多，系统会kill掉它；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>4. **耗电量大小**，智能手机发展到今天，最大的痛点一直是电池续航能力和发热量，如果模型计算量小，内存耗用小的话，自然会降低电量的消耗速度。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>   以上四点因素，在学术界刷paper的时候或者刷比赛时不会考虑到的(要不然Kaggle上也就不会有那么多ensemble的trick了)，学者们可以在PC平台上，甚至在GPU集群上来跑他们的模型；但是当我们评估一个模型，是不是适合在嵌入式设备、移动平台上落地使用时，我们最好针对以上四点预先估算一下它。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>   最准确的估算方式，当然是训练好模型后，把模型集成进目标硬件上，运行一下（也可以是多次，以避免系统状态波动），测量它的速度和内存占用；但是这样还是要先有一个可以运行在目标硬件上的模型！</p><p class=\"ztext-empty-paragraph\"><br/></p><p>   本文介绍怎么**在模型训练之前，从理论上分析模型的计算量和内存占用大小**，毕竟，训练模型的代价也是挺昂贵的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 计算量评价指标</p><p class=\"ztext-empty-paragraph\"><br/></p><p>一个朴素的评估模型速度的想法是评估它的计算量。一般我们用[FLOPS](&lt;<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/FLOPS\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">en.wikipedia.org/wiki/F</span><span class=\"invisible\">LOPS</span><span class=\"ellipsis\"></span></a>&gt;)，即每秒浮点操作次数**FL**oating point **OP**erations per **S**econd这个指标来衡量GPU的运算能力。这里我们用[MACC](&lt;<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Multiply%25E2%2580%2593accumulate_operation\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">en.wikipedia.org/wiki/M</span><span class=\"invisible\">ultiply%E2%80%93accumulate_operation</span><span class=\"ellipsis\"></span></a>&gt;)，即乘加数**M**ultiply-**ACC**umulate operation，或者叫MADD，来衡量模型的计算量。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>```</p><p>不过这里要说明一下，用MACC来估算模型的计算量只能**大致地**估算一下模型的速度。模型最终的的速度，不仅仅是和计算量多少有</p><p>关系，还和诸如**内存带宽**、优化程度、CPU流水线、Cache之类的因素也有很大关系。</p><p>```</p><p class=\"ztext-empty-paragraph\"><br/></p><p>为什么要用乘加数来评估计算量呢？因为CNN中很多的计算都是类似于y = w[0]\\*x[0] + w[1]\\*x[1] + w[2]\\*x[2] + ... + w[n-1]\\*x[n-1]这样的点乘然后累加的形式，其中w和x是向量，结果y是标量。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在CNN中最常见的卷积层和全连接层中，w是学习到的权重值，而x是该层的输入特征图，y是该层的输出特征图。一般来说，每层输出不止一张特征图，所以我们上面的乘加计算也要做多次。这里我们约定w[0]\\*x[0] + ...算一次乘加运算。这样来算，像上面两个长度为n的向量w和x相乘，就有n次乘法操作和n-1次加法操作，大约可等于n次乘加操作。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## CNN常用层计算量分析</p><p class=\"ztext-empty-paragraph\"><br/></p><p>下面我们逐个分析常见的CNN层的计算量：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 全连接层</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在全连接层，所有的出都是所有的输入的加权求和而来。对于一个有I和输入和J和输出的全连接层来说，它的权重W可以存储在一个IxJ的矩阵里面，这个全连接层执行的计算就是**y = matmul(x, W) + b**，这里x是I个输入值的向量，W是包含层权重的IxJ矩阵，b是包含J个元素的偏置值向量。结果y包含由层计算的输出值，也是大小为J的向量。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>为了计算MACC的数量，我们看点乘发生的位置matmul(x, W)。矩阵乘法matmul只包含一大堆的点积运算。每个点积都在输入x和矩阵W的一列间发生。两者都有个I元素，因此这算作I个MACC。我们必须计算J个这样的点积，因此**MACC的总数IxJ**与权重矩阵的大小相同。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>加偏置b并不会太影响MACC的数量，毕竟加偏置的操作次数远少于矩阵乘法里面的乘加次数。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>示例：具有300个输入神经元和100个输出神经元的全连接层执行300 × 100 = 30,000个MACC。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>```</p><p>注意：有时，全连接层的公式是在没有明确偏置值的情况下编写的。在这种情况下，偏置向量作为一行添加到权重矩阵中，所以是(I + 1)</p><p>× J，但这实际上更像是一种数学简化。在任何情况下，它只会增加J个额外的乘法，所以无论如何MACC的数量都不会受到太大影响。</p><p>记住这是一个近似值。</p><p>```</p><p class=\"ztext-empty-paragraph\"><br/></p><p>总之，一个长度为I的向量与一个I x J维度的矩阵相乘（这就是全连接呀）得到一个长度为J的输出向量，需要**I x J次MACC或者(2xI - 1) x J和FLOPS**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如果全连接层直接跟随卷积层，则其输入大小可能不会被指定为单个矢量长度I，但是可能被指定为具有诸如形状(512, 7, 7)的特征图。例如Keras要求你先将这个输入“压扁flatten”成一个向量，这样就可以得到I = 512×7×7个输入。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>```</p><p>注意：在所有这些计算中，我假设批量大小为1.如果您想知道批量大小为B的MACC，那么只需将结果乘以B。</p><p>```</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 激活函数</p><p class=\"ztext-empty-paragraph\"><br/></p><p>通常深度学习模型层的后面会串联一个非线性激活函数，例如ReLU或者Sigmoid函数。这些激活函数自然也会消耗时间。但是我们不用MACC来计算它们的计算量，而是使用FLOPS，因为它们不完全是乘加运算。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>有些激活函数的计算比其他激活函数更难，例如，ReLU：y = max(x, 0)，这只是GPU上的一次单次操作。对于一个有J个输出神经元的全连接层来说，ReLU只做J次这样的运算，所以算**J次FLOPS**。对于Sigmoid函数y = 1 / (1 + exp(-x))来说，因为它涉及到指数运算和倒数，所以它有更多的计算量。当我们计算FLOPS时，我们通常把加、减、乘、除、取幂、求根等看做一次FLOPS。因为Sigmoid函数有四种（减、取幂、加、除），所以它每个输出对应四个FLOPS，对于J个输出单元的全连接层后的Sigmoid激活层，有**J x 4次FLPOS**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>通常我们不计算激活函数的计算量，因为他们只占整个网络计算量中的很小一部分，我们主要关心**大矩阵乘法和点乘运算**，直接认为激活函数的运算是免费的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>总结：**不需要担忧激活函数**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 卷积层</p><p class=\"ztext-empty-paragraph\"><br/></p><p>卷积层的输入和输出不是矢量，而是三维特征图H × W × C，其中H是特征图的高度，W宽度和C是通道数。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>今天使用的大多数卷积层都是方形核。**对于具有核大小K的卷积层，MACC的数量为：K × K × Cin × Hout × Wout × Cout**。这个公式可以这么理解：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 首先，输出特征图中有Hout × Wout × Cout个像素；</p><p>+ 其次，每个像素对应一个立体卷积核K x K x Cin在输入特征图上做立体卷积卷积出来的；</p><p>+ 最后，而这个立体卷积操作，卷积核上每个点都对应一次MACC操作</p><p class=\"ztext-empty-paragraph\"><br/></p><p>同样，我们在这里为了方便忽略了偏置和激活。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>我们不应该忽略的是层的stride，以及任何dilation因子，padding等。这就是为什么我们需要参看层的输出特征图的尺寸Hout × Wout，因它考虑到了stride等因素。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>示例：对于3×3，128个filter的卷积，在112×112带有64个通道的输入特征图上，我们执行MACC的次数是：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3 × 3 × 64 × 112 × 112 × 128 = 924,844,032</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这几乎是10亿次累积运算！GPU将忙于计算......</p><p class=\"ztext-empty-paragraph\"><br/></p><p>```</p><p>注意：在此示例中，我们使用“same”填充和stride = 1，以便输出特征图与输入特征图具有相同的大小。通常看到卷积层使用</p><p>stride = 2，这会将输出特征图大小减少一半，在上面的计算中，我们将使用56 × 56而不是112 × 112。</p><p>```</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 深度可分离卷积层</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这里对于MobileNet V1中的深度可分离卷积只列个结论，更详细的讨论可见本黑板报我前面写的[depthwise separable convolutions in mobilenet](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230004-depthwise_separable_convolutions_in_mobilenet/D%25230004.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)一文。MobileNet V1深度可分层的总MACC是：**MACC_v1 = (K × K × Cin × Hout × Wout) + (Cin × Hout × Wout × Cout)**，其中K是卷积核大小，Cin是输入特征图通道数，Hout, Wout是DW卷积核输出尺寸（PW卷积只改变输出通道数，不改变输入输出尺寸）。深度可分离卷积的计算量和传统卷积计算量的比为(K × K + Cout) / K × K × Cout，约等于 1 / (K x K)。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>下面我们详细讨论下MobileNet V2中的MACC。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>MobileNet V2相比与V1，主要是由DW+PW两层变成了下面的三层PW+DW+PW：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 一个1×1卷积，为特征图添加更多通道（称为expansion layer）</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2. 3×3深度卷积，用于过滤数据（depthwise convolution）</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3. 1×1卷积，再次减少通道数（projection layer，bottleneck convolution）</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>这种扩展块中MACC数量的公式：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**Cexp = (Cin × expansion_factor)**，（expansion_factor用于创建深度层要处理的额外通道，使得Cexp在此块内使用的通道数量）</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**MACC_expansion_layer = Cin × Hin × Win × Cexp**，(参照上面传统卷积，把卷积核设置为1x1即得)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**MACC_depthwise_layer = K × K × Cexp × Hout × Wout**(参照MoblieNet V1分析)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**MACC_projection_layer = Cexp × Hout × Wout × Cout**(参照MoblieNet V1分析，或者传统卷积把卷积核设置为1x1即得)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>把所有这些放在一起：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**MACC_v2 = Cin × Hin × Win × Cexp + (K × K + Cout) × Cexp × Hout × Wout**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如果stride = 1，则简化为：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**(K × K + Cout + Cin) × Cexp × Hout × Wout**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这与MobileNet V1使用的深度可分层相比如何？如果我们使用输入特征图112×112×64扩展因子6，以及stride = 1的3×3深度卷积和128输出通道，那么MACC的总数是：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>(3 × 3 + 128 + 64) × (64 × 6) × 112 × 112 = 968,196,096</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这不是比以前更多吗？是的，它甚至超过了最初的3×3卷积。但是......请注意，由于扩展层，在这个块内，我们实际上使用了64 × 6 = **384通道**。因此，**这组层比原始的3×3卷积做得更多**（从64到128个通道），**而计算成本大致相同**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### BN层</p><p class=\"ztext-empty-paragraph\"><br/></p><p>前面提到了，对于激活函数，我们认为他们是免费的。但是对于BN层呢？在近年的网络结构中在卷积层后面串一个BN层已是常规操作。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>BN层获取上层的输出，并将以下公式应用于每个输出值：z = gamma * (y - mean) / sqrt(variance + epsilon) + beta。这y是前一层输出特征图中的元素。我们首先通过减去输出**通道的**mean并除以标准偏差来标准化该值（epsilon用于确保我们不除以0，它通常是这样的0.001）。然后我们按比例缩放gamma并添加偏差或偏移量beta。每个通道有自己的gamma，beta，mean，和variance的值，因此，如果在卷积层有C个输出通道，则该批量标准化层学习到C×4个参数。看起来有相当多的FLOPS，因为上面的公式应用于输出特征图中的每个元素。然而......通常批量标准化应用于卷积层的输出但*在*非线性（ReLU）之前。在这种情况下，我们可以做一些数学运算来使批量标准化层消失（详见本黑板报我前面写的[Batch-Normalization层原理与分析](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230020-Batch-Normalization%25E5%25B1%2582%25E5%258E%259F%25E7%2590%2586%25E4%25B8%258E%25E5%2588%2586%25E6%259E%2590/D%25230020.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)）！</p><p class=\"ztext-empty-paragraph\"><br/></p><p>简而言之：我们可以完全忽略批量标准化层的影响，因为我们在进行推理时实际上将其从模型中融合进前面的卷积层。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>```</p><p>注意：此技巧仅在层的顺序为：卷积-&gt;BN-&gt;ReLU时才有效；不适用于：卷积-&gt;ReLU-&gt;BN。ReLU是一个非线性操作，它会把数据弄乱。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>（但如果批量标准化后面紧跟一个新的卷积层，你可以反过来折叠参数）</p><p>```</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 其它层</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 池化层</p><p class=\"ztext-empty-paragraph\"><br/></p><p>到此我们研究了卷积层和全连接层，这两个是现代神经网络中最重要的组成部分。但是也有其他类型的层，例如池化层。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这些其他层类型肯定需要时间，但它们不使用点积，因此不能用MACC测量。如果你对计算FLOPS感兴趣，只需获取特征图大小并将其乘以表示处理单个输入元素的难度的常量。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>示例：在112×112具有128通道的特征图上具有过滤器大小2和步幅2的最大池化层需要112 × 112 × 128 = 1,605,632　FLOPS或1.6兆FLOPS。当然，如果步幅与滤波器尺寸不同（例如3×3窗口，2×2步幅），则这些数字会稍微改变。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>但是，在确定网络的复杂性时，通常会忽略这些附加层。毕竟，与具有100个MFLOPS的卷积/全连接层相比，1.6 MFLOPS非常小。因此，它成为网络总计算复杂度的舍入误差。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### Concate层</p><p class=\"ztext-empty-paragraph\"><br/></p><p>某些类型的操作，例如结果的连接，通常甚至可以免费完成。不是将两个层分别写入自己的输出张量中，然后有一个将这两个张量复制到一个大张量的连接层。相反，第一层可以直接写入大张量的前半部分，第二层可以直接写入后半部分。不需要单独的复制步骤。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>|                   |                            计算量                            | 相对比值           | 备注                                                        |</p><p>| ----------------- | :----------------------------------------------------------: | ------------------ | ----------------------------------------------------------- |</p><p>| 全连接层          |             (2 x I - 1) x J FLOPS或者I x J MACC              |                    | 耗时大户                                                    |</p><p>| 激活函数(Sigmoid) |                         J x 4 FLOPS                          |                    | 比例很小，通常不关心                                        |</p><p>| 卷积层            |            K × K × Cin × Hout × Wout × Cout MACC             | 1                  | 耗时很多                                                    |</p><p>| MobileNet V1      | (K × K × Cin × Hout × Wout) + (Cin × Hout × Wout × Cout) MACC | 约等于 1 / (K x K) | 传统卷积的(K × K + Cout) / K × K × Cout，约等于 1 / (K x K) |</p><p>| MobileNet V2      | Cin × Hin × Win × Cexp + (K × K + Cout) × Cexp × Hout × Wout MACC |                    | 计算量没有变小，但是表达能力变大了                          |</p><p>| BN层              |                             N/A                              |                    | BN融合，通常不关心                                          |</p><p>| 池化层            |                    Cin × Hin × Win FLOPS                     |                    | 很小，通常不关心                                            |</p><p>| Concate层         |                             N/A                              |                    | 通常不需要拷贝，成本为零                                    |</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如上表，本文讨论了在CNN网络中常见的层的计算量MACC，没有涉及到的层也可以参考本文的分析方法来算MACC或者FLOPS。后面在工作中遇到新的模型的时候，不需要实际实现它就可以按照本文的讨论粗略地估计模型在计算量上的可行性；更重要的是，以上分析在自己设计、改造网络的时候也有很强的指导性作用。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [How fast is my model?](&lt;<a href=\"https://link.zhihu.com/?target=http%3A//machinethink.net/blog/how-fast-is-my-model/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">How fast is my model?</a>&gt;)</p><p>+ [Convolutional Neural Networks at Constrained Time Cost](&lt;<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1412.1710\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Convolutional Neural Networks at Constrained Time Cost</a>&gt;)</p><p>+ [Learning both Weights and Connections for Efficient Neural Networks](&lt;<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1506.02626\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Learning both Weights and Connections for Efficient Neural Networks</a>&gt;)</p><p>+ [depthwise separable convolutions in mobilenet](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230004-depthwise_separable_convolutions_in_mobilenet/D%25230004.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p>+ [Batch-Normalization层原理与分析](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230020-Batch-Normalization%25E5%25B1%2582%25E5%258E%259F%25E7%2590%2586%25E4%25B8%258E%25E5%2588%2586%25E6%259E%2590/D%25230020.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "优化", 
                    "tagLink": "https://api.zhihu.com/topics/19570512"
                }, 
                {
                    "tag": "高性能", 
                    "tagLink": "https://api.zhihu.com/topics/19616380"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/82739255", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 0, 
            "title": "机器学习中的过拟合及其解决办法", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓</b></p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>机器学习的目标是从训练集中学习一个模型去预测没有见过的测试集，最终判断一个模型好不好是要它在看测试集上的性能。如果一个模型，在训练集上性能很好，但是在测试集上性能很差的话，那么就是发生过拟合。本文介绍了机器学习中的过拟合和解决过拟合的常用手段。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 过拟合和欠拟合</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 过拟合</p><p class=\"ztext-empty-paragraph\"><br/></p><p>做模型训练时，我们会把收集到的所有样本分为互不相交的三类：训练集，验证集，测试集。我们在训练集上进行模型训练，跑出来训练集上的的误差叫**训练误差**，在验证集上进行调参，这个“训练-&gt;调参-&gt;训练-&gt;...”迭代完之后，我们会拿得到的的模型在测试集上跑一下，得出来一个**测试误差**。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f5c705bbffc987fa45c4967ab2daa891_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1083\" data-rawheight=\"790\" class=\"origin_image zh-lightbox-thumb\" width=\"1083\" data-original=\"https://pic2.zhimg.com/v2-f5c705bbffc987fa45c4967ab2daa891_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1083&#39; height=&#39;790&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1083\" data-rawheight=\"790\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1083\" data-original=\"https://pic2.zhimg.com/v2-f5c705bbffc987fa45c4967ab2daa891_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f5c705bbffc987fa45c4967ab2daa891_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>有时候，我们为了减少训练误差，会倾向于使用更多的特征（如上图），更复杂的模型（有时候更多的特征和更复杂的模型是一回事）或者更精巧的训练，但是这些模型在测试集上一跑，性能很差（如下图）。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ffd780b64b483ec1d7b56beab39df04c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"362\" class=\"origin_image zh-lightbox-thumb\" width=\"500\" data-original=\"https://pic1.zhimg.com/v2-ffd780b64b483ec1d7b56beab39df04c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;500&#39; height=&#39;362&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"362\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"500\" data-original=\"https://pic1.zhimg.com/v2-ffd780b64b483ec1d7b56beab39df04c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ffd780b64b483ec1d7b56beab39df04c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这种随着模型容量的增大，或者训练次数的增加，模型性能在训练集上很好，但是在测试集上很差的现象叫做**过拟合(Over-fitting)**（模型过度over拟合fitting了训练集内的数据）。模型在训练集上性能很好，在多大程度上也能将这种性能推广到**从未见过**的测试集的能力叫做模型的**泛化能力**，我们要泛化能力好的模型。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>过拟合的原因有很多，**本质原因还是因为模型的容量(Capacity)相对于数据的复杂度和学习任务的难度太强了，导致学到了太多的抽样误差**，类似于一个记忆力很强的学生，通过死记硬背的方式把《三年高考五年模拟》都记下来了题目和标准答案（训练集），在《三年高考五年模拟》这本书上的题目都会做，扣分很少（训练集上误差小），但是缺少举一反三的能力，但是在真实高考的时候得分很差（测试集上误差大）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 欠拟合</p><p class=\"ztext-empty-paragraph\"><br/></p><p>欠拟合就是过拟合的反面，模型容量太小或者训练不充分，学习能力差，在训练集上误差都很大（更不要谈测试集上的误差了）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 不要老怪过拟合</p><p class=\"ztext-empty-paragraph\"><br/></p><p>上面介绍了过拟合和欠拟合的一些内容，过拟合也是模型训练的基本概念和训练中经常经常碰到的，但有的新手听多了过拟合，一看到模型在验证集上性能不好就怪过拟合，就跑去爬更多的数据，这是很低效和不负责任的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在验证集上性能不好，有可能是优化求解过程的问题，比如：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-9922fe584c222dbd3ffacd15c5058b2d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"608\" data-rawheight=\"324\" class=\"origin_image zh-lightbox-thumb\" width=\"608\" data-original=\"https://pic2.zhimg.com/v2-9922fe584c222dbd3ffacd15c5058b2d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;608&#39; height=&#39;324&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"608\" data-rawheight=\"324\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"608\" data-original=\"https://pic2.zhimg.com/v2-9922fe584c222dbd3ffacd15c5058b2d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-9922fe584c222dbd3ffacd15c5058b2d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图是[ResNet](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1512.03385\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deep Residual Learning for Image Recognition</a>)中的，56层的网络在训练集上性能还没有20层的好，是**训练过程退化**的原因，不是过拟合；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>又比如有的网络，本身就太简单，**在训练集上性能就不咋地！**在验证集上那当然也是不咋地啦，这种情况也不是过拟合，那是欠拟合。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 过拟合解决办法</p><p class=\"ztext-empty-paragraph\"><br/></p><p>上面介绍了过拟合的原因，我们很自然就可以从**(1)数据**、**(2)模型**、**(3)训练方法**三个方面去思考解决过拟合的方法啦。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 数据增强</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f39c288e19363c153ab4ade1069ec5dd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1130\" data-rawheight=\"485\" class=\"origin_image zh-lightbox-thumb\" width=\"1130\" data-original=\"https://pic2.zhimg.com/v2-f39c288e19363c153ab4ade1069ec5dd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1130&#39; height=&#39;485&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1130\" data-rawheight=\"485\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1130\" data-original=\"https://pic2.zhimg.com/v2-f39c288e19363c153ab4ade1069ec5dd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f39c288e19363c153ab4ade1069ec5dd_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图所示，在同样模型假设空间的情况下，数据越多，学习到的模型（红线）过拟合越轻。一般来说，训练数据的数目不应该小于模型参数的若干倍（比如5或10）。这里就不继续介绍数据增强的方法了，感兴趣的朋友可以参见[《深度学习中不平衡样本的处理》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230016-%25E6%25B7%25B1%25E5%25BA%25A6%25E5%25AD%25A6%25E4%25B9%25A0%25E4%25B8%25AD%25E4%25B8%258D%25E5%25B9%25B3%25E8%25A1%25A1%25E6%25A0%25B7%25E6%259C%25AC%25E7%259A%2584%25E5%25A4%2584%25E7%2590%2586/D%25230016.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)一文中的“数据增强”小节。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 改造网络Capacity</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 减层，减特征图通道数</p><p class=\"ztext-empty-paragraph\"><br/></p><p>通过去掉CNN中的层，或者减少特征图的数量可以快速减小模型的学习能力，减轻过拟合。这个原理很直观，不过哪些层可以去掉哪些层不能去，这需要反复实验和深入的分析之后才能做。比如利用BN层的gamma系数来做[网络剪枝](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1708.06519\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Learning Efficient Convolutional Networks through Network Slimming</a>)。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 早停Early Stop</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d612b12cb9b67f968ed7bf63dee5d27e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"475\" data-rawheight=\"284\" class=\"origin_image zh-lightbox-thumb\" width=\"475\" data-original=\"https://pic3.zhimg.com/v2-d612b12cb9b67f968ed7bf63dee5d27e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;475&#39; height=&#39;284&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"475\" data-rawheight=\"284\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"475\" data-original=\"https://pic3.zhimg.com/v2-d612b12cb9b67f968ed7bf63dee5d27e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d612b12cb9b67f968ed7bf63dee5d27e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>当训练一个有足够大表达能力的大模型时，随着训练的迭代，训练集的损失和**验证集**的损失都会会慢慢变小，到了过拟合点之后，训练集的损失继续降低，而验证集的损失会回升，这种现象几乎一定会出现。如果我们训练时检测模型在这两个样本集上的性能，在验证集损失降低到最小（此时也有理由预期在测试集上损失也最小）的时候终止迭代，这种训练方式叫**早停**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 限制权重的惩罚的正则化</p><p class=\"ztext-empty-paragraph\"><br/></p><p>正则化是机器学习中通过显示**控制模型复杂度来避免模型过拟合**的一种有效方式。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-20ebc6a0cd7e5a59afe8e7dc02b5585d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"578\" data-rawheight=\"183\" class=\"origin_image zh-lightbox-thumb\" width=\"578\" data-original=\"https://pic2.zhimg.com/v2-20ebc6a0cd7e5a59afe8e7dc02b5585d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;578&#39; height=&#39;183&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"578\" data-rawheight=\"183\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"578\" data-original=\"https://pic2.zhimg.com/v2-20ebc6a0cd7e5a59afe8e7dc02b5585d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-20ebc6a0cd7e5a59afe8e7dc02b5585d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图所示，如果将模型原始的假设空间比作“天空”，那么天空中自由的小鸟就是模型可能收敛到的一个个最优解。在模型加了正则化之后，就好比将原假设空间（“天空”）约束到一定空间范围内（“笼子”），这样一来，鸟的活动范围（可能得到的最优解）也变得相对有限了，这个笼子就是正则化。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-9a617ddd202d878db226b0920cb030b9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"382\" class=\"origin_image zh-lightbox-thumb\" width=\"550\" data-original=\"https://pic2.zhimg.com/v2-9a617ddd202d878db226b0920cb030b9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;550&#39; height=&#39;382&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"382\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"550\" data-original=\"https://pic2.zhimg.com/v2-9a617ddd202d878db226b0920cb030b9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-9a617ddd202d878db226b0920cb030b9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>那这个笼子应该怎么造呢？还是继续PRML中的例子，如上图中，我们可以发现，当模型阶数越高时，有一些特征所对应的系数绝对值也特别大，这些特别大的绝对值对应的就是模型曲线比较Sharp的地方（**噪音都是比较Sharp的，去学习噪音去了**），而好的模型，不会出现这些Sharp的地方，就没有这些特别大的系数，如果我们可以以某种方式约束最后的模型里面没有这样大的参数，那不就可以了吗。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### L2正则化</p><p class=\"ztext-empty-paragraph\"><br/></p><p>L2正则化，在训练时的目标函数里面添加模型所有系数的平方和![](images/Selection_356.png)，其中lamda是调节L2正则化强度的超参数。可以看到，如果学习到的解有一些比较大的系数，那么损失也比较大，这样，就在优化过程中诱导优化学习方法去寻找那些具有小绝对值权重系数的解。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-074a392473cad85c3a5acb641d4d13e2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1081\" data-rawheight=\"398\" class=\"origin_image zh-lightbox-thumb\" width=\"1081\" data-original=\"https://pic3.zhimg.com/v2-074a392473cad85c3a5acb641d4d13e2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1081&#39; height=&#39;398&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1081\" data-rawheight=\"398\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1081\" data-original=\"https://pic3.zhimg.com/v2-074a392473cad85c3a5acb641d4d13e2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-074a392473cad85c3a5acb641d4d13e2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>正则化不是越大越好的，如上图所示lamda越大，那么最终的模型越趋近于一条直线（针对PRML中用多项式模型去学习Sin分布的例子），太简单了也不好。当取不同lamda大小的时候对模型权重和误差的影响如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-afaee99f1ded629be56471f2b0971db6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"507\" data-rawheight=\"396\" class=\"origin_image zh-lightbox-thumb\" width=\"507\" data-original=\"https://pic3.zhimg.com/v2-afaee99f1ded629be56471f2b0971db6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;507&#39; height=&#39;396&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"507\" data-rawheight=\"396\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"507\" data-original=\"https://pic3.zhimg.com/v2-afaee99f1ded629be56471f2b0971db6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-afaee99f1ded629be56471f2b0971db6_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a4f7d83b539735bea7ec0b79b711b5d5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"501\" data-rawheight=\"369\" class=\"origin_image zh-lightbox-thumb\" width=\"501\" data-original=\"https://pic2.zhimg.com/v2-a4f7d83b539735bea7ec0b79b711b5d5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;501&#39; height=&#39;369&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"501\" data-rawheight=\"369\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"501\" data-original=\"https://pic2.zhimg.com/v2-a4f7d83b539735bea7ec0b79b711b5d5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a4f7d83b539735bea7ec0b79b711b5d5_b.jpg\"/></figure><p>可以看到，正则太强了（对应上上表最右列），学习到的权重都太小，模型没有分辨力（上上上图右边红色曲线）；正则太弱了（对应上上表最左列），学到的权重都太大，过拟合；只有适当的正则强度（上上图中间列），才能学习到好的模型（上上上图左边红色曲线）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>L2正则化在深度学习中常见的说法叫**“权重衰减”(Weight decay)**，在机器学习中常叫为“岭回归”(ridge regression)。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### L1正则化</p><p class=\"ztext-empty-paragraph\"><br/></p><p>L1正则化，就是以绝对值的和作为正则项去限制模型优化空间，原理和L2正则化类似，不赘述。不过L1与L2不同的是，L1比L2相比，还可以驱赶权重往0靠，有**稀疏化和特征选择**的作用，推理详见本人前作[《拉格朗日乘子法》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/tree/master/D%25230008-%25E6%258B%2589%25E6%25A0%25BC%25E6%259C%2597%25E6%2597%25A5%25E4%25B9%2598%25E5%25AD%2590%25E6%25B3%2595\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 集成学习方法</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 随机失活Dropout</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在训练时，对网络的全连接层做随机失活，可以理解为是一种高效的**集成学习**方法（但不需要学习多个模型）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>随即失活就是在训练的时候，随机将一些神经元的激活输出**以概率p设置为0(dropout)**，减小了该神经元和未失活神经元的复杂协同效应(complex co-adaptation)，减小了模型的容量，然后在预测的时候，对各个失活的神经元的**激活输出又乘以(1-p)**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>失活的神经元无法参与训练，因此每次训练（一次前向，一次反向）相当于在训练一个新的网络。而在预测的时候，又相当于若干个子网络的平均集成(**ensemble**)。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>另外，需要注意在深度学习训练框架（例如Caffe）的随机失活层代码(caffe_root/src/caffe/layers/dropout_layer.cpp)中，并没有完全遵照前面的原理，而是在训练阶段直接将未失活神经元的激活输出乘以**scale = 1/(1-p)**，这样的好处是**在测试阶段就不需要再做scale调整了**，这种失活的方式称之为**“倒置随机失活”(inverted dropout)**，很聪明。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### Bagging</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Bagging（也称Bootstrapping）方法是通过**综合几个模型的结果**来降低泛化误差的技术。主要思路是训练几个不同的模型，然后以某种规则综合考虑各个模型的输出来做预测。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Bagging学习奏效的原因是**不同的模型通常不会在测试集上产生完全相同的误差**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Bagging虽然是减少泛化误差非常强大可靠的方法，但是在学术中通常不鼓励使用集成学习的方法来提升整体性能，因为任何机器学习算法都可以从集成学习中获得性能提升（几倍地牺牲计算和存储为代价）。在实践中，因为涉及到成倍增加的训练工作量，在落地的时候计算量和存储也是成倍地增加，所以相比于其它缓解过拟合的方法，Bagging也不大使用。用Bagging最多的地方，应该是一些刷榜刷分的比赛吧？</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文先介绍了过拟合，欠拟合，和模型泛化能力的基本意义，然后从数据，模型和训练的角度介绍了一些基本的常用的缓解过拟合的手段。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [《机器学习与应用》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/30445238/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">book.douban.com/subject</span><span class=\"invisible\">/30445238/</span><span class=\"ellipsis\"></span></a>)</p><p>+ [CNN_book](<a href=\"https://link.zhihu.com/?target=http%3A//210.28.132.67/weixs/book/CNN_book.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">210.28.132.67/weixs/boo</span><span class=\"invisible\">k/CNN_book.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [《深度学习》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/27087503/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度学习 (豆瓣)</a>)</p><p>+ [《Pattern Recognition And Machine Learning》](<a href=\"https://link.zhihu.com/?target=https%3A//www.douban.com/group/471521/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Pattern Recognition And Machine Learning</a>)</p><p>+ [Deep Residual Learning for Image Recognition](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1512.03385\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deep Residual Learning for Image Recognition</a>)</p><p>+ [Learning Efficient Convolutional Networks through Network Slimming](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1708.06519\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Learning Efficient Convolutional Networks through Network Slimming</a>)</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "过拟合", 
                    "tagLink": "https://api.zhihu.com/topics/20683622"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/82448855", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 1, 
            "title": "Batch-Normalization层原理与分析", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>BN层发表于2015年，由于其对随机梯度下降SGD和S函数激活良好的训练加速效果，且还能在一定程度上提升网络泛化能力，在之后的CNN网络中被广泛应用。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 问题提出</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 内部协变量漂移(Internal Covariate Shift)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>首先来看一下什么是协变量漂移。我们知道，基于统计的机器学习理论里面，有一个基础性的假设，就是**“源空间(Source domain)”和“目标空间(Target domain)”的数据分布是一致的**（如果不一致，那需要用到迁移学习(Transfer learning/Domain adaptation)）。协变量漂移属于分布不一致的一个分支情况，它是指**源空间和目标空间的条件分布一样，而边缘分布不一样的情况**。即对所有样本x，有P_s(Y|X=x) = P_t(Y|X=x)，但是P_s(X) != P_t(X)。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>然后来看深度卷积神经网络，后面的层的输入是前面的层的输出，所以，这些串联起来的层可以看成是一层一层的**复合函数**。考虑最简单的两层卷积级联的情况</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-2e777c348ce7939249b351999749a250_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"233\" data-rawheight=\"33\" class=\"content_image\" width=\"233\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;233&#39; height=&#39;33&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"233\" data-rawheight=\"33\" class=\"content_image lazy\" width=\"233\" data-actualsrc=\"https://pic1.zhimg.com/v2-2e777c348ce7939249b351999749a250_b.png\"/></figure><p>，令第一层网络的输出x = F_1(u,theta_1)，它也是第二层网络F_2的输入，那么前面第二层的输出也可以表达为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ce2f669262c11cab42fe39b3a536dc5f_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"147\" data-rawheight=\"34\" class=\"content_image\" width=\"147\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;147&#39; height=&#39;34&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"147\" data-rawheight=\"34\" class=\"content_image lazy\" width=\"147\" data-actualsrc=\"https://pic4.zhimg.com/v2-ce2f669262c11cab42fe39b3a536dc5f_b.png\"/></figure><p>。在用梯度下降法优化更新第二层网络参数theta_2的时候，参数更新规则</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b47aad7438bc142c8e2d09a551e089e7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"338\" data-rawheight=\"80\" class=\"content_image\" width=\"338\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;338&#39; height=&#39;80&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"338\" data-rawheight=\"80\" class=\"content_image lazy\" width=\"338\" data-actualsrc=\"https://pic4.zhimg.com/v2-b47aad7438bc142c8e2d09a551e089e7_b.jpg\"/></figure><p>，可以看到**这就正是网络F_2在学习x的分布。**每次mini-batch跑一次，梯度下降法**更新一次theta_1的值，都会导致x的分布发生变化（即前文所说P_s(X) != P_t(X）），而标签没变（即上文所说的P_s(Y|X=x) = P_t(Y|X=x)），网络层数如果深了（深度学习的网络经常这样），那么这种分布的变化就会层层累积！这就称之为内部协变量漂移(Internal Covariate Shift)**，之所以叫内部，是因为这是对层间信号的分析。如果每次mini-batch跑一次去参数更新来学习的时候，所要学习的输入的分布和上一次mini-batch跑的时候是稳定的的话当然是好事。所以可以合理假设，如果**x的分布在整个网络优化期间可以更稳定**的话（例如，有同样的均值和方差），那么优化theta_2也会更加有效率，毕竟在每次mini-batch迭代的时候，**theta_2不必去补偿上一次迭代由于theta_1的变化而带来的x分布的变化。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>另外，在网络**使用S形函数这类有饱和区的激活函数的时候**，如果每层的输出x（也就是下一层的输入）不稳定的话，随着网络的优化参数的迭代可能就会漂移到激活函数饱和区，饱和区的梯度很小，梯度下降时，每次更新参数的幅度很低，训练效率很低，拉长了训练时间。而且**随着网络层数的增加**，ICS使训练效率下降的这个问题更加严重。虽然这个问题可以通过**使用不带饱和区的激活函数（比如ReLU）**，或者**仔细挑选网络初始化参数**，又或者**选择小的学习率**来缓解，但毕竟不是从根本上解决了ICS，毕竟有的网络使用S函数比ReLU好（比如LSTM和GRU）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>### BN层作用</p><p class=\"ztext-empty-paragraph\"><br/></p><p>上面介绍了ICS和它带来的不好的影响，而BN层的作用，就是为消除ICS而提出的解决方案，使训练时每层的输入的分布更加**稳定**，从而：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. **加速训练**，首先，网络**不必学习每次迭代带来的分布变化**（学习任务更加简单了），其次，训练时可以**选择比较大的学习率**，可以减去drop，Local Response Normalization层，减少训练时对careful weight initialnization依赖，减轻梯度爆炸和梯度消失，在论文中，模型收敛速度提高x10倍以上；</p><p>2. **正则化**功能，提升泛化能力，减轻对dropout和L2的依赖；</p><p>3. **网络可以采用具有非线性饱和特性的激活函数**（比如：S型激活函数）；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>第一点加速训练和第三点可以采用有饱和区的激活函数，前面都有解释，这两点都是比较好理解的。关于第二点，可以这么理解，使用BN训练时，一个样本只与mini-batch中其他样本有相互关系；**对于同一个训练样本，网络的输出会发生变化**，这些效果有助于提升网络泛化能力。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## BN流程</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-048ecff11c67affd65cef78a9c7cb23c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"597\" data-rawheight=\"484\" class=\"origin_image zh-lightbox-thumb\" width=\"597\" data-original=\"https://pic1.zhimg.com/v2-048ecff11c67affd65cef78a9c7cb23c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;597&#39; height=&#39;484&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"597\" data-rawheight=\"484\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"597\" data-original=\"https://pic1.zhimg.com/v2-048ecff11c67affd65cef78a9c7cb23c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-048ecff11c67affd65cef78a9c7cb23c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图为对一个mini-batch做BN层处理的流程。一共分为四步：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 计算当前mini-batch所有样本的**均值**；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2. 计算当前mini-batch所有样本的**方差**；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3. 对当前mini-batch内每个样本用前面的均值和方差做**归一化**；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>4. 对归一化后的样本，乘以一个**缩放**系数，再做一次**平移**；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>前面的三步，都是直接为了稳定x的分布，缓解ICS而做的归一化处理。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>第四步的缩放和平移（缩放和平移参数都是学习到的），是为了使BN层的输出有**重构**原来没有加BN层的时候的网络的输出的能力！</p><p class=\"ztext-empty-paragraph\"><br/></p><p>有1~3步不就可以缓解ICS了吗？为什么还要加这个重构步骤呢？</p><p class=\"ztext-empty-paragraph\"><br/></p><p>可以这么理解，1~3步简单地对样本进行normalize处理会把数据限制在均值0，方差1的范围内，对于型激活函数来说，只使用了其线性部分，会**限制模型的表达能力**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>可能这样说还是不好理解，这样理解：如果是仅仅使用上面的1~3步对网络某一层A的输出数据做归一化，然后送入网络下一层B，这样是会影响到本层网络A所学习到的特征的。打个比方，比如我网络中间某一层学习到特征数据本身就分布在S型激活函数的两侧，你强制把它给我归一化处理、标准差也限制在了1，把数据变换成分布于S函数的中间部分，这样就相当于我这一层网络所学习到的特征分布被你搞坏了，这可怎么办？加上这两个可训练的缩放系数正是为了缓解**最大限度地保存模型的表达能力**（如果真的特征数据分布在S激活函数的两侧，学习一个方差一个均值还是可以学习回来的嘛）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## BN部署</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在部署阶段，我们模型的输入一般就是一个图片，没有了mini-batch的概念，那么均值和方差和缩放平移系数从哪里来呢？其实这些参数都是在训练完成后就**固定不变**了的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 部署细节</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-fc4e7fc71f9bccff69f80e0bd08c513c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"231\" data-rawheight=\"76\" class=\"content_image\" width=\"231\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;231&#39; height=&#39;76&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"231\" data-rawheight=\"76\" class=\"content_image lazy\" width=\"231\" data-actualsrc=\"https://pic1.zhimg.com/v2-fc4e7fc71f9bccff69f80e0bd08c513c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>部署时候的均值和方差，都是用的每个mini-batch均值、方差的无偏估计。再配上学习来的缩放、平移系数，最终的BN层在预测时候的输出就是</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-33b999784410bb7fc10dbe18afd1c298_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"378\" data-rawheight=\"48\" class=\"content_image\" width=\"378\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;378&#39; height=&#39;48&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"378\" data-rawheight=\"48\" class=\"content_image lazy\" width=\"378\" data-actualsrc=\"https://pic1.zhimg.com/v2-33b999784410bb7fc10dbe18afd1c298_b.png\"/></figure><p>。可以看到，**BN层总的效果就是做了一次缩放，一次平移。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### BN在CNN中的变化</p><p class=\"ztext-empty-paragraph\"><br/></p><p>通过上面的介绍，我们知道BN层是对于每个神经元做归一化处理，而不是对一整层网络的神经元进行归一化。既然BN是对单个神经元的运算，那么在CNN中卷积层上要怎么搞？假如某一层卷积层有6个特征图，每个特征图的大小是100x100，这样就相当于这一层网络有6x100x100个神经元，如果采用BN，就会有6x100x100个参数γ、β，这样岂不是太恐怖了。因此卷积层上的BN使用，其实也是使用了类似**权值共享**的策略，把一整张特征图当做一个神经元进行处理：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-25479c4c00bb036fcf157ea1047bfad2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"561\" data-rawheight=\"403\" class=\"origin_image zh-lightbox-thumb\" width=\"561\" data-original=\"https://pic3.zhimg.com/v2-25479c4c00bb036fcf157ea1047bfad2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;561&#39; height=&#39;403&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"561\" data-rawheight=\"403\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"561\" data-original=\"https://pic3.zhimg.com/v2-25479c4c00bb036fcf157ea1047bfad2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-25479c4c00bb036fcf157ea1047bfad2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>CNN中卷积层输出的是一系列的特征图，如果训练的min-batch sizes为m，那么网络下一层输入数据可以表示为四维矩阵(m,f,p,q)，m为min-batch sizes，f为特征图个数，p、q分别为特征图的宽高。在CNN中我们可以把每个特征图看成是一个特征处理（一个神经元），因此在使用Batch Normalization的时候，mini-batch size的大小就是：**mxpxq**，于是对于**每个通道的特征图都只有一对可学习参数：γ、β**。说白了，这就是相当于求取所有样本所对应的一个特征图的所有像素的平均值、方差，然后对这个特征图每个像素统一做归一化。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 加速方法——BN融合</p><p class=\"ztext-empty-paragraph\"><br/></p><p>BN层在训练时学习到了参数缩放γ、平移β、均值u和σ，可以单独作为预测时候的一个层，但更好的方案是和上一个层做融合，消去一个单独BN层带来的计算量。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>假设上一层卷积输出为w\\*x + b，BN层输出为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b1e064925e4eeff78228e4d3620f84a1_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"155\" data-rawheight=\"48\" class=\"content_image\" width=\"155\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;155&#39; height=&#39;48&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"155\" data-rawheight=\"48\" class=\"content_image lazy\" width=\"155\" data-actualsrc=\"https://pic2.zhimg.com/v2-b1e064925e4eeff78228e4d3620f84a1_b.png\"/></figure><p>，则从卷积层的输入x到BN层的输出为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-78179a185b64cf320e2d23ebe8db96dc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"498\" data-rawheight=\"107\" class=\"origin_image zh-lightbox-thumb\" width=\"498\" data-original=\"https://pic1.zhimg.com/v2-78179a185b64cf320e2d23ebe8db96dc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;498&#39; height=&#39;107&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"498\" data-rawheight=\"107\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"498\" data-original=\"https://pic1.zhimg.com/v2-78179a185b64cf320e2d23ebe8db96dc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-78179a185b64cf320e2d23ebe8db96dc_b.jpg\"/></figure><p>。所以在预测阶段，跟在卷积层后面的BN层可以直接融合到这个卷积层中，只需要改造一下卷积核参数和偏移量，而且并没有增加卷积层的计算量。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 另一种声音</p><p class=\"ztext-empty-paragraph\"><br/></p><p>BN论文里面提出BN的原因是在于解决ICS问题，但是2018年NIPS也有[论文](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1805.11604\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">How Does Batch Normalization Help Optimization?</a>)对此提出质疑，并且主张：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. **BN起作用与ICS无关，就算有ICS，BN也可以正常工作**</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-31bd6e19ddca515d74c92e6906926efa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"967\" data-rawheight=\"598\" class=\"origin_image zh-lightbox-thumb\" width=\"967\" data-original=\"https://pic3.zhimg.com/v2-31bd6e19ddca515d74c92e6906926efa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;967&#39; height=&#39;598&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"967\" data-rawheight=\"598\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"967\" data-original=\"https://pic3.zhimg.com/v2-31bd6e19ddca515d74c92e6906926efa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-31bd6e19ddca515d74c92e6906926efa_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>2. **BN有效的本质原因在于其能够使优化空间optimization landscape变得平滑**</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9ef6aa2ef08634a5713756aacd97c3eb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"966\" data-rawheight=\"503\" class=\"origin_image zh-lightbox-thumb\" width=\"966\" data-original=\"https://pic4.zhimg.com/v2-9ef6aa2ef08634a5713756aacd97c3eb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;966&#39; height=&#39;503&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"966\" data-rawheight=\"503\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"966\" data-original=\"https://pic4.zhimg.com/v2-9ef6aa2ef08634a5713756aacd97c3eb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-9ef6aa2ef08634a5713756aacd97c3eb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>3. **其他的Normalization方法，例如L1，L2，L无穷大，也可以起到和BN一样的效果**</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-997f959372de6e4aaf1eff64667291f5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"977\" data-rawheight=\"848\" class=\"origin_image zh-lightbox-thumb\" width=\"977\" data-original=\"https://pic2.zhimg.com/v2-997f959372de6e4aaf1eff64667291f5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;977&#39; height=&#39;848&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"977\" data-rawheight=\"848\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"977\" data-original=\"https://pic2.zhimg.com/v2-997f959372de6e4aaf1eff64667291f5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-997f959372de6e4aaf1eff64667291f5_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1d37f20e22125c27db9a504f6030e18f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"967\" data-rawheight=\"401\" class=\"origin_image zh-lightbox-thumb\" width=\"967\" data-original=\"https://pic4.zhimg.com/v2-1d37f20e22125c27db9a504f6030e18f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;967&#39; height=&#39;401&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"967\" data-rawheight=\"401\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"967\" data-original=\"https://pic4.zhimg.com/v2-1d37f20e22125c27db9a504f6030e18f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1d37f20e22125c27db9a504f6030e18f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>BN层技术对网络训练的加速作用使其在提出之后迅速被广泛应用，本文从BN论文的脉络出发，先介绍了ICS，然后介绍了BN层的技术细节和部署方式，最后略带了一下别的研究者对BN的理解和拓展。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1502.03167\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>)</p><p>+ [&lt;深度学习优化策略-1&gt;Batch Normalization（BN）](<a href=\"https://zhuanlan.zhihu.com/p/26702482\" class=\"internal\">lqfarmer：&lt;深度学习优化策略-1&gt;Batch Normalization（BN）</a>)</p><p>+ [Batch Normalization 学习笔记](<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/hjimce/article/details/50866313\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/hjimce/ar</span><span class=\"invisible\">ticle/details/50866313</span><span class=\"ellipsis\"></span></a>)</p><p>+ [为什么Batch Normalization那么有用？](<a href=\"https://zhuanlan.zhihu.com/p/52749286\" class=\"internal\">autocyz：为什么Batch Normalization那么有用？</a>)</p><p>+ [How Does Batch Normalization Help Optimization?](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1805.11604\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">How Does Batch Normalization Help Optimization?</a>)</p><p>+ [网络inference阶段conv层和BN层的融合](<a href=\"https://zhuanlan.zhihu.com/p/48005099\" class=\"internal\">autocyz：网络inference阶段conv层和BN层的融合</a>)</p><p>+ [《机器学习与应用》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/30445238/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习与应用 (豆瓣)</a>)</p><p>+ [CNN_book](<a href=\"https://link.zhihu.com/?target=http%3A//210.28.132.67/weixs/book/CNN_book.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">210.28.132.67/weixs/boo</span><span class=\"invisible\">k/CNN_book.pdf</span><span class=\"ellipsis\"></span></a>)</p>", 
            "topic": [
                {
                    "tag": "批量归一化", 
                    "tagLink": "https://api.zhihu.com/topics/20687433"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "优化", 
                    "tagLink": "https://api.zhihu.com/topics/19570512"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/82448662", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 0, 
            "title": "DeepID1,DeepID2,DeepID2+和DeepID3", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>DeepID系列是比较早地(2014年)将CNN引入人脸识别的算法，出自于著名的香港中文大学和中科院高等技术研究所的汤晓鸥，王晓刚团队。本文按照次系列论文发表的时间顺序逐一进行解读和介绍。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## [DeepID1](<a href=\"https://link.zhihu.com/?target=http%3A//mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">mmlab.ie.cuhk.edu.hk/pd</span><span class=\"invisible\">f/YiSun_CVPR14.pdf</span><span class=\"ellipsis\"></span></a>)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 简单介绍</p><p class=\"ztext-empty-paragraph\"><br/></p><p>此文发表于**2014年CVPR**上，以人脸识别的子领域**人脸验证(Face Verification)**为目标，文章**并没有用直接用二类分类CNN实现人脸验证**，而是通过学习一个多类（10000类，每个类大约有20个实例）人脸识别任务来学习特征，并把学到的特征使用到face verification和其他unseen新的identification。它使用人脸上不同的patch训练多个单独的ConvNet，每个ConvNet的最后一个隐层为提取到的特征，称之为DeepID(Deep hidden IDentitiy feature)。最后将这些patch提取到的DeepID **concate**起来作为整个Face的feature送入Joint Bayesian分类器（当然也可以是其他的分类器）做二分类就可以做人脸验证了。文章主要的贡献在于用很多不同的Face patch分别训练以一个很难的分类任务（~10000个不同身份的人）的ConvNet，得到一些分辨力很强的over-complete representations，最后在不需要严格对齐的前提下于LFW上取得了97.45%的人脸对比精度。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 算法流程</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f0f05f38b28e2a52a49a5749a7a2d101_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"578\" data-rawheight=\"554\" class=\"origin_image zh-lightbox-thumb\" width=\"578\" data-original=\"https://pic2.zhimg.com/v2-f0f05f38b28e2a52a49a5749a7a2d101_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;578&#39; height=&#39;554&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"578\" data-rawheight=\"554\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"578\" data-original=\"https://pic2.zhimg.com/v2-f0f05f38b28e2a52a49a5749a7a2d101_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f0f05f38b28e2a52a49a5749a7a2d101_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图所示的为特征提取部分（本文主要在于特征提取，验证任务就用联合贝叶斯做二分类就好了）的示意图。首先，从输入人脸中扣除一些face patches，然后把这些patch输入同一个结构不同参数的ConvNet中提取160维DeepID特征，把这些DeepID特征concate起来就是这个人脸的feature了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这里需要注意如下几点：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 不是一整张人脸作为输入的，**输入的是人脸上抠图出来的一些patch**；</p><p>2. **每个patch都有单独的一个ConvNet**提取对应的子DeepID特征；</p><p>3. DeepID特征是在**最后一个隐层提取**，而不是输出层，那个10000个节点的输出层，是训练时候才有的；</p><p>4. 多个patch的DeepID concate成最后整个人脸的特征，所以最后一张人脸的**特征长度是160乘以patch的个数**，而不是仅仅160维。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 网络结构</p><p class=\"ztext-empty-paragraph\"><br/></p><p>上面介绍了整体的特征提取流程，下面介绍特征提取所用的子ConvNet的结构</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-35b4496f9f130424aa42516014c623b5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"580\" data-rawheight=\"419\" class=\"origin_image zh-lightbox-thumb\" width=\"580\" data-original=\"https://pic2.zhimg.com/v2-35b4496f9f130424aa42516014c623b5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;580&#39; height=&#39;419&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"580\" data-rawheight=\"419\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"580\" data-original=\"https://pic2.zhimg.com/v2-35b4496f9f130424aa42516014c623b5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-35b4496f9f130424aa42516014c623b5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>共5层网络，越往上的神经元的个数就越少，到最后就剩下160个神经元的输出，上面的Face patches是对齐过后的的人脸块，也就是说已左（右）眼为中心的人脸区域块，嘴角为中心的人脸区域块等等，这样就有多个不同的输入块输入到CNN中，采用了把**Max pooling layer3+Conv layer4的输出作为特征**（不同层次的特征抽象程度不一样；而且经过连续的降采样，Conv layer4所含的神经元已经太少，前面损失了太多的信息，所以加入Max pooling layer3里面的信息中和一下）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>输入图像：39x31xk 个矩形人脸图像块+31x31xk（这里k在彩色图像时为3，灰度时k为1）个人脸正方形块（因为后面要考虑到是全局图像还是局部图像，灰度图像和彩色图像，且需要考虑到尺度问题），使用ReLU非线性处理，用随机梯度下降优化网络。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 一个人脸最后的特征</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-103e5202601433b5aeeb6ec77003b61c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"579\" data-rawheight=\"516\" class=\"origin_image zh-lightbox-thumb\" width=\"579\" data-original=\"https://pic1.zhimg.com/v2-103e5202601433b5aeeb6ec77003b61c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;579&#39; height=&#39;516&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"579\" data-rawheight=\"516\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"579\" data-original=\"https://pic1.zhimg.com/v2-103e5202601433b5aeeb6ec77003b61c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-103e5202601433b5aeeb6ec77003b61c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图所示，每个人脸会提取**3个Scale**的**10个patch**(以鼻头，左眼，右眼，左嘴角，右嘴角为中心)，然后**彩色图加上灰度图**，加上**水平翻转**数量又翻倍，一共是**3scalex10patchx2typex2flip=120**个输入图片分别送入**60个Conv Net**（水平翻转和原图，用同一个Conv Net），提取了一共120x160=**19200维**的特征向量，作为这个脸的最终送入联合贝叶斯做分类或者验证的特征。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>另外有些特例，两个以嘴角和两个眼球为中心的这四个patch，不做翻转操作，直接用他们的对称patch最为他们翻转图的特征。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 网络训练</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 样本准备</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在DeepID的实验过程中，使用的外部数据集为CelebFaces+，有10177人，202599张图片；8700人训练DeepID，1477人训练Joint Bayesian分类器。切分的patch（也就是上图这样的数据）数目为100，使用了五种不同的scale。每张图片最后形成的向量长度为32000，使用PCA降维到150。如此，达到97.20%的效果。使用某种Transfer Learning的算法后，在LFW上达到97.45%的验证精度。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 目标函数</p><p class=\"ztext-empty-paragraph\"><br/></p><p>训练Conv Net使用10000分类的**交叉熵**最为目标函数。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 实验结论</p><p class=\"ztext-empty-paragraph\"><br/></p><p>论文最后作者做了一系列实验，比较重要的结论有：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 使用Multi-scale patches的Conv Net比只使用一个只有整张人脸的patch的效果要好；</p><p>2. DeepID自身的分类错误率在40%到60%之间震荡，虽然较高，但**DeepID是用来学特征的**，并不需要要关注自身分类错误率；</p><p>3. 如果只DeepID神经网络的最后一层Softmax层作为特征表示，效果很差；</p><p>4. 随着DeepID的训练集的增长，DeepID本身的分类正确率和LFW的验证正确率都在增加。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## [DeepID2](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1406.4773\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deep Learning Face Representation by Joint Identification-Verification</a>)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 简单介绍</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在DeepID1取得了97.45%的人脸验证准确率之后，作者团队再接再厉，**同一年在NIPS**上发表《Deep Learning Face Representation by Joint Identification-Verification》一文，将LFW上人脸验证的准确率从97.45%提升到了99.15%，因为本文的方法直接迭代于上文中的DeepID1，大家就叫它DeepID2。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>从论文题目可以看出来，DeepID2的主要创新点在于在训练网络的时候**同时使用了分类损失信号和鉴定损失信号**，这两种互补的信号来监督训练，得到的特征比DeepID1只用分类信号来比，更具有辨别力（不同类之间的距离大，同类之间的距离小）。这个思路和[线性判别分析LDA](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230014-%25E6%2595%25B0%25E6%258D%25AE%25E9%2599%258D%25E7%25BB%25B4%25E5%25B8%25B8%25E7%2594%25A8%25E6%2596%25B9%25E6%25B3%2595%25E6%2580%25BB%25E7%25BB%2593%28LDA%252CPCA%29/D%25230014.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)思路相似，不同的是线性判别分析用的是线性映射，而这里的卷积神经网络用的是复杂的非线性映射。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 算法流程</p><p class=\"ztext-empty-paragraph\"><br/></p><p>DeepID2的大致流程和DeepID1一样的，都是单独训练网络用来提取各个patch的DeepID特征，然后输入给另外单独训练的贝叶斯分类器去做人脸验证。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 网络结构</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4f245cc76671cb5694eb28e7c96662fd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"792\" data-rawheight=\"253\" class=\"origin_image zh-lightbox-thumb\" width=\"792\" data-original=\"https://pic2.zhimg.com/v2-4f245cc76671cb5694eb28e7c96662fd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;792&#39; height=&#39;253&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"792\" data-rawheight=\"253\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"792\" data-original=\"https://pic2.zhimg.com/v2-4f245cc76671cb5694eb28e7c96662fd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-4f245cc76671cb5694eb28e7c96662fd_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图所示为DeepID2的网络结构示意图。输入稍微变大了一些，但是同样使用了Max pooling layer3和Conv layer4的输出联合起来作为160维的DeepID特征。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 一个人脸最后的特征</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e2712f91e32500e74573cb121f3831c4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"964\" data-rawheight=\"266\" class=\"origin_image zh-lightbox-thumb\" width=\"964\" data-original=\"https://pic1.zhimg.com/v2-e2712f91e32500e74573cb121f3831c4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;964&#39; height=&#39;266&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"964\" data-rawheight=\"266\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"964\" data-original=\"https://pic1.zhimg.com/v2-e2712f91e32500e74573cb121f3831c4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e2712f91e32500e74573cb121f3831c4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 作者使用400个face patch训练了200个ConvNet；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2. 使用了**forward-backward greedy**算法从400个face patch挑选了如上图所示的25个patch，提取出25个160维特征，也就是25x160=4000维的特征向量；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3. 然后，利用**PCA**做特征选择，进一步将这4000维的特征向量压缩到180维。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>顺便提一句，作者最后利用这180维特征向量训练出来一个联合贝叶斯分类器作为人脸验证任务的分类器。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 网络训练</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 样本准备</p><p class=\"ztext-empty-paragraph\"><br/></p><p>DeepID2使用的外部数据集仍然是CelebFaces+，但先把CelebFaces+进行了切分，切分成了CelebFaces+A(8192个人)和CelebFaces+B(1985个人)。首先，训练DeepID2，CelebFaces+A做训练集，此时CelebFaces+B做验证集；其次，CelebFaces+B切分为1485人和500人两个部分，进行特征选择，选择25个patch。最后在CelebFaces+B整个数据集上训练联合贝叶斯模型，然后在LFW上进行测试。在上一段描述的基础上，进行了组合模型的加强，即在选取特征时进行了七次。第一次选效果最好的25个patch，第二次从剩余的patch中再选25个，以此类推。然后将七个联合贝叶斯模型使用SVM进行融合。最终达到了99.15%的结果。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 目标函数</p><p class=\"ztext-empty-paragraph\"><br/></p><p>同时使用识别**交叉熵损失**</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b0cd8b6c04d5bde435c761495d6559dc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"431\" data-rawheight=\"74\" class=\"origin_image zh-lightbox-thumb\" width=\"431\" data-original=\"https://pic1.zhimg.com/v2-b0cd8b6c04d5bde435c761495d6559dc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;431&#39; height=&#39;74&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"431\" data-rawheight=\"74\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"431\" data-original=\"https://pic1.zhimg.com/v2-b0cd8b6c04d5bde435c761495d6559dc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b0cd8b6c04d5bde435c761495d6559dc_b.jpg\"/></figure><p>加上验证**L2损失**</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5262a2db871c824c383449382ee1d371_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"689\" data-rawheight=\"85\" class=\"origin_image zh-lightbox-thumb\" width=\"689\" data-original=\"https://pic2.zhimg.com/v2-5262a2db871c824c383449382ee1d371_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;689&#39; height=&#39;85&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"689\" data-rawheight=\"85\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"689\" data-original=\"https://pic2.zhimg.com/v2-5262a2db871c824c383449382ee1d371_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5262a2db871c824c383449382ee1d371_b.png\"/></figure><p>作为网络训练的目标函数。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**交叉熵损失主要是为了增大不同id的人(inter-personal)提取出来的特征的距离；而L2损失是为了使同一个id的人(intra-personal)的特征，尽可能的聚拢在一起。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 实验结论</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 作者验证了lamda（即验证loss相对于识别loss的权值）对人脸验证准确率的影响。当lamda=0（相当于不用验证loss）或lamda=正无穷大(相当于不用识别loss)，人脸验证效果都不如lamda在俩者之间的取值。作者采用不同的lamda取值测试L2人脸验证模型和联合贝叶斯人脸验证模型，从实验结果可以看出lamda从0增加到+无穷大时，两种人脸验证模型的准确率都是先升高后降低；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2. 当用于训练DeepID2的人脸类别越丰富（即人脸类别数），通过CNN学习的特征在人脸识别阶段会越有效，该结论与DeepID1是类似的；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3. 作者测试了不同形式的验证loss函数（L2+ loss, L2- loss, 余弦loss）对于人脸验证结果的影响，此处不作介绍；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>4. 作者选取了七组不重复的特CNN特征组合，用联合贝叶斯方法处理后，进一步采用SVM对结果融合，得到最终结果在LFW上达到了99.15%的验证精度（太过Trick了）</p><p class=\"ztext-empty-paragraph\"><br/></p><p>总之，作者通过**修改CNN网络模型**（输入分辨率）和**Loss**（最重要的修改）的方式训练得到新的DeepID2特征，通过进化版本的特征组合方式，实现了99.15%的人脸验证准确率。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## [DeepID2+](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1412.1265v1.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1412.1265</span><span class=\"invisible\">v1.pdf</span><span class=\"ellipsis\"></span></a>)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 简单介绍</p><p class=\"ztext-empty-paragraph\"><br/></p><p>DeepID2+论文《Deeply learned face representations are sparse, selective, and robust》发表于**2015年CVPR上**。其主要做了如下两点工作：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 通过在DeepID2的基础上，**增加网络中间层的维度和对浅层卷积增加监督训练的方法，用新的DeepID2+网络**，提升LFW上的识别精度到了99.47%；</p><p>2. 通过对DeepID2+网络的的实验分析，得出三点结论：</p><p>   + **稀疏性：网络的神经元输出有适度的稀疏性**，适度的稀疏性有助于最大化输出特征的辨别力；甚至使在**二值化**后，网络的输出还能保证良好的识别精度；</p><p>   + **选择性：高层神经元对个体相关的属性比较敏感**，即对同一个人的人脸来说，总有一些特定的高层神经元的输出要么一直处于激活状态，要么一直处于抑制状态；</p><p>   + **鲁棒性：虽然训练集中没有部分遮挡的样本，但是DeepID2+依然对遮挡很鲁棒**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 算法流程</p><p class=\"ztext-empty-paragraph\"><br/></p><p>大致和DeepID2一致的，不再赘述。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 网络结构</p><p class=\"ztext-empty-paragraph\"><br/></p><p>和DeepID2的网络结构类似，不过输出**特征从DeepID的160维增大到了512维**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 网络训练</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 样本准备</p><p class=\"ztext-empty-paragraph\"><br/></p><p>与DeepID2的8千人的16W训练样本相比，DeepID2+**扩充了训练数据集**，合并了CelebFaces+，WDRef数据集还有新增的一些人脸样本，一共收集了来自于1.2W人的29W人脸样本。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 目标函数</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-93644cb2f8fd58c73c14b2cfd794c8dc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"672\" class=\"origin_image zh-lightbox-thumb\" width=\"594\" data-original=\"https://pic1.zhimg.com/v2-93644cb2f8fd58c73c14b2cfd794c8dc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;594&#39; height=&#39;672&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"672\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"594\" data-original=\"https://pic1.zhimg.com/v2-93644cb2f8fd58c73c14b2cfd794c8dc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-93644cb2f8fd58c73c14b2cfd794c8dc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>和DeepID2一样，识别信号用**交叉熵**，验证信号用**L2损失**，两者加权为网络训练时候的总损失。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>所不同的是，如上图所示，在训练的时候，对Conv layer1, Conv layer2这些浅层也**增加了监督信号**来训练。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 实验结论</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这里有两个比较有价值的实验：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 二值化实验：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>   作者发现，DeepID2+的网络输出的特征具有稀疏性和选择性，即对每个人，最后的DeepID层都大概有半数的单元是激活的，半数的单元是抑制的。**而不同的人，激活或抑制的单元是不同的**。基于此性质，如下图所示，使用阈值对最后输出的512维向量进行了二值化处理，发现效果降低有限。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5787daf8089e3b5e6f3768f4886eb49f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"595\" data-rawheight=\"515\" class=\"origin_image zh-lightbox-thumb\" width=\"595\" data-original=\"https://pic4.zhimg.com/v2-5787daf8089e3b5e6f3768f4886eb49f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;595&#39; height=&#39;515&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"595\" data-rawheight=\"515\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"595\" data-original=\"https://pic4.zhimg.com/v2-5787daf8089e3b5e6f3768f4886eb49f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5787daf8089e3b5e6f3768f4886eb49f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>   二值化后会有好处，即通过计算**汉明距离**就可以进行检索了。然后精度保证的情况下，可以使人脸检索变得速度更快，更接近实用场景。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2. 遮挡实验：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>   在训练数据中没有遮挡数据的情况下，DeepID2+自动就对遮挡有了很好的鲁棒性。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9087a6567244213cd58048ba30954ef4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"334\" class=\"origin_image zh-lightbox-thumb\" width=\"594\" data-original=\"https://pic1.zhimg.com/v2-9087a6567244213cd58048ba30954ef4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;594&#39; height=&#39;334&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"334\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"594\" data-original=\"https://pic1.zhimg.com/v2-9087a6567244213cd58048ba30954ef4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-9087a6567244213cd58048ba30954ef4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>   如上图所示，有两种方式对人脸进行多种尺度的遮挡，第一种是从下往上进行遮挡，从10%-70%。第二种是不同大小的黑块随机放，黑块的大小从10×10到70×70。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>   两种遮挡的实验对性能的影响如下图所示，</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6c981a81b90dd1d2d4d3a3fa96dd86d9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"537\" class=\"origin_image zh-lightbox-thumb\" width=\"594\" data-original=\"https://pic2.zhimg.com/v2-6c981a81b90dd1d2d4d3a3fa96dd86d9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;594&#39; height=&#39;537&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"537\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"594\" data-original=\"https://pic2.zhimg.com/v2-6c981a81b90dd1d2d4d3a3fa96dd86d9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6c981a81b90dd1d2d4d3a3fa96dd86d9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-08af4de08084ec8e6722f88f67640750_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"595\" data-rawheight=\"419\" class=\"origin_image zh-lightbox-thumb\" width=\"595\" data-original=\"https://pic1.zhimg.com/v2-08af4de08084ec8e6722f88f67640750_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;595&#39; height=&#39;419&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"595\" data-rawheight=\"419\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"595\" data-original=\"https://pic1.zhimg.com/v2-08af4de08084ec8e6722f88f67640750_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-08af4de08084ec8e6722f88f67640750_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>   结论是遮挡在20%以内，块大小在30×30以下，DeepID2+的输出的向量的验证正确率几乎不变。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## [DeepID3](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1502.00873.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1502.0087</span><span class=\"invisible\">3.pdf</span><span class=\"ellipsis\"></span></a>)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 简单介绍</p><p class=\"ztext-empty-paragraph\"><br/></p><p>DeepID3是DeepID系列最后一篇，**主要是探索性质为主**，亮点并不是很多，发表于**2015年**，它在DeepID2+的基础上，借鉴ILSVRC 2014表现不俗的VGG和GoogLeNet的启发，尝试使用更深的网络和Inception层来提升识别率。最终识别率在LFW上达到了99.53%，和DeepID2+的最终结果99.47%差不多，而且LFW数据集里面有三对人脸被错误地标记了，在更正这些错误的label后，两者准确率均为99.52％。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 算法流程</p><p class=\"ztext-empty-paragraph\"><br/></p><p>与DeepID2+一样，不再赘述。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 网络结构</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-23d172b4c7273c6aaf84f293dca576bd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"439\" data-rawheight=\"862\" class=\"origin_image zh-lightbox-thumb\" width=\"439\" data-original=\"https://pic2.zhimg.com/v2-23d172b4c7273c6aaf84f293dca576bd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;439&#39; height=&#39;862&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"439\" data-rawheight=\"862\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"439\" data-original=\"https://pic2.zhimg.com/v2-23d172b4c7273c6aaf84f293dca576bd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-23d172b4c7273c6aaf84f293dca576bd_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-efce882791dfd949d961bd4763f7c4ab_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"440\" data-rawheight=\"868\" class=\"origin_image zh-lightbox-thumb\" width=\"440\" data-original=\"https://pic4.zhimg.com/v2-efce882791dfd949d961bd4763f7c4ab_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;440&#39; height=&#39;868&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"440\" data-rawheight=\"868\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"440\" data-original=\"https://pic4.zhimg.com/v2-efce882791dfd949d961bd4763f7c4ab_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-efce882791dfd949d961bd4763f7c4ab_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>DeepID3分别借鉴于VGG和GoogLeNet提出了两种网络结构，与DeepID2+相比，不同点在于：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. **网络深度更深了**；</p><p>2. 出现了**连续两个conv layer直接相连**的情况，这样使得网络具有更大的receptive fields和更复杂的nonlinearity，同时还能限制参数的数量；</p><p>3. DeepID3 Net2使用了**Inception结构**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 网络训练</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 样本准备</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在训练样本上，DeepID3仍采用**原来DeepID2+中使用的样本**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 目标函数</p><p class=\"ztext-empty-paragraph\"><br/></p><p>同样的交叉熵加上L2，不同的是在网络的前段加入了**更多的监督分支**，目的有：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 监督前段网络学习更好的信号；</p><p>2. 使深网络更容易训练。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 实验结论</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d91de18a9a285472565a9da9dc164123_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"537\" data-rawheight=\"411\" class=\"origin_image zh-lightbox-thumb\" width=\"537\" data-original=\"https://pic4.zhimg.com/v2-d91de18a9a285472565a9da9dc164123_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;537&#39; height=&#39;411&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"537\" data-rawheight=\"411\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"537\" data-original=\"https://pic4.zhimg.com/v2-d91de18a9a285472565a9da9dc164123_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d91de18a9a285472565a9da9dc164123_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>DeepID3在LFW上的face verification准确率为99.53％，性能上并没有比DeepID2+的99.47％提升多少。而且LFW数据集里面有三对人脸被错误地标记了，在更正这些错误的label后，两者准确率均为99.52％。因此，作者对于具有更深的架构网络是否具有更强的优势没有下定论，可能是数据集少了，这么深的网络训练不充分，也可能是别的原因。为之后的研究方向。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 综合比较</p><p>|  论文  |  发表年份  |  创新点  | 目标函数 | 分类信号 | 验证信号 | LFW性能 |</p><p>| :----: | :------: | :-----: | :-----: | :----: | :-----: | :----: |</p><p>| DeepID | 2014CVPR | **多patch训练；多层特征图融合** | 交叉熵 | YES | NO | 97.45% |</p><p>| DeepID2| 2014NIPS | **增大网络输入分辨率；加入验证信号** | 交叉熵，L2 | YES | YES | 99.15% |</p><p>| DeepID2+ | 2015CVPR | **训练增加浅层的监督信号；讨论稀疏性，选择性和遮挡鲁棒性** | 交叉熵，L2 | YES | YES | 99.52% |</p><p>| DeepID3 | 2015N/A | **借鉴VGG和GoogLeNet，堆叠Conv，添加Inception层** | 交叉熵，L2 | YES | YES | 99.52% |</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文总结了2014~2015年出现的DeepID系列人脸识别模型，虽然属于比较早期的算法思路，但是其基本思想和创新点，比如DeepID1的不同层的特征图做融合，比如DeepID2的验证信号和识别信号做融合，DeepID2+的多层信号监督训练，DeepID2+的二值化和遮挡实验，DeepID3的借鉴经典网络这些思路，都是做算法研究和实际项目时候可以学习的东西。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>温故而知新，可以为师矣。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [Deep Learning Face Representation from Predicting 10,000 Classes](<a href=\"https://link.zhihu.com/?target=http%3A//mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">mmlab.ie.cuhk.edu.hk/pd</span><span class=\"invisible\">f/YiSun_CVPR14.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [Deep Learning Face Representation by Joint Identification-Verification](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1406.4773\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deep Learning Face Representation by Joint Identification-Verification</a>)</p><p>+ [Deeply learned face representations are sparse, selective, and robust](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1412.1265v1.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1412.1265</span><span class=\"invisible\">v1.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [DeepID3: Face Recognition with Very Deep Neural Networks](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1502.00873.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1502.0087</span><span class=\"invisible\">3.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [DeepID1,DeepID2](<a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/venus024/p/5632243.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">DeepID1,DeepID2 - venus024 - 博客园</a>)</p><p>+ [DeepID人脸识别算法之三代](<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/stdcoutzyx/article/details/42091205\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/stdcoutzy</span><span class=\"invisible\">x/article/details/42091205</span><span class=\"ellipsis\"></span></a>)</p>", 
            "topic": [
                {
                    "tag": "人脸识别", 
                    "tagLink": "https://api.zhihu.com/topics/19559196"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/82448302", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 0, 
            "title": "MTCNN和FaceBoxes", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>人脸检测是人工智能落地运用的最充分的一个领域，它是属于通用目标检测的一个子领域（只判断是不是人脸）。本文介绍两种实际项目中运用的比较多的深度学习人脸检测算法，MTCNN和FaceBoxes。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 人脸检测简要历史回顾（2018之前）</p><p class=\"ztext-empty-paragraph\"><br/></p><p>１．[VJ算法]((<a href=\"https://link.zhihu.com/?target=http%3A//www.face-rec.org/algorithms/Boosting-Ensemble/16981346.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">face-rec.org/algorithms</span><span class=\"invisible\">/Boosting-Ensemble/16981346.pdf</span><span class=\"ellipsis\"></span></a>))</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在2001年，Viola和Jones在IJCV上发表了论文[Robust Real-Time Face Detection](<a href=\"https://link.zhihu.com/?target=http%3A//www.face-rec.org/algorithms/Boosting-Ensemble/16981346.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">face-rec.org/algorithms</span><span class=\"invisible\">/Boosting-Ensemble/16981346.pdf</span><span class=\"ellipsis\"></span></a>)，文中设计了一种快速人脸检测算法，其利用简单地Haar特征训练**级联的**AdaBoost分类器作为分类器，再利用**滑动窗口方法**穷举人脸可能区域送入分类器进行人脸/非人脸的分类（检测）。这个算法的速度较之前的算法有2个数量级的提高，并且有很高的精度，人们称之为**VJ框架**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2. [Cascade CNN](<a href=\"https://link.zhihu.com/?target=http%3A//users.eecs.northwestern.edu/~xsh835/assets/cvpr2015_cascnn.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">users.eecs.northwestern.edu</span><span class=\"invisible\">/~xsh835/assets/cvpr2015_cascnn.pdf</span><span class=\"ellipsis\"></span></a>)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>论文发表于2015年CVPR上，其集成了VJ框架的分类器级联结构，并利用三个**级联**的**检测网络**去由简单到复杂地过滤人脸，并穿插三个**校准网络**对每个检测子网络的输出进行校正。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3. [Faceness Net](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1701.08393.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1701.0839</span><span class=\"invisible\">3.pdf</span><span class=\"ellipsis\"></span></a>)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Faceness Net论文发表于2017年PAMI上，其主要创新点是用多个卷积网络分别对人脸各个部件，例如头发，眼睛，鼻子，嘴巴，胡须等进行检测，再加一个卷积网络对这个检测到的部件进行优化，输出检测到的结果。由于使用了**部件检测器**，其对人脸的遮挡具有非常好的鲁棒性。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## MTCNN</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 简单介绍</p><p class=\"ztext-empty-paragraph\"><br/></p><p>MTCNN是**M**ulti **T**ask **C**ascaded **C**onvolutional **N**et的缩写，出自中科院深圳先进技术研究院和香港中文大学，它是一个利用多个小CNN网络级联来做人脸检测的模型。[论文](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1604.02878\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks</a>)发表于2016年的ECCV，性能属于当时的STOA。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>它借鉴了VJ算法的级联分类器框架，借鉴了Cascade CNN算法的多个卷积网络由粗到细的分类，不同点在于将bounding regression，landmark regression和face classification这**三个互补的任务放在一起训练**，另外还加入了**在线难例挖掘**(Online Hard Sample Mining)，最终在减少了子网络的数量和计算量的同时保持了良好的性能。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 算法流程</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3b37ddc866cc1920eb72fd9aaf7d39a7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"689\" data-rawheight=\"940\" class=\"origin_image zh-lightbox-thumb\" width=\"689\" data-original=\"https://pic4.zhimg.com/v2-3b37ddc866cc1920eb72fd9aaf7d39a7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;689&#39; height=&#39;940&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"689\" data-rawheight=\"940\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"689\" data-original=\"https://pic4.zhimg.com/v2-3b37ddc866cc1920eb72fd9aaf7d39a7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3b37ddc866cc1920eb72fd9aaf7d39a7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图是MTCNN算法工作流程示意图，整个流程分为三个Stage：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ Stage1，用**P**roposal Net输出人脸候选框和五个关键点坐标，值得注意的是，P-Net是一个**全卷积网络**，可以接受**任意尺寸**的图片，输出的特征图可以上采样到输入图片的坐标，那样就**只需要做一次前向操作**，所以处理速度比滑动窗口要快很多。本阶段的**主要任务是提供人脸Proposals，要求比较高的Recall**（可以有适当的误报，假阳性可以在后面的阶段处理掉）；</p><p>+ Stage2，用**R**efinement Net对Stage1输出的Proposals进行细化，抛弃掉非人脸Proposals，并进一步**细化微调**框和特征点的位置；</p><p>+ Stage3，**O**utput Net以Stage2输出的人脸框为输入，进一步进行人脸的分类，框和特征点的回归和细化，输出最终的检测结果。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 网络结构</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-40313cb4ea401f822b64503de764a374_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1291\" data-rawheight=\"473\" class=\"origin_image zh-lightbox-thumb\" width=\"1291\" data-original=\"https://pic1.zhimg.com/v2-40313cb4ea401f822b64503de764a374_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1291&#39; height=&#39;473&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1291\" data-rawheight=\"473\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1291\" data-original=\"https://pic1.zhimg.com/v2-40313cb4ea401f822b64503de764a374_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-40313cb4ea401f822b64503de764a374_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图是P-Net，R-Net和O-Net的网络结构示意图。**这里为了加速考虑，让P-Net的输入最小，为了精度考虑，后面两个网络的输入分辨率成倍递增。**每个子网络都同时输出三个信息：人脸分类，人脸框回归和人脸特征点回归。和前面介绍的Cascade CNN模型相比，主要改动在</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 将5x5卷积核改成了更小的3x3的卷积核，以减小模型计算量；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2. **增加了卷积核的通道数，以学习到更多的更丰富更有辨别力（discriminative description）的特征**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 网络训练</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 样本准备</p><p class=\"ztext-empty-paragraph\"><br/></p><p>因为是联合三种任务进行训练，所以训练样本需要如下四种不同标注的不同图片：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**1. 负样本图片**：和所有真值人脸的IoU(Intersection over Union)小于0.3的图片作为人脸负样本；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**2. 正样本图片**：和任意真值人脸的IoU大于0.65的图片作为人脸正样本图片；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**3. 部分人脸图片**：和任意真值人脸的IoU在(0.4,0.65)这个区间的图片作为部分人脸图片；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**4. 特征点训练图片**：标记好了五个点坐标信息的人脸图片。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>每个任务所用到的图片为：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>|     任务     | 负样本 | 正样本 | 部分样本 | 特征点训练样本 |</p><p>| :----------: | :---: | :---: | :-----: | :---------: |</p><p>|    人脸分类   |   Y    |   Y   |   N    |      N       |</p><p>|   人脸框回归  |   N    |   Y   |   Y     |     N       |</p><p>| 面部特征点回归 |   N    |   N   |   N     |     Y       |</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 目标函数</p><p class=\"ztext-empty-paragraph\"><br/></p><p> 每个网络的训练，都用了人脸分类，人脸框回归和人脸特征点回归三种任务同时进行。对人脸分类，取**二分类交叉熵**</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8255465d5dd491207fea6222adea955e_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"559\" data-rawheight=\"45\" class=\"origin_image zh-lightbox-thumb\" width=\"559\" data-original=\"https://pic3.zhimg.com/v2-8255465d5dd491207fea6222adea955e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;559&#39; height=&#39;45&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"559\" data-rawheight=\"45\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"559\" data-original=\"https://pic3.zhimg.com/v2-8255465d5dd491207fea6222adea955e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8255465d5dd491207fea6222adea955e_b.png\"/></figure><p>作为目标函数；对人脸框回归取**L2损失**</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5d03d6d93c1eca1df0e739b3e8f3fed3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"262\" data-rawheight=\"46\" class=\"content_image\" width=\"262\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;262&#39; height=&#39;46&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"262\" data-rawheight=\"46\" class=\"content_image lazy\" width=\"262\" data-actualsrc=\"https://pic4.zhimg.com/v2-5d03d6d93c1eca1df0e739b3e8f3fed3_b.jpg\"/></figure><p>作为目标函数；对人脸面部五个特征点回归也是取**L2损失**</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-606ae48bba01afd9126fa51014939545_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"448\" data-rawheight=\"53\" class=\"origin_image zh-lightbox-thumb\" width=\"448\" data-original=\"https://pic2.zhimg.com/v2-606ae48bba01afd9126fa51014939545_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;448&#39; height=&#39;53&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"448\" data-rawheight=\"53\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"448\" data-original=\"https://pic2.zhimg.com/v2-606ae48bba01afd9126fa51014939545_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-606ae48bba01afd9126fa51014939545_b.png\"/></figure><p>作为目标函数。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>最终每个网络训练的时候，对以上三个任务的目标函数取加权和</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-2821664d57818544a9ee5b47f575847d_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"420\" data-rawheight=\"52\" class=\"content_image\" width=\"420\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;420&#39; height=&#39;52&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"420\" data-rawheight=\"52\" class=\"content_image lazy\" width=\"420\" data-actualsrc=\"https://pic2.zhimg.com/v2-2821664d57818544a9ee5b47f575847d_b.png\"/></figure><p>为子网络的目标函数。对于P-Net和R-Net，aplha_det = 1.0, alpha_box = 0.5, alpha_landmark = 0.5，检测任务的损失权重高，因为P-Net和R-Net的主要任务就是提取人脸Proposals，而不是进行人脸框的精确回归和人脸特征点的精确定位，后两个任务只是辅助性的（其实最后部署的时候，这两个网络的输出也只用了分类结果和框回归，**特征点回归信息没用上**）。对于R-Net，aplha_det = 1.0, alpha_box = 0.5, alpha_landmark = 1.0，和前面两个网络相比，**O-Net的训练提升了人脸特征点回归任务的权重**。**Beta代表样本类型指示器**，因为当样本是背景的时候，没有必要去做人脸框回归和特征点回归的任务。各个网络训练的时候采用随机梯度下降法(Stochastic Gradient Descent)作为求解器。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 在线难例挖掘</p><p class=\"ztext-empty-paragraph\"><br/></p><p>有别于先训练好网络再做难例挖掘的传统做法，MTCNN方法**对人脸分类任务**采用了**在线难例挖掘技术**。首先，在每一个mini-batch前向做完得到loss的输出后，将每个训练样本的loss进行从大到小的排序，**取loss排序靠前的70%做难例样本**；然后，在误差反向传播阶段，**只计算这70%的难例的梯度**，忽略其余30%的简单样本（这些样本提供的信息没有难例好）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 实践中可能的优化点</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 拼接大图</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在论文默认实现中，为了检测不同大小的人脸，需要把输入的图片做一系列的金字塔，分别输入进P-Net网络中，这样P-Net就会做多次前向计算。如果把这些不同层的金字塔图片拼接成一个大的图片，并记住他们的位置信息（对应金字塔的哪一层），将这一大张图片送进P-Net，这样P-Net就只需要做一次前向运算，可能会对速度有所优化。当然，因为拼接成一张大图需要计算量，还有输入P-Net的分辨率也大了，优化效果需要在落地平台上实际测量为准。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2. 网络裁剪</p><p class=\"ztext-empty-paragraph\"><br/></p><p>可以假设网络中每个通道的特征图的重要性不是一样的，可能有些特征图重要些，有些不那么重要，可以通过某些优化方法裁剪掉那些不重要的特征图。另外，Mobile Net中的可分离卷积核也可能对网络速度优化有好处。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3. 进一步回归</p><p class=\"ztext-empty-paragraph\"><br/></p><p>已经有人给MTCNN后面又加了一层网络，专门细化回归人脸特征点，如果需要精细化的特征点信息（比如人脸对齐，美颜等），可以考虑在O-Net后面再加一层专门做特征点回归的网络。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>4. 超参数调整</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在论文中，很多参数都是启发式设置的，比如三个网络训练时，不同任务损失的权重参数，正负样本和部分样本的定义阈值，还有非极大值抑制的IoU阈值等，这些参数是不是可以考虑用[《深度学习调参常用方法总结》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230013-%25E6%25B7%25B1%25E5%25BA%25A6%25E5%25AD%25A6%25E4%25B9%25A0%25E8%25B0%2583%25E5%258F%2582%25E5%25B8%25B8%25E7%2594%25A8%25E6%2596%25B9%25E6%25B3%2595%25E6%2580%25BB%25E7%25BB%2593/D%25230013.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)做进一步研究和优化？</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## FaceBoxes</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 简单介绍</p><p class=\"ztext-empty-paragraph\"><br/></p><p>FaceBoxes是2017年首次发表在IJCB上，出自**中科院自动化所李子青**教授团队，属于2017年的STOA。与MTCNN相比，它只用了一个网络实现了**CPU上实时**高精度检测人脸，而且检测速度和图片中**人脸个数无关**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>其主要创新点在于：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 提出**R**apdily **D**igested **C**onvolutional **L**ayers，RDCL快速摘要卷积层**加速特征提取过程**</p><p>2. 提出**M**ultiple **S**cale **C**onvolutional **L**ayers，MSCL多尺度卷积层**丰富感受野**和离散化**不同anchor到不同分辨率特征图**上（类似于SSD）</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3. 采用**anchor密集采样**策略，对**浅层特征图上的锚点做更密集的采样**，使不同类型的锚点在原图上都有一样的采样密度，提升对小尺度人脸的召回率。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 网络结构和算法流程</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5f4dc7fb0dd2f756b7836528f04b7f56_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1370\" data-rawheight=\"384\" class=\"origin_image zh-lightbox-thumb\" width=\"1370\" data-original=\"https://pic3.zhimg.com/v2-5f4dc7fb0dd2f756b7836528f04b7f56_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1370&#39; height=&#39;384&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1370\" data-rawheight=\"384\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1370\" data-original=\"https://pic3.zhimg.com/v2-5f4dc7fb0dd2f756b7836528f04b7f56_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5f4dc7fb0dd2f756b7836528f04b7f56_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图所示，网络的主体部分，按照功能划分有RDCL层和MSCL层两个部分。**RDCL任务是加速。快速降低特征图分辨率，在提取特征的同时，减少计算量。而MSCL是综合不同层的特征图进行人脸的二分类和人脸框的回归，可以类比SSD。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ **理解RDCL层**可以从以下三个方面看：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. RDCL层串联两个卷积层和两个池化层，每层的stride分别为4,2,2,2，**快速收缩特征图**到输入图片的**1/32**的分辨率；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2. RDCL层卷积层分别用7x7和5x5的**大尺度卷积核**，增大感受野；这一点需要和1.1配合起来理解，1.1中快速收缩的特征图分辨率丢失了很多信息，用大尺度的卷积核可以增加感受野对这些信息的损失进行**一定层度的中和**；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3. 使用[**C-ReLU**](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1603.05201\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units</a>)代替普通的ReLU层，通过**减少卷积核的数目**来加速，但经过C-ReLU，特征图的通道数又可以翻倍，不使后面MSCL层有太多信息损失；</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0474cca188d4b6d9dd96f2cc2984f4a5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"239\" data-rawheight=\"429\" class=\"content_image\" width=\"239\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;239&#39; height=&#39;429&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"239\" data-rawheight=\"429\" class=\"content_image lazy\" width=\"239\" data-actualsrc=\"https://pic2.zhimg.com/v2-0474cca188d4b6d9dd96f2cc2984f4a5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>   这里需要说一下ICML 2016的C-ReLU激活函数的原理，论文中作者在AlexNet的模型上做了一个有趣的实验，发现：**低层卷积层中的一些滤波器核存在着负相关程度很高的滤波器核，而层次越高的卷积层，这一现象越不明显**，作者把这一现象称为**Pairing Phenomenon**。这样在CNN中较低的层（FaceBoxes中就是RDCL层），如上图所示，**C-ReLU减少一半输出通道(output channels)的数量，通过简单的concate相同的输出和其negation使其变成双倍数量**，即达到原来输出的数量，这使得**2倍的速度提升**而没有损失信息；个人理解，**减少一半channel数目是根据conv filters数目降低一半得到的**，**先减少一半channels的feature map，再通过C-ReLU可以扩充一倍的channels数目**，这样就达到了对比原先channel扩充2倍的conv filters的目的；**conv filters减少了一倍，自然速度就提升了两倍**；C-ReLU can double the number of output channels by simply concatenating negated outputs before applying ReLU；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ **下面来看MSCL层**：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>MSCL目的为**丰富感受野**和**离散化anchor至不同的feature map**上，可以让FaceBoxes处理大尺度变化的人脸目标。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>从Figure 1上可以看出，MSCL层使用了三个Inception模块，多任务分类和回归使用了采自Inception3，Conv3_2和Conv4_2三层的特征图信息。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-26f272b07c4549f00044613c543f1531_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"382\" data-rawheight=\"432\" class=\"content_image\" width=\"382\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;382&#39; height=&#39;432&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"382\" data-rawheight=\"432\" class=\"content_image lazy\" width=\"382\" data-actualsrc=\"https://pic2.zhimg.com/v2-26f272b07c4549f00044613c543f1531_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 使用类似GoogLeNet **Inception结构**（见上图），不同分辨率的卷积核进行卷积，**丰富感受野**，以在同层的特征图上处理不同尺度的人脸；使用Inception模块的出发点为：作者认为与anchor关联的检测层的特征，应该包含尽可能多的不同尺度的感受野信息(output  features of the anchor-associated layers should correspond to various  sizes of receptive fields)，这样这样就可以更方便的检出不同尺度的人脸。而Inception结构在不同卷积分支上使用不同大小的卷积核，这种在feature map宽度上增加多尺度卷积核的方式可以丰富每层feature map的感受野信息；</p><p>2. 离散化不同的anchor到不同层的特征图，**综合不同抽象程度的特征，处理人脸的尺度变化**；原理类似于SSD，常规操作。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 锚点密集采样策略</p><p class=\"ztext-empty-paragraph\"><br/></p><p>  **对浅层feature map**(如FaceBoxes中的Inception3)检测的小尺度目标，其对应anchor(小目标对应的anchor一般预定义比较小，如32x32、64x64等)，**做更加密集的anchor采样**，使得小目标anchor的采样密集与大目标采样密度一致，这样可以提升对小目标的召回率。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 网络训练</p><p>#### 样本准备</p><p class=\"ztext-empty-paragraph\"><br/></p><p>使用Wider Face训练集内的12800张图片作为训练集。并用颜色扭曲，随机抠图，尺度变换（patch缩放到1024x1024），水平翻转等进行数据增强；删除长宽小于20像素的人脸。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 匹配策略</p><p class=\"ztext-empty-paragraph\"><br/></p><p>训练阶段的anchor与gt bbox匹配策略，与ssd几乎一致：**与gt bbox iou最大的anchor作为正样本，与gt bbox iou大于阈值T的也为正样本。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 目标函数</p><p class=\"ztext-empty-paragraph\"><br/></p><p>和前面介绍的MTCNN是一样的，2类Softmax loss + 回归的bbox Smooth-L1 loss。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 在线难例挖掘</p><p class=\"ztext-empty-paragraph\"><br/></p><p>也比较常规，anchor中负样本比较多，跑一遍forward，根据loss(对应分类score)降序排序，控制正负样本比例为1:3。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文先介绍了在2018年之前的一些主流人脸检测算法，VJ架构，Cascade CNN和Faceness Net。然后详细介绍了在CPU上速度比较快比较实用的2016年的MTCNN和2017年的FaceBoxes。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [Robust Real-Time Face Detection](<a href=\"https://link.zhihu.com/?target=http%3A//www.face-rec.org/algorithms/Boosting-Ensemble/16981346.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">face-rec.org/algorithms</span><span class=\"invisible\">/Boosting-Ensemble/16981346.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [A Convolutional Neural Network Cascade for Face Detection](<a href=\"https://link.zhihu.com/?target=http%3A//users.eecs.northwestern.edu/~xsh835/assets/cvpr2015_cascnn.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">users.eecs.northwestern.edu</span><span class=\"invisible\">/~xsh835/assets/cvpr2015_cascnn.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [Faceness-Net: Face Detection through Deep Facial Part Responses](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1701.08393.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1701.0839</span><span class=\"invisible\">3.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1604.02878\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks</a>)</p><p>+ [FaceBoxes: A CPU Real-time Face Detector with High Accuracy](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1708.05234\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">FaceBoxes: A CPU Real-time Face Detector with High Accuracy</a>)</p><p>+ [Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1603.05201\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/abs/1603.0520</span><span class=\"invisible\">1</span><span class=\"ellipsis\"></span></a>)</p><p>+ [IJCB2017_FaceBoxes](<a href=\"https://zhuanlan.zhihu.com/p/39183856\" class=\"internal\">胡孟：IJCB2017_FaceBoxes</a>)</p><p>+ [《机器学习与应用》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/30445238/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习与应用 (豆瓣)</a>)</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "刷脸背后：人脸检测 人脸识别 人脸检索（书籍）", 
                    "tagLink": "https://api.zhihu.com/topics/20839821"
                }, 
                {
                    "tag": "加速", 
                    "tagLink": "https://api.zhihu.com/topics/19609182"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/82448116", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 1, 
            "title": "深度学习中不平衡样本的处理", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在目标检测问题中，负样本更容易采集，所以我们能得到的负样本数量一般会比正样本数量多很多。但是负样本多了，就会引起训练数据类别不平衡问题，这会带来：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 大量容易负样本（不提供有用的学习信息）会导致训练过程无效。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2. 大量容易负样本产生的loss会压倒少量正样本的loss（即容易负样本的梯度占主导），导致模型性能衰退。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>样本不平衡问题不是目标检测问题独有的困难，在反欺诈，灾害预测，垃圾邮件预测等等问题中也会有正样本过少导致训练集样本不平衡的问题。要解决这个问题，可以采用本文介绍的从数据层面和算法两个层面要思考解决方案。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 数据层面</p><p>通过对训练集合数据的处理，让正样本的数量和负样本的数量比例趋于平衡（例如1:3）。常见的方式有数据重采样和数据增强。</p><p>### 数据重采样</p><p>数据重采样，是指在训练之前或者训练时候，对样本多的类别采样频率减少，对样本少的类别采样频率增大，从而使在训练的时候各类类别样本数目比较平衡。</p><p>#### 多数样本下采样</p><p>拿二分类人脸检测来说，背景是数量样本较多的类别，而人脸是样本较少的类别。对于负样本来说，可以选择**随机抛弃**一部分样本来减少训练时背景样本的数量来达到平衡，但是这样做会降低训练数据的多样性而影响模型泛化能力一般不采用。正确的下采样方式是这样的，比如假设训练时负样本和正样本的比例为3:1（这个比例需要根据实际问题来做出合理的假设），那么**在批训练时候，每批样本随机采集3个负样本（而不是更多）的时候就随机采集1个正样本，使每个批次的训练数据保持负样本比正样本比例大致为3:1。**</p><p>另外，根据训练模型在验证集上测试结果，**观察假阳性的规律**（比如手掌部位的假阳性较多），可以特地选取一些手掌的图片作为负样本进行训练（对负样本进行了约束，自然也就降低了负样本的数量）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 少数样本上采样</p><p>少数样本的上采样是指：在训练时，对少数样本那一类进行**有放回的抽样**，以用来增加负样本在训练批次里面的数量比例。或者在训练之前，对少数样本那一类进行简单复制，也属于少数样本上采样的一种方式。</p><p>需要注意的是，仅仅采用少数样本上采样，因为本质上没有真正增加少数样本的多样性，没有带来更多的信息，可能会引起模型**过拟合**问题。更保险和有效的方式是采取多数样本下采样和少数样本上采样**相结合使用**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 数据增强</p><p class=\"ztext-empty-paragraph\"><br/></p><p>正样本不够，可以采取一些处理方式，增加正样本，这是一种简单易行的方式。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 图像处理增加少类样本</p><p>对少数样本的图片添加噪音（例如高斯噪音），进行直方图均衡化操作，进行裁剪，小角度旋转，翻转等等，这些都可以在不改变样本种类的前提下增加少类样本的数量。</p><p>#### SMOTE</p><p>SMOTE[Chawla et a., 2002]是通过对少数样本进行**插值**合成新的样本。比如对于每个少数类样本a，从a最邻近的样本中选取样本b，然后在[a,b]区间中随机选择一点作为新样本。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### Fancy PCA</p><p>在AlexNet提出的同时，Krizhevsky等人还提出了使用&#34;Fancy PCA&#34;的方法进行数据扩充，这个方法讲Top-1错误率降低了一个百分点（很显著的提升）。</p><p>具体操作如下：首先，对所有数据的R,G,B通道进行主成分分析，得到对应特征向量p_i和特征值lamda_i。再根据特征向量和特征值计算一组新的特征值[p1,p2,p3]\\*[alpha_1\\*lamda_1,alpha_2\\*lamda_2,alpha_3\\*lamda_3]，其中alpha为均值为0，方差为0.1的高斯分布随机值。最后将其作为扰动加入原来图像的像素值即可。在每一个epoch之后，再次采样新的alpha值进行扰动。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Krizhevsky等人提到“**Fancy PCA可以近似地捕获自然图像的一个重要特性,即物体特质与光照强度和颜色变化无关**”。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>#### 监督式数据扩充</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在某些情况下，图像分类所用的是整个场景的高级语义特征，此时如果采用随机裁剪作为数据增强方式，可能会裁剪到部分样本而破坏了原来的高级语义，那样的数据，连标签都错了，完全不能用来做训练。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a69dd161b919bcfe41e35e4ff92ebd82_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"289\" data-rawheight=\"215\" class=\"content_image\" width=\"289\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;289&#39; height=&#39;215&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"289\" data-rawheight=\"215\" class=\"content_image lazy\" width=\"289\" data-actualsrc=\"https://pic3.zhimg.com/v2-a69dd161b919bcfe41e35e4ff92ebd82_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图所示的一个场景分类的样本，样本的标签是“海滩”，随机裁剪可能裁剪到了“天空”或者“树”上面去了，就完全不能代表“海滩”这一类场景了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在2016年ImageNet竞赛场景分类任务中，国内的**海康威视**团队提出了一种利用原始数据标签的监督式数据增强方法：如下图所示，首先，利用已有的数据训练出一个初始模型；然后，利用该模型跑原始训练样本集，对每个样本生成对应的热力图（可直接将分类模型最后一层卷积层特征**按照深度方向加和**得到），这**张热力图可以指示图像区域与场景标记之间的概率关系**，然后对热力图概率高的地方为中心对应原图取裁剪图像块就可以了。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5fe9d3f22ec2e46ba1d109f7a87f2593_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"498\" data-rawheight=\"449\" class=\"origin_image zh-lightbox-thumb\" width=\"498\" data-original=\"https://pic4.zhimg.com/v2-5fe9d3f22ec2e46ba1d109f7a87f2593_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;498&#39; height=&#39;449&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"498\" data-rawheight=\"449\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"498\" data-original=\"https://pic4.zhimg.com/v2-5fe9d3f22ec2e46ba1d109f7a87f2593_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5fe9d3f22ec2e46ba1d109f7a87f2593_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>## 算法层面</p><p>在算法层面减轻类别不平衡的方法基本上是**改造优化时的目标函数**，使目标函数倾向于减轻多数样本的惩罚力度，加大少数样本的惩罚力度；或者加大难分样本的惩罚力度，减轻容易分样本的惩罚力度。这其中最具有典型性的是**Focal Loss**。关于Focal Loss的介绍，可以参见[《深度学习常用损失函数》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230015-%25E6%25B7%25B1%25E5%25BA%25A6%25E5%25AD%25A6%25E4%25B9%25A0%25E5%25B8%25B8%25E7%2594%25A8%25E6%258D%259F%25E5%25A4%25B1%25E5%2587%25BD%25E6%2595%25B0/D%25230015.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)一文。</p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文对于深度学习实践中常遇到的数据集不平衡问题的处理方式，从数据层面和算法层面两个角度总结了一些实际可行的缓解方法。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [《深度学习》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/27087503/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">book.douban.com/subject</span><span class=\"invisible\">/27087503/</span><span class=\"ellipsis\"></span></a>)</p><p>+ [CNN_book](<a href=\"https://link.zhihu.com/?target=http%3A//210.28.132.67/weixs/book/CNN_book.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">210.28.132.67/weixs/boo</span><span class=\"invisible\">k/CNN_book.pdf</span><span class=\"ellipsis\"></span></a>)</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "数据", 
                    "tagLink": "https://api.zhihu.com/topics/19554449"
                }, 
                {
                    "tag": "算法", 
                    "tagLink": "https://api.zhihu.com/topics/19553510"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/81956896", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 0, 
            "title": "深度学习常用损失函数", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p># 　　　　　　深度学习常用损失函数</p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>深度学习的任务本质上来说是拟合一个函数Fcnn，将输入的图片映射到对应的标签，这个映射的好坏需要用损失函数来表达，学习训练的过程也是由损失函数来指导。本文总结了一些在深度学习，更具体地说，实在卷积神经网络中常见的损失函数及其特性和使用场合。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**请注意：本文为了讨论方便，不严格区分目标函数，损失函数。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>卷积神经网络常见的任务有两种，一种是分类（输出离散值，例如预测输入图片的类别），一种是回归（输出连续值，例如预测回归框的相对偏移量）。这两种任务都有各自适合的损失函数，**有时候是多种损失函数的组合**。这里为了方便，就以任务的不同来分别讨论各自的损失函数。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 分类问题常用损失函数</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 交叉熵</p><p class=\"ztext-empty-paragraph\"><br/></p><p>一般来说，分类网络最后一层是用Softmax来输出预测各类别的概率值，KL散度是评价两个概率值的常见手段，交叉熵损失就是脱胎于KL散度，用来评价Softmax输出的概率分布与训练样本真值概率之间的差异性。而且，在优化的角度来讲，交叉熵损失也适合Softmax。关于交叉熵损失更详细的理解和说明，可以参见我前面写的[《为什么选交叉熵作为分类问题的损失函数》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230012-%25E4%25B8%25BA%25E4%25BB%2580%25E4%25B9%2588%25E9%2580%2589%25E4%25BA%25A4%25E5%258F%2589%25E7%2586%25B5%25E4%25BD%259C%25E4%25B8%25BA%25E5%2588%2586%25E7%25B1%25BB%25E9%2597%25AE%25E9%25A2%2598%25E7%259A%2584%25E6%258D%259F%25E5%25A4%25B1%25E5%2587%25BD%25E6%2595%25B0/D%25230012.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 合页损失函数</p><p class=\"ztext-empty-paragraph\"><br/></p><p>合页损失函数L = max(0, 1-x)，其函数曲线如下图蓝色线所示，类似于一个合页而得名。合页损失函数起源于SVM，它的特点是对错分类的点，考虑到它距离分类平面的远近，距离越大则损失越大；但是对于能正确分类的样本，如果这些样本离分类平面的距离不够大的话，也会有一点损失。这样就强迫了在优化的时候，优化过程去寻找最大间隔划分平面。关于它的讨论，可以参见我前面写的[《支持向量机》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230009-SVM/D%25230009.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)一文。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2803e599fa7bbef714322e7a0f82b766_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"358\" data-rawheight=\"329\" class=\"content_image\" width=\"358\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;358&#39; height=&#39;329&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"358\" data-rawheight=\"329\" class=\"content_image lazy\" width=\"358\" data-actualsrc=\"https://pic3.zhimg.com/v2-2803e599fa7bbef714322e7a0f82b766_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>### 坡道损失函数</p><p class=\"ztext-empty-paragraph\"><br/></p><p>合页损失函数对于错误越大的样本，施加的惩罚越严重。可是样本毕竟不是那么完美的，如果里面有一些**标签错误的**，或者本身就是**离群点（outlier）**，那么就会误导合页损失，从而影响分类超平面的学习。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>为了缓解这个问题，坡道损失函数（其函数图像如上图红色曲线），限制了损失的最大值，在一定程度上缓解了前面讨论的错误标签和离群点带来的问题。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 中心损失函数</p><p class=\"ztext-empty-paragraph\"><br/></p><p>上面介绍的损失函数，仅仅只考虑了学到让不同类与类之间间隔增大的特征；而**良好的特征，不仅仅要考虑不到不同类之间间隔要大，应该也要保证同类之间的间隔要小，这样的特征更有辨别力**。中心损失函数，就可以指导网络学习到的特征，在同类的时候尽量聚拢在一起。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-23eecd7684fe70a834d91b371797cfd4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"327\" data-rawheight=\"98\" class=\"content_image\" width=\"327\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;327&#39; height=&#39;98&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"327\" data-rawheight=\"98\" class=\"content_image lazy\" width=\"327\" data-actualsrc=\"https://pic1.zhimg.com/v2-23eecd7684fe70a834d91b371797cfd4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上式为中心损失函数的表达式，其中c_yi为第yi类训练样本深度特征的均值点。网络学习到特征离离中心点越远，则惩罚越大，这就促使网络去学让每个同类别样本能聚拢在一块的特征。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在实际使用中，由于中心点损失函数值考虑类内差异性，而交叉熵损失函数只考虑类间差异性，一般会把中心损失函数和交叉熵损失函数配合起来用各取所长。这样网络最终的目标函数可以表示为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-fc06692de302343ca903dab0add0d330_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"480\" data-rawheight=\"47\" class=\"origin_image zh-lightbox-thumb\" width=\"480\" data-original=\"https://pic1.zhimg.com/v2-fc06692de302343ca903dab0add0d330_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;480&#39; height=&#39;47&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"480\" data-rawheight=\"47\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"480\" data-original=\"https://pic1.zhimg.com/v2-fc06692de302343ca903dab0add0d330_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-fc06692de302343ca903dab0add0d330_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其中，lamda代表了两种损失的相对比例，用来调节学习到的特征类间分离性和类内聚拢性之间的相对强度，lamda越大，类内聚拢越强烈。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4de8f6c5a009140971f9ca9b79220494_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"565\" data-rawheight=\"416\" class=\"origin_image zh-lightbox-thumb\" width=\"565\" data-original=\"https://pic1.zhimg.com/v2-4de8f6c5a009140971f9ca9b79220494_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;565&#39; height=&#39;416&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"565\" data-rawheight=\"416\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"565\" data-original=\"https://pic1.zhimg.com/v2-4de8f6c5a009140971f9ca9b79220494_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-4de8f6c5a009140971f9ca9b79220494_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图所示，为做0~9这十个手写数字分类任务时，用L_final目标函数优化网络，取不同lamda值所学习到的特征投影到二维平面的图像。可以明显地观察到，当中心损失函数比重增大时，网络学习到的特征更集中，类间差异更小，学习到的特征更好。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在分类性能方面，**中心损失函数搭配传统交叉熵损失函数作为目标函数学习到的特征要优于只使用传统交叉熵损失函数作为目标函数所学习到的特征**，特别是在人脸识别问题上可能有较大的性能提升。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Focal Loss</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Focal Loss首次提出于2017年ICCV最佳学生论文奖《Focal Loss for Dense Object Detection》，主要是为了解决在目标检测这个问题中，训练样本负样本和正样本不平衡的问题。训练样本不平衡会带来：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 大量容易负样本（不提供有用的学习信息）会导致训练过程无效。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2. 大量容易负样本产生的loss会压倒少量正样本的loss（即容易负样本的梯度占主导），导致模型性能衰退。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Focal Loss表达式</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-cbe25032a020532182e4df84bc788057_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"230\" data-rawheight=\"45\" class=\"content_image\" width=\"230\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;230&#39; height=&#39;45&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"230\" data-rawheight=\"45\" class=\"content_image lazy\" width=\"230\" data-actualsrc=\"https://pic4.zhimg.com/v2-cbe25032a020532182e4df84bc788057_b.jpg\"/></figure><p>，其中</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-23dc8b46949c69174eee7f8d1395a1be_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"205\" data-rawheight=\"52\" class=\"content_image\" width=\"205\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;205&#39; height=&#39;52&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"205\" data-rawheight=\"52\" class=\"content_image lazy\" width=\"205\" data-actualsrc=\"https://pic3.zhimg.com/v2-23dc8b46949c69174eee7f8d1395a1be_b.jpg\"/></figure><p>，p代表网络预测得到的标签为1的概率（此处以二分类问题为例），**p_t是网络分对了的概率**，y是真值的标签，取值正负1，gamma值为可以调节的聚焦参数，(1 - p_t)^gamma为调制因子。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-290e9dc1ad475dc3a986bcd19eb935b4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"599\" data-rawheight=\"562\" class=\"origin_image zh-lightbox-thumb\" width=\"599\" data-original=\"https://pic1.zhimg.com/v2-290e9dc1ad475dc3a986bcd19eb935b4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;599&#39; height=&#39;562&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"599\" data-rawheight=\"562\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"599\" data-original=\"https://pic1.zhimg.com/v2-290e9dc1ad475dc3a986bcd19eb935b4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-290e9dc1ad475dc3a986bcd19eb935b4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>gamma聚焦参数对Focal Loss的影响如上图。可以看到：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 当网络正确分类的时候(上图中是p_t &gt; 0.6)，大多都是负样本这样的容易样本，损失很小；当网络分类错误的时候，损失很大；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2. 调制因子gamma可以调节对错误分类损失的**相对放大程度**，当gamma越大的时候，越是关注与那些难分错分的样本，让他们的损失大一点，引导网络学习。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3. 当gamma = 0的时候，Focal Loss退化成交叉熵。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>直观的来看，调制因子gamma增大，会减少容易样本的loss，并且扩大得到低loss值的样本范围从而增加错误分类样本的重要性。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>实际使用时，使用的是α-balanced变体的Focal Loss![](images/160509.png)，让alpha参数调节对正样本和负样本的关注程度（一般更关注于数量更少的正样本），gamma参数调节容易样本带来的相对损失（让容易样本带来的相对损失小）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 回归问题常用损失函数</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a7511418072ff0a5ab6ab056016e7ea0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"777\" data-rawheight=\"272\" class=\"origin_image zh-lightbox-thumb\" width=\"777\" data-original=\"https://pic1.zhimg.com/v2-a7511418072ff0a5ab6ab056016e7ea0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;777&#39; height=&#39;272&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"777\" data-rawheight=\"272\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"777\" data-original=\"https://pic1.zhimg.com/v2-a7511418072ff0a5ab6ab056016e7ea0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a7511418072ff0a5ab6ab056016e7ea0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>### L1 Loss</p><p class=\"ztext-empty-paragraph\"><br/></p><p>L1损失如上图最左边所示，它常作为正则项来减轻模型过拟合或者使网络稀疏化（可参看[《拉格朗日乘子法》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230008-%25E6%258B%2589%25E6%25A0%25BC%25E6%259C%2597%25E6%2597%25A5%25E4%25B9%2598%25E5%25AD%2590%25E6%25B3%2595/D%25230008.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)一文介绍），做特征选择，模型压缩用。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>L1损失直接对网络输出的预测值和真值的差值取绝对值，计算简单，原理直观。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### L2 Loss</p><p class=\"ztext-empty-paragraph\"><br/></p><p>L2损失如上图中间所示，它经常作为正则项来减轻模型的过拟合。在回归问题中，L2相对于L1来说，同样幅度的误差带来的损失更大，而且它的导数在网络训练的后期更小，所以在某些情况下，其能达到的精度也要高于L1损失；另外，因为L2的导数大于L1的导数（在误差大于1的情况下），所以，在训练时，用L2作为损失函数，其收敛速度也快于L1。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Tukey&#39;s biweight</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Tukey&#39;s biweight损失函数和坡道损失函数一样，是一种克服噪声和离群点的鲁棒损失函数。其定义为![](images/162310.png)，函数图像如上图最右边所示，常数c指定了函数图像拐点的位置，**一般指定为4.6851**，此时Tukey&#39;s biweight可取得与L2损失函数在最小化符合正态分布的参差类似的（95%渐进）的回归效果。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Smooth-L1 Loss</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Smooth L1损失首次是在Fast RCNN算法里面作为Bounding Box Regression的损失函数提出，它是L1损失的改良版本，和普通L1损失和L2损失相比，有如下三点优势:</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. **第一，它在训练的后期**，当预测值与真值相差很小的时候，Smooth L1的导数比L1的导数更小，可以**使训练达到更高的精度**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2. **第二，在训练的前期**，当预测值和真值相差很大的时候，Smooth L1的导数相比于L2的导数不会太大，**避免了训练不稳定**的情况出现。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3. **第三，对于数据噪音和离群点**，Smooth L1的损失比L2的损失小，不容易误导网络训练。换句话说就是，对噪音和离群点更鲁棒。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>基于以上优点，在Fast RCNN之后，很多流行的目标检测算法，比如Faster RCNN和SSD，也都采用了Smooth L1这种损失。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文以深度学习的分类问题和回归问题介绍了常见的几种损失函数，其中这些损失函数有的可以互用（比如L2损失函数也经常用在分类问题上）。损失函数的构造或者选择，需要根据具体要解决的问题来，总的思路是，引导网络学习到辨别力强（类间距离大，类内距离小）的特征，而且还要容易优化。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [《深度学习》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/27087503/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度学习 (豆瓣)</a>)</p><p>+ [CNN_book](<a href=\"https://link.zhihu.com/?target=http%3A//210.28.132.67/weixs/book/CNN_book.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">210.28.132.67/weixs/boo</span><span class=\"invisible\">k/CNN_book.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [Soft Sampling：探索更有效的采样策略](<a href=\"https://zhuanlan.zhihu.com/p/63954517\" class=\"internal\">陀飞轮：Soft Sampling：探索更有效的采样策略</a>)</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "损失函数", 
                    "tagLink": "https://api.zhihu.com/topics/20682932"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/81956680", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 1, 
            "title": "数据降维常用方法总结(LDA,PCA)", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p># 　　　　　　数据降维常用方法总结(LDA,PCA)</p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>做机器学习的时候，一般来说特征越多，我们对训练样本的信息越多，越有利于学习。但是特征太多，也会带来两个问题，1.**维数灾难**，为了学习到每个维数的规律，那个维数的样本就不能太少，而样本的数量是跟着特征维数的增大而指数性增大的；2.**资源开销大**，我们提取的特征太多，这些特征中就有可能有相关和**冗余**，给数据存储，学习和优化都带来负担。有鉴于此，在特征维数过大的时候，我们有必要对这些特征进行辨别和合并，有选择性地压缩特征的维度。本文介绍了几种常用的特征选择方法。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 线性判别分析(Linear Discriminant Analysis)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>什么是好的特征？好的特征有两点才叫好，</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**1. 同类样本在特征空间中应该尽可能靠近；**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**2. 不同类样本在特征空间中应该尽可能远离。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**同时满足**这两个的要求的特征，在做分类时才好分，线性判别分析正是根据上面两点来做特征降维的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**线性判别分析的思想非常朴素：给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、不同类样例的投影点尽可能远离。在对新样本进行分类时，将其投影到同样的这条直线上，再根据新样本投影点的位置来确定它的类别。**</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-dbf4aa6284c5db831d9f247c0988b341_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"626\" data-rawheight=\"459\" class=\"origin_image zh-lightbox-thumb\" width=\"626\" data-original=\"https://pic2.zhimg.com/v2-dbf4aa6284c5db831d9f247c0988b341_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;626&#39; height=&#39;459&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"626\" data-rawheight=\"459\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"626\" data-original=\"https://pic2.zhimg.com/v2-dbf4aa6284c5db831d9f247c0988b341_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-dbf4aa6284c5db831d9f247c0988b341_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>LDA由二维降维到一维的最简单情况如上图所示，我们有在由x1,x2张成的二维特征空间内有一些正样本+和一些负样本-，LDA就是找一个由二维到一维的投影Y = W_t \\* X（这里只做降维，为了分析简单，所以投影直线的截距设置为0，过二维原点），Ｗ是直线的法向量，**在这个投影上，这两类新的中心点要尽量的远离，但每类内部的点要尽量地靠拢。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在下面公式推导之前，有一个重要的结论就是这里每个点投影后的值W_t \\* x代表的是投影后的点到二维原点o的距离，可以利用向量的内积在几何上去理解，W_t \\* x = &lt;w, x&gt; = |W|\\*|x|\\*cos(theta) = |x|\\*cos(theta)，W是直线的法向量，长度为1。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8fe26c5be4033e94ed142fd6b61f3433_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"625\" data-rawheight=\"770\" class=\"origin_image zh-lightbox-thumb\" width=\"625\" data-original=\"https://pic4.zhimg.com/v2-8fe26c5be4033e94ed142fd6b61f3433_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;625&#39; height=&#39;770&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"625\" data-rawheight=\"770\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"625\" data-original=\"https://pic4.zhimg.com/v2-8fe26c5be4033e94ed142fd6b61f3433_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8fe26c5be4033e94ed142fd6b61f3433_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这里推导很直观，不再赘述。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 主成分分析(Principal Components Analysis)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>线性判别分析压缩需要用到不同类的标签，如果同一类要做特征压缩，那么我们可以用主成分分析。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>主成分分析的目标是通过**旋转坐标轴**（也就是线性变换！）将各个原始特征组合成新的特征，而这些新的特征有的特征所携带的信息量大，有的特征所携带的信息量小，PCA**删除一些携带信息量小的特征**，留下信息量大的特征，最大程度的保留原始特征所能表达的信息。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f6fed6719a23ea2e2bab9468c8cca791_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"374\" data-rawheight=\"325\" class=\"content_image\" width=\"374\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;374&#39; height=&#39;325&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"374\" data-rawheight=\"325\" class=\"content_image lazy\" width=\"374\" data-actualsrc=\"https://pic2.zhimg.com/v2-f6fed6719a23ea2e2bab9468c8cca791_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图所示，在原始特征X1和X2张成的特征空间，有一群样本点，这些样本点在X1轴方向分布的范围比较大，在X2轴方向分布的范围也比较大。直观地想，分布范围比较大，相当于数据分布比较乱，分布没啥规律，从信息论的角度理解就是数据在X1和X2方向上的熵比较大，携带信息比较多，不能简单地丢掉某一维度来降维。而PCA是先旋转原始X1,X2轴到新的方向F1,F2，在这个F1,F2张成的坐标系中，F1方向携带的信息多，F2方向携带的信息少（因为F2方向数据投影的取值范围很集中），这样就可以省略F2这个特征，取用F1特征达到降维的目的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**主成分分析的这种坐标轴变化是通过将原来的坐标轴进行线性组合完成的。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>为了形式化找到X1,X2到F1,F2的旋转矩阵，需要先计算原始数据**协方差矩阵**X_t\\*X的特征值和单位特征向量，特征值大的特征向量方向所含的信息多（上图中F1），特征值越大，所含信息越多。PCA选取特征值大的特征向量组成旋转矩阵对原始数据降维。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**如果特征根小于1，说明该主成分的解释力度还不如直接引入一个原变量的平均解释力度大，因此一般可以用特征根大于1作为纳入标准。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>更详细的推导过程可见[理解主成分分析 (PCA)](<a href=\"https://zhuanlan.zhihu.com/p/37810506\" class=\"internal\">SIGAI：理解主成分分析 (PCA)</a>)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文总结了两种常用的数据降维方法，第一个是用于**多类**的线性判别分析LDA降维，第二个是用于**单类**的主成分分析PCA方法降维。两者都有比价扎实的数学基础和压缩指导。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [《深度学习》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/27087503/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度学习 (豆瓣)</a>)</p><p>+ [主成分分析原理介绍-笔记](<a href=\"https://zhuanlan.zhihu.com/p/55981609\" class=\"internal\">Foolin：主成分分析原理介绍-笔记</a>)</p><p>+ [理解主成分分析(PCA)](<a href=\"https://zhuanlan.zhihu.com/p/37810506\" class=\"internal\">SIGAI：理解主成分分析 (PCA)</a>)</p><p>+ [奇异值分解(SVD)](<a href=\"https://zhuanlan.zhihu.com/p/29846048\" class=\"internal\">漫漫成长：奇异值分解（SVD）</a>)</p>", 
            "topic": [
                {
                    "tag": "LDA", 
                    "tagLink": "https://api.zhihu.com/topics/19565464"
                }, 
                {
                    "tag": "Principal Component Analysis", 
                    "tagLink": "https://api.zhihu.com/topics/19806816"
                }, 
                {
                    "tag": "数据降维", 
                    "tagLink": "https://api.zhihu.com/topics/20010182"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/81956439", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 10, 
            "title": "深度学习调参常用方法总结", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p># 　　　　　　深度学习调参常用方法总结</p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在深度学习工程师的日常工作中，很大一部分时间都是在训练模型，或者说调参。深度学习中的超参数很多，学习率，正则系数，dropout，weight decay，kernel size这些都是需要在训练之前设置的超参。而且每次模型训练，都要花很多时间，很多资源成本。是故，本文总结了常用的几种调参策略。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 试错法（Baby Sitting）</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d6494d137231caacd67986c71c947ba4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"891\" data-rawheight=\"645\" class=\"origin_image zh-lightbox-thumb\" width=\"891\" data-original=\"https://pic1.zhimg.com/v2-d6494d137231caacd67986c71c947ba4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;891&#39; height=&#39;645&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"891\" data-rawheight=\"645\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"891\" data-original=\"https://pic1.zhimg.com/v2-d6494d137231caacd67986c71c947ba4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d6494d137231caacd67986c71c947ba4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>试错法也叫照看法，是最简单的调参策略。就是设计一组超参数，然后跑训练，看训练的结果满意不？不满意，就看哪里出问题了，梳理数据，查看Loss，看特征图等等，**不断根据前面的结果迭代**修改超参数，继续下一次训练，如此往复，直到满意，或者项目Deadline逼近。试错法简单易懂，技术门槛低，容易实现，是很多机器学习从业者的首选，不过缺点在于试错法最终得到的结果是**高度依赖调参者水平和经验**的一种方法（强行抹黑，哈哈）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 网格搜索（Grid Search）</p><p class=\"ztext-empty-paragraph\"><br/></p><p>网格搜索，稍微比试错法效率好一点。因为方法简单，**适用于超参数种类比较少，取值也不多的情况下**，也会被很多工程师用到。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>基本思路是这样的，比如现在要寻找一组Dropout rate和Learning rate的最佳组合，让验证集误差最小。网格搜索就是指定一些离散的Dropout rate，比如(0.4,0.5,0.6,0.7)这四个取值；然后再给Learning rate也同样指定一些可能的取值，比如(1e-4,1e-3,1e-2)这三个取值，如果以Dropout rate为横轴，以Learning rate为纵轴建立一个平面直角坐标系，那么这些取值点就是盘面上的4x3=12个点，这些点连成一个网格。下一步就是以这12个点为参数训练12个模型，取效果最好的参数。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-924f19eabbcc74e314a26847d7cc2225_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"672\" data-rawheight=\"338\" class=\"origin_image zh-lightbox-thumb\" width=\"672\" data-original=\"https://pic2.zhimg.com/v2-924f19eabbcc74e314a26847d7cc2225_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;672&#39; height=&#39;338&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"672\" data-rawheight=\"338\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"672\" data-original=\"https://pic2.zhimg.com/v2-924f19eabbcc74e314a26847d7cc2225_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-924f19eabbcc74e314a26847d7cc2225_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这个方法的优点在于容易理解，概念简单，易于多机并行化，缺点在于穷举方法太暴力，**成本太高**，特别是需要调节的参数数量比较多的时候，训练次数是成**指数性增长**的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 二分搜索（Binary Search）</p><p class=\"ztext-empty-paragraph\"><br/></p><p>网格搜索太暴力，成本太高。其实我们对所要调节的参数要干什么，会对最终结果（验证集误差）会产生什么影响应该是略有知晓的。网格搜索没有利用这个信息。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>那如何改进呢？</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如果某个参数的取值，对验证集误差来说是平滑而且有一个极值的，那么我们可以用二分搜索来减少训练次数，加快调参。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>举个例子，在MTCNN人脸检测中，用的人脸分类损失是二分类交叉熵，人脸框回归和特征点定位用的是L2损失。在训练每个子网络(P-Net,R-Net和O-Net)的时候会对交叉熵和L2损失用不同的加权求和作为各自网络训练时的损失函数。以P-Net来说，L_pnet = L_cls + alpha x (L_boxreg + L_landmark)，alpha这个权重系数，就是一个可以手动调节的超参数。alpha调节的是训练时分类损失和其他两个损失的权重对比，如果alpha取值比较大，那么人脸检测任务可能表现的不好，反之alpha取值太小，框回归和特征点定位任务可能表现不好。在设定了一个最低的人脸召回率后，我们可以用二分法在(0.0,1.0)这个区间去用二分法寻找最合适的alpha值，比网格搜索要高效地多。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>二分法比网格搜索效率成**指数性**的提高，但是需要超参数的取值对验证集误差是一个**凸函数**，这在仅一类超参数调节的时候还好，在多类超参数需调节的时候，一般很难保证找到全局最优。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 随机搜索（Random Search）</p><p class=\"ztext-empty-paragraph\"><br/></p><p>二分搜索前提条件太苛刻，网格搜索效率太低，而且不适合于同时优化太多超参数。而随机搜索前提条件比较松，而且适合于同事优化多个超参数，编程也不复杂，在同样的训练次数约束下效果一般也不比网格搜索差，是一种比较重要的调参思路。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>随机搜索的思路是这样的：一般我们需要调整的超参数很多，但是**并不是所有的超参数都有同样的重要性**（这一前提很重要，也很容易满足），在训练次数一定（资源约束）的情况下，**训练更多的参数的组合**（而不是像网格搜索一样，很多参数的组合都是一个参数**不动**，动其余的参数），更有可能找到那些重要的参数的最佳值（采样点更多选最优嘛）。随机搜索和网格搜索比较示意图如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-16f22cb6fe4b6ac8d2fc33e5b1808206_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"778\" data-rawheight=\"388\" class=\"origin_image zh-lightbox-thumb\" width=\"778\" data-original=\"https://pic3.zhimg.com/v2-16f22cb6fe4b6ac8d2fc33e5b1808206_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;778&#39; height=&#39;388&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"778\" data-rawheight=\"388\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"778\" data-original=\"https://pic3.zhimg.com/v2-16f22cb6fe4b6ac8d2fc33e5b1808206_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-16f22cb6fe4b6ac8d2fc33e5b1808206_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>图中需要优化（最大化）的函数是f(x,y) = g(x) + h(y)，这里x,y可以看成是超参数，f(x,y)是验证集上的损失。图上黄色的是h(y)的函数曲线，绿色是g(x)是的函数曲线。这里h(y)函数值比较均匀，在最优值处贡献比g(x)小，所以自然在x轴上采样更多的点去训练比较比较好。而左边网格搜索，9个点（9次实验）只有3种不同的x取值；右边随机搜索，里面的点都是随机找（比如在x和y上按照均匀分布找）的，同样9个点，可以找到9个不同的x值的配置来训练，**没有浪费的实验**，明显比网格搜索的3个点更容易找到g(x)的最大值，也就是找到了f(x,y)的最大值。当有几个超参数都性能没有显著影响时，随机搜索比网格搜索**指数级的高效**。另外，随机搜索不用离散化超参数的值，**有更大的搜索空间**，而不产生额外的代价而更容易搜索到超参的最佳值，这也是随机搜索比网格搜索优秀的地方。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 贝叶斯优化（Bayesian Optimization）</p><p class=\"ztext-empty-paragraph\"><br/></p><p>上面的网格搜索和随机搜索，都有一个共同的缺点，“每一次的训练都独立于之前的训练”。相比之下，试错法和二分搜索，都会根据上一次的训练结果来做分析，根据分析结果指导下一次训练超参数的设置。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>贝叶斯超参数优化方法则是有别于照看法和二分法的另一种更精巧的**可以根据前面的训练结果，来指导后面训练超参数设置**的超参数优化方法。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>超参数搜索其实可以转化为一个优化问题，L = f(x)，其中x是超参数的取值，f是从这组超参数到训练出来的模型在验证集误差的映射。在及其简化的条件下，可以计算验证集上可导误差函数关于超参数的梯度来求最佳超参数，但更多的时候这个导数是找不到的。贝叶斯优化不求找到这个导数，它使用一个代理模型（例如高斯过程），这个代理模型**不仅将预测转化为一个值，还为我们提供在其它地方不确定性的范围（均值和方差）**，然后下一次选择的超参数就在**不确定性最大的**地方选择，因为选择不确定性最大的地方的超参数，可以提供**最大量的信息**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>下面以一个超参数的贝叶斯优化来举例：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3c9afcf0ae832a398991227e068ca7fa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"864\" data-rawheight=\"562\" class=\"origin_image zh-lightbox-thumb\" width=\"864\" data-original=\"https://pic3.zhimg.com/v2-3c9afcf0ae832a398991227e068ca7fa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;864&#39; height=&#39;562&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"864\" data-rawheight=\"562\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"864\" data-original=\"https://pic3.zhimg.com/v2-3c9afcf0ae832a398991227e068ca7fa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-3c9afcf0ae832a398991227e068ca7fa_b.jpg\"/></figure><p>上图中红色点线是验证集误差的真值，黑色曲线是我们对真值预测的平均值，灰色代表预测的不确定性，黑点是我们已经跑过的实验，不确定性为0，肯定和误差的真值一样，但离黑点远的地方，我们拥有的信息少，不确定性大。而底下蓝色的是不确定性，蓝色的点是指示下一次超参数应该选择的地方（就是不确定性最大的地方）。下图是根据上图的指示的点进行的一次迭代：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-bb725bc982ff6ab6853edfcf8a132d33_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"863\" data-rawheight=\"549\" class=\"origin_image zh-lightbox-thumb\" width=\"863\" data-original=\"https://pic4.zhimg.com/v2-bb725bc982ff6ab6853edfcf8a132d33_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;863&#39; height=&#39;549&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"863\" data-rawheight=\"549\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"863\" data-original=\"https://pic4.zhimg.com/v2-bb725bc982ff6ab6853edfcf8a132d33_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-bb725bc982ff6ab6853edfcf8a132d33_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>迭代之后，可以看到，多了一个黑点，我们知道了更多的关于f的信息，并更新了不确定性的分布。再在下一次挑选超参数的时候，依然选择不确定性最大的地方的超参数值来训练，如此往复，直到预先设定的标准满足或者资源耗尽。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>下图是如此往复实验8个超参数的结果：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-dab96b358cbed1206670600c37ce9ece_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"870\" data-rawheight=\"549\" class=\"origin_image zh-lightbox-thumb\" width=\"870\" data-original=\"https://pic3.zhimg.com/v2-dab96b358cbed1206670600c37ce9ece_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;870&#39; height=&#39;549&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"870\" data-rawheight=\"549\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"870\" data-original=\"https://pic3.zhimg.com/v2-dab96b358cbed1206670600c37ce9ece_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-dab96b358cbed1206670600c37ce9ece_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>贝叶斯方法关键涉及到一个代理模型的选择，例如上图中的EI(x)的定义，现在**还没有很成熟可靠**的方法。有时候贝叶斯方法表现的像人类专家，在某些问题上可以取得比较好的结果，但有时候在某些问题上又像白痴一样。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>不过如果这个方法能够有一天成熟起来，必将贡献于整个机器学习领域，节约大量的调参时间，让机器学习算法从业者把更宝贵的时间专注于考虑更本质的问题。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>下面为了方便以后检索，把以上五种总结如下表：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>|          超参数搜索方法           |  ML |   DL | 自动化 | 资源代价 | 利用历史信息 | 多机并行 |         适用于         |</p><p>| :------------------------:        |:---:| :---:| :----: | :------: | :----------: | :------: | :--------------------: |</p><p>|   照看法(Baby Sitting)            | YES | ***  |   NO   |  *       | YES          | NO       | 参数少，机器少，水平高 |</p><p>|   网格搜索(Grid Search)           | YES | *    |   YES  |  ****    | NO           | YES      | 参数少，机器多，信息少 |</p><p>|   二分搜索(Binary Search)         | YES | **   |   NO   |  *       | YES          | NO       | 凸函数                 |</p><p>|   随机搜索(Random Search)         | YES | ***  |   YES  |  ****    | NO           | YES      | 参数多，机器多         |</p><p>| 贝叶斯优化(Bayesian Optimization) | YES | **** |   YES  |  *       | YES          | N/A      | 某些特殊情况           |</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文总结了常用的几种深度学习调参的方法，这些方法都有各自的优缺点，有的方法之间还有相互借鉴的交集，需要在实际工作中多揣摩，多实验，多分析。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [深度学习超参数搜索实用指南](<a href=\"https://zhuanlan.zhihu.com/p/46278815\" class=\"internal\">阿里云云栖社区：深度学习超参数搜索实用指南</a>)</p><p>+ [超参数搜索不够高效？这几大策略了解一下](<a href=\"https://zhuanlan.zhihu.com/p/46718023\" class=\"internal\">机器之心：超参数搜索不够高效？这几大策略了解一下</a>)</p><p>+ [Practical guide to hyperparameters search for deep learning models](<a href=\"https://link.zhihu.com/?target=https%3A//blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Practical Guide to Hyperparameters Optimization for Deep Learning Models</a>)</p><p>+ [How To Make Deep Learning Models That Don’t Suck](<a href=\"https://link.zhihu.com/?target=https%3A//blog.nanonets.com/hyperparameter-optimization/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">A Practical Guide To Hyperparameter Optimization</a>)</p><p>+ [Random Search for Hyper-Parameter Optimization](<a href=\"https://link.zhihu.com/?target=http%3A//www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">jmlr.org/papers/volume1</span><span class=\"invisible\">3/bergstra12a/bergstra12a.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [《深度学习》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/27087503/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度学习 (豆瓣)</a>)</p>", 
            "topic": [
                {
                    "tag": "调参", 
                    "tagLink": "https://api.zhihu.com/topics/20682988"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "学习方法", 
                    "tagLink": "https://api.zhihu.com/topics/19566266"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/81956135", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 2, 
            "title": "为什么选交叉熵作为分类问题的损失函数", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p># 　　　　　　为什么选交叉熵作为分类问题的损失函数</p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在深度学习中，经常（如果不是100%的话）选交叉熵作为分类问题的损失函数，为什么选交叉熵，而不是传统机器学习更熟悉的均方差或者别的损失函数？这在深度学习中是一个很重要的和基础的问题。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A，纯数学公式推导，无代码</p><p class=\"ztext-empty-paragraph\"><br/></p><p>为了回答本文题目提出的问题，下面从两个方面来做讨论：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 交叉熵**能不能**做分类问题的损失函数？</p><p>2. 交叉熵与均方差相比，做深度学习分类问题损失函数，**优势在哪里**？</p><p class=\"ztext-empty-paragraph\"><br/></p><p>我想回答了上面两个子问题，本文题目的问题，也就迎刃而解了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 交叉熵能不能做分类问题损失函数</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 信息论中的熵</p><p class=\"ztext-empty-paragraph\"><br/></p><p>熵是一个物理学中的概念，代表事物的**混乱程度**，越混乱，越没有规律的事物，熵越大，反之越小。在信息论和计算机科学中，熵代表**信息量**的多少，信息量越多，熵越大。**信息量的多少，用编码这个信息所需要的最少要多少个bit表示**，比如吃没吃饭这个信息，只有两个不同状态，就可以只用一个bit（0代表没吃，1代表吃了）编码；今天周几这个信息，有7个不同的状态，就至少要用3个bit来编码。编码位数用公式表达就是</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8ba1e35ad3691f4f7601b47fa05e4524_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"371\" data-rawheight=\"85\" class=\"content_image\" width=\"371\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;371&#39; height=&#39;85&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"371\" data-rawheight=\"85\" class=\"content_image lazy\" width=\"371\" data-actualsrc=\"https://pic1.zhimg.com/v2-8ba1e35ad3691f4f7601b47fa05e4524_b.jpg\"/></figure><p>，p_i代表某个状态i出现的概率（本文的对数，没有特别声明都是以2为底）。再取所有可能的状态去加权平均，就是这个信息（分布）的熵</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-00af84a14d203be6a4d1d78f46962c1a_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"688\" data-rawheight=\"82\" class=\"origin_image zh-lightbox-thumb\" width=\"688\" data-original=\"https://pic3.zhimg.com/v2-00af84a14d203be6a4d1d78f46962c1a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;688&#39; height=&#39;82&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"688\" data-rawheight=\"82\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"688\" data-original=\"https://pic3.zhimg.com/v2-00af84a14d203be6a4d1d78f46962c1a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-00af84a14d203be6a4d1d78f46962c1a_b.png\"/></figure><p>，**代表编码这个事件所需要的总的字节数**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>另外，还有一点很重要的结论，那就是一个事件**按照真实分布编码的时候，需要的字节数，也就是熵H(X)最小**。证明很简单，类似于贪心算法，出现概率最大的可能性，分配最少的字节去编码，最小可能性，分配最多的字节去编码，更严谨的证明，参见[Gibbs&#39; inequality](<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Gibbs%2527_inequality\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">en.wikipedia.org/wiki/G</span><span class=\"invisible\">ibbs%27_inequality</span><span class=\"ellipsis\"></span></a>)。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 衡量两个分布的相似度－KL散度</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如果同一个事件A，分布p是它的真实分布（如果找得到的话），另外有一个分布q是我们**猜测的**事件A的分布，我们希望我们猜测的分布q尽量地和实际分布p一致。我们按照q分布去给事件A编码，那么每个事件A的可能性需要的字节数乘以该可能性发生的概率，就得到了**p分布和q分布的交叉熵**</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-561e647d2abf026622d8f0a71ae6b30c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"202\" data-rawheight=\"50\" class=\"content_image\" width=\"202\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;202&#39; height=&#39;50&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"202\" data-rawheight=\"50\" class=\"content_image lazy\" width=\"202\" data-actualsrc=\"https://pic1.zhimg.com/v2-561e647d2abf026622d8f0a71ae6b30c_b.jpg\"/></figure><p>。根据上节中的讨论，**交叉熵可以理解为，用猜测的q分布去编码实际为p分布的事件所需要的字节数**。而这样编码，和最佳编码（按照实际分布p来编码），所**额外用的字节数**，叫做**相对熵**，也叫做[**KL散度**](<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Kullback%25E2%2580%2593Leibler_divergence\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">en.wikipedia.org/wiki/K</span><span class=\"invisible\">ullback%E2%80%93Leibler_divergence</span><span class=\"ellipsis\"></span></a>)</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-082b2024536b7569686bd24a426b6cbd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"296\" data-rawheight=\"55\" class=\"content_image\" width=\"296\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;296&#39; height=&#39;55&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"296\" data-rawheight=\"55\" class=\"content_image lazy\" width=\"296\" data-actualsrc=\"https://pic2.zhimg.com/v2-082b2024536b7569686bd24a426b6cbd_b.jpg\"/></figure><p>，额外用的字节数越少，那么分布q就和分布p越接近，所以，KL散度是衡量两个分布相似度的指标。额外提一句，**KL散度不满足距离要求的对称性，不能当作距离用**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### KL散度和交叉熵的关系</p><p class=\"ztext-empty-paragraph\"><br/></p><p>由上面的讨论可知，交叉熵代表了用q分布编码p分布需要的字节数，KL散度表示这个字节数和最佳编码字节数之间的差值，也就是多用的字节数。**当分布p一定的时候，H(p)也就定了，那么交叉熵和KL散度同增同减，是等价的**。因为</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. KL散度可以衡量两个分布q和p的相似程度</p><p>2. **在分布p一定时**，交叉熵和KL散度同增同减</p><p class=\"ztext-empty-paragraph\"><br/></p><p>所以，在优化时，如果要衡量两个分布相似度，交叉熵是一个合适的选择。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Softmax输出的是什么</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在[《从线性回归到对率回归到Softmax激活函数》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230010-%25E4%25BB%258E%25E7%25BA%25BF%25E6%2580%25A7%25E5%259B%259E%25E5%25BD%2592%25E5%2588%25B0%25E5%25AF%25B9%25E7%258E%2587%25E5%259B%259E%25E5%25BD%2592%25E5%2588%25B0Softmax%25E6%25BF%2580%25E6%25B4%25BB%25E5%2587%25BD%25E6%2595%25B0/D%25230010.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)文中已经论述了，我们可以把Softmax的输出看作是每类的概率分布。真实的Ground Truth就代表了**真实分布**。这两个分布一个是上文提到的q，一个是p，那么对这两个分布的距离，用交叉熵来表示（将底改为e，不影响最值），也就是自然而然的事情了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>到此为止，我们也就解答了“交叉熵能不能做分类问题损失函数”这个问题。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 交叉熵比均方差好在哪里</p><p class=\"ztext-empty-paragraph\"><br/></p><p>除了交叉熵比均方差完美地配合了Softmax输出概率这一个前提之外，交叉熵比均方差做损失函数好还有一些比较现实的原因。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-af57cd5c4e61c2b68c4dd8dc0d70e22c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1147\" data-rawheight=\"804\" class=\"origin_image zh-lightbox-thumb\" width=\"1147\" data-original=\"https://pic1.zhimg.com/v2-af57cd5c4e61c2b68c4dd8dc0d70e22c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1147&#39; height=&#39;804&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1147\" data-rawheight=\"804\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1147\" data-original=\"https://pic1.zhimg.com/v2-af57cd5c4e61c2b68c4dd8dc0d70e22c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-af57cd5c4e61c2b68c4dd8dc0d70e22c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图中，左上角y帽代表网络输出的真值的估计，y是真值。而y帽是从Softmax激活函数得来的，a_i是Softmax激活函数每一路的输入值。J代表损失函数，在优化的时候，损失函数会通过梯度下降法反向传播来优化网络，减小J的值。这里以J反向传播到a_i为例，直观感受一下为什么Softmax作为输出层的时候，J用交叉熵比用均方差好。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>首先对于交叉熵的损失式１，对它关于a_i求导，得到式4，式4里面有两项（4-1和4-2）相乘。对于均方差的损失式2，也对它关于a_i求导，得到式5，式5里面有三项（5-1,5-2和5-3）相乘。因为y帽和y都是概率，所以4-1,4-2个5-1,5-2,5-3都是绝对值小于1的，一些小于1的数相乘，越乘越接近0，而且项数越多，越接近0。接近0是我们不愿意发生的，因为误差反向传播需要根据这些导数来更新网络的参数，接近0就代表更新量接近0了，不利于网络优化。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>然后我们假设，y_i = 1的时候，看看学习时会发生什么。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4b4f568a62224ff2eb0ee72c167357f3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1162\" data-rawheight=\"696\" class=\"origin_image zh-lightbox-thumb\" width=\"1162\" data-original=\"https://pic4.zhimg.com/v2-4b4f568a62224ff2eb0ee72c167357f3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1162&#39; height=&#39;696&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1162\" data-rawheight=\"696\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1162\" data-original=\"https://pic4.zhimg.com/v2-4b4f568a62224ff2eb0ee72c167357f3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4b4f568a62224ff2eb0ee72c167357f3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>可以看到，在y_i　 = 1（假设学习one-hot目标）时，**交叉熵相对于Softmax的输入a_i的导数delta_ce和误差A成正比，误差越大，导数越大，学习越快**；而用**最小二乘的delta_mse，随着误差增大，反而学习变慢了，学习效率不高**。而且，delta_mse的绝对值一直比delta_ce小，也印证了我们在上一段的讨论。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>除了和Softmax配合之外，如果前面一层的激活函数是Sigmoid激活的话，也有类似的问题，这里不再证明。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>至此，我们也回答了文首提出的第二个问题“交叉熵与均方差相比，做深度学习分类问题损失函数，优势在哪里？”</p><p class=\"ztext-empty-paragraph\"><br/></p><p>总结一句就是，交叉熵做Softmax分类的损失函数，它可以做，也做得比其它损失函数好，所以深度学习的分类问题，一般就是它啦。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文从信息论和统计学KL散度的角度论证了用交叉熵作为深度学习中分类问题损失函数的合理性，并结合Softmax作为网络最后输出层这个例子，在误差反向传播和参数迭代优化的角度，比较了交叉熵比均方差在分类问题上的优势。这些讨论可以加深我们对经典方法的理解和学习，在工程实践中，也有一定作用。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [《深度学习》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/27087503/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度学习 (豆瓣)</a>)</p><p>+ [Gibbs&#39; inequality](<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Gibbs%2527_inequality\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">en.wikipedia.org/wiki/G</span><span class=\"invisible\">ibbs%27_inequality</span><span class=\"ellipsis\"></span></a>)</p><p>+ [KL散度](<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Kullback%25E2%2580%2593Leibler_divergence\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">en.wikipedia.org/wiki/K</span><span class=\"invisible\">ullback%E2%80%93Leibler_divergence</span><span class=\"ellipsis\"></span></a>)</p><p>+ [《从线性回归到对率回归到Softmax激活函数》](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230010-%25E4%25BB%258E%25E7%25BA%25BF%25E6%2580%25A7%25E5%259B%259E%25E5%25BD%2592%25E5%2588%25B0%25E5%25AF%25B9%25E7%258E%2587%25E5%259B%259E%25E5%25BD%2592%25E5%2588%25B0Softmax%25E6%25BF%2580%25E6%25B4%25BB%25E5%2587%25BD%25E6%2595%25B0/D%25230010.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)</p><p>+ [如何通俗的解释交叉熵与相对熵?-Noriko Oshima](<a href=\"https://www.zhihu.com/question/41252833/answer/108777563\" class=\"internal\">如何通俗的解释交叉熵与相对熵?</a>)</p><p>+ [为什么使用交叉熵作为损失函数](<a href=\"https://zhuanlan.zhihu.com/p/63731947\" class=\"internal\">weiyinfu：为什么使用交叉熵作为损失函数</a>)</p>", 
            "topic": [
                {
                    "tag": "激活函数", 
                    "tagLink": "https://api.zhihu.com/topics/20682949"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "优化", 
                    "tagLink": "https://api.zhihu.com/topics/19570512"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/81955899", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 0, 
            "title": "kNN", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p># 　　　　　　kNN</p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>k近邻算法是最简单最容易理解和实现的**惰性**机器**分类与回归**算法，它只是将全部的训练样本都记录下来，在预测的时候根据一定的距离度量准则和决策规则选出最合适的类别或则数值输出。和SVM或者CNN相比，它**没有显示的学习过程**，是一种惰性的学习算法。本文以kNN分类算法为例对其做简单介绍。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A，纯数学公式推导，无代码</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 算法描述</p><p class=\"ztext-empty-paragraph\"><br/></p><p>当给定训练集合D，模型记录下所有的训练集合，在新的测试样本t来的时候，kNN方法在训练集合中寻找k个和t距离最近的样本，根据这k个样本中占主要类别的类作为t的类。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 算法关键因素</p><p>从上面的描述可以知道，在给定了训练集合之后，kNN算法中的关键因素有如下三点：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. k值的选择</p><p>2. 距离的定义</p><p>3. 决策规则</p><p class=\"ztext-empty-paragraph\"><br/></p><p>下面对这三点做逐一分析</p><p>### k值的选择</p><p class=\"ztext-empty-paragraph\"><br/></p><p>先讨论特殊的情况，如果k=1，那么就是选训练集合中与测试样本最近的样本的类作为预测的类，此时模型复杂，任何在训练样本中出现过的样本都会被正确分类，但是，每次分类结果仅取决于训练集合中的单一样本，这样也就对训练集合中的噪音非常敏感，类似于过拟合。另一个极端，如果k=训练集合中的样本个数，此时模型简单，对于任意的测试样本，输出都是训练集合中占主导类别的类，较远的（不相似的）点也对分类结果起作用，明显分类效果很差，类似于欠拟合。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>那么怎么选择合适的k值呢？</p><p class=\"ztext-empty-paragraph\"><br/></p><p>首先，从训练集合的数据量来看。在训练集合比较小的情况下，建议先认真清理数据，然后选择比较小的k值；在训练集合比较大的情况下，选择比较大的k值提高对噪音的鲁棒程度。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>其次，用k折交叉验证来选择合适的k值是一个比较稳妥的方法。另外，可以利用网格搜索技术来加快超参数挑选。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 距离的定义</p><p class=\"ztext-empty-paragraph\"><br/></p><p>特征空间中两个相似点的距离远近代表它们的相似度。有很多计算空间中点点相似度的方法，比如L1距离，L2距离，L无穷大距离，还有余弦距离等。需要注意的是，**相同的两个点，取不同的距离度量方式，得出的相似度可能是不一样的！**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>具体的距离度量方式取决于具体的问题（要选距离越小，相似性越大的距离，比如文本分类选余弦距离比欧氏距离更合适），这里很难一概而论。不过，有几点需要注意一下的：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 预处理，要将不同维度的**取值范围归一化**，避免少数维度在距离度量中占主导地位；</p><p>2. 特征选择，特征空间中各个维度的特征关联性越小越好，最好**正交**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 决策规则怎么选</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在选取了k个距离最近的训练样本后，按什么方式决策测试样本的类别呢？一般用多数表决法，即选择这k个最近邻中出现次数做多的类别作为模型的输出类别。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>可以证明，在损失函数取0-1损失函数时候，**多数表决法等价于经验风险最小化**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 算法性能分析</p><p class=\"ztext-empty-paragraph\"><br/></p><p>研究表明，在一定条件下，**最近邻**的分类错误率不会超过两倍的贝叶斯错误率。而且，**一般kNN**方法的错误率会收敛到贝叶斯错误率，所以我们**可以将kNN算法作为贝叶斯的近似**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>kNN算法是一种不需要训练过程的惰性算法，可以用于分类任务也可用于回归任务。它思路简单，容易理解，而且性能也不错。本文从算法原理、关键因素、超参数选择和性能分析等方面对其进行了初步介绍和总结。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [《统计学习方法》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/10590856/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">统计学习方法 (豆瓣)</a>)</p><p>+ [《数据挖掘十大算法》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/24735417/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">数据挖掘十大算法 (豆瓣)</a>)</p><p>+ [《深度学习》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/27087503/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度学习 (豆瓣)</a>)</p>", 
            "topic": [
                {
                    "tag": "K近邻算法", 
                    "tagLink": "https://api.zhihu.com/topics/20688152"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "聚类算法", 
                    "tagLink": "https://api.zhihu.com/topics/19627785"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/81791660", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 0, 
            "title": "从线性回归到对率回归到Softmax激活函数", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p># 　　　　　　从线性回归到对率回归到Softmax激活函数</p><p>## 引言</p><p class=\"ztext-empty-paragraph\"><br/></p><p>线性回归是最基础最简单的机器学习算法，很多其他的算法（比如对率回归、广义线性回归）或者衍生于它，或者可以借助它加深理解（比如感知机、Softmax激活函数、交叉熵），本文从线性回归开始，串联对率回归和Softmax激活函数的原理，做一个简单的介绍和总结。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A，纯数学公式推导，无代码</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 线性回归</p><p class=\"ztext-empty-paragraph\"><br/></p><p>给定一组训练数据{(x1,y1), (x2,y2), ..., (xN,yN)}，其中x是特征向量，y是标签，取值区间在[0,1]。一元线性回归就是将特征空间的每个分量，赋予一定的权重将他们加起来，然后加上一个偏置，将其和作为标签y的估计值。或者说，寻找一个W，使h(x) = Wx + b接近训练集y。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 一元线性回归</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-bc49c0384c9c588a6d4d467cfa5e6118_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"727\" data-rawheight=\"711\" class=\"origin_image zh-lightbox-thumb\" width=\"727\" data-original=\"https://pic1.zhimg.com/v2-bc49c0384c9c588a6d4d467cfa5e6118_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;727&#39; height=&#39;711&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"727\" data-rawheight=\"711\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"727\" data-original=\"https://pic1.zhimg.com/v2-bc49c0384c9c588a6d4d467cfa5e6118_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-bc49c0384c9c588a6d4d467cfa5e6118_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>当特征空间只有一维的时候，就是一维线性回归。此时，W是标量，b也是标量，h(x) = wx + b。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>那么如何通过训练数据估计参数w和b呢？关键在于找到一个函数，度量估计值和真实值的总的差距大小，然后找到使这个总的差距最小的的w和b就可以了。均方差就是这么一个度量工具。因此，我们可以通过均方差最小化来找w和b的解。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图E(w,b)是总的平方误差（和均方差只差一个系数1/N，不影响讨论），**E是关于w,b的凸函数**，所以直接可以对w,b分别求偏导数然后令偏导数等于0就可以求得w和b的在最小二乘意义下的**全局最优解**。上面说的**基于均方误差最小化来进行模型求解的方法成为“最小二乘法”**。在**线性回归中，最小二乘法的几何意义是试图找到一条直线h(x)，使这条直线到所有样本点的欧式距离平方的和最小**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 多元线性回归</p><p class=\"ztext-empty-paragraph\"><br/></p><p>当一元线性回归的样本特征维数大于1的时候，一元线性回归就泛化成了多元线性回归。求解的方法同样是最小二乘法。先对w，b求导，然后分别令其为0即可。为了简便，可以把w和b组合成一个新的变量W&#39; = (w;b)，然后h(x) = W&#39;x。最后可以求得W&#39;的解为inv(X_t\\*X) \\* X_t \\* y，X_t是X的转置。需要特别注意的是，当数据集中**样本个数小于特征维数的时候，X_t\\*X是不满秩的**，此时有无数多个W&#39;，此时就要根据具体问题的实际情况的偏好，对W&#39;的取值范围进行约束，常见的做法是引入正则项。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>一元线性回归是多元线性回归的特殊情况，它只有一个特征维度，但是有大于一个的学习样本，样本数大于特征的个数，所以，肯定不会出现X_t\\*X不满秩的情况的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 联系函数和广义线性回归</p><p class=\"ztext-empty-paragraph\"><br/></p><p>线性回归模型虽然简单，但是却可以产生出丰富的变化。如果我们前面的模型W\\*x + b不是直接预测标签y，而是预测y的变换g(y)，那么这就是**广义线性模型**，W\\*x + b的值对于y本身来说，不是线性的，但是对于y的变换域g(y)来说是线性的。这里的函数g()成为**联系函数**。特别地，如果g取自然对数ln的话，那就是“对数线性回归”，它使用W\\*x + b线性拟合ln(y)，换一种说法，它使用e^(W\\*x+b)来线性拟合y。g(y) = W\\*x + b可以推出y = g_inv(W\\*x + b)，所以**联系函数g()必须是可逆函数**，要不然就不能把W\\*x + b直接映射到y了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 对率回归</p><p class=\"ztext-empty-paragraph\"><br/></p><p>上面讲的都是回归的内容，回归简单地将就是将一组输入特征变换一下，输出一个**实数值**。但是如果要用特征的线性组合W\\*x + b做**分类**问题怎么办？很简单，找一个联系函数，把特征的线性组合W\\*x + b映射到不同类别就行。最简单的映射当然是指定一个值t，当W\\*x + b　&gt; t时，映射到1类，反之映射到0类，h(z)对应于一个阶跃函数，如下图中情况A所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-68cc92d516746b9f6dccf51f1eed21bf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1341\" data-rawheight=\"716\" class=\"origin_image zh-lightbox-thumb\" width=\"1341\" data-original=\"https://pic4.zhimg.com/v2-68cc92d516746b9f6dccf51f1eed21bf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1341&#39; height=&#39;716&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1341\" data-rawheight=\"716\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1341\" data-original=\"https://pic4.zhimg.com/v2-68cc92d516746b9f6dccf51f1eed21bf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-68cc92d516746b9f6dccf51f1eed21bf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>但是，阶跃函数不连续，求解过程中不好求导；更重要的是阶跃函数**不可逆**，那就不能作为联系函数来用（原因请见联系函数一节）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 对率函数</p><p class=\"ztext-empty-paragraph\"><br/></p><p>有人就想到了用对数几率函数（上图情况B所用的h(z)函数）来替代阶跃函数，对数几率函数是一个S形函数，单调可微，任意阶可导，取值范围(0,1)，**正好也可以对应到标签的两端，而且也是概率的取值范围**。另外，它还有其它一些很好的数学性质，比如关于(0,0.5)这个点点对称，比如求导简单等。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>使用对数几率函数作为联系函数的（其实是逆函数）广义线性回归，就叫对率回归。**对率回归的输出值可以看做是预测为1的概率**。那么当使用对率回归做分类时，很自然地加一个阈值就可以了，比如对率函数输出大于0.5就判断为1类，否则判断为0类。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>题外话，对率函数其实是Logistic函数簇的一种特殊情况，为什么把它叫对率函数呢？其实对率函数的全称应该是**“对数几率线性回归函数”**。几率定义为样本x作为1类的概率除以作为0类的概率，即odds = y / (1-y)。如果对odds去自然对数，那么在对率函数的情况下，这个对数的结果ln(y/(1-y))是由W\\*x + b线性表示的，所以叫对数、几率、线性、回归！所以就将这个特殊的S型的函数成为对率函数，因为它将W\\*x + b和对数几率直接联系了起来。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 对率回归参数估计</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6c87ff2079067ef89190b6bf7f263e8b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1308\" data-rawheight=\"905\" class=\"origin_image zh-lightbox-thumb\" width=\"1308\" data-original=\"https://pic4.zhimg.com/v2-6c87ff2079067ef89190b6bf7f263e8b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1308&#39; height=&#39;905&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1308\" data-rawheight=\"905\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1308\" data-original=\"https://pic4.zhimg.com/v2-6c87ff2079067ef89190b6bf7f263e8b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6c87ff2079067ef89190b6bf7f263e8b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>见上图，式7是对率回归的表达式h(x)，将其视为y=1的概率，然后写出可以用极大似然估计函数式10。式10是对w和b的高阶连续可导凸函数，理论上可以直接求导找到最大值，但是直接像线性回归那样找到解析解十分困难。所以一般是用凸优化理论的方法，比如牛顿法、梯度下降法来迭代，找到数值解，因为怎么求解和模型本身的思想关系不大，这里不做详述。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 从对率回归到Softmax激活函数</p><p>先说明一下，这里讲的Softmax不是Softmax Function = ln(sum_1toN(e^x_i))，而是Softmax Activation Function。</p><p>Softmax激活函数，是如下图式11所示</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6cd39edcbb3888f631a199d576acbf37_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1325\" data-rawheight=\"787\" class=\"origin_image zh-lightbox-thumb\" width=\"1325\" data-original=\"https://pic4.zhimg.com/v2-6cd39edcbb3888f631a199d576acbf37_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1325&#39; height=&#39;787&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1325\" data-rawheight=\"787\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1325\" data-original=\"https://pic4.zhimg.com/v2-6cd39edcbb3888f631a199d576acbf37_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6cd39edcbb3888f631a199d576acbf37_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>通过对率回归的一个结论：事件发生的几率odds等于e^(wx)，可以知道，Softmax的输出的K个分量就对应了这个输入x属于该分量对应类别的几率大小。式11分母部分，是为了赋予一个概率的意义，把各个输出分量用他们的和归一化而已（不太准确的说，就是将几率转化为概率）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 一点学习心得</p><p class=\"ztext-empty-paragraph\"><br/></p><p>从最简单直观的线性回归，到不那么好理解的Softmax激活函数，各种巧妙的方法，思路都是从基础简单的问题和思路一步步推演过来的，要么是为了发展旧方法来解决一个具体的小问题（例如对率函数的引入，就是为了解决阶跃函数不可导不可逆的问题），要么是对原来旧方法的拓展（例如从多元线性回归到广义线性回归）。另外，有些方法由于历史原因，名字不是很合适，例如，&#34;Logistic Regression&#34;这个名字就很不恰当，应该叫对数几率回归比较好。另外，Softmax称为Softargmax也会更恰当。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文首先介绍了一元到多元的线性回归，并从联系函数引入了广义线性回归。当用对数函数联系几率和特征的线性组合时就成了对率回归，并介绍了对率函数和对率回归的参数估计。最后将对率回归从二分类推广到多分类引入了Softmax激活函数。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [《统计学习方法》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/10590856/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">统计学习方法 (豆瓣)</a>)</p><p>+ [《PRML》](<a href=\"https://link.zhihu.com/?target=https%3A//www.douban.com/group/471521/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Pattern Recognition And Machine Learning</a>)</p><p>+ [《The Elements of Statistical Learning》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/3294335/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">The Elements of Statistical Learning</a>)</p><p>+ [Logistic function](<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Logistic_function%23cite_note-4\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">en.wikipedia.org/wiki/L</span><span class=\"invisible\">ogistic_function#cite_note-4</span><span class=\"ellipsis\"></span></a>)</p><p>+ [《深度学习》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/27087503/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度学习 (豆瓣)</a>)</p>", 
            "topic": [
                {
                    "tag": "线性回归", 
                    "tagLink": "https://api.zhihu.com/topics/19650500"
                }, 
                {
                    "tag": "激活函数", 
                    "tagLink": "https://api.zhihu.com/topics/20682949"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/81791276", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 1, 
            "title": "支持向量机", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p># 　　　　　　支持向量机</p><p>## 引言</p><p>在机器学习中，[支持向量机](<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Support-vector_machine\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">en.wikipedia.org/wiki/S</span><span class=\"invisible\">upport-vector_machine</span><span class=\"ellipsis\"></span></a>)是一种有效并且常用的二分类模型，它的基本模型是定义在特征空间中的间隔最大化的线性分类器（加上核技巧后也可变成非线性分类器）。它的数学理论基础扎实，且适用于小样本的学习，在现在深度学习流行的今天，也是一种值得学习的基础模型。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A，纯数学公式推导，无代码</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 线性支持向量机</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 从二分类模型到线性支持向量机基本型</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-9ad070ebb1bb96efba8b3735c2737ea5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"971\" data-rawheight=\"790\" class=\"origin_image zh-lightbox-thumb\" width=\"971\" data-original=\"https://pic2.zhimg.com/v2-9ad070ebb1bb96efba8b3735c2737ea5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;971&#39; height=&#39;790&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"971\" data-rawheight=\"790\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"971\" data-original=\"https://pic2.zhimg.com/v2-9ad070ebb1bb96efba8b3735c2737ea5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-9ad070ebb1bb96efba8b3735c2737ea5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图给定一组训练数据{(x1,y1), (x2,y2), ..., (xN,yN)}，其中x是特征向量，y是标签（取值-1，+1）。二分类问题就是要找到一个假设函数h(x)，使得对所有的训练数据有h(xi) = yi，数学描述等价于h(xi) \\* yi = 1。如果我们假设要找的这个假设函数为线性函数，即假设h(x) = sign(Wx + b)这种形式，那么就是线性二分类问题，Wx + b = 0为**划分超平面**，h(x) = Wx + b为**分类决策函数**，超平面将整个特征空间一分为二，符合h(x) &gt; 0的决策为+1类，否则决策为-1类。数学描述就是要找到合适的W和b参数，使对所有h(xi) \\* yi = sign(Wxi + b) \\* yi = 1，这等价于对所有的xi都有(Wx + b) * yi &gt; 0。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这样的超平面可能有无数个，如上图所示的a和b就是两个，那用什么标准来选择最好的呢？支持向量机的核心思想就是，选择和距离正样本集合和负样本集合**间隔最大**的超平面为划分超平面(而空间点和平面的距离公式为下图中式3所示)，所以支持向量机的原始形式可以见下图式4和式5，翻译成人话就是，在分类所有训练样本正确的前提下，寻找W和b，使其构成的超平面Wx + b = 0可以最大距离划分正样本集和负样本集，所谓的最大距离，就是超平面和任意一个样本点的距离都尽可能地大。也就是，支持向量机是想找到图中b所代表的划分平面比a要好。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ae9828d02478ca3f72bf0336cf67fc61_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"751\" data-rawheight=\"813\" class=\"origin_image zh-lightbox-thumb\" width=\"751\" data-original=\"https://pic2.zhimg.com/v2-ae9828d02478ca3f72bf0336cf67fc61_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;751&#39; height=&#39;813&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"751\" data-rawheight=\"813\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"751\" data-original=\"https://pic2.zhimg.com/v2-ae9828d02478ca3f72bf0336cf67fc61_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ae9828d02478ca3f72bf0336cf67fc61_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>但是这里原始形式很难处理，为了能在能在现实中好优化，需要继续把它做简化，简化成一个经典的凸二次规划(QP)问题。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在**式4中，里面的求最小，是相对于所有的样本点求最小，和W,b的取值无关**，所以可以将式4简化为式6，然后可以注意到，同时用相同的系数缩放W和b，不改变式6和式5的解，我们就可以进一步令min|Wxi+b| = 1（反正可以任意缩放咯），这里的集合意义是让样本点中离超平面最近的距离归一化一下，然后式6就可以转化到式7(等价可以到式8)，同样式5可以转化为式9。最后联立式8和式9就是**线性支持向量机的基本型**了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 对偶问题</p><p>可以证明，式8和式9描述的线性支持向量机的基本行中所描述的优化属于d+1个优化变量（W和b），N个约束的**凸二次规划问题**。我们可以用[D#0008](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230008-%25E6%258B%2589%25E6%25A0%25BC%25E6%259C%2597%25E6%2597%25A5%25E4%25B9%2598%25E5%25AD%2590%25E6%25B3%2595/D%25230008.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)中提到的拉格朗日乘子法引入拉格朗日乘子将带约束的原问题转换为不带约束的对偶问题来方便地求解。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ade94d25e75a24f77128cac11ff12acc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"757\" data-rawheight=\"745\" class=\"origin_image zh-lightbox-thumb\" width=\"757\" data-original=\"https://pic1.zhimg.com/v2-ade94d25e75a24f77128cac11ff12acc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;757&#39; height=&#39;745&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"757\" data-rawheight=\"745\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"757\" data-original=\"https://pic1.zhimg.com/v2-ade94d25e75a24f77128cac11ff12acc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ade94d25e75a24f77128cac11ff12acc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>由上面的W\\*和b\\*的表达式可以知道，当lambda_i\\* = 0时，对应的x_i和y_i对Ｗ\\*的取值无影响，进而b\\*这些x_i和y_i也影响不到b\\*。所以可知，**只有那些对应于lambda_i\\* != 0的样本才对最终的决策平面(Wx + b = 0)有影响**。而这些样本就是支持向量机的**支持向量**，第一张图中两个x和一个o这三个点就是支持向量。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>由上面的讨论现在可以有以下结论，1.支持向量机的划分平面由最重要的少数样本点(支持向量)决定；2.在预测时，样本点到划分平面的距离，可以**在一定程度上**表示为置信度。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 非线性支持向量机</p><p class=\"ztext-empty-paragraph\"><br/></p><p>上面介绍的线性支持向量机处理的样本都有个假设，假设样本线性可分，但是我们更可能遇到的是一些线性不可分的数据，例如下图所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e36fbc3a91de211b47c4a0ca9c8ebd34_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"4032\" data-rawheight=\"3024\" class=\"origin_image zh-lightbox-thumb\" width=\"4032\" data-original=\"https://pic1.zhimg.com/v2-e36fbc3a91de211b47c4a0ca9c8ebd34_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;4032&#39; height=&#39;3024&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"4032\" data-rawheight=\"3024\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"4032\" data-original=\"https://pic1.zhimg.com/v2-e36fbc3a91de211b47c4a0ca9c8ebd34_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e36fbc3a91de211b47c4a0ca9c8ebd34_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>对于这种情况，**T. M. Cover模式可分性定理**证明了，对于低维度特征，可以通过非线性映射z = T(x)嵌入到高维度(甚至无穷维度)空间内，在那个空间内特特征线性可分！这样我们又可以在这个高维度空间内愉快地使用线性SVM来分类了。我们把T(x)当作ｘ代入上上图式11中，解出高维空间中新的W* = sum_1toN(lambda_i \\* y_i \\* T(x_i))，b* = 1 - W*x_+1。不失一般性，让b\\* = 0（相当于整个样本和划分平面一起平移，使划分平面经过原点），那么我们有划分平面W\\*T(x) = 0，也就是**sum_1toN(lambda_i \\* y_i \\* T(x_i)) \\* T(x) = 0**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 核技巧基本原理</p><p class=\"ztext-empty-paragraph\"><br/></p><p>但是，这里有两个问题，**第一，这个从低维度特征空间到高维度特征空间的映射函数T(x)很难找；第二，高维度特特征空间维度可能很高，甚至于无穷维，那么要直接计算T(x)也是很困难的。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>继续观察到，我们找到划分平面，**其实不需要去计算T(x)的具体值是多少，甚至连T(x)的函数形式都不需要，我们只需要知道T(x_i) \\* T(x)是多少就可以了。**即，找到一个新的函数K(x1, x2)使其等于T(x_i) \\* T(x)。这个函数K可以找的比较简单的形式(起码比T函数要简单的多)，找到的K就叫做**核函数**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 核函数的讨论和常用核函数</p><p class=\"ztext-empty-paragraph\"><br/></p><p>核函数是否一定存在？什么样的函数可以做核函数呢？这涉及到更深的希尔伯特空间中核矩阵的判别问题，这里就不多做讨论了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>通过上面的讨论可知，我们希望通过T将低维度空间内线性不可分问题转换为高维度空间内线性可分问题，那么高维度特征空间的好坏，也就是T的好坏，就决定了支持向量机的性能好坏。让人悲伤的是，我们并不知道高维度特征空间的样子，也不知道T的形式，我们只是为了简单计算，**间接地通过核函数K来定义T**。所以，核函数的选择好坏，直接决定了高维度特征空间的好坏，如果和函数选择不好，可能直接就决定了支持向量机性能也不好。**“核函数”的选择成为支持向量机方法成败的关键变量**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>一般来说，当特征维数D大于样本数量N时（例如文本分类），使用线性核；当特征维数D比较小，而样本数量N适中时，选择高斯核（也称RBF核），或者情况不明时，也选择高斯核；当特征维数D比较小，样本数N特别大时，选择深度神经网络一般会比SVM好。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>下面列出了一些常见的核函数：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-24f93131cf314953841594d8b5d64dbe_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"4032\" data-rawheight=\"3024\" class=\"origin_image zh-lightbox-thumb\" width=\"4032\" data-original=\"https://pic3.zhimg.com/v2-24f93131cf314953841594d8b5d64dbe_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;4032&#39; height=&#39;3024&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"4032\" data-rawheight=\"3024\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"4032\" data-original=\"https://pic3.zhimg.com/v2-24f93131cf314953841594d8b5d64dbe_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-24f93131cf314953841594d8b5d64dbe_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 软间隔和Hinge Loss</p><p>在上面的探讨中，都是要求支持向量机得到的划分平面能**100%正确划分**正样本和负样本，即**假设样本都是线性可分的**，虽然在理论上，我们总能找到一个高维映射使数据线性可分，但是在实际任务中，寻找这样合适的核函数很难而且计算也困难。另外，因为样本中噪音的存在，一味要求100%的正确分类，也很容易陷入过拟合的困境。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>继续回到式8和式9表示的基本问题。在基本问题的约束，式9中加入松弛变量e_i(大于0)，使约束条件变得宽松一点，然后为了防止过度宽松，在式8中也加上相应的惩罚，于是基本问题就变成了下图中的式12和13。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c88a28c5f823e6ced9715c171e50790b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1230\" data-rawheight=\"1010\" class=\"origin_image zh-lightbox-thumb\" width=\"1230\" data-original=\"https://pic4.zhimg.com/v2-c88a28c5f823e6ced9715c171e50790b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1230&#39; height=&#39;1010&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1230\" data-rawheight=\"1010\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1230\" data-original=\"https://pic4.zhimg.com/v2-c88a28c5f823e6ced9715c171e50790b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c88a28c5f823e6ced9715c171e50790b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其中，式12中的C表示一个大于零的加权系数，当C越大的时候，越不允许有样本错分。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这里的e_i的大小是和每个样本x_i有关的，它的形式应该满足于上图中1,2,3条件才行。在这里常用Hinge Loss来表示。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>对于Hinge Loss，如果分类正确，且间隔较大，那么Loss = 0；如果**分类正确，但是间隔不是很大，那么也还是有Loss的，只是Loss比分类不正确的时候要小点而已**。这样的目的是为了在优化的时候，**诱导分类平面往间隔最大的方向调整**，这也是支持向量机的基本思想。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 传统机器学习与深度学习讨论</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在深度学习大行其道的今天，是否还有学习传统的支持向量机呢？我觉得是有必要的，理由如下：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 支持向量机的整套体系有扎实的数学理论基础，例如最大间隔，拉格朗日对偶问题，核技巧等，对这些理论的学习和研究，也可以用在深度学习项目中；</p><p>2. 就传统图像处理、机器学习和深度学习的关系而言，很多技巧和思路，都是相同的哇。比如图像处理中的卷积核可分离和MoblieNet中深度可分离卷积的思路，图像金字塔和特征金字塔都是解决不同分辨率提取特征来解决尺度变化的问题，AdaBoost目标检测中在不同尺度做窗口扫描和SSD目标检测中在不同层次特征图做预选框生成，这些思路都是可以联系起来的；</p><p>3. 君子不器，作为工程师，不应囿于对某种特定的方法的偏见，应联系实际问题选择方法来解决具体问题，再说，天下没有免费的午餐嘛。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文从支持向量机的提出的动机开始，先介绍了线性支持向量机，然后介绍了非线性支持向量机和核技巧，后面介绍了软间隔和合页损失的内容，最后对传统机器学习和深度学习的关系做了一点评论。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [《统计学习方法》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/10590856/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">统计学习方法 (豆瓣)</a>)</p><p>+ [《PRML》](<a href=\"https://link.zhihu.com/?target=https%3A//www.douban.com/group/471521/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Pattern Recognition And Machine Learning</a>)</p><p>+ [《The Elements of Statistical Learning》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/3294335/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">The Elements of Statistical Learning</a>)</p><p>+ [南京大学-张皓-从零推导支持向量机](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/HaoMood/File/blob/master/%25E4%25BB%258E%25E9%259B%25B6%25E6%258E%25A8%25E5%25AF%25BC%25E6%2594%25AF%25E6%258C%2581%25E5%2590%2591%25E9%2587%258F%25E6%259C%25BA%28SVM%29.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/HaoMood/File</span><span class=\"invisible\">/blob/master/%E4%BB%8E%E9%9B%B6%E6%8E%A8%E5%AF%BC%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA(SVM).pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [《数据挖掘概念与技术》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/2038599/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">book.douban.com/subject</span><span class=\"invisible\">/2038599/</span><span class=\"ellipsis\"></span></a>)</p><p>+ [《数据挖掘十大算法》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/24735417/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">数据挖掘十大算法 (豆瓣)</a>)</p><p>+ [Hinge Loss](<a href=\"https://link.zhihu.com/?target=https%3A//www.wikiwand.com/en/Hinge_loss\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Hinge loss | Wikiwand</a>)</p><p>+ [支持向量机SVM - 从入门到放弃](<a href=\"https://zhuanlan.zhihu.com/p/30596284\" class=\"internal\">微调：支持向量机SVM - 从入门到放弃</a>)</p><p>+ [现在还有必要对 SVM 深入学习吗？](<a href=\"https://www.zhihu.com/question/41066458\" class=\"internal\">现在还有必要对 SVM 深入学习吗？</a>)</p>", 
            "topic": [
                {
                    "tag": "SVM", 
                    "tagLink": "https://api.zhihu.com/topics/19583524"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "算法", 
                    "tagLink": "https://api.zhihu.com/topics/19553510"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/81790672", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 0, 
            "title": "拉格朗日乘子法", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p># 　　　　　　拉格朗日乘子法</p><p>## 引言</p><p>在最优化理论中，[拉格朗日乘子法](<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Lagrange_multiplier\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">en.wikipedia.org/wiki/L</span><span class=\"invisible\">agrange_multiplier</span><span class=\"ellipsis\"></span></a>)是求解等式约束下极值问题的常用手段(KKT条件可以推广到求解不等式约束问题)，在机器学习中是一个很重要很基础的理论，[最大熵模型](<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Maximum-entropy_Markov_model\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">en.wikipedia.org/wiki/M</span><span class=\"invisible\">aximum-entropy_Markov_model</span><span class=\"ellipsis\"></span></a>)，L1[正则](<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Regularization_%2528mathematics%2529\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">en.wikipedia.org/wiki/R</span><span class=\"invisible\">egularization_%28mathematics%29</span><span class=\"ellipsis\"></span></a>)项为什么可以获得比L2正则获得更稀疏的解，[SVM](<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Support-vector_machine\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">en.wikipedia.org/wiki/S</span><span class=\"invisible\">upport-vector_machine</span><span class=\"ellipsis\"></span></a>)的理论推导的理解都和拉格朗日乘子法有关系。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>N/A，纯数学公式推导，无代码</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 拉格朗日乘子法介绍</p><p class=\"ztext-empty-paragraph\"><br/></p><p>拉格朗日乘子法的基本思想，就是将一个含有n个变量个k个（等式）约束条件的**有约束**最优化问题（也叫原问题）通过引入拉格朗日乘子转化为求(n+k)个变量的**无约束**最优化问题（也叫对偶问题），然后求解无约束最优化问题的极值解就是原来有约束最优化问题的解。其背后的数学原理是原问题极值处的梯度为各个约束条件梯度所张成的空间中的一个向量。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>上面的说法有点不好理解，下面以两个(n=2)变量，一个约束(k=1)条件下的最优化问题来具体讲。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>设目标函数是f(x,y)，约束条件是h(x,y)=0，需要求目标函数最小值。即</p><p>min f(x,y), st. h(x,y) = 0，求解这个问题可以用高斯消元法（但是有时候隐函数不容易消元），也可以引入拉格朗日系数lambda，通过求解对偶问题F(x, y, lambda) = f(x,y) + lambda\\*h(x,y)的极值就可以得到原问题的极值（是极大值还是极小值，需要进一步判定），求对偶问题的极值，就是F(x, y, lambda)函数对x,y和lambda三个变量分别求偏导数，令偏导数等于0，再联立得到的3个方程，3个方程3个未知数，求解x，y和lambda三个值就得到了原问题的解。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 几何实例</p><p class=\"ztext-empty-paragraph\"><br/></p><p>为什么可以这么做，李航老师的《统计学习基础》附录有比较简单的公式推导，这里可以通过下面的具体例子来直观解释一下：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>假设约束方程为h(x,y) = xy - 1 = 0（双曲线）;优化目标min f(x,y) = x^2+y^2。换句话说就是求解双曲线上距离坐标原点最近的点(距离要开根号，不过这里开不开根号，最优解都一样，根号可以忽略)。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-91e3b02f499dd8d130514514964ef50d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1162\" data-rawheight=\"524\" class=\"origin_image zh-lightbox-thumb\" width=\"1162\" data-original=\"https://pic2.zhimg.com/v2-91e3b02f499dd8d130514514964ef50d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1162&#39; height=&#39;524&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1162\" data-rawheight=\"524\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1162\" data-original=\"https://pic2.zhimg.com/v2-91e3b02f499dd8d130514514964ef50d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-91e3b02f499dd8d130514514964ef50d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图所示，我们要求的目标就是在**双曲线上**的所有点中，找到**距离原点最近**的点。f(x,y)函数簇就是一些以原点为圆心，半径不一样的圆，图中画出了圆a,b,c,d只是几个具体实例。</p><p>那么，什么样的点，是即在双曲线上（即满足约束条件xy-1=0），又让圆半径最小（半径最小是优化目标）呢？直观看来，应该是圆c和双曲线**相切**的点（因为如果不相切，则圆和双曲线必有交点，那么必有双曲线上的某些点在圆內，而这些点距离原点的距离，都比交点短，更优于交点）。那么，两个曲线相切，有什么必要条件需要满足呢？那就是法向量必须平行。所以有下图中的式１。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2dc844865f6e090b3c13a5dada5c2a72_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1165\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb\" width=\"1165\" data-original=\"https://pic3.zhimg.com/v2-2dc844865f6e090b3c13a5dada5c2a72_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1165&#39; height=&#39;500&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1165\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1165\" data-original=\"https://pic3.zhimg.com/v2-2dc844865f6e090b3c13a5dada5c2a72_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2dc844865f6e090b3c13a5dada5c2a72_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>由式1可以推出3和4，然后联立方程2,3,4可以解出对偶问题最优解是x = y = 1和x = y = -1。</p><p>而式3,4和2，其实就是上节提到的对偶问题F(x,y,lambda)分别对x,y和lambda分别求导。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>所以这里就以一个具体的例子，体会了一下拉格朗日乘子法的思想，为什么拉格朗日乘子法可以将原有约束优化问题转换为对偶无约束优化问题，然后通过求解对偶问题求解原问题。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 拉格朗日乘数法基本形态</p><p class=\"ztext-empty-paragraph\"><br/></p><p>上面是以具体的例子体验了一下，这里做进一步的推广。</p><p>首先复述一下拉格朗日乘子法：原问题，求函数z = f(x,y)在约束条件 h(x,y) = 0下的条件极值，可以引入拉格朗日乘子lambda转换为对偶问题F(x, y, lambda) = f(x, y) + lambda\\*h(x, y)的无条件极值问题。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>可以画图来辅助我们理解：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d7ae92ffa4dde4cd16e7edbaaf98f653_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"918\" data-rawheight=\"642\" class=\"origin_image zh-lightbox-thumb\" width=\"918\" data-original=\"https://pic4.zhimg.com/v2-d7ae92ffa4dde4cd16e7edbaaf98f653_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;918&#39; height=&#39;642&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"918\" data-rawheight=\"642\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"918\" data-original=\"https://pic4.zhimg.com/v2-d7ae92ffa4dde4cd16e7edbaaf98f653_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d7ae92ffa4dde4cd16e7edbaaf98f653_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>红线是优化目标的函数簇（同一个线上，z值相等，可以理解为等高线），蓝线为约束条件h(x, y)，小箭头代表法线方向。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>可以直观地看出，在最优解处，f和h的法线平行。剩下怎么求解就跟上节几何实例中一样了，首先加入拉格朗日乘子lambda体现法向量平行，然后分别对x,y求导得到两个方程，结合约束方程h(x,y)，三个方程三个未知数求解，不赘述。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 在机器学习中的应用</p><p class=\"ztext-empty-paragraph\"><br/></p><p>拉格朗日乘子法作为一种凸优化的基础方法，在机器学习中有基础而广泛的应用。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 离散分布的最大熵</p><p class=\"ztext-empty-paragraph\"><br/></p><p>下面是一个拉格朗日乘子法在概率论和机器学习里面的应用，证明均匀分布是最大熵分布</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-216313a032d286815c6ff4f3a1f3c72b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"4032\" data-rawheight=\"3024\" class=\"origin_image zh-lightbox-thumb\" width=\"4032\" data-original=\"https://pic4.zhimg.com/v2-216313a032d286815c6ff4f3a1f3c72b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;4032&#39; height=&#39;3024&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"4032\" data-rawheight=\"3024\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"4032\" data-original=\"https://pic4.zhimg.com/v2-216313a032d286815c6ff4f3a1f3c72b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-216313a032d286815c6ff4f3a1f3c72b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>### L1，L2正则分析</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在机器学习问题中，随着模型参数的增多复杂度越大，模型的表达能力越强，越倾向于学到太多的**噪音**。表现为当模型的复杂度增加时，训练误差越来越小，而测试误差会先逐渐减小，达到最小值后又逐渐增大。这就是发生了**过拟合**问题，如下图所示：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f3ed929c73a858ea3705ffb7a7be2b40_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"4032\" data-rawheight=\"3024\" class=\"origin_image zh-lightbox-thumb\" width=\"4032\" data-original=\"https://pic1.zhimg.com/v2-f3ed929c73a858ea3705ffb7a7be2b40_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;4032&#39; height=&#39;3024&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"4032\" data-rawheight=\"3024\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"4032\" data-original=\"https://pic1.zhimg.com/v2-f3ed929c73a858ea3705ffb7a7be2b40_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f3ed929c73a858ea3705ffb7a7be2b40_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>而学习到噪音往往和极大的参数值有关，或者说噪音是通过一些极大绝对值的参数值学习到的。所以，通常就在优化目标上加上一个正则化项来表示对这些大的参数值的惩罚。正则化一般具有如下形式：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-91e3b02f499dd8d130514514964ef50d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1162\" data-rawheight=\"524\" class=\"origin_image zh-lightbox-thumb\" width=\"1162\" data-original=\"https://pic2.zhimg.com/v2-91e3b02f499dd8d130514514964ef50d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1162&#39; height=&#39;524&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1162\" data-rawheight=\"524\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1162\" data-original=\"https://pic2.zhimg.com/v2-91e3b02f499dd8d130514514964ef50d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-91e3b02f499dd8d130514514964ef50d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>当后面的正则化项为L1函数的时候，称为L1正则，为L2函数的时候，称为L2正则。L1正则比L2正则比，更容易得到稀疏（很多分量接近0）的解。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这里可以用拉格朗日乘子法在二维情况下做个理解性分析：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-13f24ab3596e1a1bf2b97cedc27a7bcf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1498\" data-rawheight=\"708\" class=\"origin_image zh-lightbox-thumb\" width=\"1498\" data-original=\"https://pic4.zhimg.com/v2-13f24ab3596e1a1bf2b97cedc27a7bcf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1498&#39; height=&#39;708&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1498\" data-rawheight=\"708\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1498\" data-original=\"https://pic4.zhimg.com/v2-13f24ab3596e1a1bf2b97cedc27a7bcf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-13f24ab3596e1a1bf2b97cedc27a7bcf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>![](images/131203.png)</p><p>上图中蓝色的圈，就是经验风险L(w)，红色的是J(w)正则化项（圆形是L2正则，方形是L1正则）。正则化的一般形式可以理解为拉格朗日乘子法的无约束条件下的对偶问题，上图，可以理解为对偶问题拆回到了原问题，L(w)是要优化目标函数，一圈一圈的圆形是它的等高线，红色的J(w)是约束条件（圈内的点符合约束）。由上图可以看到，在L1正则的情况下，等高线更容易碰到（其实是相切）方形的尖尖上，而尖尖那里，就是代表有某些系数为0！（这里是w1=0）这样就是稀疏化了。而在L2的时候，等高线不容易碰到圆形w1=0的地方，也不容易碰到w2=0的地方，所以L2就没有L1的稀疏化能力。另外，这里是二维参数的情况，在高维情况下，L1约束区域有更多的尖尖，每个尖尖有更多的分量为0，而且，等高线也更容易碰到尖尖。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文介绍了拉格朗日乘子法的基本形式，对其简单的几何证明，并介绍了该理论在机器学习中的应用。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p>+ [《统计学习方法》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/10590856/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">统计学习方法 (豆瓣)</a>)</p><p>+ [《PRML》](<a href=\"https://link.zhihu.com/?target=https%3A//www.douban.com/group/471521/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Pattern Recognition And Machine Learning</a>)</p><p>+ [《The Elements of Statistical Learning》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/3294335/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">The Elements of Statistical Learning</a>)</p>", 
            "topic": [
                {
                    "tag": "拉格朗日乘子", 
                    "tagLink": "https://api.zhihu.com/topics/20687697"
                }, 
                {
                    "tag": "算法", 
                    "tagLink": "https://api.zhihu.com/topics/19553510"
                }, 
                {
                    "tag": "优化", 
                    "tagLink": "https://api.zhihu.com/topics/19570512"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/81790260", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 0, 
            "title": "compile git commit sha1 into elf", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p>## 引言</p><p>我们自己会开发一些sdk以动态库的形式发布出去，为了便于问题回溯，我们会手动加上版本号来管理，或者在代码里面加上诸如\\_\\_DATE\\_\\_和\\_\\_TIME\\_\\_这样的宏来打印出代码编译时候的时间信息，但是如果commit比较频繁，这样还是很难将时间和源代码的版本精确对应起来，这里介绍一种方法，可以自动得获取git commit的sha1信息，将其自动化地编译进sdk。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 目标：自动化打印sdk编译时候的git commit sha1信息</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 限制条件：1. 自动化；2.精确；3. 不可以影响系统正常运行。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 操作系统：Ubuntu 16.04 LTS</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 编译器：gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验过程</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这里为了讨论方便，约定sdk库由sdk.cpp编译而成，sdk头文件为sdk.h，应用程序由main.cpp编译而成，中间调用sdk的接口。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>目录结构如下：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-47dc2c4919bbe804a8ac52903831ec10_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"327\" data-rawheight=\"274\" class=\"content_image\" width=\"327\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;327&#39; height=&#39;274&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"327\" data-rawheight=\"274\" class=\"content_image lazy\" width=\"327\" data-actualsrc=\"https://pic1.zhimg.com/v2-47dc2c4919bbe804a8ac52903831ec10_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>代码内容如下：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ef8969cd60e0979c5a751de39ad95aed_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"355\" data-rawheight=\"270\" class=\"content_image\" width=\"355\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;355&#39; height=&#39;270&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"355\" data-rawheight=\"270\" class=\"content_image lazy\" width=\"355\" data-actualsrc=\"https://pic2.zhimg.com/v2-ef8969cd60e0979c5a751de39ad95aed_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7bbec649e5588f89b7ee3b1168a375ab_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"797\" data-rawheight=\"415\" class=\"origin_image zh-lightbox-thumb\" width=\"797\" data-original=\"https://pic4.zhimg.com/v2-7bbec649e5588f89b7ee3b1168a375ab_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;797&#39; height=&#39;415&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"797\" data-rawheight=\"415\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"797\" data-original=\"https://pic4.zhimg.com/v2-7bbec649e5588f89b7ee3b1168a375ab_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7bbec649e5588f89b7ee3b1168a375ab_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4807f8e37368a12c74310f1a1cf9a2ce_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"506\" data-rawheight=\"315\" class=\"origin_image zh-lightbox-thumb\" width=\"506\" data-original=\"https://pic3.zhimg.com/v2-4807f8e37368a12c74310f1a1cf9a2ce_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;506&#39; height=&#39;315&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"506\" data-rawheight=\"315\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"506\" data-original=\"https://pic3.zhimg.com/v2-4807f8e37368a12c74310f1a1cf9a2ce_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4807f8e37368a12c74310f1a1cf9a2ce_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ce4c93d68157b09c6329658f452f9ebb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"875\" data-rawheight=\"440\" class=\"origin_image zh-lightbox-thumb\" width=\"875\" data-original=\"https://pic4.zhimg.com/v2-ce4c93d68157b09c6329658f452f9ebb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;875&#39; height=&#39;440&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"875\" data-rawheight=\"440\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"875\" data-original=\"https://pic4.zhimg.com/v2-ce4c93d68157b09c6329658f452f9ebb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ce4c93d68157b09c6329658f452f9ebb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>结果分析：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fb6a7236e16bbd0cacd8f2ceae06eb5e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"843\" data-rawheight=\"546\" class=\"origin_image zh-lightbox-thumb\" width=\"843\" data-original=\"https://pic3.zhimg.com/v2-fb6a7236e16bbd0cacd8f2ceae06eb5e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;843&#39; height=&#39;546&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"843\" data-rawheight=\"546\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"843\" data-original=\"https://pic3.zhimg.com/v2-fb6a7236e16bbd0cacd8f2ceae06eb5e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fb6a7236e16bbd0cacd8f2ceae06eb5e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>首先在Makefile中用git log, head和cut命令获取当前代码的git commit sha1值，然后将其赋给GIT_COMMIT_SHA1宏，最后通过gcc的-D选项将其告诉编译器。这样就可以在代码里面跟自己定义的字符串一样使用这个GIT_COMMIT_SHA1宏了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**注意**，一定要在代码commit后再编译，这样才能使当前仓库最新commit sha1值和代码一致。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文分享了一种将git commit sha1值自动编译进elf文件的方法，并做了简单的原理介绍，其他版本工应该也可以找到类似的方法。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [Linux C/C++ 把Git commit SHA1值编译到程序中来方便查看版本](<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/thisinnocence/article/details/79517984\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/thisinnoc</span><span class=\"invisible\">ence/article/details/79517984</span><span class=\"ellipsis\"></span></a>)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ [将git版本号编译进程序](<a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/wangqiguo/p/7191352.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">将git版本号编译进程序 - 薰衣草的旋律 - 博客园</a>)</p>", 
            "topic": [
                {
                    "tag": "软件开发", 
                    "tagLink": "https://api.zhihu.com/topics/19552332"
                }, 
                {
                    "tag": "版本管理", 
                    "tagLink": "https://api.zhihu.com/topics/19566741"
                }, 
                {
                    "tag": "软件工程", 
                    "tagLink": "https://api.zhihu.com/topics/19557552"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/81789465", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 0, 
            "title": "protect my function", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p># 　　　　　　protect my function</p><p>## 引言</p><p>我们自己会开发一些sdk以动态库的形式发布出去，为了保护自己的知识产权，往往还会搭配一个负责授权的授权库（为了模块化和维护升级方便，往往授权库也是单独动态库的形式），sdk库通过调用授权库的接口完成授权工作。最简单的方式就是授权库提供一个类似于int get_auth_result(const char* key)的接口（为了谈论简单，假设只有一个接口），sdk输入key，然后检查返回值，如果返回值为0则授权成功，非0则授权失败。但是[D#0002](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230002-hack_your_printf/D%25230002.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)中提了，这样的调用是有可能被sdk使用者破解的。这里我们针对这个授权库防劫持的问题，来讨论一下。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 目标：确保我们sdk里面调用的get_auth_result函数一定是正版授权库里面的这个函数，而不是被劫持的李鬼函数</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 限制条件：1. 不可以影响授权库升级； 2. 不可以要求系统root权限；3. 不可以影响系统正常运行。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 操作系统：Ubuntu 16.04 LTS</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 编译器：gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这里为了讨论方便，<a href=\"https://link.zhihu.com/?target=http%3A//%25E7%25BA%25A6%25E5%25AE%259A%25E6%258E%2588%25E6%259D%2583%25E5%25BA%2593%25E4%25B8%25BAlibauth.so\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">约定授权库为libauth.so</a>(由auth.cpp编译而成)，头文件为auth.h，<a href=\"https://link.zhihu.com/?target=http%3A//sdk%25E5%25BA%2593%25E4%25B8%25BAlibsdk.so\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">sdk库为libsdk.so</a>(由sdk.cpp编译而成)，sdk头文件为sdk.h，sdk库调用auth库完成授权。</p><p>文件内容如下：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-491379b71f28059600cb20c9111e2108_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"540\" data-rawheight=\"308\" class=\"origin_image zh-lightbox-thumb\" width=\"540\" data-original=\"https://pic1.zhimg.com/v2-491379b71f28059600cb20c9111e2108_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;540&#39; height=&#39;308&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"540\" data-rawheight=\"308\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"540\" data-original=\"https://pic1.zhimg.com/v2-491379b71f28059600cb20c9111e2108_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-491379b71f28059600cb20c9111e2108_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e9c3faea44823b349cb1cf4c4fc6f948_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"533\" data-rawheight=\"485\" class=\"origin_image zh-lightbox-thumb\" width=\"533\" data-original=\"https://pic1.zhimg.com/v2-e9c3faea44823b349cb1cf4c4fc6f948_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;533&#39; height=&#39;485&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"533\" data-rawheight=\"485\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"533\" data-original=\"https://pic1.zhimg.com/v2-e9c3faea44823b349cb1cf4c4fc6f948_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e9c3faea44823b349cb1cf4c4fc6f948_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-41416f75be04de89dc65bd69ef18a45f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"786\" data-rawheight=\"337\" class=\"origin_image zh-lightbox-thumb\" width=\"786\" data-original=\"https://pic4.zhimg.com/v2-41416f75be04de89dc65bd69ef18a45f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;786&#39; height=&#39;337&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"786\" data-rawheight=\"337\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"786\" data-original=\"https://pic4.zhimg.com/v2-41416f75be04de89dc65bd69ef18a45f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-41416f75be04de89dc65bd69ef18a45f_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-673b764989bfdba93260ede0b6c202d8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"754\" data-rawheight=\"450\" class=\"origin_image zh-lightbox-thumb\" width=\"754\" data-original=\"https://pic1.zhimg.com/v2-673b764989bfdba93260ede0b6c202d8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;754&#39; height=&#39;450&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"754\" data-rawheight=\"450\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"754\" data-original=\"https://pic1.zhimg.com/v2-673b764989bfdba93260ede0b6c202d8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-673b764989bfdba93260ede0b6c202d8_b.jpg\"/></figure><p>生成授权库和sdk库的过程如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0560922b31afafc0c06d6f40caf4ac8b_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"630\" data-rawheight=\"62\" class=\"origin_image zh-lightbox-thumb\" width=\"630\" data-original=\"https://pic4.zhimg.com/v2-0560922b31afafc0c06d6f40caf4ac8b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;630&#39; height=&#39;62&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"630\" data-rawheight=\"62\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"630\" data-original=\"https://pic4.zhimg.com/v2-0560922b31afafc0c06d6f40caf4ac8b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0560922b31afafc0c06d6f40caf4ac8b_b.png\"/></figure><p>这样就生成了libauth.so和libsdk.so两个库。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>按照sdk的使用管理，sdk的发布者给出一个sdk so库，一个授权so库和一个对应的头文件，不给授权库的头文件，第三方客户根据sdk的头文件里面的接口使用sdk。比如如下main函数可以视作第三方客户怎么使用sdk：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b14177ab601d0bfa31ea876a2078adf0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1201\" data-rawheight=\"486\" class=\"origin_image zh-lightbox-thumb\" width=\"1201\" data-original=\"https://pic1.zhimg.com/v2-b14177ab601d0bfa31ea876a2078adf0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1201&#39; height=&#39;486&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1201\" data-rawheight=\"486\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1201\" data-original=\"https://pic1.zhimg.com/v2-b14177ab601d0bfa31ea876a2078adf0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b14177ab601d0bfa31ea876a2078adf0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>### Case 1, api hinden</p><p class=\"ztext-empty-paragraph\"><br/></p><p>从[D#0002](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230002-hack_your_printf/D%25230002.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)已经知道了，用nm工具可以查找libauth.so里面的符号表，进而猜测出授权使用的API进行劫持（不管输入什么key，都返回验证通过，至于什么返回值代表验证通过，试）。</p><p>很自然的，我们会想到，能不能让libauth.so里面的函数，不需要客户知道的API，不要让客户看到，这样可以增大客户猜测到API和破解的难度（其实还是可以破解）。</p><p>可以的，g++有导出符号的控制，-fvisibility=hidden，这个选项可以将输出so库的符号的可见性（这里让其不可见，不出现在符号表中）。</p><p>之前的libauth.so的符号：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-aee348fe0b888b7fc61166f20ec19246_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"766\" data-rawheight=\"725\" class=\"origin_image zh-lightbox-thumb\" width=\"766\" data-original=\"https://pic3.zhimg.com/v2-aee348fe0b888b7fc61166f20ec19246_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;766&#39; height=&#39;725&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"766\" data-rawheight=\"725\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"766\" data-original=\"https://pic3.zhimg.com/v2-aee348fe0b888b7fc61166f20ec19246_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-aee348fe0b888b7fc61166f20ec19246_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>可以看到最后一行有00000000000006f0 T get_auth_result(char const\\*)，这就是授权库的接口。</p><p>看看用了-fvisibility=hidden之后：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e3a958c3f356b6ebbb3d078c193a4c09_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1133\" data-rawheight=\"807\" class=\"origin_image zh-lightbox-thumb\" width=\"1133\" data-original=\"https://pic2.zhimg.com/v2-e3a958c3f356b6ebbb3d078c193a4c09_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1133&#39; height=&#39;807&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1133\" data-rawheight=\"807\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1133\" data-original=\"https://pic2.zhimg.com/v2-e3a958c3f356b6ebbb3d078c193a4c09_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e3a958c3f356b6ebbb3d078c193a4c09_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>可以看到，get_auth_result(char const\\*)这个接口还是看得到，只是从T可见的编程了t不可见的了，看来这个方法还是不行。</p><p>其实，-fvisibility=hidden这个选项，表达为不可用比较好，不让别人用，别人如果在自己的代码里面应用get_auth_result这个函数是不可以的，会出现undefined reference to的错误，**但是不妨碍别人自己伪造一个**。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8c7e7f2ec2b125d9f3e02d3192c74533_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1124\" data-rawheight=\"125\" class=\"origin_image zh-lightbox-thumb\" width=\"1124\" data-original=\"https://pic4.zhimg.com/v2-8c7e7f2ec2b125d9f3e02d3192c74533_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1124&#39; height=&#39;125&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1124\" data-rawheight=\"125\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1124\" data-original=\"https://pic4.zhimg.com/v2-8c7e7f2ec2b125d9f3e02d3192c74533_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8c7e7f2ec2b125d9f3e02d3192c74533_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Case 2, strip</p><p class=\"ztext-empty-paragraph\"><br/></p><p>strip命令是专门用来给目标文件，动态库文件还有可执行文件去掉符号信息的，符号表内的信息也可以去除，而且不影响程序的正常运行。</p><p>这里是man strip的解释</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6afc02751fa4ce985d44466973233049_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"667\" data-rawheight=\"155\" class=\"origin_image zh-lightbox-thumb\" width=\"667\" data-original=\"https://pic2.zhimg.com/v2-6afc02751fa4ce985d44466973233049_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;667&#39; height=&#39;155&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"667\" data-rawheight=\"155\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"667\" data-original=\"https://pic2.zhimg.com/v2-6afc02751fa4ce985d44466973233049_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6afc02751fa4ce985d44466973233049_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>下面我们来实验一下：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-25d1b2433a46a26af1dcfbb01fce1820_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"863\" data-rawheight=\"1013\" class=\"origin_image zh-lightbox-thumb\" width=\"863\" data-original=\"https://pic1.zhimg.com/v2-25d1b2433a46a26af1dcfbb01fce1820_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;863&#39; height=&#39;1013&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"863\" data-rawheight=\"1013\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"863\" data-original=\"https://pic1.zhimg.com/v2-25d1b2433a46a26af1dcfbb01fce1820_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-25d1b2433a46a26af1dcfbb01fce1820_b.jpg\"/></figure><p>可以看到，在strip之前都是有符号信息的，可以看得到get_auth_result(char const\\*)这个接口，而且文件size比较大8184byte；在用strip命令处理后，符号表没有了，看不到get_auth_result(char const\\*)，文件size也将为了6160byte。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Case 3, 接口改造，加上一个magic number</p><p class=\"ztext-empty-paragraph\"><br/></p><p>我们为了防止别人伪造get_auth_result(char const\\*)接口，前面想的办法都是把接口藏起来，这样别人就不知道接口怎么写，哪些输入哪些输出，就无从伪造。还有一种思路，我们如果把get_auth_result(char const\\*)接口设计得更复杂一点，比如，返回值加入一个协助验证接口身份的密码，这个密码的产生规则，只有sdk开发者和auth库开发者知道，在sdk使用auth库的时候，去在检查授权是否成功的时候也按照之前约定的规则检查一下这个密码，那么就更保险了。</p><p>为了保险起见，这个密码**应该是动态的**。</p><p>改造后的代码如下（客户代码main.cpp未改动）：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5dcaf58f1470c9f9fcdce7051ef0ca4f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"613\" data-rawheight=\"423\" class=\"origin_image zh-lightbox-thumb\" width=\"613\" data-original=\"https://pic4.zhimg.com/v2-5dcaf58f1470c9f9fcdce7051ef0ca4f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;613&#39; height=&#39;423&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"613\" data-rawheight=\"423\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"613\" data-original=\"https://pic4.zhimg.com/v2-5dcaf58f1470c9f9fcdce7051ef0ca4f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5dcaf58f1470c9f9fcdce7051ef0ca4f_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c9e790eb17e555f928e094517351ccf8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"707\" data-rawheight=\"571\" class=\"origin_image zh-lightbox-thumb\" width=\"707\" data-original=\"https://pic1.zhimg.com/v2-c9e790eb17e555f928e094517351ccf8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;707&#39; height=&#39;571&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"707\" data-rawheight=\"571\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"707\" data-original=\"https://pic1.zhimg.com/v2-c9e790eb17e555f928e094517351ccf8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c9e790eb17e555f928e094517351ccf8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在验证库里面，加了一个magic number作为返回，这个magic生成的规则是key的首字母加末尾字母的和（当然，你也可以构造更复杂的规则）。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-643538eac09bcf29b9bb29b826f099c2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"849\" data-rawheight=\"629\" class=\"origin_image zh-lightbox-thumb\" width=\"849\" data-original=\"https://pic3.zhimg.com/v2-643538eac09bcf29b9bb29b826f099c2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;849&#39; height=&#39;629&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"849\" data-rawheight=\"629\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"849\" data-original=\"https://pic3.zhimg.com/v2-643538eac09bcf29b9bb29b826f099c2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-643538eac09bcf29b9bb29b826f099c2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在sdk使用授权库的时候，根据约定的规则（这里为了简单用key首字母加末尾字母的和）来校验auth接口的返回值，如果magic number对不上，那就是被第三方给劫持了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>以下是三种方案的小结：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>|         方案          |      实现复杂度       | 破解难度 |  升级难度   | 维护成本 |</p><p>| :-------------------: | :-------------------: | :------: | :---------: | :------: |</p><p>|      （1）api hinden    |           *           |    *     |      *      |    *     |</p><p>|         （2）strip         |           *           |    \\*\\*\\*    |    *    | * |</p><p>|     （3）magic number   |        \\*\\*\\*        |  \\*\\*\\*\\*\\*  |    \\*\\*\\*\\*\\*    | \\*\\*\\* |</p>", 
            "topic": [
                {
                    "tag": "C++ 编程", 
                    "tagLink": "https://api.zhihu.com/topics/19836485"
                }, 
                {
                    "tag": "软件开发", 
                    "tagLink": "https://api.zhihu.com/topics/19552332"
                }, 
                {
                    "tag": "Linux 开发", 
                    "tagLink": "https://api.zhihu.com/topics/19610306"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/81683945", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 1, 
            "title": "图像处理中的卷积核分离", 
            "content": "<p><b>船长黑板报所有文章和代码的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p>## 引言</p><p>在计算机图像处理中，有一种经常进行的操作，就是图像滤波，也叫图像卷积（深度学习中的卷积概念也是衍生于它，只不过深度学习中的卷积核是三维的，图像处理中的卷积核是二维的），比如用Canny卷积提取图像中的边缘信息，用Gaussian卷积构造金字塔等等。在深度学习中，深度可分离卷积（Depth-wise Separable Convolution）取代传统卷积，可以起到加速（减少计算量）和减小模型大小（参数数量）的作用；类似地，在图像处理中，往往也可以用两个独立的小的卷积串联，取代一个大的卷积，也可以起到减少计算量和减小参数数量的作用。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**请注意，本文所用的术语“卷积convolution”并不是很恰当，恰当的称呼应该叫“滤波filter”或者“空间相关”，卷积的话卷积核要旋转180度。**但太多情况下并不区分这两者，这里将错就错称之为“卷积”，但这并不影响我们的结论。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 操作系统：Ubuntu 16.04 LTS，Ubuntu 18.04 LTS</p><p>+ 编译器：g++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609</p><p class=\"ztext-empty-paragraph\"><br/></p><p>  　　　　gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 1，二维Gaussian卷积核的可分离性分析</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 推导过程：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c675f28d71d3c051c1b6860476e6ad44_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"4032\" data-rawheight=\"3024\" class=\"origin_image zh-lightbox-thumb\" width=\"4032\" data-original=\"https://pic1.zhimg.com/v2-c675f28d71d3c051c1b6860476e6ad44_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;4032&#39; height=&#39;3024&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"4032\" data-rawheight=\"3024\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"4032\" data-original=\"https://pic1.zhimg.com/v2-c675f28d71d3c051c1b6860476e6ad44_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c675f28d71d3c051c1b6860476e6ad44_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>可以从上图推导过程中看出，一个m行乘以n列的高斯卷积可以分解成一个1行乘以n列的行卷积，之后串联一个m行乘以1列的列卷积的形式，输出保持不变。行卷积的卷积核参数（均值和方差）等于原始m行n列卷积核在列方向（Y方向）的均值和方差，列卷积的卷积核参数等于原始m行n列卷积核在行方向（X方向）上的均值和方差。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 计算量分析：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>原始卷积的mxn的卷积和，卷积一个图像，乘加次数等于图像的像素个数WxH，乘以以单个像素为中心店做卷积时卷积的乘加次数mxn，总次数等于MxHxmxn。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在把原始卷积拆分为行卷积和列卷积后，行卷积的乘加次数等于输入图像的像素点个数WxH，乘以单个行卷积的乘加次数n，列卷积的乘加次数等于上一步行卷积输出图像的像素点个数WxH，乘以单个列卷积的乘加次数m，所以，总的次数是WxHxn+WxHxm。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>由此可见，将卷积分离后的计算量比原始计算量等于(m+n)/(mxn)，常见的宽高相等，假设为k，则计算量的比可以简化为2/k。**计算复杂度从原来的O(k^2)将为了O(k)。**是一个很大的提速。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 参数数量分析：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>原始卷积的参数数量为mxn个，卷积拆分后的参数数量为n+m。考虑到高斯核的对称性，这个数还略有冗余。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 2，二维Gaussian卷积核的可分离代码实测</p><p class=\"ztext-empty-paragraph\"><br/></p><p>代码都放在code/目录下，都是按照高斯函数的定义写的，不过假设了卷积核是11x11的正方形框，x和y方向的方差都一样。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>速度实测如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-45d631ecc783a89427e1abc30462ffe0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"682\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb\" width=\"682\" data-original=\"https://pic1.zhimg.com/v2-45d631ecc783a89427e1abc30462ffe0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;682&#39; height=&#39;126&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"682\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"682\" data-original=\"https://pic1.zhimg.com/v2-45d631ecc783a89427e1abc30462ffe0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-45d631ecc783a89427e1abc30462ffe0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>取kernalsize=11, sigma=0.8,处理一副490x490的彩色图像，传统高斯滤波耗时498200us，卷积核分离之后滤波，耗时114679us。加速比为4.3443:1，和理论值(k^2)/(2k)=11:2=5.5:1稍微差一点。**卷积核的size越大，加速效果越明显**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 3，理论推广，什么样的核才可分离</p><p class=\"ztext-empty-paragraph\"><br/></p><p>上面已经在理论和实测中证明了高斯核的可分离性和加速效果。那么就有个很自然的问题，除了高斯核，还有哪些核也是可分离的，核的可分离性的充分必要条件是什么，弄清楚了这些问题，我们就可以在碰到新核的时候考虑一下它是否可用可分离性加速，或者我们自己设计新的满足可分离性的核。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>先说结论，**一个卷积核K，如果可以表达成一个水平核h和一个竖直核v相乘的形式，即K=v*transpose(h)那么K就是可以分离的**；反之，一个卷积核是可分离的，那么必然可以分解成一个水平核h和一个竖直核v相乘的形式。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>证明如下（这里只以K=v*transpose(h)的形式证明充分性，必要性反推即可）：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d04921e5395de66aabc986a065685bca_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3024\" data-rawheight=\"4032\" class=\"origin_image zh-lightbox-thumb\" width=\"3024\" data-original=\"https://pic3.zhimg.com/v2-d04921e5395de66aabc986a065685bca_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;3024&#39; height=&#39;4032&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3024\" data-rawheight=\"4032\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"3024\" data-original=\"https://pic3.zhimg.com/v2-d04921e5395de66aabc986a065685bca_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d04921e5395de66aabc986a065685bca_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>所以，我们按照这个思路，可以发现，除了高斯卷积可以分离，方框卷积、双线性卷积、高斯二阶导数卷积，也是可以分离的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 4，理论再推广，把不能分离的核做近似分离</p><p class=\"ztext-empty-paragraph\"><br/></p><p>毕竟，不是每个核K都一定是可以分解成**一个**水平核和一个竖直核相乘的形式的。不过我们可以把核看成一个mxn的矩阵，对其进行[奇异值分解](<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Singular_value_decomposition\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">en.wikipedia.org/wiki/S</span><span class=\"invisible\">ingular_value_decomposition</span><span class=\"ellipsis\"></span></a>)，就可以得到**一些**水平核和一些竖直核相乘再累加的形式K=sum(sigma_i\\*u_i\\*transpose(v_i))。这里面的sqrt(sigma_i)\\*u_i和sqrt(sigma_i)\\*v_i就是每一组的水平和竖直的卷积核。组数取得越多，计算的结果越和原始K卷积的结果相近，但是计算量也会增加；组数取得越少，计算的结果越和原始卷积结果相远，但是速度也越快。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这种分离方式加速的多少和原卷积核效果近似的程度，取决于a. 原始核K的size大小，1节已经证明了卷积核分离算法的加速情况，原始核K的size越大，加速比越大；b. 原始核K的奇异值分解得到的奇异值数量和分布，分解得到的奇异值的绝对值如果都集中在头部，那么可以忽略很多不重要的奇异值，那么加速效果好，质量损失小（**其实可分离性，是奇异值分解的一个特例**，如果奇异值分解后得到的奇异值，除了一个不为0，其他都为0，那就是前面几节讨论到的可分离）；c. 加速效果还和具体的实现有关，比如内存的访问，cache等等。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>把一个大卷积操作分解为一个水平卷积串联一个竖直卷积的过程叫卷积可分离，卷积可分离是图像处理中一个常见的优化手法，其效果和原始卷积相同，但是计算量是原始卷积计算量的2k/k^2，卷积核越大，计算量下降越明显。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>卷积核只是一个矩阵，对这个矩阵做奇异值分解，如果有且仅有一个不为零的奇异值，那么它就是可以分离的；如果还有其它的奇异值，那么它就是可近似分离的，分离的效果取决于奇异值的分布。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>最后值得一提的是，这种将一个大卷积分解成多个小卷积的串联来减小计算量和内存消耗的思路，在[深度学习的研究](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1512.00567.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1512.0056</span><span class=\"invisible\">7.pdf</span><span class=\"ellipsis\"></span></a>)中，也有很大的影响。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p>+ [《数字图像处理》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/6434627/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">数字图像处理 (豆瓣)</a>)</p><p>+ [高斯滤波器详解](<a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/wangguchangqing/p/6407717.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">图像处理基础(4)：高斯滤波器详解 - Brook_icv - 博客园</a>)</p><p>+ [《计算机视觉算法与应用》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/10465997/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">计算机视觉 (豆瓣)</a>)</p><p>+ [SVD论文](<a href=\"https://link.zhihu.com/?target=http%3A//www-users.math.umn.edu/~lerman/math5467/svd.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">www-users.math.umn.edu/</span><span class=\"invisible\">~lerman/math5467/svd.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [Inception V2&amp;V3](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1512.00567.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1512.0056</span><span class=\"invisible\">7.pdf</span><span class=\"ellipsis\"></span></a>)</p>", 
            "topic": [
                {
                    "tag": "图像处理", 
                    "tagLink": "https://api.zhihu.com/topics/19556376"
                }, 
                {
                    "tag": "卷积", 
                    "tagLink": "https://api.zhihu.com/topics/19678959"
                }, 
                {
                    "tag": "加速", 
                    "tagLink": "https://api.zhihu.com/topics/19609182"
                }
            ], 
            "comments": [
                {
                    "userName": "ZJY25", 
                    "userLink": "https://www.zhihu.com/people/3c9ab3b048d8d5948fd42e724e907a10", 
                    "content": "感谢", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "船长", 
                            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
                            "content": "<p>互相交流学习</p>", 
                            "likes": 0, 
                            "replyToAuthor": "ZJY25"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/81683709", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 3, 
            "title": "depthwise separable convolutions in mobilenet", 
            "content": "<p><b>船长黑板报所有文章的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p>## 引言</p><p>深度学习模型的精简和加速，在实际项目落地的过程中，越来越重要。模型文件的大小，直接决定了运行时内存的消耗，而且越大的模型，往往也意味着部署时更多的运算量(乘加数)和更慢的运行速度，更多的功耗。而深度可分离卷积(Depth-wise separable convolution)把一个传统的卷积层分解为一个深度卷积层(Depth-wise convolution)串联一个点卷积层(Point-wise convolution)的形式，这样的分解在保持输入特征图和输出特征图维度不变的前提下(假设stride=1,加padding)，使参数数量个运算量都大大得减少了，而且对模型预测精度牺牲小，可推广性强，易于理解和实现，是深度学习中一个重要而且实用的概念。本文就[MobileNet-V1论文](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1704.04861\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/abs/1704.0486</span><span class=\"invisible\">1</span><span class=\"ellipsis\"></span></a>)中关于深度可分离卷积的讲解来做个笔记。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 1, 深度可分离卷积的结构</p><p class=\"ztext-empty-paragraph\"><br/></p><p>因为MobileNet-V1论文中的配图只画出了卷积核，没有体现输入输出的特征图，为了便于理解，我加了特征图重画了一个：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3a8955ae7f4b8cb2edf70f18705338cf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3024\" data-rawheight=\"4032\" class=\"origin_image zh-lightbox-thumb\" width=\"3024\" data-original=\"https://pic4.zhimg.com/v2-3a8955ae7f4b8cb2edf70f18705338cf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;3024&#39; height=&#39;4032&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3024\" data-rawheight=\"4032\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"3024\" data-original=\"https://pic4.zhimg.com/v2-3a8955ae7f4b8cb2edf70f18705338cf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3a8955ae7f4b8cb2edf70f18705338cf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>先约定一下，输入特征图维度为1xMxDfxDf（与caffe中Blob排列顺序一致，NxCxHxW），经过卷积层（kernalsize为Dk）后输出特征图维度为1xNxDfxDf（做了卷积核stride=1，加padding，保证输入特征图和输出特征图分辨率不变），在论文原文中，感觉Df和Dg有点乱，这里简化一下。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>输入特征图走路径1的话，就是传统卷积形式：一张MxDfxDf的特征图（三维的），经过N个MxDkxDk的（立体）卷积核分别卷积，然后合并之后，生成了一个NxDfxDf的特征图。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>输入特征图走路径2的话，就是深度可分离卷积的形式：首先，同样一张MxDfxDf的特征图（三维的），先经过M个1xDkxDk的卷积后，生成了一张MxDfxDf的特征图。这里的卷积过程和传统卷积不一样，需要说明一下，这里有M个深度为1，kernalsize大小为DkxDk的小卷积核，每个小卷积核，**只和输入的M通道的特征图的对应通道做卷积**，卷积的结果，也放在输出特征图的**对应通道**上。传统的卷积操作，一组卷积核里面的每一个小卷积，都会和输入特征图每个通道相作用，而这里的卷积操作，一组卷积核里面的每一个小卷积通道数（深度）都是1，小卷积只和输入特征图的某一个通道相作用，故称**深度卷积**（英文Depth-wise convolution的Depth-wise这个单词可能更好理解，按深度来卷积），后面简称DW卷积；然后，在DW输出的1个MxDfxDf的特征图作为输入，经过N个Mx1x1的卷积核，进行**传统的卷积操作**之后，生成1个NxDfxDf的特征图，其实这一步**就是一个传统的卷积**，特殊之处只在于它的卷积核的kernalsize等于一，是一个point，所以把它命名为点卷积（Point-wise convolution），区别于kernalsize不等于一的时候的卷积（那种卷积，是不是应该叫Patch-wise convolution?哈哈），后面称PW卷积，PW卷积的作用是让DW卷积输出的特征图每个通道之间的信息能够互通组合，不损伤特征图的表达能力。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 2, 计算量和参数数量分析</p><p class=\"ztext-empty-paragraph\"><br/></p><p>MobileNet中使用深度可分离卷积的目的还是在于减少计算量和模型大小，下面是关于深度可分离卷积的计算量分析，以及和传统卷积的对比。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 2.1 计算量分析</p><p class=\"ztext-empty-paragraph\"><br/></p><p>首先，看传统卷积的计算量：</p><p>每个输出特征图上的一个像素点，对应的计算量就是一组NxCxHxW卷积核中的一个卷积核CxHxW中的卷积乘加次数，这个次数是CxHxW（卷积核中每个“像素”对应一次乘加运算），输出特征图上所有像素的个数是输出特征图的体积CxHxW。所以在1.1节图中，传统卷积路径1的计算量为：输出特征图的体积x一组卷积核中的一个卷积核的体积，为**DfxDfxNxDkxDkxM**。</p><p>然后，看深度可分离卷积的计算量：</p><p>深度可分离卷积的计算量分为两部分，第一部分是DW卷积的计算量，DW卷积的输出特征图体积是DfxDfxM，特征图中每个像素是由一个1xDkxDk的小卷积核卷积出来的，一个小卷积核做卷积对应的乘加数就是它的体积DkxDk，所以，**DW卷积的计算量为DfxDfxMxDkxDk**；第二部分，PW卷积输出NxDfxDf个像素的特征图，每个像素点都是一个Mx1x1卷积卷出来的，对于的乘加数为M(也就是小卷积核的体积)，所以，**PW卷积的计算量为NxDfxDfxM**。DW卷积的计算量和PW卷积的计算量之和就是整个**深度可分离卷积的计算量DfxDfxMxDkxDk+NxDfxDfxM**。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>所以，深度可分离卷积的计算量比上传统卷积的计算量就为：（DfxDfxMxDkxDk+NxDfxDfxM)/(DfxDfxNxDkxDkxM)=**1/N+1/(DkxDk)**，当N比较大（输出特征图的通道数比较多）的时候约等于1/(DkxDk)，也就是说，深度可分离卷积的计算量(与传统卷积的比值)和卷积核的平方成反比。当卷积核kernalsize为3的时候，计算量（理论上）为传统卷积的**1/9**，当卷积核kernalsize为5的时候，计算量为传统卷积的**1/25**，是一个很大的进步。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 2.2 参数数量分析</p><p class=\"ztext-empty-paragraph\"><br/></p><p>传统卷积的参数数量，就是所有小卷积核的体积之和，**NxMxDkxDk**；深度可分离卷积的参数数量，等于DW卷积的参数数量**Mx1xDkxDk**和PW卷积的参数数量**NxMx1x1**之和**Mx1xDkxDk+NxMx1x1**。</p><p>所以，深度可分离卷积的参数数量比传统卷积的参数数量就是：</p><p>(Mx1xDkxDk+NxMx1x1)/(NxMxDkxDk)=**1/N+1/(DkxDk)**，和2.1节计算量分析一致，进一步的分析也和2.1节一致。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 2.3 深度可分离卷积层内部分析</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2.1和2.2小节分析的都是深度可分离卷积和传统卷积之间的对比。这节对深度可分离卷积内部的DW卷积和PW卷积之间做做分析。</p><p>由于2.1和2.2小节已经分析地很清楚了，这里拿来数据直接用。</p><p>PW层计算量比上DW层计算量等于(NxDfxDfxM)/(DfxDfxMxDkxDk)=**N/(DkxDk)**，也就是说，如果Dk等于3时，在输出特征图的通道数N大于9的时候，PW层比DW层更耗计算资源。而这个N，一般确实都很大（比如Dk=3,M=512,N=512,Df=14），所以**在深度可分离卷积中，更多的计算(和参数数量)是发生在PW层**。</p><p>参数数量的分析，也是类似的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### 3, Caffe实现</p><p class=\"ztext-empty-paragraph\"><br/></p><p>深度可分离卷积里面的PW卷积，就是一个传统的kernalsize=1的卷积，只需把kernalsize参数设置为1就行了；里面的DW卷积，可以用分组卷积里面的[group参数](<a href=\"https://link.zhihu.com/?target=http%3A//caffe.berkeleyvision.org/tutorial/layers/convolution.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Convolution Layer</a>)设置为1来实现。需要注意的是，为了保证输入特征图和输出特征图的分辨率不变，我们在DW卷积的时候需要设置一下stride=1,pad=1(kernalsize=3时)。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>[DW层](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/shicai/MobileNet-Caffe\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">shicai/MobileNet-Caffe</a>)：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>&gt;layer {</p><p>&gt;  name: &#34;conv5_1/dw&#34;</p><p>&gt;  type: &#34;Convolution&#34;</p><p>&gt;  bottom: &#34;conv4_2/sep&#34;</p><p>&gt;  top: &#34;conv5_1/dw&#34;</p><p>&gt;  param {</p><p>&gt;    lr_mult: 1</p><p>&gt;    decay_mult: 1</p><p>&gt;  }</p><p>&gt;  convolution_param {</p><p>&gt;    num_output: 512</p><p>&gt;    bias_term: false</p><p>&gt;    **pad: 1**</p><p>&gt;    **kernel_size: 3**</p><p>&gt;    **group: 512**</p><p>&gt;    engine: CAFFE</p><p>&gt;    **stride: 1**</p><p>&gt;    weight_filler {</p><p>&gt;      type: &#34;msra&#34;</p><p>&gt;    }</p><p>&gt;  }</p><p>&gt;}</p><p class=\"ztext-empty-paragraph\"><br/></p><p>[PW层](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/shicai/MobileNet-Caffe\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">shicai/MobileNet-Caffe</a>)：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>&gt;layer {</p><p>&gt;  name: &#34;conv5_1/sep&#34;</p><p>&gt;  type: &#34;Convolution&#34;</p><p>&gt;  bottom: &#34;conv5_1/dw&#34;</p><p>&gt;  top: &#34;conv5_1/sep&#34;</p><p>&gt;  param {</p><p>&gt;    lr_mult: 1</p><p>&gt;    decay_mult: 1</p><p>&gt;  }</p><p>&gt;  convolution_param {</p><p>&gt;    num_output: 512</p><p>&gt;    bias_term: false</p><p>&gt;    **pad: 0**</p><p>&gt;    **kernel_size: 1**</p><p>&gt;    **stride: 1**</p><p>&gt;    weight_filler {</p><p>&gt;      type: &#34;msra&#34;</p><p>&gt;    }</p><p>&gt;  }</p><p>&gt;}</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>以上差不多就是深度可分离卷积的原理和计算量，参数数量的理论分析了，最后顺带加了用Caffe实现。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p>+ [MobileNet-V1论文](<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1704.04861\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a>)</p><p>+ [MobileNet-Caffe](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/shicai/MobileNet-Caffe\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">shicai/MobileNet-Caffe</a>)</p>", 
            "topic": [
                {
                    "tag": "卷积神经网络（CNN）", 
                    "tagLink": "https://api.zhihu.com/topics/20043586"
                }, 
                {
                    "tag": "卷积", 
                    "tagLink": "https://api.zhihu.com/topics/19678959"
                }, 
                {
                    "tag": "加速", 
                    "tagLink": "https://api.zhihu.com/topics/19609182"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/81682950", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 9, 
            "title": "优化余弦距离的计算", 
            "content": "<p><b>船长黑板报所有文章的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p># optimizing cosine distance searching in a million feature-set</p><p>## 引言</p><p>求解两个向量之间的距离(或者说相似度)，广泛应用于数据挖掘，图像处理和深度学习中。例如自然语言处理中，根据文章中关键字的出现频率来组成一个特征向量，两个文章的特征向量的距离来表示文章主题的相似度。很多人脸识别的模型，在网络的最后一层都是计算出输入人脸的一个固定维数的特征向量，然后根据两个人脸对应的两个特征向量之间的距离来做人脸验证和识别。在真实的应用场景中，文章底库和人脸底库保存的特征向量数量很大(十万百万级)，计算特征向量之间的距离函数就会被调用很多次，所以优化这个搜索过程和最佳距离的计算函数就显得尤为重要。这里，我以优化在100万底库的场景下，找出底库中与给定512维向量之间[余弦距离](<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Cosine_similarity\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">en.wikipedia.org/wiki/C</span><span class=\"invisible\">osine_similarity</span><span class=\"ellipsis\"></span></a>)最近的那个样本的索引值为例，试验一下在通用PC平台上到底能优化多少(这里为了演示方便，实验中数据很多次取平均的过程没有展示，请注意)。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 操作系统：Ubuntu 16.04 LTS</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 编译器：g++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609</p><p class=\"ztext-empty-paragraph\"><br/></p><p>  　　　　gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 硬件配置：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>  1. CPU Intel(R) Core(TM) i7-7700 CPU @ 3.60GHz</p><p class=\"ztext-empty-paragraph\"><br/></p><p>    2. Memory 8G DDR2</p><p>    3. GPU N/A</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Step 1,最基础的实现方式</p><p class=\"ztext-empty-paragraph\"><br/></p><p>最基础的实现方式也是最容易的方式，作为我们的起点，实验代码：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>main函数和时间测量代码，main函数首先构造一些模拟数据，然后调用SearchBest做搜索，这里我们主要关心的就是SearchBest函数的耗时</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-295de8cb143095ccc1f4330b51d54854_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1057\" data-rawheight=\"967\" class=\"origin_image zh-lightbox-thumb\" width=\"1057\" data-original=\"https://pic1.zhimg.com/v2-295de8cb143095ccc1f4330b51d54854_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1057&#39; height=&#39;967&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1057\" data-rawheight=\"967\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1057\" data-original=\"https://pic1.zhimg.com/v2-295de8cb143095ccc1f4330b51d54854_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-295de8cb143095ccc1f4330b51d54854_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>SearchBest函数，这个函数调用Cosine_similarity计算余弦相似度，返回最接近的人脸的索引</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-92f789df79efadc85dd6c4cc8225573a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1093\" data-rawheight=\"809\" class=\"origin_image zh-lightbox-thumb\" width=\"1093\" data-original=\"https://pic3.zhimg.com/v2-92f789df79efadc85dd6c4cc8225573a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1093&#39; height=&#39;809&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1093\" data-rawheight=\"809\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1093\" data-original=\"https://pic3.zhimg.com/v2-92f789df79efadc85dd6c4cc8225573a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-92f789df79efadc85dd6c4cc8225573a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>Cosine_similarity函数，计算两个一维向量之间的余弦相似度</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-84b281d3496b46d719af0c511a30bcbb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"989\" data-rawheight=\"674\" class=\"origin_image zh-lightbox-thumb\" width=\"989\" data-original=\"https://pic4.zhimg.com/v2-84b281d3496b46d719af0c511a30bcbb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;989&#39; height=&#39;674&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"989\" data-rawheight=\"674\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"989\" data-original=\"https://pic4.zhimg.com/v2-84b281d3496b46d719af0c511a30bcbb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-84b281d3496b46d719af0c511a30bcbb_b.jpg\"/></figure><p>这里为了数据类型切换方便，我把SearchBest和Cosine_similarity函数写成了模板的形式。</p><p>测试结果：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f841e0192e49368eeaa33ff41787a628_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"699\" data-rawheight=\"198\" class=\"origin_image zh-lightbox-thumb\" width=\"699\" data-original=\"https://pic1.zhimg.com/v2-f841e0192e49368eeaa33ff41787a628_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;699&#39; height=&#39;198&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"699\" data-rawheight=\"198\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"699\" data-original=\"https://pic1.zhimg.com/v2-f841e0192e49368eeaa33ff41787a628_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f841e0192e49368eeaa33ff41787a628_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>耗用了**3855587us**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Step 2, 打开优化开关-O3</p><p class=\"ztext-empty-paragraph\"><br/></p><p>测试结果：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b7a8b16690dc97f0c31f5e416aebeff3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"704\" data-rawheight=\"233\" class=\"origin_image zh-lightbox-thumb\" width=\"704\" data-original=\"https://pic4.zhimg.com/v2-b7a8b16690dc97f0c31f5e416aebeff3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;704&#39; height=&#39;233&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"704\" data-rawheight=\"233\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"704\" data-original=\"https://pic4.zhimg.com/v2-b7a8b16690dc97f0c31f5e416aebeff3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b7a8b16690dc97f0c31f5e416aebeff3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>耗时**793150us**，仅仅加了一个-O3的编译选项，什么代码也没改，就加速了**x4.86**倍。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这里明显很多乘除开方之类的数学操作，如果再开激进一点的优化呢？</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Step 3，激进优化数学函数-Ofast -ffast-math(会牺牲精度)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>测试结果：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7979f9a70e03de1996f43fb6123432dd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"928\" data-rawheight=\"177\" class=\"origin_image zh-lightbox-thumb\" width=\"928\" data-original=\"https://pic2.zhimg.com/v2-7979f9a70e03de1996f43fb6123432dd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;928&#39; height=&#39;177&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"928\" data-rawheight=\"177\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"928\" data-original=\"https://pic2.zhimg.com/v2-7979f9a70e03de1996f43fb6123432dd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7979f9a70e03de1996f43fb6123432dd_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>耗用了**433704us**，差不多又优化了加速了**x1.82倍**。不过，这里要注意一下，**最佳人脸的index改变了**，这是由于(1)我们在计算的时候都是用的浮点数数据类型，计算结果很精确，在开了-Ofast -ffast-math优化之后，牺牲了计算精度，来换取了计算速度。(2)我们的模拟数据也比较特殊，底库中有很多的特征向量，之间的余弦距离其实很近很密集，这也放大了精度损失带来的影响。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>小结一下，由上面Step2和Step3的结果看来，开一下编译器的优化选项，就可以很快地收获到速度上的快速提升(差不多x10倍)，但是也要注意，对计算精度的影响会不会影响业务，有没有规避方式(例如对Feature做特别的预处理让他们变得稀疏)。</p><p>下面我们继续</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Step 4, 数据类型优化，double-&gt;float(会牺牲精度)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>C/C++里面浮点型有两种float和double，float占用字节小，计算速度快，我们把double改成float试试看，修改数据类型：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-005ee26790d7d3fae29c5271753e9db3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"692\" data-rawheight=\"159\" class=\"origin_image zh-lightbox-thumb\" width=\"692\" data-original=\"https://pic4.zhimg.com/v2-005ee26790d7d3fae29c5271753e9db3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;692&#39; height=&#39;159&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"692\" data-rawheight=\"159\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"692\" data-original=\"https://pic4.zhimg.com/v2-005ee26790d7d3fae29c5271753e9db3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-005ee26790d7d3fae29c5271753e9db3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>测试结果：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-12bef1eccc13cf5da452986d8357e7a8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"928\" data-rawheight=\"183\" class=\"origin_image zh-lightbox-thumb\" width=\"928\" data-original=\"https://pic1.zhimg.com/v2-12bef1eccc13cf5da452986d8357e7a8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;928&#39; height=&#39;183&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"928\" data-rawheight=\"183\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"928\" data-original=\"https://pic1.zhimg.com/v2-12bef1eccc13cf5da452986d8357e7a8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-12bef1eccc13cf5da452986d8357e7a8_b.jpg\"/></figure><p>耗用了**201012us**，加速**x2.16倍**(可能是在我的电脑上，一个double是8字节，一个float是4字节，所以取数据时一个cache line填满刚好可以取2x的float数据)。注意，这里index有改变化，讨论和分析见Step2。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Step 5, OpenMP(会牺牲精度)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>OpenMP是一种支持多平台共享存储器多处理器的C/C++编程规范和API，使用简单，只需要在需要做并行的地方加上编译制导语句，然后GCC的编译命令中加上-fopenmp选项即可。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>现在的CPU都是多核心的了，我们试试：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-115aaf7021218303acda0e2ac9044091_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1096\" data-rawheight=\"311\" class=\"origin_image zh-lightbox-thumb\" width=\"1096\" data-original=\"https://pic2.zhimg.com/v2-115aaf7021218303acda0e2ac9044091_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1096&#39; height=&#39;311&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1096\" data-rawheight=\"311\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1096\" data-original=\"https://pic2.zhimg.com/v2-115aaf7021218303acda0e2ac9044091_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-115aaf7021218303acda0e2ac9044091_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>测试结果：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-568851f31ee0164d8addf52eaf9bb13c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1058\" data-rawheight=\"177\" class=\"origin_image zh-lightbox-thumb\" width=\"1058\" data-original=\"https://pic1.zhimg.com/v2-568851f31ee0164d8addf52eaf9bb13c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1058&#39; height=&#39;177&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1058\" data-rawheight=\"177\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1058\" data-original=\"https://pic1.zhimg.com/v2-568851f31ee0164d8addf52eaf9bb13c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-568851f31ee0164d8addf52eaf9bb13c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>耗时**123842us**，加速了**1.62倍**，还不错。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Step 6, 循环展开</p><p>因为处理器有多个ALU，如果循环展开能充分利用这些ALU的话，速度应该还有提升。</p><p>但是，实验结果显示在循环展开后没有怎么变化，这里就不贴图了。原因的话我觉得可能是在OpenMP开了多线程之后，就已经用满了所有的计算单元吧。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Step 7, SIMD</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在向量运算中，作用在向量元素上的操作都是一样的，只是数据不一样，这样就很适合用处理器的向量指令集来做加速了。这里我选的是AVX(CPU支持的指令集可以用lscpu或者cat /proc/cpuinfo命令来查看)，AVX指令每次最多可以操作256位的数据，也就是8个32位的浮点数，最理想的情况下，代码可以加速x8倍。代码如下：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>辅助函数</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-17bac466d67292bcfb97017dc4b6d41d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"757\" data-rawheight=\"494\" class=\"origin_image zh-lightbox-thumb\" width=\"757\" data-original=\"https://pic2.zhimg.com/v2-17bac466d67292bcfb97017dc4b6d41d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;757&#39; height=&#39;494&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"757\" data-rawheight=\"494\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"757\" data-original=\"https://pic2.zhimg.com/v2-17bac466d67292bcfb97017dc4b6d41d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-17bac466d67292bcfb97017dc4b6d41d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>计算相似度的函数</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5894abf003b64c52582dd79f92ad9061_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1252\" data-rawheight=\"752\" class=\"origin_image zh-lightbox-thumb\" width=\"1252\" data-original=\"https://pic2.zhimg.com/v2-5894abf003b64c52582dd79f92ad9061_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1252&#39; height=&#39;752&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1252\" data-rawheight=\"752\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1252\" data-original=\"https://pic2.zhimg.com/v2-5894abf003b64c52582dd79f92ad9061_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5894abf003b64c52582dd79f92ad9061_b.jpg\"/></figure><p>另外，因为SIMD指令需要内存对齐，所以在main函数里面申请内存的时候也要申请对齐了的内存。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>测试结果：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5dd6919a2cc908888089a8512b27709a_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1138\" data-rawheight=\"162\" class=\"origin_image zh-lightbox-thumb\" width=\"1138\" data-original=\"https://pic3.zhimg.com/v2-5dd6919a2cc908888089a8512b27709a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1138&#39; height=&#39;162&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1138\" data-rawheight=\"162\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1138\" data-original=\"https://pic3.zhimg.com/v2-5dd6919a2cc908888089a8512b27709a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5dd6919a2cc908888089a8512b27709a_b.png\"/></figure><p>搜索耗时122652us，和前面优化好像不是很明显。原因可能有(1)可能前面Step2,Step3,Step4已经做了向量化了;(2)AVX优势在提高CPU的计算资源的利用率上，但是也受限于内存带宽，如果内存带宽不够，**在数据量大的情况下**，很多CPU的时钟周期都浪费在等待数据从内存load进寄存器上浪费了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Step 8, 优化开方函数sqrt()</p><p class=\"ztext-empty-paragraph\"><br/></p><p>一般来说，标准库提供的函数都是由高级的程序员打磨到很极致了的，但是，限于可移植性和跨平台要求，可能还有针对某些平台，某些应用优化的余地。试一下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4c885533e9116c0d4a4688228a02a7c0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"527\" data-rawheight=\"231\" class=\"origin_image zh-lightbox-thumb\" width=\"527\" data-original=\"https://pic1.zhimg.com/v2-4c885533e9116c0d4a4688228a02a7c0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;527&#39; height=&#39;231&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"527\" data-rawheight=\"231\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"527\" data-original=\"https://pic1.zhimg.com/v2-4c885533e9116c0d4a4688228a02a7c0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-4c885533e9116c0d4a4688228a02a7c0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图是QUAKE-III源代码里面计算平方根的倒数的快速计算方法，更进一步的讨论请看[Chris Lomont论文](<a href=\"https://link.zhihu.com/?target=http%3A//www.matrix67.com/data/InvSqrt.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">matrix67.com/data/InvSq</span><span class=\"invisible\">rt.pdf</span><span class=\"ellipsis\"></span></a>)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>测试结果：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-362bf7b3dc3e4e71f0af8b18798623d7_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1116\" data-rawheight=\"149\" class=\"origin_image zh-lightbox-thumb\" width=\"1116\" data-original=\"https://pic4.zhimg.com/v2-362bf7b3dc3e4e71f0af8b18798623d7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1116&#39; height=&#39;149&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1116\" data-rawheight=\"149\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1116\" data-original=\"https://pic4.zhimg.com/v2-362bf7b3dc3e4e71f0af8b18798623d7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-362bf7b3dc3e4e71f0af8b18798623d7_b.png\"/></figure><p>搜索耗时123637us，也不是很明显，一个原因是计算sqrt()这个函数不是热点，它的调用次数和for(int i = 0; i &lt; step; i++){...}这个循环里面的\\_mm256_fmadd_ps函数调用次数来说不算很多，第二个原因可能数学库里面已经做了很好的优化了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Step 9，预计算</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这是个取巧的办法，现在在特征数据库中的数据存的是100万裸的Feature向量，而在计算余弦距离的时候，每个特征向量都要计算它们的模(长度)，我们可以事前将这100万向量和它们的模一起保存(新入库的特征也一样，加上模一起入库保存)，增加了1/512的(如果加上数据库里面别的字段，这个数字还要稍微小一点)存储空间，但是这样理论上就可以减少1000,000x512次乘加和100,000次开方计算量了。很多查表法也是类似的套路，将一些复杂但是可预测的计算结果预计算，预存储，空间换时间。</p><p>实验如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6e3f8aa2c751c06cfcafe28d059e57f5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1144\" data-rawheight=\"358\" class=\"origin_image zh-lightbox-thumb\" width=\"1144\" data-original=\"https://pic2.zhimg.com/v2-6e3f8aa2c751c06cfcafe28d059e57f5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1144&#39; height=&#39;358&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1144\" data-rawheight=\"358\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1144\" data-original=\"https://pic2.zhimg.com/v2-6e3f8aa2c751c06cfcafe28d059e57f5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6e3f8aa2c751c06cfcafe28d059e57f5_b.jpg\"/></figure><p>测试结果：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-50d262c7da3913507849e56d2a77696d_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1131\" data-rawheight=\"146\" class=\"origin_image zh-lightbox-thumb\" width=\"1131\" data-original=\"https://pic2.zhimg.com/v2-50d262c7da3913507849e56d2a77696d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1131&#39; height=&#39;146&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1131\" data-rawheight=\"146\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1131\" data-original=\"https://pic2.zhimg.com/v2-50d262c7da3913507849e56d2a77696d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-50d262c7da3913507849e56d2a77696d_b.png\"/></figure><p>速度121683us，变化不大，原因参见Step 7第二条。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Step 10，数据模归一化</p><p>余弦距离，只跟两个向量之间的夹角有关，和向量的模(长度)无关。如果我们在存特征的时候，就把特征向量做了模归一化，对新的特征向量也做同样的处理，那么在计算相似度的函数Cosine_similarity_avx里面就无需再计算向量的模了，这样在乘加数上算，理论上可以减少2/3的计算量，加速x3倍。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>代码修改处如下，</p><p class=\"ztext-empty-paragraph\"><br/></p><p>首先模归一化为单位长度1：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ca73a748c3f64fbbe7768634bbe998ab_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"968\" data-rawheight=\"928\" class=\"origin_image zh-lightbox-thumb\" width=\"968\" data-original=\"https://pic4.zhimg.com/v2-ca73a748c3f64fbbe7768634bbe998ab_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;968&#39; height=&#39;928&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"968\" data-rawheight=\"928\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"968\" data-original=\"https://pic4.zhimg.com/v2-ca73a748c3f64fbbe7768634bbe998ab_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ca73a748c3f64fbbe7768634bbe998ab_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>然后把Cosine_similarity函数里面计算两向量模的代码注释掉：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fd1b1c788f23861cc5d34492b5265e8e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1121\" data-rawheight=\"648\" class=\"origin_image zh-lightbox-thumb\" width=\"1121\" data-original=\"https://pic3.zhimg.com/v2-fd1b1c788f23861cc5d34492b5265e8e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1121&#39; height=&#39;648&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1121\" data-rawheight=\"648\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1121\" data-original=\"https://pic3.zhimg.com/v2-fd1b1c788f23861cc5d34492b5265e8e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fd1b1c788f23861cc5d34492b5265e8e_b.jpg\"/></figure><p>测试一下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-460ba59debdd87d318aaab6670a9e5b4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"514\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb\" width=\"514\" data-original=\"https://pic1.zhimg.com/v2-460ba59debdd87d318aaab6670a9e5b4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;514&#39; height=&#39;168&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"514\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"514\" data-original=\"https://pic1.zhimg.com/v2-460ba59debdd87d318aaab6670a9e5b4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-460ba59debdd87d318aaab6670a9e5b4_b.jpg\"/></figure><p>耗时123436us，和前面的差别不大，没有达到预期的x3倍加速。感觉还是因为内存带宽的问题，从内存搬运数据到cache的速度，远小于CPU把cache上数据计算完的速度，所以CPU的计算能力已经冗余了。在我们的优化里面，只减少了CPU的计算量，但是内存访问没有减少（因为还有计算mult_add这一步，所以在内存访问vectorA[i]和vectorB[i]一点都没有少），所以，速度也没用什么提升。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Step 11，特征选择</p><p>特征向量里面有512维的特征，但是这512维特征的重要性不一定都是一样的，如果我们能有选择性地抽取里面信息量最大的特征，减少特征向量的维数，那么我们就减少了计算量，**更重要的是，加大了内存访问数据量**，增加了CPU的利用率。[PCA](<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Principal_component_analysis\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">en.wikipedia.org/wiki/P</span><span class=\"invisible\">rincipal_component_analysis</span><span class=\"ellipsis\"></span></a>)就是一种在减少数据集的维度的同时，保持对方差贡献最大(信息量保持最多)的特征的特征选择压缩算法。</p><p>因为我们在这里不是探讨PCA算法的过程，我们就直接假设512维的特征向量压缩到了256维。</p><p>改动如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e93a4e23f16c2f22160c1fb4b45a530b_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1114\" data-rawheight=\"133\" class=\"origin_image zh-lightbox-thumb\" width=\"1114\" data-original=\"https://pic4.zhimg.com/v2-e93a4e23f16c2f22160c1fb4b45a530b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1114&#39; height=&#39;133&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1114\" data-rawheight=\"133\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1114\" data-original=\"https://pic4.zhimg.com/v2-e93a4e23f16c2f22160c1fb4b45a530b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e93a4e23f16c2f22160c1fb4b45a530b_b.png\"/></figure><p>测试结果：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1d765f550b87d79c082d83f4bafdba11_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"493\" data-rawheight=\"163\" class=\"origin_image zh-lightbox-thumb\" width=\"493\" data-original=\"https://pic2.zhimg.com/v2-1d765f550b87d79c082d83f4bafdba11_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;493&#39; height=&#39;163&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"493\" data-rawheight=\"163\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"493\" data-original=\"https://pic2.zhimg.com/v2-1d765f550b87d79c082d83f4bafdba11_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1d765f550b87d79c082d83f4bafdba11_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>速度**63201us**，加速大约**x2倍**。乘加数减少2倍，内存效率提升2倍，加速2倍符合预期。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Step 12，OpenBLAS</p><p>OpenBLAS是BLAS的跨平台开源项目，可以加速各种矩阵和线性代数的计算，广泛应用在机器学习应用和框架中，Caffe里面用的BLAS库就是OpenBLAS。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这里我们用OpenBLAS的矩阵与向量相乘的接口cblas_sgemv来做相似度计算。输入矩阵是底库中所有特征向量的集合，向量是需要搜索的那一个特征向量。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>实验代码如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ba3c075744e0e4c32a1731e7753f4042_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1506\" data-rawheight=\"775\" class=\"origin_image zh-lightbox-thumb\" width=\"1506\" data-original=\"https://pic3.zhimg.com/v2-ba3c075744e0e4c32a1731e7753f4042_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1506&#39; height=&#39;775&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1506\" data-rawheight=\"775\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1506\" data-original=\"https://pic3.zhimg.com/v2-ba3c075744e0e4c32a1731e7753f4042_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ba3c075744e0e4c32a1731e7753f4042_b.jpg\"/></figure><p>实验过程结果如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3c64aa062734270b55888e891504dd89_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"943\" data-rawheight=\"223\" class=\"origin_image zh-lightbox-thumb\" width=\"943\" data-original=\"https://pic2.zhimg.com/v2-3c64aa062734270b55888e891504dd89_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;943&#39; height=&#39;223&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"943\" data-rawheight=\"223\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"943\" data-original=\"https://pic2.zhimg.com/v2-3c64aa062734270b55888e891504dd89_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3c64aa062734270b55888e891504dd89_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>耗时63865us，改变不是很大。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Step 13，浮点转定点</p><p>在很多通用CPU上，特别是计算能力有限的嵌入式设备中，浮点数的运算要比定点数的计算慢。如果我们把float型的已经归一化到[0.0,1.0]了的浮点型特征值(因为余弦距离只与向量夹角有关，与模还有分量的缩放取值范围无关，所以很容易做归一化)，转换为[0,65536]的unsigned short定点类型，看看能不能有提升。</p><p>先做一点理论分析：</p><p>（1）在我的电脑上,sizeof(float)等于4，sizeof(short)等于2，如果用unsigned short代替float，内存读取效率可以提升2倍。</p><p>（2）unsigned short替换float误差分析</p><p>问题描述：两个N维浮点向量X,Y，已经归一化到模为1，即![](images/Selection_216.png)，其中![](images/Selection_217.png)，归一化后的余弦距离![](images/Selection_218.png)</p><p>假设转定点数时把[0.0, 1.0] 映射到 [0, M], 则分量最大损失为1/M，考虑到四舍五入则为0.5/M。</p><p>那么量化导入的误差为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-395656607e55442010c17a4fffb64a67_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"506\" data-rawheight=\"50\" class=\"origin_image zh-lightbox-thumb\" width=\"506\" data-original=\"https://pic4.zhimg.com/v2-395656607e55442010c17a4fffb64a67_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;506&#39; height=&#39;50&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"506\" data-rawheight=\"50\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"506\" data-original=\"https://pic4.zhimg.com/v2-395656607e55442010c17a4fffb64a67_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-395656607e55442010c17a4fffb64a67_b.png\"/></figure><p>，其中</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-92326543629569996c0ba618a0608ba6_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"145\" data-rawheight=\"34\" class=\"content_image\" width=\"145\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;145&#39; height=&#39;34&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"145\" data-rawheight=\"34\" class=\"content_image lazy\" width=\"145\" data-actualsrc=\"https://pic3.zhimg.com/v2-92326543629569996c0ba618a0608ba6_b.png\"/></figure><p>，带入则有</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-715fd3866031e6a99d74f1c0dce116fd_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"204\" data-rawheight=\"31\" class=\"content_image\" width=\"204\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;204&#39; height=&#39;31&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"204\" data-rawheight=\"31\" class=\"content_image lazy\" width=\"204\" data-actualsrc=\"https://pic2.zhimg.com/v2-715fd3866031e6a99d74f1c0dce116fd_b.png\"/></figure><p>，两遍加上取绝对值</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-fa61cc4b2208d1e6bcf6c8912a2009b7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"264\" data-rawheight=\"211\" class=\"content_image\" width=\"264\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;264&#39; height=&#39;211&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"264\" data-rawheight=\"211\" class=\"content_image lazy\" width=\"264\" data-actualsrc=\"https://pic4.zhimg.com/v2-fa61cc4b2208d1e6bcf6c8912a2009b7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>N是特征向量的维数，N=256，M是unsigned short可以表示的最大整形值，M=65536，带进去可以得知相似度的量化误差在万分之三以内。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>实际改动：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c67200049bbeedc3eff7949675fb3456_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"503\" data-rawheight=\"82\" class=\"origin_image zh-lightbox-thumb\" width=\"503\" data-original=\"https://pic3.zhimg.com/v2-c67200049bbeedc3eff7949675fb3456_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;503&#39; height=&#39;82&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"503\" data-rawheight=\"82\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"503\" data-original=\"https://pic3.zhimg.com/v2-c67200049bbeedc3eff7949675fb3456_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c67200049bbeedc3eff7949675fb3456_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>测试结果：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6d23aeb95f09f7aed64f85e3f05c625c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"652\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"652\" data-original=\"https://pic1.zhimg.com/v2-6d23aeb95f09f7aed64f85e3f05c625c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;652&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"652\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"652\" data-original=\"https://pic1.zhimg.com/v2-6d23aeb95f09f7aed64f85e3f05c625c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6d23aeb95f09f7aed64f85e3f05c625c_b.jpg\"/></figure><p>运行时间**31298us**，加速**x2.02倍**。主要原因在于乘加数没有增加，但是**内存效率提升了2倍**。如果换成4字节的unsigned int，实验证明速度没有什么提升。所以这里的定点化对速度提升的主要收益在于内存效率提升，而不在CPU的定点计算能力比浮点计算能力强。可能也是因为我是用的是PC电脑的通用CPU，浮点计算能力丝毫不逊色与定点计算，也许在嵌入式设备上会不一样。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>注意，上面的分析基于特征值没有归一化到[0,1]区间，模没有归一化到1，而且量化误差对相似度影响不大的基础上的，另外，也需要注意溢出问题。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 杂项</p><p class=\"ztext-empty-paragraph\"><br/></p><p>以下是几点心得：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ **不要抱怨**你的硬件不好，考虑一下你自己有没有优化好你的程序是不是充分优化了，是不是充分利用了所有可用的计算资源(CPU,GPU,Memory)，是不是把石头里面最后一滴油压出来了;</p><p>+ **优化无定法**，一定要根据具体的问题来做来实验。要在优化前确定优化的目标和约束，优化是一个**性能**（程序运行时间），**资源**（CPU、GPU、内存占用，甚至手机电池电量），代码可维护性和跨平台与**成本**（开发测试时间、人力投入）多者的平衡，在开始动手前最好能对能牺牲什么，一定不能牺牲什么，优化的目标，有个基本的想法，不打无准备的仗;</p><p>+ **算法优化要在代码优化前面**，算法才是程序的核心灵魂，一辆汽车发动机不行，外观再流线型设计也跑不快，牛顿下降比梯度下降快，二分查找比线性搜索快，做代码优化前，先考虑一下算法是不是不能优化了？</p><p>+ 优化要先**找到热点**，根据[28法则](<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/28%25E6%25B3%2595%25E5%2588%2599/4524352\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">28法则_百度百科</a>)，80%的时间都是被20%的代码消耗的，所以，我们应该放80%的注意力去重点优化那20%的代码。根据[阿姆达尔定律](<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E9%2598%25BF%25E5%25A7%2586%25E8%25BE%25BE%25E5%25B0%2594%25E5%25AE%259A%25E5%25BE%258B/10386960\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">阿姆达尔定律_百度百科</a>)，最终你程序的加速比，取决于你对关键热点的优化程度。特别是大的程序，找到程序中最耗时(或耗资源)的地方，下大力气挖潜力;放过非热点部分的优化;要不然就本末倒置，重拳击在棉花上，累死也无功;热点是变动的，一个热点优化了，可能原先排名第二的热点就成了现在排第一的了;</p><p>+ 优化过程要先易后难，**先吃肉后啃骨头**，原因有三：(1)一般水平的程序员写的代码和算法或者学究训练出来的模型，都有很大的优化空间，往往开个编译器的-O3，改改数据类型，就能取得不错的效果。(2)从人的心理学上讲，先吃几块肉，看到点优化的效果，也能给自己一点啃骨头的信心和动力。(3)有的优化是重复的，烂程序各有各的烂法，但是优秀的优化，手段都是类似的，开-O3和手动写AVX代码，仔细调整流水线调整memory cache，效果可能是一样的，但是-O3成本多低。先开编译器优化选项，后做缓存优化和汇编，肯定没错的;</p><p>+ 啃骨头的时候，要善于**借助工具**来指导优化，各个平台都有各自profiling的工具，也有各自的小实用程序(例如htop, gprof)或者自己开发的收集的小工具，找自己顺手的用，要依赖于自己的经验，分析，甚至直觉，但是要更相信工具和数字，数字不会撒谎;</p><p>+ 在优化的过程中要试，要记下各个优化点和优化的时间大小，还要**仔细检查优化没有影响输入和输出**，分析输入输出不一样的原因，最好做个表格，万一修改没有取得好的效果，及时回滚;优化好了也要思考背后的道理，不能知其然不知其所以然，及时总结经验;</p><p>+ 如果优化手段比较trick，**做好记录**和代码注释;</p><p>+ **优化不必太早**，在开发阶段，更重要的是代码的规范性，结果的正确性和开发进度;高神有言:</p><p>&gt; 程序员浪费了太多的时间去思考和担忧程序中那些**非关键部位**的速度，而且考虑到调试和维护，这些为优化而进行的修改实际上是有很大负面影响的。我们应当忘记小的性能改善，97%的情况下，**过早地优化都是万恶之源**。</p><p>&gt;                                                                                                                                                                ——高德纳</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 优化也是有成本的，例如人力投入，资源占用等，而且优化往往会使代码难懂难维护，还可能引入新的bug，使代码产生对平台的依赖，**不到万不得已，能不优化就不要优化**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文只是抛砖引玉，初步涉及了优化汪洋大海中的一点点。光优化的层次来分就有系统级的，应用级，代码级别的，优化还要区分是网络瓶颈，IO瓶颈，内存瓶颈还是CPU瓶颈，本文的这个小demo只涉及到代码级别的单个通用CPU和内存优化，还没有涉及到用上GPU(十分适合做大规模并行计算)，负责均衡。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>|          优化步骤          |       时间(us)        | 加速倍数(与上一步相比) | 总加速比(和基准线比) |   备注   |</p><p>| :------------------------: | :-------------------: | :--------------------: | :------------------: | :------: |</p><p>|       Step 1(基准线)       |        3855587        |          x1.0          |         x1.0         |  基准线  |</p><p>|        Step 2(-O3)         |        793150         |         x4.86          |        x4.86         |          |</p><p>| Step 3(-Ofast -ffast-math) |        433704         |         x1.82          |        x8.90         |          |</p><p>|   Step 4(double-&gt;float)    |        201012         |         x2.16          |        x19.18        |          |</p><p>|       Step 5(OpenMP)       |        123842         |         x1.62          |        x31.13        |          |</p><p>|      Step 6(循环展开)      | N/A(与第五步相差不大) |         ~x1.0          |       ~x31.13        |          |</p><p>|        Step 7(SIMD)        |        122652         |         ~x1.0          |       ~x31.13        |          |</p><p>|     Step 8(优化sqrt())     |        123637         |         ~x1.0          |       ~x31.13        |          |</p><p>|       Step 9(预计算)       |        121683         |         ~x1.0          |       ~x31.13        |          |</p><p>|   Step 10(数据模归一化)    |        123436         |         ~x1.0          |       ~x31.13        |          |</p><p>|     Step 11(特征选择)      |         63201         |         ~x1.96         |        x61.01        |          |</p><p>|     Step 12(OpenBLAS)      |         63865         |         ~x1.0          |       ~x61.01        |          |</p><p>|    Step 13(浮点转定点)     |         31298         |         ~x2.02         |     **x123.19**      | 内存效率 |</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p>+ [《深入理解计算机系统》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/1896753/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深入理解计算机系统 (豆瓣)</a>)</p><p>+ [《编程珠玑》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/3227098/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">编程珠玑 (豆瓣)</a>)</p><p>+ [《代码大全》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/1477390/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">代码大全（第2版） (豆瓣)</a>)</p><p>+ [《让你的软件飞起来》](<a href=\"https://link.zhihu.com/?target=https%3A//wenku.baidu.com/view/1fb647bdf121dd36a32d8290.html%3Fsxts%3D1550723886017\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">让你的软件飞起来 - 图文 - 百度文库</a>)</p><p>+ [《并行编程方法优化与实践》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/26600702/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">并行编程方法与优化实践 (豆瓣)</a>)</p><p>+ [《并行算法设计与性能优化》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/26413096/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">并行算法设计与性能优化 (豆瓣)</a>)</p><p>+ [《Linux性能优化》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/27051000/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">book.douban.com/subject</span><span class=\"invisible\">/27051000/</span><span class=\"ellipsis\"></span></a>)</p><p>+ [《C++性能优化指南》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/27666339/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">C++性能优化指南 (豆瓣)</a>)</p><p>+ [《英特尔C++编译器开发者参考手册》](<a href=\"https://link.zhihu.com/?target=https%3A//software.intel.com/en-us/cpp-compiler-developer-guide-and-reference\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Intel® C++ Compiler 19.0 Developer Guide and Reference</a>)</p><p>+ [ what-does-gccs-ffast-math-actually-do](<a href=\"https://link.zhihu.com/?target=https%3A//stackoverflow.com/questions/7420665/what-does-gccs-ffast-math-actually-do\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">What does gcc&#39;s ffast-math actually do?</a>)</p><p>+ [ memalign](<a href=\"https://link.zhihu.com/?target=https%3A//linux.die.net/man/3/memalign\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">memalign(3): allocate aligned memory</a>)</p><p>+ [ FAST INVERSE SQUARE ROOT](<a href=\"https://link.zhihu.com/?target=http%3A//www.matrix67.com/data/InvSqrt.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">matrix67.com/data/InvSq</span><span class=\"invisible\">rt.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ [左庆误差分析](<a href=\"https://zhuanlan.zhihu.com/p/35904005\" class=\"internal\">左庆：人脸特征向量用整数存储精度损失多少？</a>)</p><p>+ [Why GEMM is at the heart of deep learning](<a href=\"https://link.zhihu.com/?target=https%3A//petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">petewarden.com/2015/04/</span><span class=\"invisible\">20/why-gemm-is-at-the-heart-of-deep-learning/</span><span class=\"ellipsis\"></span></a>)</p><p>+ [OpenBLAS gemm 入门和分析](<a href=\"https://link.zhihu.com/?target=https%3A//www.jianshu.com/p/26f24f464016\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">抱歉，你访问的页面不存在。 - 简书</a>)</p><p>+ [浮点峰值那些事儿](<a href=\"https://zhuanlan.zhihu.com/p/28226956\" class=\"internal\">高洋：浮点峰值那些事儿</a>)</p><p>+ [探求计算性能的极限](<a href=\"https://link.zhihu.com/?target=https%3A//wenku.baidu.com/view/788ba23dfc4ffe473268ab07.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">探求计算性能极限 - 图文 - 百度文库</a>)</p><p>+ [OpenBLAS项目与矩阵乘法优化 | AI 研习社](<a href=\"https://link.zhihu.com/?target=https%3A//www.leiphone.com/news/201704/Puevv3ZWxn0heoEv.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">OpenBLAS项目与矩阵乘法优化 | AI 研习社</a>)</p>", 
            "topic": [
                {
                    "tag": "性能优化", 
                    "tagLink": "https://api.zhihu.com/topics/19633850"
                }, 
                {
                    "tag": "C++ 编程", 
                    "tagLink": "https://api.zhihu.com/topics/19836485"
                }, 
                {
                    "tag": "余弦相似度", 
                    "tagLink": "https://api.zhihu.com/topics/20687325"
                }
            ], 
            "comments": [
                {
                    "userName": "搬砖行者", 
                    "userLink": "https://www.zhihu.com/people/72b7ef6794cbc75448a92929f5dd2dab", 
                    "content": "好看不火可还行", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "船长", 
                            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
                            "content": "<p>哈哈，感谢认可，后面船长还会继续回馈社区</p><a class=\"comment_sticker\" href=\"https://pic4.zhimg.com/v2-ba306425d0a7aee2c7260381f1bf7b97.gif\" data-width=\"\" data-height=\"\">[欢呼]</a>", 
                            "likes": 0, 
                            "replyToAuthor": "搬砖行者"
                        }
                    ]
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>很好的实验，谢谢分享！另外关于您提到的“所以这里的定点化对速度提升的主要收益在于内存效率提升，而不在CPU的定点计算能力比浮点计算能力强”，我再多说一句，这个应该是expected的，浮点数运算的确需要的cycle的确是整数运算的几倍（尤其除法是最慢的），但是这个差距在实际中的确难以体现出来，因为当下的存储体系结构和乱序执行，bottleneck主要在于cache miss的时候访问内存，访问内存导致的stall比浮点运算可多多了</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "船长", 
                            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
                            "content": "<p>是的，一个cache missing消耗的时钟周期比浮点数相对于定点数计算多得时钟周期要多很多</p>", 
                            "likes": 0, 
                            "replyToAuthor": "知乎用户"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/81682358", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 0, 
            "title": "hack your printf", 
            "content": "<p><b>船长黑板报所有文章的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p>## 引言</p><p>在某些情况下，我们用到了第三方的库，比如一个lib3rd_lib.so文件，但是又不想在我们调用它的时候让库里面的打印信息显示出来，只想打印我们自己代码里面的信息，让我们的日志文件或者Terminal更加整洁；或者我们开发了自己的库发布出去，但是里面有打印信息（比如版权信息，作者联系方式等），一定要打印出来，不想让使用者抑制打印信息。该怎么办呢？这是矛和盾的问题，这里船长以前者（抑制第三方库的打印）为例，即抑制printf函数打印信息为例(其他的方式，比如std::cout，打印文件，可以举一反三)，列出几种方法和大家讨论一下。矛的缺点，就是盾的优势，搞懂了怎么抑制，那怎么反抑制就不难了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 目标：抑制第三方库的printf函数打印信息，不让它显示在Terminal</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 限制条件：1. 不可以影响第三方库除打印外的功能； 2. 不可以要求系统root权限；3. 不可以影响系统正常运行；４. 不可以影响我们自己代码里面的打印功能。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 操作系统：Ubuntu 18.04 LTS</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 编译器：g++ (Ubuntu 5.5.0-12ubuntu1) 5.5.0 20171010</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Case 1, 狸猫换太子——劫持printf</p><p class=\"ztext-empty-paragraph\"><br/></p><p>我们的计算机系统里面的软硬件是分**层次**的，最底下是计算机的硬件（显示器，键盘，内存条，显卡等），硬件上面是硬件驱动和操作系统内核（比如大名鼎鼎的Linux kernal），操作系统内核的上层是运行时库，运行时库上面就是我们的日常用到的Firefox浏览器，git，Vim文本编辑器这样的应用程序和gcc编译器，ld链接器这样的开发工具。底下的层通过API来为上层提供功能，比如显卡通过显卡驱动的API来为操作系统内核提供显示和并行计算的功能，操作系统内核通过系统调用API以软件中断方式（Linux 为0x80号中断）来为运行时库提供支持，而运行时库又以库API的方式(比如我们这里要抑制的printf)为应用程序提供支持。所以，从我们在应用程序的代码里面写下一行printf(&#34;Hello world!\\n&#34;)之后，它要经过一系列的**调用链**（运行时库-&gt;操作系统-&gt;显卡驱动）才能在显示器Terminal里面显示出来一行Hello world字符串。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>说到这里，就清楚了，只要我们能在这个调用链的任何一环劫持到用到的API，我们就能干扰到最上面的printf()函数，在里面做我们爱做的事情，比如，抑制第三方库的printf函数往Terminal打印。这里以劫持printf调用到的puts()函数来演示一下。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>实验代码：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>首先是第三方库的代码：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-96d6636a08c3e87d81747ce093b977d9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"422\" data-rawheight=\"270\" class=\"origin_image zh-lightbox-thumb\" width=\"422\" data-original=\"https://pic2.zhimg.com/v2-96d6636a08c3e87d81747ce093b977d9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;422&#39; height=&#39;270&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"422\" data-rawheight=\"270\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"422\" data-original=\"https://pic2.zhimg.com/v2-96d6636a08c3e87d81747ce093b977d9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-96d6636a08c3e87d81747ce093b977d9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-fc1e354977351370730cffce13549871_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"797\" data-rawheight=\"266\" class=\"origin_image zh-lightbox-thumb\" width=\"797\" data-original=\"https://pic2.zhimg.com/v2-fc1e354977351370730cffce13549871_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;797&#39; height=&#39;266&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"797\" data-rawheight=\"266\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"797\" data-original=\"https://pic2.zhimg.com/v2-fc1e354977351370730cffce13549871_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-fc1e354977351370730cffce13549871_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>第三方库很简单，就是在_3rd_lib_func()这个函数里面调了printf()打印了一句话。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>接着是main.cpp代码，在main函数里面调用了第三方库里面的_3rd_lib_func()，自己也调用了printf打印了一句话。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-239172834bb35c8f8b8ba49fc9ffeaeb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"621\" data-rawheight=\"348\" class=\"origin_image zh-lightbox-thumb\" width=\"621\" data-original=\"https://pic4.zhimg.com/v2-239172834bb35c8f8b8ba49fc9ffeaeb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;621&#39; height=&#39;348&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"621\" data-rawheight=\"348\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"621\" data-original=\"https://pic4.zhimg.com/v2-239172834bb35c8f8b8ba49fc9ffeaeb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-239172834bb35c8f8b8ba49fc9ffeaeb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>最后是我们最重要的，劫持了库里面的int puts ( const char * str )的函数：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0beb1a125042995210c71249c95320f1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"438\" data-rawheight=\"220\" class=\"origin_image zh-lightbox-thumb\" width=\"438\" data-original=\"https://pic2.zhimg.com/v2-0beb1a125042995210c71249c95320f1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;438&#39; height=&#39;220&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"438\" data-rawheight=\"220\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"438\" data-original=\"https://pic2.zhimg.com/v2-0beb1a125042995210c71249c95320f1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0beb1a125042995210c71249c95320f1_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-21b527ccba9ecf508b880572f4882d63_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"597\" data-rawheight=\"302\" class=\"origin_image zh-lightbox-thumb\" width=\"597\" data-original=\"https://pic4.zhimg.com/v2-21b527ccba9ecf508b880572f4882d63_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;597&#39; height=&#39;302&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"597\" data-rawheight=\"302\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"597\" data-original=\"https://pic4.zhimg.com/v2-21b527ccba9ecf508b880572f4882d63_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-21b527ccba9ecf508b880572f4882d63_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这个my_hack.cpp就是改写了系统默认的puts函数，而**puts函数是printf到系统内核调用链条中的一环**，所以，我们在自己写的puts函数里面，就可以为所欲为了。实验过程如下：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c7ff7a602b6aa7f8dac8a58df21b5cbb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"994\" data-rawheight=\"368\" class=\"origin_image zh-lightbox-thumb\" width=\"994\" data-original=\"https://pic4.zhimg.com/v2-c7ff7a602b6aa7f8dac8a58df21b5cbb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;994&#39; height=&#39;368&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"994\" data-rawheight=\"368\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"994\" data-original=\"https://pic4.zhimg.com/v2-c7ff7a602b6aa7f8dac8a58df21b5cbb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c7ff7a602b6aa7f8dac8a58df21b5cbb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>可以看到，在开始按照正常的过程编出来的a.out打印出来了3rd_lib.cpp里面的log，在第二个加入了my_hack.cpp编出来的a.out，打印的过程被hack了，打印出来了原本不应该出来的HA, hacked printf。这里，我们可以选择把my_hack.cpp文件的6,7,8三行注释掉，那么就让第三方库的printf哑巴了。然后，如果我们自己的main.cpp函数里面调用到的printf函数，该怎么办才能正常输出呢？方法很简单，在想正常输出的字符串里面加一个TAG，在my_hack.cpp的puts函数里面，检查一下有没有这个TAG关键字，如果没有，就让它哑巴掉，如果有，就用fputs打印出来。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这种方法有没有缺点呢？缺点其实很明显，它只能劫持特定范围（比它高级且依赖于它）的函数，如果lib3rd_lib.so里面不是用printf，而是用std::cout，或者直接用接近调用链底层的puts, fputs这样的函数，此法就失败了，除非你能找到调用链条最底层（精确点说，应该是比第三方库用的打印函数更底层）的那个函数，还好[Linux是开源的](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/1231584/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">book.douban.com/subject</span><span class=\"invisible\">/1231584/</span><span class=\"ellipsis\"></span></a>)，代码都有，感兴趣的可以试试看。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>那有没有更好更根本一点的解决办法呢？</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Case 2, 丢进垃圾桶——输出重定向</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Unix系统的设计哲学里面有一点很重要，**一切皆为文件**（不仅仅txt文档是文件，jpg图像是文件，目录、套节字也是文件，显示器也是文件）。在操作系统的眼里，printf往显示器打印信息，就是操作系统往stdout(默认绑定了显示器，是显示器在操作系统里面的抽象)这个文件里面写信息。多说一句，标准错误stderr这个文件也是默认绑定了显示器的，标准输入stdin这个文件绑定的是键盘。除了上面说的标准输入，标准输出，标准错误三个特别的文件之外，还有一个很特殊的文件[/dev/null](<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Devnull\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">en.wikipedia.org/wiki/D</span><span class=\"invisible\">evnull</span><span class=\"ellipsis\"></span></a>)，这个文件在Unix生态里面的行话也叫位桶(bit bucket)或者黑洞(black hole)，向这个黑洞文件输出的所有流，都会被它吞掉丢弃，就好像进了黑洞一样。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如果我们能在程序里把printf文件的输出对象（stdout）文件，重新定向到黑洞文件，就可以把printf的所有输出都吞掉。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>实验代码：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6615b6373d66243d5c337a1b4d6e70ff_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"770\" data-rawheight=\"562\" class=\"origin_image zh-lightbox-thumb\" width=\"770\" data-original=\"https://pic4.zhimg.com/v2-6615b6373d66243d5c337a1b4d6e70ff_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;770&#39; height=&#39;562&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"770\" data-rawheight=\"562\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"770\" data-original=\"https://pic4.zhimg.com/v2-6615b6373d66243d5c337a1b4d6e70ff_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6615b6373d66243d5c337a1b4d6e70ff_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上面的代码第13行，用freopen函数把/dev/null这个垃圾桶和标准输出stdout绑定了，所以，在绑定之后的语句里面，再往标准输出stdout输出，就什么也看不到了。不过，这时标准错误stderr还是和屏幕绑定的，往标准错误输出的字符串还是能正确现实。运行过程和结果如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3fdade29225c374c0d7230a8fc4367ce_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"704\" data-rawheight=\"215\" class=\"origin_image zh-lightbox-thumb\" width=\"704\" data-original=\"https://pic3.zhimg.com/v2-3fdade29225c374c0d7230a8fc4367ce_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;704&#39; height=&#39;215&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"704\" data-rawheight=\"215\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"704\" data-original=\"https://pic3.zhimg.com/v2-3fdade29225c374c0d7230a8fc4367ce_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-3fdade29225c374c0d7230a8fc4367ce_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上面代码还有个两个缺点，第一，如果在调用freopen函数之前的打印，那还是打印得出来;第二点，就是把我们自己的printf的输出也给吞了，这时，我们是可以用fprintf这个函数，来指定把我们的信息输出到stderr（前文讲了，stderr这个文件也是默认绑定显示器的），这样就绕开了标准输出重定向/dev/null的尴尬。或者我们可以在调用第三方库的之前，进行重定向，之后，再重定向回来。但是这样做还是不太优雅，显得很没有品味。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Case 3, 大力出奇迹——重写字符串</p><p class=\"ztext-empty-paragraph\"><br/></p><p>源文件里面的字符串常量，在编译后的二进制文件里面，是有专门存储字段来存储的，而且是**连续存储**的，而且是**明文**连续存储的。验证如下</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5dd108333d901e902201af524c11c05e_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"653\" data-rawheight=\"90\" class=\"origin_image zh-lightbox-thumb\" width=\"653\" data-original=\"https://pic3.zhimg.com/v2-5dd108333d901e902201af524c11c05e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;653&#39; height=&#39;90&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"653\" data-rawheight=\"90\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"653\" data-original=\"https://pic3.zhimg.com/v2-5dd108333d901e902201af524c11c05e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5dd108333d901e902201af524c11c05e_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>既然知道了上面这些，而且我们知道了存储的字符串内容（跑一下程序看看就知道了），那么我们就可以暴力扫描这个lib3rd_lib.so文件，找到他要打印的字符串，改写成我们想要的（注意不要溢出）。暴力扫描和替换的代码如下（为了能截屏到一张图，代码风格不佳，很多返回值也没有检查，仅作示意）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-bfaac57f31e662e04f28328748ea16ea_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"869\" data-rawheight=\"954\" class=\"origin_image zh-lightbox-thumb\" width=\"869\" data-original=\"https://pic3.zhimg.com/v2-bfaac57f31e662e04f28328748ea16ea_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;869&#39; height=&#39;954&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"869\" data-rawheight=\"954\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"869\" data-original=\"https://pic3.zhimg.com/v2-bfaac57f31e662e04f28328748ea16ea_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-bfaac57f31e662e04f28328748ea16ea_b.jpg\"/></figure><p>实验结果如下：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8c47e4ce8750f9a3ea2e844b3d565721_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1150\" data-rawheight=\"380\" class=\"origin_image zh-lightbox-thumb\" width=\"1150\" data-original=\"https://pic2.zhimg.com/v2-8c47e4ce8750f9a3ea2e844b3d565721_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1150&#39; height=&#39;380&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1150\" data-rawheight=\"380\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1150\" data-original=\"https://pic2.zhimg.com/v2-8c47e4ce8750f9a3ea2e844b3d565721_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8c47e4ce8750f9a3ea2e844b3d565721_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>可见，把lib3rd_lib.so这个文件里面的打印字符串我们成功修改了，而且没有影响原来库的功能。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这个方法缺点，毫无技术含量。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文以抑制第三方库的打印信息为目的，提出了三种方案，给出了实验过程和技术分析，并且分析了他们各自的优缺点。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p>+ <a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Devnull\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">en.wikipedia.org/wiki/D</span><span class=\"invisible\">evnull</span><span class=\"ellipsis\"></span></a></p><p>+ [《Linux内核源代码情景分析》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/1231584/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Linux内核源代码情景分析（上册） (豆瓣)</a>)</p>", 
            "topic": [
                {
                    "tag": "C / C++", 
                    "tagLink": "https://api.zhihu.com/topics/19601705"
                }, 
                {
                    "tag": "Linux 开发", 
                    "tagLink": "https://api.zhihu.com/topics/19610306"
                }, 
                {
                    "tag": "Hack", 
                    "tagLink": "https://api.zhihu.com/topics/19599672"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/81681440", 
            "userName": "船长", 
            "userLink": "https://www.zhihu.com/people/89f0b9e35e459846140118beaf57d1d8", 
            "upvote": 2, 
            "title": "\"undefined reference to XXX\"问题总结", 
            "content": "<p><b>船长黑板报所有文章的最新版本均在</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4971ae7a4b322276d02d781b52875642_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a><p><b>维护，知乎不做维护</b></p><p><b>您的Star是对我的鼓励</b></p><p>## 引言</p><p>我们在Linux下用C/C++工作的时候，经常会遇到&#34;undefined reference to XXX&#34;的问题，直白地说就是在**链接**(从.cpp源代码到可执行的ELF文件，要经过预处理-&gt;编译-&gt;链接三个阶段，此时预处理和编译已经通过了)的时候，链接器找不到XXX这个函数的定义了。这个问题在网上随便搜搜就有很多网页提供解决思路，要么是错的，要么不全面，要么只给结果没有具体分析思路。偶尔没头脑也可以轻易解决，但有的时候又隐藏的很深很细，需要花很长时间去排查。船长这里通过几个小例子，试着总结一下&#34;undefined reference to XXX&#34;问题的直接原因和解决方法，以后大家遇到这个问题，可以按照本文一条一条Check，可以节省时间，提高工作效率。背后的深层原因牵扯到编译，链接的基础知识、计算机原理和Linux系统环境，先挖个坑，以后有时间再专门写文来埋。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>**欢迎探讨，本文持续维护。**</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 实验平台：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 操作系统：Ubuntu 16.04 LTS，Ubuntu 18.04 LTS</p><p>+ 编译器：g++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609</p><p class=\"ztext-empty-paragraph\"><br/></p><p>  　　　　gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Case 1, 链接时缺少定义了XXX的源文件或者目标文件或者库文件</p><p class=\"ztext-empty-paragraph\"><br/></p><p>实验代码：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-21f175a4a5d61ea213eb00ada674a91f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"728\" data-rawheight=\"784\" class=\"origin_image zh-lightbox-thumb\" width=\"728\" data-original=\"https://pic4.zhimg.com/v2-21f175a4a5d61ea213eb00ada674a91f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;728&#39; height=&#39;784&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"728\" data-rawheight=\"784\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"728\" data-original=\"https://pic4.zhimg.com/v2-21f175a4a5d61ea213eb00ada674a91f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-21f175a4a5d61ea213eb00ada674a91f_b.jpg\"/></figure><p>+ 1.1 缺源文件。下面显示的是首先直接用g++编译main.cpp，出现了&#34;undefined reference to foo()&#34;的问题，未能编译出可执行程序a.out；然后在编译命令行加上foo.cpp(foo函数的定义文件)后，成功编译出a.out，而且执行起来更是非常顺滑～</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-822709be74e850959deb92c523e077cd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"765\" data-rawheight=\"261\" class=\"origin_image zh-lightbox-thumb\" width=\"765\" data-original=\"https://pic2.zhimg.com/v2-822709be74e850959deb92c523e077cd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;765&#39; height=&#39;261&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"765\" data-rawheight=\"261\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"765\" data-original=\"https://pic2.zhimg.com/v2-822709be74e850959deb92c523e077cd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-822709be74e850959deb92c523e077cd_b.jpg\"/></figure><p>+ 1.2 缺目标文件。同样，首先把foo.cpp编译成目标文件foo.o之后，也可以用foo.o来编译，也可以解决&#34;undefined reference to XXX&#34;的问题：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-51180c1c46df48fc72168e54d335295d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"733\" data-rawheight=\"352\" class=\"origin_image zh-lightbox-thumb\" width=\"733\" data-original=\"https://pic2.zhimg.com/v2-51180c1c46df48fc72168e54d335295d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;733&#39; height=&#39;352&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"733\" data-rawheight=\"352\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"733\" data-original=\"https://pic2.zhimg.com/v2-51180c1c46df48fc72168e54d335295d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-51180c1c46df48fc72168e54d335295d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>+ １.3 缺库文件。最后，把foo.cpp编译成动态库（静态库一样的）<a href=\"https://link.zhihu.com/?target=http%3A//xn--foo-tu9dq54h.so/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">文件foo.so</a>，我们来试试：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-cadace7476d5ec4e70058dc36dece1da_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"969\" data-rawheight=\"199\" class=\"origin_image zh-lightbox-thumb\" width=\"969\" data-original=\"https://pic3.zhimg.com/v2-cadace7476d5ec4e70058dc36dece1da_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;969&#39; height=&#39;199&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"969\" data-rawheight=\"199\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"969\" data-original=\"https://pic3.zhimg.com/v2-cadace7476d5ec4e70058dc36dece1da_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-cadace7476d5ec4e70058dc36dece1da_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>### Case 2, 链接顺序不对</p><p class=\"ztext-empty-paragraph\"><br/></p><p>接上节1.3，请看下面的代码：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-90fc7952a69eb17462642b91ca896853_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"768\" data-rawheight=\"182\" class=\"origin_image zh-lightbox-thumb\" width=\"768\" data-original=\"https://pic4.zhimg.com/v2-90fc7952a69eb17462642b91ca896853_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;768&#39; height=&#39;182&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"768\" data-rawheight=\"182\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"768\" data-original=\"https://pic4.zhimg.com/v2-90fc7952a69eb17462642b91ca896853_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-90fc7952a69eb17462642b91ca896853_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这段代码与1.3节相比，只是调换了foo.so库文件和main.cpp文件的前后顺序，就会出现&#34;undefined reference to XXX&#34;的问题。原因在[gcc手册](<a href=\"https://link.zhihu.com/?target=https%3A//gcc.gnu.org/onlinedocs/gcc-5.5.0/gcc/Link-Options.html%23Link-Options\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Using the GNU Compiler Collection (GCC): Link Options</a>)里面有提及，*It makes a difference where in the command you write this option; the linker searches and processes libraries and object files in the order they are specified. Thus, ‘foo.o -lz bar.o’ searches library ‘z’ after file foo.o but before bar.o. **If bar.o refers to functions in ‘z’, those functions may not be loaded***。不难懂就不翻译了，总之在给编译器输入源文件，目标文件或者动态库静态库文件时，如果B文件依赖A文件中的内容，那么B文件应该放在A文件的**左边**。这个问题很隐蔽，而且莫名其妙，权且当个结论记住吧。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Case 3, 函数符号修饰不一样</p><p class=\"ztext-empty-paragraph\"><br/></p><p>先说一下符号修饰（Name Mangling），函数从源代码编译到目标文件时，函数在目标文件中的名字是会改变的（这个改变的规则是编译器厂商定的，一般会包含函数参数列表信息、name space信息等），后面在链接阶段，链接器是按照函数改变后的名字来索引函数的实现机器码，函数改变后的名字不一样的函数，在链接器看来就是不一样的函数。至于为什么要有符号修饰这个机制，能不能直接用函数名来在目标文件内表示不同函数呢？答案是，能，而且在远古时期，就是这么干的。但是随着语言各种特性的发展，渐渐地就不能这么简单粗暴了，继续这样搞很容易造成符号冲突，而符号修饰在函数名上加上了名字空间信息和参数列表等信息，就可以允许不同名字空间里面的同名函数不冲突，允许函数重载机制。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>啰嗦这么多，记住一句话，链接器看到的名字和要找的名字，是经过符号修饰之后的名字，和你在源代码里看到的不一样。至于怎么看链接器要找的名字，和怎么看目标文件和库文件里有的名字，后面我会讲。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>下面我们具体列举一下，函数签名不一样（导致符号修饰不一样，继而导致链接器懵逼报&#34;undefined reference to XXX&#34;）导致的&#34;undefined reference to XXX&#34;错误和解决方法。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 3.1 函数定义和声明不一致</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这个很好理解，我们在写代码时，都是用.cpp文件写函数的定义，用.h头文件来写函数的接口声明来实现和接口的分离（那些写#include &#34;xxx.cpp&#34;的牲口，应该祭天～）。但是如果我们在.h文件里面写的声明和.cpp文件里面写的函数定义不一样的话，那就会出&#34;undefined reference to XXX&#34;这个问题，实验代码请看：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8d1b11c9dd97d7c7703214a855449cfb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"723\" data-rawheight=\"776\" class=\"origin_image zh-lightbox-thumb\" width=\"723\" data-original=\"https://pic4.zhimg.com/v2-8d1b11c9dd97d7c7703214a855449cfb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;723&#39; height=&#39;776&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"723\" data-rawheight=\"776\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"723\" data-original=\"https://pic4.zhimg.com/v2-8d1b11c9dd97d7c7703214a855449cfb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8d1b11c9dd97d7c7703214a855449cfb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在bar.h文件内bar函数的声明是void bar(void)，在实现文件内实现是void bar(int)，参数列表变了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>编译一下：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f39670eff80cbb49e9285fadc9fc569a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"779\" data-rawheight=\"139\" class=\"origin_image zh-lightbox-thumb\" width=\"779\" data-original=\"https://pic3.zhimg.com/v2-f39670eff80cbb49e9285fadc9fc569a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;779&#39; height=&#39;139&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"779\" data-rawheight=\"139\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"779\" data-original=\"https://pic3.zhimg.com/v2-f39670eff80cbb49e9285fadc9fc569a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f39670eff80cbb49e9285fadc9fc569a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>怎么解决？把声明和定义整一致了就行，很简单就不赘述了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 3.2 C和C++混合编程</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如果我们的开发的代码用的是C++语言，但是有些第三方库或者代码，用的是C语言编写编译，在应用C语言的库的时候，就要加上extern &#34;C&#34;，告诉C++的编译器按照C语言的符号修饰规则去找这些符号。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这是实验代码，baz.c是C源文件，里面定义了函数void baz(void)，在C++文件main.cpp里面，引用到了这个函数</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-afb62cfa60d21137d20889f6e011b645_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"718\" data-rawheight=\"749\" class=\"origin_image zh-lightbox-thumb\" width=\"718\" data-original=\"https://pic2.zhimg.com/v2-afb62cfa60d21137d20889f6e011b645_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;718&#39; height=&#39;749&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"718\" data-rawheight=\"749\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"718\" data-original=\"https://pic2.zhimg.com/v2-afb62cfa60d21137d20889f6e011b645_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-afb62cfa60d21137d20889f6e011b645_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>先用gcc编译C文件baz.c得到目标文件baz.o，然后用g++编译C++文件main.cpp和baz.o，就出现了错误：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6c12000ce808b63c16e0518cf929eaa3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"799\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb\" width=\"799\" data-original=\"https://pic4.zhimg.com/v2-6c12000ce808b63c16e0518cf929eaa3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;799&#39; height=&#39;204&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"799\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"799\" data-original=\"https://pic4.zhimg.com/v2-6c12000ce808b63c16e0518cf929eaa3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6c12000ce808b63c16e0518cf929eaa3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>解决方案，加上extern &#34;C&#34;，就可以了：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-283e16a9a73b8fb1b63ace00e63a66e8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"776\" data-rawheight=\"619\" class=\"origin_image zh-lightbox-thumb\" width=\"776\" data-original=\"https://pic1.zhimg.com/v2-283e16a9a73b8fb1b63ace00e63a66e8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;776&#39; height=&#39;619&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"776\" data-rawheight=\"619\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"776\" data-original=\"https://pic1.zhimg.com/v2-283e16a9a73b8fb1b63ace00e63a66e8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-283e16a9a73b8fb1b63ace00e63a66e8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 3.3 编译器版本或者编译器选项不一致</p><p class=\"ztext-empty-paragraph\"><br/></p><p>不同的编译器版本，或者编译器版本和链接器版本不一致，用的运行时库不一样，都不一定兼容；编译器选项不一样，也可能会影响到函数签名。在开发时，最好每个技术团队内都每个程序员都用一套标准的编译器和编译器选项配置。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Case 4，把模板函数写进了cpp文件中</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如果把模板函数写进了实现文件.cpp中，那么编译器就会认为这是一个独立的编译单元进行编译。然而，因为它是模板函数，编译器不能确定到底要将它特化到哪个实现(是特化成int的，还是float的，还是别的什么类型)，编译器也不会搜索你的整个工程里面别的cpp来确定特化什么类型(要搜索得话，万一你有10000000个.cpp呢？)，所以干脆就没有特化，所以你在链接的时候就找不到该函数的定义了，也就是&#34;undefined reference to XXX&#34;。这个情况发生不多，就不单独介绍了，[imred](<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/imred/article/details/80261632\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/imred/art</span><span class=\"invisible\">icle/details/80261632</span><span class=\"ellipsis\"></span></a>)这里写的生动详细，还附送了一个关于编译时弱符号(.weak)的知识点，感兴趣的可以看这里。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>### Case 5，api hinden</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如果把动态库libA.so的确实现了foo(...)，但是在编译成so的时候使用了-fvisibility=hidden类似的开关，那么使用者也是链接不了的（但是nm一下还是看得到），也会出现undefined reference to foo(...)这样的报错。代码实验过程请参考[D#0006](<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Captain1986/CaptainBlackboard/blob/master/D%25230006-protect_my_function/D%25230006.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Captain1986/CaptainBlackboard</a>)的Case 1, api hinden节。这种情况常见于一些开源库里面的不兼容，前面的版本这个api是可以用的，库升级后，这个版本变成了hinden了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 杂项</p><p>有时候我们使用第三方的库文件，又找不到相应的文档；或者我们查到了函数修饰后的名字，但是太晦涩难懂，怎么翻译成源代码里面的函数声明呢？</p><p>+ 怎么查看动态库文件里面提供了哪些函数符号？</p><p>  首先，我们把bar.cpp文件编译成动态库bar.so文件，这个文件提供了函数void bar(int)的实现</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-41a9ff1bd18b24571bc2b1c7f19ebbe5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1028\" data-rawheight=\"447\" class=\"origin_image zh-lightbox-thumb\" width=\"1028\" data-original=\"https://pic2.zhimg.com/v2-41a9ff1bd18b24571bc2b1c7f19ebbe5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1028&#39; height=&#39;447&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1028\" data-rawheight=\"447\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1028\" data-original=\"https://pic2.zhimg.com/v2-41a9ff1bd18b24571bc2b1c7f19ebbe5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-41a9ff1bd18b24571bc2b1c7f19ebbe5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>  要查看bar.so这个库里面提供了哪些函数符号，可以用nm命令</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-77ca1a8c7aeff379459d2c9104312877_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1208\" data-rawheight=\"950\" class=\"origin_image zh-lightbox-thumb\" width=\"1208\" data-original=\"https://pic4.zhimg.com/v2-77ca1a8c7aeff379459d2c9104312877_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1208&#39; height=&#39;950&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1208\" data-rawheight=\"950\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1208\" data-original=\"https://pic4.zhimg.com/v2-77ca1a8c7aeff379459d2c9104312877_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-77ca1a8c7aeff379459d2c9104312877_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>  这里我们可以看到，在bar.so文件的代码段（.Text段），有函数_Z3bari的定义。这个_Z3bari就是bar(int)这个函数在名字修饰后在库文件中的名字，链接器就是看这个名字来进行链接活动。nm -C可以查看函数名字修饰之前的名字，大家可以试试看。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 知道了函数修饰后的名字，怎么推函数声明？</p><p class=\"ztext-empty-paragraph\"><br/></p><p>  那么，如果我们知道了某个库里面的函数修饰后的名字，有没有什么方法可以反推函数源文件中的声明呢？用c++filt这个命令就可以。还是以上文中_Z3bari为例：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-67da7c424fbc66ec761607bcc1d56baa_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"692\" data-rawheight=\"74\" class=\"origin_image zh-lightbox-thumb\" width=\"692\" data-original=\"https://pic3.zhimg.com/v2-67da7c424fbc66ec761607bcc1d56baa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;692&#39; height=&#39;74&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"692\" data-rawheight=\"74\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"692\" data-original=\"https://pic3.zhimg.com/v2-67da7c424fbc66ec761607bcc1d56baa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-67da7c424fbc66ec761607bcc1d56baa_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>  可以看到，_Z3bari这个符号是bar(int)这个函数修饰而成。可能有人会问，为什么不是void bar(int)，是不是由于是void型的返回值，所以略去了？这里要注意，函数修饰不会管函数的返回值是什么。函数的返回值不同，而函数名和参数列表一样，经过函数修饰产生的符号是一样的，这也为什么C++规定，函数只有返回值不一样的话，是不能算重载的原因。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>+ 题外话，有时候我们不愿意让别人看到我们发布的库文件的符号表（泄露内部实现信息），我们也可以将符号信息从库文件中删除（strip命令，还有-fvisibility=hidden)。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 总结</p><p class=\"ztext-empty-paragraph\"><br/></p><p>以上差不多就是&#34;undefined reference to XXX&#34;的这个问题的常见原因和解决方案了，总结起来就是三点：１．是不是编译器找不到定义了XXX的文件；２．是不是定义了XXX的文件，由于函数修饰的原因里面没有想要的XXX符号;3．找到了想要的符号，但是该符号是隐藏属性，不能链接使用。如果不确定库里面有没有这个XXX符号，用nm找，用c++filt可以从修饰后的符号找函数声明。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>## 参考资料</p><p>+ [《How to write a shared library》](<a href=\"https://link.zhihu.com/?target=https%3A//www.akkadia.org/drepper/dsohowto.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">akkadia.org/drepper/dso</span><span class=\"invisible\">howto.pdf</span><span class=\"ellipsis\"></span></a>)</p><p>+ <a href=\"https://link.zhihu.com/?target=https%3A//www.cprogramming.com/tutorial/shared-libraries-linux-gcc.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">cprogramming.com/tutori</span><span class=\"invisible\">al/shared-libraries-linux-gcc.html</span><span class=\"ellipsis\"></span></a></p><p>+ [《程序员的自我修养：链接、装载与库》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/3652388/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">程序员的自我修养 (豆瓣)</a>)</p><p>+ [《深入理解计算机系统》](<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/1896753/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深入理解计算机系统 (豆瓣)</a>)</p><p>+ [关于模板函数为什么要写进.h文件的解释](<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/imred/article/details/80261632\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/imred/art</span><span class=\"invisible\">icle/details/80261632</span><span class=\"ellipsis\"></span></a>)</p>", 
            "topic": [
                {
                    "tag": "C++ 编程", 
                    "tagLink": "https://api.zhihu.com/topics/19836485"
                }, 
                {
                    "tag": "Linux 开发", 
                    "tagLink": "https://api.zhihu.com/topics/19610306"
                }, 
                {
                    "tag": "嵌入式开发", 
                    "tagLink": "https://api.zhihu.com/topics/19610823"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_1074249022222409728"
}
