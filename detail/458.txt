{
    "title": "机器学习笔记", 
    "description": "机器学习相关知识总结", 
    "followers": [
        "https://www.zhihu.com/people/huang-jia-ri-gou-dui-dui-chang", 
        "https://www.zhihu.com/people/liu-yang-44-75", 
        "https://www.zhihu.com/people/mysoulmq", 
        "https://www.zhihu.com/people/haoyuan-34", 
        "https://www.zhihu.com/people/zb-quan", 
        "https://www.zhihu.com/people/tree-1-13", 
        "https://www.zhihu.com/people/samurai3701", 
        "https://www.zhihu.com/people/li-peng-41-64", 
        "https://www.zhihu.com/people/wang-cl", 
        "https://www.zhihu.com/people/ha-ha-23-33-40", 
        "https://www.zhihu.com/people/yu-guang-liang-62", 
        "https://www.zhihu.com/people/zjms-85", 
        "https://www.zhihu.com/people/lin-yi-17-84", 
        "https://www.zhihu.com/people/a-bo-98-16", 
        "https://www.zhihu.com/people/tie-dan-91-52", 
        "https://www.zhihu.com/people/sun-wei-feng-85-34", 
        "https://www.zhihu.com/people/wu-ze-peng-35-54", 
        "https://www.zhihu.com/people/teng-yu-75-88", 
        "https://www.zhihu.com/people/zhangyuting", 
        "https://www.zhihu.com/people/lee-37-58-78", 
        "https://www.zhihu.com/people/silent-is-gold", 
        "https://www.zhihu.com/people/qing-ge-45", 
        "https://www.zhihu.com/people/misserror-47", 
        "https://www.zhihu.com/people/zzx-47-7", 
        "https://www.zhihu.com/people/liesben", 
        "https://www.zhihu.com/people/xierry-41", 
        "https://www.zhihu.com/people/hai-dao-chuan-chuan-fu-1738", 
        "https://www.zhihu.com/people/meng-li-35", 
        "https://www.zhihu.com/people/niceworld-49-6", 
        "https://www.zhihu.com/people/antonioqiao", 
        "https://www.zhihu.com/people/wang-zong-98-28", 
        "https://www.zhihu.com/people/da-da-18-6-40", 
        "https://www.zhihu.com/people/qichao-tang", 
        "https://www.zhihu.com/people/zhong-robin", 
        "https://www.zhihu.com/people/minzi-77", 
        "https://www.zhihu.com/people/IsingZhang", 
        "https://www.zhihu.com/people/rua-1", 
        "https://www.zhihu.com/people/wangcheny91", 
        "https://www.zhihu.com/people/wang-wei-31-91", 
        "https://www.zhihu.com/people/henter", 
        "https://www.zhihu.com/people/tom-pareto", 
        "https://www.zhihu.com/people/ping.love", 
        "https://www.zhihu.com/people/xu-yin-da-58", 
        "https://www.zhihu.com/people/yu-hou-shi-cha-hai", 
        "https://www.zhihu.com/people/he-en-dong", 
        "https://www.zhihu.com/people/lucky-45-9-77", 
        "https://www.zhihu.com/people/joyce-58-73-53", 
        "https://www.zhihu.com/people/she-liang", 
        "https://www.zhihu.com/people/ta-you-38", 
        "https://www.zhihu.com/people/cszhiyue", 
        "https://www.zhihu.com/people/xie-zhao-peng-6", 
        "https://www.zhihu.com/people/hijackjave", 
        "https://www.zhihu.com/people/xiaoyu4122", 
        "https://www.zhihu.com/people/wang-dong-70-96", 
        "https://www.zhihu.com/people/aaa-79-26", 
        "https://www.zhihu.com/people/zhaotiechui", 
        "https://www.zhihu.com/people/lei-xiao-yu-49", 
        "https://www.zhihu.com/people/liu-fei-94-95", 
        "https://www.zhihu.com/people/ming-ming-89-83", 
        "https://www.zhihu.com/people/sudalv0313", 
        "https://www.zhihu.com/people/xiao-gu-wei-dao-zhu"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/54874433", 
            "userName": "皓轩", 
            "userLink": "https://www.zhihu.com/people/688c1bab727a1ff5ff1b05707ff647e6", 
            "upvote": 33, 
            "title": "自然语言处理中句向量", 
            "content": "<p>如何求解一个sentence vector的表达？过去，我们常见的就是从word vector到sentence vector，这种从小unit到大一级unit的方法，统一称为<b>“composion”</b>；这方面的工作有以下的1、2、3、4、5、6.1、7。其实，除了以上方法，还有一种基于<b>distributed</b>的思想，这种方法就像word2vec一样，一个word的向量表达由它周围的contexts来展现；那么同理我们也可以把一个sentence当作一个word，即一个unit，用sentence上下文的前后sentence作为它的context来求得；这方面的工作有6.2、6.3、6.4。其余的从多任务、对话中学习等等。</p><h2>1. Bag of Words(BOW)</h2><p>无监督</p><p>基于统计的词袋模型：</p><ul><li>单个词的One-Hot表示</li><li>基于频数的词袋模型</li><li>(重点)基于TF-IDF的词袋模型：这个也和TF-IDF相关；Unsupervised Sentence Representations as Word Information Series: Revisiting TF--IDF（参考13）</li></ul><p>基于词向量的词袋模型：</p><ul><li>最简单的做法是拿预训练的词向量求平均。例如：Word2Vec、Glove、FastText等。</li><li>以每个词的tf-idf为权重，对所有词的word vector加权平均，获得sentence embedding。</li></ul><p>好处是<b>计算速度较快</b>，但是缺点是它<b>忽略了词序</b>，在一些对于词语顺序比较敏感的任务中，比如情感分析(sentiment analysis)等，效果不佳。</p><ul><li>(重点)从词的角度出发考虑的，最后的效果非常好，就是怎么样从词的向量得到句子的向量。首先选出一个词库，比如说10万个词，然后用w2v跑出所有词的向量，然后对于每一个句子，构造一个10万维的向量，向量的每一维是该维对应的词和该句子中每一个词的相似度的最大值。这种方法实际上是bag of words的一个扩展，比如说对于我喜欢用苹果手机这么一句话对应的向量，会在三星、诺基亚、小米、电脑等词上也会有比较高的得分。这种做法对于bag of words的稀疏性问题效果非常好。（句子中每个词保留和它最相似的十个词，所以最终非零维度的个数&lt;= 10 * (句子中词的个数））</li></ul><p><a href=\"https://www.zhihu.com/question/29978268/answer/55338644\" class=\"internal\">如何用 word2vec 计算两个句子之间的相似度？</a></p><ul><li>共现矩阵(Cocurrence matrix)，然后一般配合PCA或SVD将其进行降维。</li></ul><p><a href=\"https://zhuanlan.zhihu.com/p/42310942\" class=\"internal\">stawary：NLP中的文本表示方法</a></p><h2>2. Deep Averaging Network(DAN)</h2><p>有监督</p><p>来自ACL2015的一篇文章Deep Unordered Composition Rivals Syntactic Methods for Text Classification。</p><p>在BOW的基础上变<b>deep</b>（每deep一层，更加abstract）。</p><p>文中和BOW模型和RecNNs模型进行了<b>对比</b>。注：RecNNs是递归神经网络基于解析树的方式去考虑句法和次序对文本分类的影响（Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection）。</p><ul><li>BOW模训练速度快，但对次序不敏感，准确度不高。</li><li>RecNNs模型性能上虽然更好，但是代价高，训练速度慢。</li><li>DAN既能沾上BOW训练快、代价小的优点；又能考虑RecNNs在句法上的信息提取，达到和RecNNs媲美的准确度。</li></ul><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/miemasha8413/article/details/80590588\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-29c96e801c14bb5965442e45c32443f8_ipico.jpg\" data-image-width=\"830\" data-image-height=\"1032\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">怎么理解DAN，deep averaging network 模型</a><p>DAN既能沾上BOW训练快、代价小的优点；又能考虑RecNNs在句法上的信息提取，达到和RecNNs媲美的准确度。</p><p><b>Word Dropout Improves Robustness</b>。（参考自5）</p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/miemasha8413/article/details/80590588\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">怎么理解DAN，deep averaging network 模型</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/miyyer/dan\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">dan python</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/aravindsiv/dan_qa\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">利用dan做qa任务 keras</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.socher.org/index.php/Main/DynamicPoolingAndUnfoldingRecursiveAutoencodersForParaphraseDetection\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">RecNNs</a></p><h2>3. CNN for sentence modeling(各种基于CNN的模型)</h2><p>有监督</p><ul><li>(重点)TextCNN 2014 （Convolutional neural networks for sentence classification）</li><li>DCNN 2014 （A Convolutional Neural Network for Modelling Sentences）：<b>动态pooling</b></li><li>句子匹配中的句子建模 2014 （Convolutional neural network architectures for matching natural language sentences）</li><li>其中的句子表征 2015 （Multi-perspective sentence similarity modeling with convolutional neural networks）</li><li>其中的句子分析模型CNN-SM 2015 （Convolutional Neural Network for Paraphrase Identification）（这部分模型主要使用了上述Kal在2014年提出的模型DCNN，针对句子本身<b>提取出四种粒度的特征表示</b>：词、短ngram、长ngram和句子粒度。多种粒度的特征表示是非常必要的，一方面提高模型的性能，另一方面增强模型的鲁棒性。）</li><li>TextCNNN的分析 2015 （Sensitivity Analysis of Convolutional Neural Networks for Sentence Classification）</li></ul><p><a href=\"https://link.zhihu.com/?target=http%3A//www.jeyzhang.com/cnn-apply-on-modelling-sentence.html%255D\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">卷积神经网络(CNN)在句子建模上的应用</a></p><h2>4. Doc2vec</h2><p>(重点)</p><p>无监督</p><p>将一个句子甚至一篇短文也用一个向量来表示。</p><p>涉及到短文，最常用的固定长度的向量方法是词袋模型（bag-of-words）。尽管它很流行，但是<b>词袋模型存在两个主要的缺点</b>：一个是词袋模型忽略词序；另一个是词袋模型忽略语法。</p><p>Doc2vec又叫Paragraph Vector，基于word2vec模型提出，具有一些优点，比如<b>不固定句子长度，接受不同长度的句子做训练样本</b>，Doc2vec是一个<b>无监督学习算法</b>。</p><p>模型中，<b>每一句话用唯一的向量表示，每一个词也用唯一的向量表示</b>。增加了一个新句子向量Paragraph Vector，它可以被看作另一个词向量，扮演了一个记忆的功能。因为<b>Paragraph Vector在一个句子的若干次训练中是共享的</b>，它被看作是句子的主旨。</p><h2>PV-DM（Distributed Memory Model of paragraph vector）</h2><p>类似word2vec中的CBOW模型。</p><h2>PV-DBOW（Distributed Bag of Words of parageaph vector）</h2><p>类似word2vec中的skip-gram模型。</p><h2>预测过程</h2><p>Doc2vec怎么预测新的句子Paragraph Vector？</p><p>在预测新句子的过程中，将Paragraph Vector随机初始化，然后再根据随机梯度下降不断迭代最终求得最终稳定下来的句子向量。但是在预测过程中，<b>模型里的词向量和投影到输出层的softmax weights是不会变的，这样不断迭代的过程中只会更新Paragraph Vector</b>。</p><h2>代码实现</h2><div class=\"highlight\"><pre><code class=\"language-text\">from gensim.model.doc2vec import Doc2Vec</code></pre></div><h2>5. 基于RNN的模型</h2><p>有监督</p><p>比较早期的应用，通常取<b>最后的时序输出算作句子表征</b>。</p><p>很显然利用RNN(GRU或者LSTM)是一种不错的解决方案，它完全克服了BOW中忽略语序的缺点。但是它往往和supervised task结合在一起，缺乏可扩展性或者说迁移性(transferrable)，在某个task中可以取得不错的成绩，但是遇到其他的问题就得重新进行训练。LSTM往往开销比较大，而且不适合GPU的并行处理。</p><h2>5.1 Infersent</h2><p>(重点)</p><p>2017 Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</p><p>在SNLI语料上训练的位于句子编码器顶层的分类器，两个句子共用同一个编码器，这里的编码器采用max-pooling操作实现的biLSTM。（参考9）</p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/triplemeng/article/details/82106615\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">InferSent的代码实现</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/triplemeng/article/details/82106615\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">InferSent的代码实现</a></p><h2>6. Sentence2Vec</h2><p>无监督</p><h2>6.1 SIF</h2><p>(重点)</p><p>可以看作基于词袋模型的改进。原文模型仅用于分类，但也可用于有监督的学习Sentence Embedding。</p><p>以smooth inverse frequency（SIF）为权重，对所有词的word vector加权平均，最后<b>从中减掉principal component</b>，得到sentence embedding。</p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/sinat_31188625/article/details/72677088\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">A simple but tough to beat baseline for sentence</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/peter3125/sentence2vec\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">github1</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/jx00109/sentence2vec\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">github2</a></p><h2>6.2 skip-thought vectors</h2><p>NIPS15 Skip-Thought Vectors</p><p>skip-thought模型结构<b>借助了skip-gram的思想</b>。在skip-gram中，是以中心词来预测上下文的词；在skip-thought同样是利用中心句子来预测上下文的句子。</p><p>skip-thought模型的神经网络结构是在机器翻译中最常用的Encoder-Decoder架构，而在Encoder-Decoder架构中所使用的模型是GRU模型。<b>因此在训练句子向量时同样要使用到词向量，编码器输出的结果为句子中最后一个词所输出的向量</b>。</p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/jiangxinyang/p/9638991.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">skip-thought vector 实现Sentence2vector</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.sohu.com/a/129290647_473283\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">TensorFlow 自动句子语义编码，谷歌开源机器学习模型 Skip-Thoughts</a></p><h2>6.3 Quick-Thought Vectors</h2><p>(重点)</p><p>2018</p><p>本文是基于Skip-Thought Vector的改进。训练速度比Skip-Thought Vector快，后者需要训练3个RNN模块。（参考7）（参考11）</p><h2>6.4 An Exploration of Discourse-Based Sentence Spaces for Compositional Distributional Semantics</h2><p>这篇工作的出发点非常明确，就是去探究compositional vs distributional两类方法的basic setting有多大差别，是否有优劣之分。文章分别用基于compositional的思想和distributed的思想（所谓discourse-based）构造了一些feature，进行sentence表达，最后用实验来展现两者之间是否存在gap.结论是，几乎不存在gap。</p><h2>7. Power Mean均值模型</h2><p>无监督</p><p>2018</p><p>也是基于词袋模型的改进，通过引入<b>幂均值</b>（Power Mean）来捕捉序列中的其他信息。（参考10）</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/UKPLab/arxiv2018-xling-sentence-embeddings\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">github</a></p><h2>8. 基于Attention的模型</h2><h2>8.1 self-attention</h2><p>(重点)</p><p>2017 A Structured Self-attentive Sentence Embedding</p><p>本文提出使用二维矩阵作为句子表征，矩阵的行表示在句子不同位置的关注度，以解决句子被压缩成一维向量时的信息损失。（参考7）</p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/sinat_31188625/article/details/78344404\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING</a></p><h2>8.2 Learning Sentence Representation with Guidance of Human Attention IJCAI</h2><h2>8.3 Hierarchical Attention</h2><p>(重点)</p><p>Hierarchical Attention Networks for Document Classification</p><h2>9. 多任务学习</h2><p>(重点)</p><p>多任务学习试图在一次训练中组合不同的训练目标。</p><h2>9.1 基于多任务的Sentence Embedding</h2><p>2018 Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning</p><ul><li>本文认为为了能够推广到各种不同的任务，需要对同一句话的多个方面进行编码。</li><li>简单来说，模型同时在多个任务和多个数据源上进行训练，但是共享相同的Sentence Embedding。</li></ul><h2>9.2 Universal Sentence Encoder（谷歌）</h2><p>2018 Universal Sentence Encode</p><p>这篇文章基于InferSent，也是<b>想找到一个universal encoder</b>。</p><p>本文使用类似的多任务框架，区别在于使用的Encoder不同。</p><p>以两种模型作为Encoder：</p><ul><li>Transformer，更高的精度</li><li>DAN(Deep Averaging Network)，更快的速度</li></ul><h2>10. 从对话中学习</h2><p>(重点)</p><p>Learning Semantic Textual Similarity from Conversations</p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/triplemeng/article/details/81905480\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">关于句子embedding的一些工作简介（五）---- 从对话中学习</a></p><h2>github开源代码</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Separius/awesome-sentence-embedding\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">awesome-sentence-embedding</a></p><p><a href=\"https://www.zhihu.com/question/299549788/answer/561907291\" class=\"internal\"><span class=\"invisible\">https://www.</span><span class=\"visible\">zhihu.com/question/2995</span><span class=\"invisible\">49788/answer/561907291</span><span class=\"ellipsis\"></span></a></p><h2>参考</h2><p><a href=\"https://link.zhihu.com/?target=http%3A//www.flickering.cn/ads/2015/02/%25E8%25AF%25AD%25E4%25B9%2589%25E5%2588%2586%25E6%259E%2590%25E7%259A%2584%25E4%25B8%2580%25E4%25BA%259B%25E6%2596%25B9%25E6%25B3%2595%25E4%25BA%258C/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">1语义分析的一些方法1</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.flickering.cn/ads/2015/02/%25E8%25AF%25AD%25E4%25B9%2589%25E5%2588%2586%25E6%259E%2590%25E7%259A%2584%25E4%25B8%2580%25E4%25BA%259B%25E6%2596%25B9%25E6%25B3%2595%25E4%25B8%2580/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">2语义分析的一些方法2</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.infosec-wiki.com/%3Fp%3D167761\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">3Sentence Vector 的一些进展</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/lipengcn/article/details/80465468\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">4从compositional到distributed，从无监督到有监督再到多任务学习 —— 漫谈句向量 Sentence Embedding</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//baijiahao.baidu.com/s%3Fid%3D1591526385672382663%26wfr%3Dspider%26for%3Dpc\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">5深度学习在文本分类中的应用</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//baijiahao.baidu.com/s%3Fid%3D1591526385672382663%26wfr%3Dspider%26for%3Dpc\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">6深度学习在文本分类中的应用</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/imhuay/Algorithm_Interview_Notes-Chinese/blob/master/B-%25E8%2587%25AA%25E7%2584%25B6%25E8%25AF%25AD%25E8%25A8%2580%25E5%25A4%2584%25E7%2590%2586/B-%25E4%25B8%2593%25E9%25A2%2598-%25E5%258F%25A5%25E5%25B5%258C%25E5%2585%25A5.md%232017-self-attention\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">7专题 句向量</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/triplemeng/article/details/81045221\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">8关于句子embedding的一些工作简介（一）</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/triplemeng/article/details/81195026\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">9关于句子embedding的一些工作简介（二）---- InferSent</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/triplemeng/article/details/81298100\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">10关于句子embedding的一些工作简介（三）---- Concatenated p-mean Word Embeddings</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/triplemeng/article/details/81561320\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">11关于句子embedding的一些工作简介（四）---- Quick Thoughts</a></p><p><a href=\"https://zhuanlan.zhihu.com/p/37761272\" class=\"internal\">12当前最好的词句嵌入技术概览：从无监督学习转向监督、多任务学习</a></p><p><a href=\"https://zhuanlan.zhihu.com/p/53569058\" class=\"internal\">13NLP预训练模型大集合！</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650743256%26idx%3D3%26sn%3D8273e7def6b8b64027e8aeb84212b6e9%26chksm%3D871ae5a6b06d6cb0b4a26620a3b566a8c3a64d54db814d64754a8d952d927b05e7e8e73a969f%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">14当前最好的词句嵌入技术概览：从无监督学习转向监督、多任务学习</a></p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "向量", 
                    "tagLink": "https://api.zhihu.com/topics/19660929"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": [
                {
                    "userName": "新人类", 
                    "userLink": "https://www.zhihu.com/people/7c38fdd2d1d11be6f85ecebd8566817d", 
                    "content": "<p>很全啊！多谢多谢~</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/54772042", 
            "userName": "皓轩", 
            "userLink": "https://www.zhihu.com/people/688c1bab727a1ff5ff1b05707ff647e6", 
            "upvote": 1, 
            "title": "自然语言处理中的词向量", 
            "content": "<p>下面链接里面也有很多相关的词向量模型，但是有些其实不太常用。以下总结了一些比较常用的一些词向量的相关博文和实战代码。</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Separius/awesome-sentence-embedding\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">awesome-sentence-embedding</a></p><p><a href=\"https://zhuanlan.zhihu.com/p/37761272\" class=\"internal\">当前最好的词句嵌入技术概览：从无监督学习转向监督、多任务学习</a></p><h2>1. Word Embedding</h2><h2>1.1 Word2Vec</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//www.leiphone.com/news/201706/PamWKpfRFEI42McI.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">一文详解 Word2vec 之 Skip-Gram 模型（结构篇）</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.leiphone.com/news/201706/eV8j3Nu8SMqGBnQB.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">一文详解 Word2vec 之 Skip-Gram 模型（训练篇）</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.leiphone.com/news/201706/QprrvzsrZCl4S2lw.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">一文详解 Word2vec 之 Skip-Gram 模型（实现篇）</a></p><p>基于分布假设（在相同的上下文中出现的单词往往具有相似的含义）的无监督学习方法</p><h2>1.2 GloVe</h2><p>GloVe(Global Vectors for Word Representation)是一种基于共现矩阵分解的词向量。</p><p><a href=\"https://link.zhihu.com/?target=https%3A//nlp.stanford.edu/projects/glove/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Glove词向量</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/sinat_26917383/article/details/54847240\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">NLP︱高级词向量表达（一）——GloVe（理论、相关测评结果、R&amp;python实现、相关应用）</a></p><p>基于分布假设（在相同的上下文中出现的单词往往具有相似的含义）的无监督学习方法</p><h2>1.3 WordRank</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/sinat_26917383/article/details/54852214\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">NLP︱高级词向量表达（三）——WordRank（简述）</a></p><h2>1.4 Polyglot</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//sites.google.com/site/rmyeid/projects/polyglot%23TOC-Abstract\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Polyglot，里面有英文词向量、中文词向量、字向量</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//nbviewer.jupyter.org/gist/aboSamoor/6046170\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">如何读取Polyglot词向量文件</a></p><h2>1.5 fastText</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">fastText预训练的157种语言的词向量</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/sinat_26917383/article/details/54850933\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">NLP︱高级词向量表达（二）——FastText（简述、学习笔记）</a></p><p>有监督 + ngram（可以解决OOV问题） + 速度快</p><h2>2. Contextualized Word Embedding</h2><p><a href=\"https://zhuanlan.zhihu.com/p/54448555\" class=\"internal\">从Word Embedding到Bert模型-自然语言处理中的预训练技术发展史(理论+实践)</a></p><h2>2.1 Transformer</h2><p>Transformer代码分析：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/JepsonWong/Transformer\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/JepsonWong/T</span><span class=\"invisible\">ransformer</span><span class=\"ellipsis\"></span></a></p><h2>2.2 ELMO</h2><p>代码很清晰，可读性很好。<a href=\"https://link.zhihu.com/?target=https%3A//github.com/codertimo/ELMO-tf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/codertimo/EL</span><span class=\"invisible\">MO-tf</span><span class=\"ellipsis\"></span></a></p><p>注意：ELMo模型的输入是字符而不是单词。</p><h2>2.3 BERT</h2><p>BERT代码实战（中英文文本分类）：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/JepsonWong/BERT\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/JepsonWong/B</span><span class=\"invisible\">ERT</span><span class=\"ellipsis\"></span></a></p><h2>3. 其他</h2><h2>3.1 基于笔画的中文词的word embedding</h2><p>cw2vec</p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/quxing10086/article/details/80332538\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">cw2vec理论及其实现</a></p><h2>4. 词嵌入模型的最优维度</h2><p><a href=\"https://zhuanlan.zhihu.com/p/53958685\" class=\"internal\">NeurIPS 2018 oral论文解读：如何给词嵌入模型选择最优维度</a></p><p>里面有<b>各个词向量的最优维度</b>以及最优维度的选择原理。</p><h2>github资源</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/3Top/word2vec-api\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">word2vec-api</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Kyubyong/wordvectors\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Pre-trained word vectors of 30+ languages，里面有中文word2vec和fastText的词向量</a></p><h2>博客资源</h2><p>注意：训练好的word2vec和glove词向量均可以用gensim导入。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Google&#39;s trained Word2Vec model in Python</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//mccormickml.com/2016/04/27/word2vec-resources/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Word2Vec Resources</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/Darwin2000/p/5786984.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">60维训练好的中文词向量-gensim版本要对应，要不然没法用！</a></p><p><a href=\"https://zhuanlan.zhihu.com/Gensim%20Word2vec%20%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97\" class=\"internal\">Gensim Word2vec 使用指南</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/philosophyatmath/article/details/52354413\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">python 环境下gensim中的word2vec的使用笔记</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/u010041824/article/details/70832295\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">gensim词向量之加载word2vec和glove</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/lixintong1992/article/details/51607372\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Gensim Word2vec简介</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.360doc.com/content/17/0126/23/40028542_624946612.shtml\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Keras模型中使用预训练的词向量</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//albertxiebnu.github.io/fasttext/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">玩转Fasttext</a></p><h2>论文</h2><h2>Mikolov的原始论文</h2><p>《DistributedRepresentations of Words and Phrases and their Compositionality》</p><p>《Efficient Estimation ofWord Representations in Vector Space》</p><h2>DL在language model中应用的祖先文章（2003年）</h2><p>《A Neural ProbabilisticLanguage Model》 by Yoshua Bengio et. al.</p><h2>Word2Vec在文章种被解释为一种变种PCA</h2><p>《Neural Word Embedding asMatrix Factorization》by Yoav Goldberg et. al.</p><h2>以unsupervised的方式产生embedding</h2><p>Stanford的Chris Manning的 Glove：《Global Vectors for WordRepresentation》</p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "词向量", 
                    "tagLink": "https://api.zhihu.com/topics/20142325"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/54448555", 
            "userName": "皓轩", 
            "userLink": "https://www.zhihu.com/people/688c1bab727a1ff5ff1b05707ff647e6", 
            "upvote": 40, 
            "title": "从Word Embedding到Bert模型-自然语言处理中的预训练技术发展史(理论+实践)", 
            "content": "<p>针对本文的github地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/JepsonWong/Pre-training_Techniques_For_NLP\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/JepsonWong/P</span><span class=\"invisible\">re-training_Techniques_For_NLP</span><span class=\"ellipsis\"></span></a></p><p>这篇文章主要讲基于语言模型的词向量，其实词向量还有基于统计方法的（例如：基于共现矩阵、SVD）。</p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.meiwen.com.cn/subject/xtcemftx.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">1 词向量技术-从word2vec到ELMo</a></p><h2>下游任务如何使用Word Embedding</h2><p>下游NLP任务在使用Word Embedding的时候也类似图像有两种做法，一种是Frozen，就是Word Embedding那层网络参数固定不动；另外一种是Fine-Tuning，就是Word Embedding这层参数使用新的训练集合训练也需要跟着训练过程更新掉。</p><h2>语言模型(神经网络语言模型NNLM)</h2><p>2003年Bengio提出。</p><p>学习任务是输出入某个句中某个单词w前面的t-1个单词，要求网络正确预测单词w，单词的word embedding是这个模型的副产物。</p><h2>语言模型(Word2Vec/Glove)</h2><p>Word2Vec网络结构和NNLM类似，但尽管网络结构相近，而且也是做<b>语言模型任务</b>，但是其训练方法不太一样。Word2Vec有两种训练方法，一种叫CBOW，核心思想是从一个句子里面把一个词抠掉，用这个词的上文和下文去预测被抠掉的这个词；第二种叫做Skip-gram，和CBOW正好反过来，输入某个单词，要求网络预测它的上下文单词。而你回头看看，NNLM是怎么训练的？是输入一个单词的上文，去预测这个单词。这是有显著差异的。为什么Word2Vec这么处理？原因很简单，因为Word2Vec和NNLM不一样，<b>NNLM的主要任务是要学习一个解决语言模型任务的网络结构</b>，语言模型就是要看到上文预测下文，而word embedding只是无心插柳的一个副产品。但是<b>Word2Vec目标</b>不一样，它<b>单纯就是要word embedding的</b>，这是主产品，所以它完全可以随性地这么去训练网络。</p><p>Word2Vec工具主要包含两个模型：跳字模型（skip-gram）和连续词袋模型（continuous bag of words，简称CBOW），以及两种近似训练法：负采样（negative sampling）和层序softmax（hierarchical softmax）。</p><p>Word2Vec得到<b>以该词作为背景词和中心词的两组词向量</b>。我们<b>会使用连续词袋模型的背景词向量，使用跳字模型的中心词向量</b>。来自链接[1]。</p><h2>TagLM</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//godweiyang.com/2017/10/03/ACL17-1161/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">讲的不错 Semi-supervised sequence tagging with bidirectional language models</a></p><p>2017年ACL会议提出，做命名实体识别任务。和ELMo是同一个作者。</p><p>利用语言模型来训练，分为Forward LM和Backward LM；输入均为一行文本，然后接Embedding层，接着接LM层（LM层是rnn或者lstm实现，可以为单向也可以为双向；Forward LM层输入为一个正向文本序列，预测下一个词；Backward LM输入为反向的文本序列，预测也为下一个词；也就是说这连两个输入序列顺序不一样，但预测结果是一样），然后两个LM层接一个Dense层（将Embedding转化为len(单词)的预测结果）。</p><p>图示如下，图中LM画的只是单向rnn，其实可以实现成单向和双向。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-405fe29fe68e56c870b110a3741a9062_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1862\" data-rawheight=\"970\" class=\"origin_image zh-lightbox-thumb\" width=\"1862\" data-original=\"https://pic3.zhimg.com/v2-405fe29fe68e56c870b110a3741a9062_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1862&#39; height=&#39;970&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1862\" data-rawheight=\"970\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1862\" data-original=\"https://pic3.zhimg.com/v2-405fe29fe68e56c870b110a3741a9062_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-405fe29fe68e56c870b110a3741a9062_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>实现：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//pypi.org/project/keras-bi-lm/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Python包 可以使用提供的接口</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/CyberZHG/keras-bi-lm\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">上述Python包的源代码</a></p><p>使用：</p><p>要使用TagLM生成的词向量，首先<b>在大型语料上</b>训练好TagLM模型，然后给定每一行输入，跑LM模型得到其两个LM模型隐藏层的输出组成词向量，<b>每个LM模型得到对应位置词的一个一个词向量</b>。</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/PoWWoP/UOI-1705.00108\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">TagLM的例子 用bi-lm做序列问题</a></p><h2>ULFMiT</h2><p>没仔细看</p><p>ULMFiT使用的是三阶段模式，在通用语言模型训练之后，加入了一个领域语言模型预训练过程，而且论文重点工作在这块，方法还相对比较繁杂，这并不是一个特别好的主意，因为领域语言模型的限制是它的规模往往不可能特别大，精力放在这里不太合适，放在通用语言模型上感觉更合理；再者，尽管ULFMiT实验做了6个任务，但是都集中在分类问题相对比较窄，不如ELMO验证的问题领域广，我觉得这就是因为第二步那个领域语言模型带来的限制。</p><h2>ELMo(Embedding from Language Models)</h2><p><b>Word Embedding存在多义词问题</b>。多义词对Word Embedding来说有什么负面影响？比如一个多义词w有两个常用含义，但是Word Embedding在对w这个单词进行编码的时候，是区分不开这两个含义的，因为它们尽管上下文环境中出现的单词不同，但是<b>在用语言模型训练的时候，不论什么上下文的句子经过word2vec，都是预测相同的单词w，而同一个单词占的是同一行的参数空间，这导致两种不同的上下文信息都会编码到相同的word embedding空间里去</b>。所以word embedding无法区分多义词的不同语义，这就是它的一个比较严重的问题。</p><p>ELMO提供了一种简洁优雅的解决方案。提出ELMO的论文题目：“Deep contextualized word representation”体现了其精髓，而精髓在哪里？在deep contextualized这个短语，一个是deep，一个是<b>context</b>，其中context更关键。在此之前的Word Embedding本质上是个静态的方式，所谓静态指的是训练好之后每个单词的表达就固定住了，以后使用的时候，不论新句子上下文单词是什么，这个<b>单词的Word Embedding不会跟着上下文场景的变化而改变</b>，所以对于一个多义词，它事先学好的Word Embedding中<b>混合了几种语义</b> ，在应用中来了个新句子，即使从上下文中明显可以看出它代表的是某种确定的含义，但是对应的Word Embedding内容也不会变，它还是混合了多种语义。这是为何说它是<b>静态的</b>，这也是问题所在。ELMO的本质思想是：<b>我事先用语言模型学好一个单词的Word Embedding</b>，此时多义词无法区分，不过这没关系。在我<b>实际使用Word Embedding的时候，单词已经具备了特定的上下文了</b>，这个时候我可以<b>根据上下文单词的语义去调整单词的Word Embedding表示</b>，这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。所以ELMO本身是个<b>根据当前上下文对Word Embedding动态调整</b>的思路。</p><p>学习任务是根据单词w的上下文去预测单词，单词w之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。分别有两个编码器，分别是正方向编码器和反方向编码器，均为双层LSTM结构；正方向编码器输入的是从左到右顺序的除了预测单词外的上文Context-before和下文Context-after，反方向编码器输入的是从右到左的逆序的句子上文和下文；</p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.she9.com/article.php%3Fid%3D295\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">开源项目ELMo：机器学习在自动翻译中的应用</a></p><p><a href=\"https://zhuanlan.zhihu.com/p/38254332\" class=\"internal\">ELMo 最好用的词向量《Deep Contextualized Word Representations》</a></p><p>ELMo的方法，可以看作是<b>TagLM的一个改进版</b>。首先，ELMo<b>不局限于sequence labeling，而是作为一个一般性的词向量表示方法</b>；其次，ELMo<b>不仅仅使用了neural language model 的最后一层的输出，而是对所有层的输出做了加权平均来构造最后的向量</b>。</p><p>公式中的s是由softmax算出来的(加起来是1)，gamma是一个需要学习的变量，加不加这个变量对performance的影响是比较大的，这两个变量都是<b>和具体的任务相关</b>。</p><p>通过这样的迁移策略，那些<b>对词义消歧有需求的任务就更容易通过训练给第二隐层一个很大的权重</b>，而<b>对词性、句法有明显需求的任务则可能对第一隐层的参数学习到比较大的值</b>（实验结论）。总之，这样便得到了一份“可以被下游任务定制”的特征更为丰富的词向量，效果比word2vec好得多也就不足为奇了。</p><p>缺点，在GPT和Bert出来之后对比发现</p><ul><li><b>LSTM抽取特征的能力远弱于Transformer</b></li><li>拼接方式双向融合特征融合能力偏弱</li></ul><p><a href=\"https://link.zhihu.com/?target=https%3A//allennlp.org/elmo\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ELMo项目主页</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/allenai/allennlp\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ElMo项目 github pytorch</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/allenai/bilm-tf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ELMo词向量 tennsorflow实现</a></p><p><a href=\"https://zhuanlan.zhihu.com/p/37915351\" class=\"internal\">里面有用法 NAACL2018 一种新的embedding方法Deep contextualized word representations ELMo原理与用法</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/sinat_26917383/article/details/81913790\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ELMo的总结 主要是实战，可以用中文，中文词向量在下方 流水账Elmo词向量中文训练过程杂记</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/HIT-SCIR/ELMoForManyLangs\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ELMo预训练的多国语言模型</a></p><h2>一些说明</h2><p>上述ELMo的目标也仅仅是学习到上下文相关的、更强大的词向量，其目的依然是为下游任务提供一个扎实的根基，还没有想要弑君称王的意思。</p><p>而我们知道，仅仅是对文本进行充分而强大的encoding（即得到每个词位非常精准丰富的特征）是远不够覆盖所有NLP任务的。在QA、机器阅读理解（MRC）、自然语言推理（NLI）、对话等任务中，<b>还有很多更复杂的模式需要捕捉</b>，比如句间关系。为此，下游任务中的网络会加入各种<b>花式attention</b>（参考NLI、MRC、Chatbot中的SOTA们）。</p><p>而随着捕捉更多神奇模式的需要，研究者们<b>为每个下游任务定制出各种各样的网络结构</b>，导致<b>同一个模型，稍微一换任务就挂掉了</b>，甚至在同一个任务的情况下换另一种分布的数据集都会出现显著的性能损失，这显然不符合人类的语言行为呀。要知道人类的generalization能力是非常强的，这就说明，或许现在整个NLP的发展轨迹就是错的，尤其是在SQuAD的带领下，穷尽各种trick和花式结构去刷榜，真正之于NLP的意义多大呢？</p><p>不过所幸，这条越走越偏的道路终于被一个模型shutdown了，那就是Google发布的Bidirectional Encoder Representations from Transformers (BERT)（GPT其实和它原理差不多）。</p><p>这两篇paper的最重要意义不在于用了什么模型，也不在于怎么训练的，而是它<b>提出一种全新的游戏规则</b>。</p><p>像之前说的，<b>为每个NLP任务去深度定制泛化能力极差的复杂模型结构其实是非常不明智的</b>，走偏了方向的。既然ELMo相比word2vec会有这么大的提升，这就说明预训练模型的潜力远不止为下游任务提供一份精准的词向量，所以我们可不可以<b>直接预训练一个龙骨级的模型呢</b>？如果它里面已经<b>充分的描述了字符级、词级、句子级甚至句间关系的特征</b>，那么在不同的NLP任务中，只需要去为任务<b>定制一个非常轻量级的输出层</b>（比如一个单层MLP）就好了，毕竟模型骨架都已经做好了。</p><p>而这两篇paper正是做了这件事情，或者说，它真的把这件事情做成了，它作为一个general的龙骨级模型轻松的挑战了11个任务上的深度定制的模型。</p><p>也就是说，然后在具体NLP任务<b>有监督微调</b>时，与<b>ELMo当成特征</b>的做法不同，OpenAI <b>GPT不需要再重新对任务构建新的模型结构</b>，而是<b>直接在Transformer这个语言模型上的最后一层接上softmax作为任务输出层，然后再对这整个模型进行微调</b>。他们还发现，如果使用语言模型作为辅助任务，能够提升有监督模型的泛化能力，并且能够加速收敛。</p><h2>GPT</h2><p>GPT是“Generative Pre-Training”的简称，从名字看其含义是指的生成式的预训练。GPT也采用<b>两阶段过程</b>，第一个阶段是<b>利用语言模型进行预训练</b>，第二阶段<b>通过Fine-tuning的模式解决下游任务</b>。</p><p><a href=\"https://zhuanlan.zhihu.com/p/52775384\" class=\"internal\">论文研读之OpenAI-Generative Pre-Training</a></p><p><b>利用语言模型来进行训练，即给定前面的单词预测下一个单词</b>。</p><p>GPT和ELMo是类似的，主要不同在于两点：</p><ul><li>特征抽取器不是用的RNN，而是用的<b>Transformer</b>，上面提到过它的特征抽取能力要强于RNN，这个选择很明显是很明智的。</li><li>GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是<b>单向的语言模型</b>，所谓“单向”的含义是指：语言模型训练的任务目标是根据w单词的上下文去正确预测单词w，w之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。ELMo在做语言模型预训练的时候，预测单词w同时使用了上文和下文，而GPT则只采用Context-before这个单词的上文来进行预测，而抛开了下文。这个选择现在看不是个太好的选择，原因很简单，它没有把单词的下文融合进来，<b>这限制了其在更多应用场景的效果</b>，比如<b>阅读理解这种任务</b>，在做任务的时候是可以允许同时看到上文和下文一起做决策的。如果预训练时候不把单词的下文嵌入到Word Embedding中，是很吃亏的，白白丢掉了很多信息。</li></ul><p><a href=\"https://zhuanlan.zhihu.com/p/52818066\" class=\"internal\">由Attention看OpenAI的网红GPT</a></p><h2>BERT</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/brightmart/bert_language_understanding\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">（没看）BERT源码理解</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.lyrn.ai/2018/11/07/explained-bert-state-of-the-art-language-model-for-nlp/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">（原理）BERT – State of the Art Language Model for NLP</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.jianshu.com/p/d110d0c13063\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">（原理）5 分钟入门 Google 最强NLP模型：BERT</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Dissecting BERT Part 1: The Encoder</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Understanding BERT Part 2: BERT Specifics</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//medium.com/dissecting-bert/dissecting-bert-appendix-the-decoder-3b86f66b0e5f\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Dissecting BERT Appendix: The Decoder</a></p><p>使用超多层Transformer + 双任务预训练 + 后期微调的训练策略。</p><p>BERT是使用Transformer的<b>编码器</b>（GPT被认为用的是Transformer的解码器）来作为语言模型，在语言模型预训练的时候，提出了两个<b>新的目标任务</b>（即遮挡语言模型MLM和预测下一个句子的任务）。</p><p>BERT模型的训练分为预训练（Pre-training）和微调（Pre-training）两步。<b>微调取决于下游的具体任务</b>。不同的下游任务意味着不同的网络扩展结构：比如一个对句子进行情感分类的任务，只需要在BERT的输出层句向量上接入几个Dense层，走个softmax。而对于SQuAD上的阅读理解任务，需要对BERT输出的词向量增加match层和softmax。</p><p>总体来说，对BERT的微调是一个轻量级任务，<b>微调主要调整的是扩展网络而非BERT本身</b>。换句话说，我们完全可以固定住BERT的参数，把BERT输出的向量编码当做一个特征（feature）信息，用于各种下游任务。</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/codertimo/BERT-pytorch\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">bert pytorch复现</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.52nlp.cn/bert-paper-%25E8%25AE%25BA%25E6%2596%2587-%25E6%2596%2587%25E7%25AB%25A0-%25E4%25BB%25A3%25E7%25A0%2581%25E8%25B5%2584%25E6%25BA%2590%25E6%25B1%2587%25E6%2580%25BB\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">BERT相关论文、文章和代码资源汇总</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/google-research/bert\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">google开源代码</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/huggingface/pytorch-pretrained-BERT\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">PyTorch版本BERT</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/soskek/bert-chainer\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Chainer版本BERT</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Separius/BERT-keras\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Keras版: Keras implementation of BERT with pre-trained weights</a></p><p>google预训练好了BERT-Base、Multilingual和BERT-Base、Chinese在内的任意模型。</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/google-research/bert%23pre-trained-models\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">BERT预训练模型</a></p><p><b>实践部分</b>：</p><p>官方实践部分包括微调预训练BERT、通过预训练BERT抽取语义特征（可以使用脚本extract_features.py抽取语义特征）。下面链接有涉及到：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650751075%26idx%3D2%26sn%3D0a3ecd1af5f8549051760775e34db342%26chksm%3D871a841db06d0d0bcf3cc4e620bb384e050ba6e92224d338a8ddc1543add97a4a4e7919ebf15%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">（原理讲的不错）谷歌终于开源BERT代码：3 亿参数量，机器之心全面解读</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650752891%26idx%3D5%26sn%3D8a44293a57da96db51b9a13feb6223d7%26chksm%3D871a8305b06d0a134e332a6831dbacc9ee79b28a79658c130fe6162f33211788cab18a55ec90%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">小数据福音！BERT在极小数据下带来显著提升的开源实现</a></p><p>我们可以用bert-as-service<b>生成句向量和ELMo词向量</b>等：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/pD4it8vQ-aE474uSMQG0YQ\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">两行代码玩转 Google BERT 句向量词向量</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/hanxiao/bert-as-service\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">bert-as-service github项目（Mapping a variable-length sentence to a fixed-length vector using BERT model）</a></p><p>下载那个中文的pretrain模型后，怎么能取出embeddings，如何评估一下这个pretrain出来的embeddings？</p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.jianshu.com/p/aa2eff7ec5c1\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">（详细讲解了run_classifier.py文件的结构）干货 | BERT fine-tune 终极实践教程</a></p><p>三个embedding的tensor名字为：</p><ul><li>bert/embeddings/word_embeddings:0</li><li>bert/embeddings/token_type_embeddings:0</li><li>bert/embeddings/position_embeddings:0</li></ul><p>可以在载入后用如下的方式获取,通过sess.run()打印或者写入文件embedding_variables = [ var for var in tf.global_variables() if var.name in embedding_name_list]</p><h2>一些trick</h2><p>1.如何理解ptr-training和fine-tune的mismatching。就是为什么不能100%用MASK代替，而要用10%的random token和10%的原token？</p><p>不全用MASK是因为在finetune到下游任务的时候（例如POS Tagging）所有词都是已知的，如果模型只在带MASK的句子上预训练过，那么模型就只知道根据其他词的信息来预测当前词，而不会直接利用这个词本身的信息，会凭空损失一部分信息，对下游任务不利。还有10%random token是因为如果都用原token，模型在预训练时可能会偷懒，不去建模单词间的依赖关系，直接照抄当前词。</p><p>2.Transformer中encoder的self-attention本身就是双向的，只是利用mask的trick来训练语言模型。</p><h2>ELMo、GPT、BERT对比</h2><p>对比一下三种语言模型结构。</p><p>BERT使用的是Transformer编码器，由于self-attention机制，所以模型<b>上下层直接全部互相连接</b>的。而OpenAI GPT使用的是Transformer解码器，它是一个需要从左到右的受限制的Transformer，而ELMo使用的是双向LSTM，虽然是双向的，但是也<b>只是在两个单向的LSTM的最高层进行简单的拼接</b>。所以只有BERT是真正在模型<b>所有层中是双向的</b>。</p><p>在模型的输入方面，BERT做了更多的细节。他们使用了WordPiece embedding作为词向量，并加入了位置向量和<b>句子切分向量</b>。此外，作者还<b>在每一个文本输入前加入了一个CLS向量</b>，后面会有这个向量作为具体的分类向量。</p><p>在语言模型预训练上，他们不再使用标准的从左到右预测下一个词作为目标任务，而是<b>提出了两个新的任务</b>。第一个任务他们称为MLM，即在输入的词序列中，随机的挡上15%的词，然后任务就是去预测挡上的这些词，可以看到相比传统的语言模型预测目标函数，MLM可以从任何方向去预测这些挡上的词，而不仅仅是单向的。</p><h2>BERT实战</h2><p>代码地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/google-research/bert\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/google-resea</span><span class=\"invisible\">rch/bert</span><span class=\"ellipsis\"></span></a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/weixin_37947156/article/details/84877254\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">BERT中文实战（文本相似度）</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.jianshu.com/p/aa2eff7ec5c1\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">干货 | BERT fine-tune 终极实践教程</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//terrifyzhao.github.io/2018/11/29/%25E4%25BD%25BF%25E7%2594%25A8BERT%25E5%2581%259A%25E4%25B8%25AD%25E6%2596%2587%25E6%2596%2587%25E6%259C%25AC%25E7%259B%25B8%25E4%25BC%25BC%25E5%25BA%25A6%25E8%25AE%25A1%25E7%25AE%2597.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">BERT中文文本相似度计算与文本分类</a></p><p>重点介绍fine-tuning过程。</p><p>代码通过tensorflow高级API—— tf.estimator进行封装(wrapper)的。因此对于不同数据集的适配，只需要修改代码中的processor部分，就能进行代码的训练、交叉验证和测试。</p><p>预训练的入口是在run_pretraining.py而fine-tune的入口针对不同的任务分别在run_classifier.py和run_squad.py。其中run_classifier.py适用的任务为分类任务。如CoLA、MRPC、MultiNLI这些数据集。而run_squad.py适用的是阅读理解(MRC)任务，如squad2.0和squad1.1。</p><p>run_classfier.py：</p><p>data_dir：指的是我们的输入数据的文件夹路径。输入数据格式在InputExample类中定义，它要求的输入分别是guid, text_a, text_b, label，其中text_b和label为可选参数。例如我们要做的是单个句子的分类任务，那么就不需要输入text_b；另外，在test样本中，我们便不需要输入lable。</p><p>task_name：用来选择processor的。</p><p>processor：任何模型的训练、预测都是需要有一个明确的输入，而BERT代码中processor就是负责对模型的输入进行处理。自定义的processor里需要继承DataProcessor，并重载获取label的get_labels和获取单个输入的get_train_examples,get_dev_examples和get_test_examples函数。其分别会在main函数的FLAGS.do_train、FLAGS.do_eval和FLAGS.do_predict阶段被调用。这三个函数的内容是相差无几的，区别只在于需要指定各自读入文件的地址。</p><p>process在得到字符串形式的输入后，在file_based_convert_examples_to_features里先是对字符串长度，加入[CLS]和[SEP]等一些处理后，将其写入成TFrecord的形式。这是为了能在estimator里有一个更为高效和简易的读入。</p><p>在create_model的函数里，除了从modeling.py获取模型主干输出之外，还有进行fine-tune时候的loss计算。因此，如果对于fine-tune的结构有自定义的要求，可以在这部分对代码进行修改。如进行NER任务的时候，可以按照BERT论文里的方式，不只读第一位的logits，而是将每一位logits进行读取。</p><p>运行命令：</p><p>训练：</p><div class=\"highlight\"><pre><code class=\"language-text\">python run_classifier.py \\\n  --data_dir=$MY_DATASET \\\n  --task_name=sim \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --output_dir=/tmp/sim_model/ \\\n  --do_train=true \\\n  --do_eval=true \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --max_seq_length=128 \\\n  --train_batch_size=32 \\\n  --learning_rate=5e-5\\\n  --num_train_epochs=2.0</code></pre></div><p>预测：</p><div class=\"highlight\"><pre><code class=\"language-text\">python run_classifier.py \\\n  --task_name=sim \\\n  --do_predict=true \\\n  --data_dir=$MY_DATASET \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=/tmp/sim_model \\\n  --max_seq_length=128 \\\n  --output_dir=/tmp/output/\n当然，我们需要在data_dir下有测试数据，测试完成后会在output_dir路径下生成一个test_results.tsv文件，该文件包含了测试用例和相似度probabilities</code></pre></div><h2>MT-DNN</h2><a href=\"https://zhuanlan.zhihu.com/p/56640922\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\">机器之心：GLUE排行榜上全面超越BERT的模型近日公布了！</a><p>研究人员认为MTL和语言模型预训练是互补的技术，因此可以结合起来改进文本表示的学习，从而提高各种NLU任务的性能。他们将2015年做的一项多任务深度神经网络（MT-DNN）模型加以拓展，将BERT合并到模型的共享文本编码层。</p><p>较低层（即文本编码层）在所有任务中共享，而顶层是任务特定的，组合不同类型的NLU任务，如单句分类、成对文本分类、文本相似性和相关性排序。与BERT模型类似，MT-DNN分两个阶段进行训练：<b>预训练和微调</b>。与BERT不同的是，MT-DNN<b>在微调阶段使用MTL</b>，在其模型架构中具有多个任务特定层。</p><h2>XLM: 跨语言版的BERT</h2><p><a href=\"https://zhuanlan.zhihu.com/p/56314795\" class=\"internal\">跨语言版BERT：Facebook提出跨语言预训练模型XLM</a></p><p>预训练语言模型：</p><ul><li>因果语言模型（CLM）—单语言</li><li>BERT 中通过掩码训练的语言模型（MLM）—单语言</li><li>翻译语言模型（TLM）—跨语言</li></ul><p>通过XNLI-15模型生成跨语言句子表征</p><p><a href=\"https://zhuanlan.zhihu.com/p/56314795\" class=\"internal\">跨语言版BERT：Facebook提出跨语言预训练模型XLM</a></p><h2>参考</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/fz-bQMAi5bs2_bvRhf3ERg\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史</a></p><p><a href=\"https://zhuanlan.zhihu.com/p/47488095\" class=\"internal\">BLMo迁移学习的理解不错 NLP的游戏规则从此改写？从word2vec, ELMo到BERT</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/LGJvvhotSg7XMn8mg3TZUw\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">预训练在自然语言处理的发展: 从Word Embedding到BERT模型 ppt</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzIwMTc4ODE0Mw%3D%3D%26mid%3D2247492317%26idx%3D1%26sn%3De823a75d9463257ed9ea7b3e4677c1ae%26chksm%3D96ea3d5da19db44be0872ff4e29043aa72c7a624a116196bfeeca092a15f9209d7cf8ce46eb5%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">自然语言处理中的语言模型预训练方法</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/robert-dlut/p/9824346.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">自然语言处理中的语言模型预训练方法</a></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "词向量", 
                    "tagLink": "https://api.zhihu.com/topics/20142325"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_1058754636478992384"
}
