{
    "title": "机器学习和chatbot备忘录", 
    "description": "", 
    "followers": [
        "https://www.zhihu.com/people/ren-pang-lu-zi-hu", 
        "https://www.zhihu.com/people/mo-yi-feng-49", 
        "https://www.zhihu.com/people/Micro-Kun", 
        "https://www.zhihu.com/people/wurentidai", 
        "https://www.zhihu.com/people/szj-ws", 
        "https://www.zhihu.com/people/liang-de-peng", 
        "https://www.zhihu.com/people/wen-qiang-51-69", 
        "https://www.zhihu.com/people/yu-lu-81-96", 
        "https://www.zhihu.com/people/crazyvr", 
        "https://www.zhihu.com/people/woshibabaa", 
        "https://www.zhihu.com/people/lei-xiao-yu-49", 
        "https://www.zhihu.com/people/martinson", 
        "https://www.zhihu.com/people/tsin-97", 
        "https://www.zhihu.com/people/calvinjku", 
        "https://www.zhihu.com/people/qichao-tang", 
        "https://www.zhihu.com/people/fan-tao-rong-66", 
        "https://www.zhihu.com/people/zzx-47-7", 
        "https://www.zhihu.com/people/chen-yu-lin-56-22", 
        "https://www.zhihu.com/people/dan-yiyi-5", 
        "https://www.zhihu.com/people/chen-hao-99-99-23", 
        "https://www.zhihu.com/people/michael-wu-25", 
        "https://www.zhihu.com/people/oshixiaoxiliu", 
        "https://www.zhihu.com/people/zhou-mo-1-15-21", 
        "https://www.zhihu.com/people/lynn-10", 
        "https://www.zhihu.com/people/qin-zheng-72-24", 
        "https://www.zhihu.com/people/ge-ge-56-49", 
        "https://www.zhihu.com/people/zhe-da-gai-jiu-shi-ren-sheng-ba", 
        "https://www.zhihu.com/people/willwinworld", 
        "https://www.zhihu.com/people/su-su-92-9", 
        "https://www.zhihu.com/people/jellyzhang0909", 
        "https://www.zhihu.com/people/nicknerd", 
        "https://www.zhihu.com/people/zhou-shan-96-59", 
        "https://www.zhihu.com/people/benying", 
        "https://www.zhihu.com/people/meng-shi-kang-45", 
        "https://www.zhihu.com/people/moni-gg", 
        "https://www.zhihu.com/people/chu-yun-fei-42-59", 
        "https://www.zhihu.com/people/huang-shuai-4", 
        "https://www.zhihu.com/people/plusign", 
        "https://www.zhihu.com/people/ju-wa-wa", 
        "https://www.zhihu.com/people/sailfishc", 
        "https://www.zhihu.com/people/ping.love", 
        "https://www.zhihu.com/people/xll-27-13", 
        "https://www.zhihu.com/people/ni-zi-xin-49-94", 
        "https://www.zhihu.com/people/likaihere", 
        "https://www.zhihu.com/people/qi-kan-bi-ye-lun-wen-zhi-dao", 
        "https://www.zhihu.com/people/zpleefly", 
        "https://www.zhihu.com/people/yang-zhi-22-53", 
        "https://www.zhihu.com/people/GammaGo", 
        "https://www.zhihu.com/people/bao-gyro", 
        "https://www.zhihu.com/people/wang-yang-18-92", 
        "https://www.zhihu.com/people/qibinchuan", 
        "https://www.zhihu.com/people/ghostkit", 
        "https://www.zhihu.com/people/deepnlp", 
        "https://www.zhihu.com/people/SimpleJian", 
        "https://www.zhihu.com/people/riddle-49", 
        "https://www.zhihu.com/people/bao-feng-yu-8-76", 
        "https://www.zhihu.com/people/ck_welder", 
        "https://www.zhihu.com/people/tsw123-78", 
        "https://www.zhihu.com/people/xu-you-21-22", 
        "https://www.zhihu.com/people/jiang-hai-yun-92", 
        "https://www.zhihu.com/people/deng-bao-bao-19-43", 
        "https://www.zhihu.com/people/xros-liang", 
        "https://www.zhihu.com/people/gu-ge-70-33", 
        "https://www.zhihu.com/people/peng-sun-74", 
        "https://www.zhihu.com/people/tengfei-57", 
        "https://www.zhihu.com/people/yorkj", 
        "https://www.zhihu.com/people/yildhd-wang", 
        "https://www.zhihu.com/people/swayblue", 
        "https://www.zhihu.com/people/tom-pareto", 
        "https://www.zhihu.com/people/wang-meng-ya-68", 
        "https://www.zhihu.com/people/bai-gua-tai-lang", 
        "https://www.zhihu.com/people/lan-lan-lan-lan-96-82", 
        "https://www.zhihu.com/people/fu-jia-22-30", 
        "https://www.zhihu.com/people/xiaoerhei2016", 
        "https://www.zhihu.com/people/chen-fei-22-6", 
        "https://www.zhihu.com/people/ju-shang-38", 
        "https://www.zhihu.com/people/liu-meng-yuan-72-95", 
        "https://www.zhihu.com/people/yijia-zhao-24", 
        "https://www.zhihu.com/people/xiao-lin-99", 
        "https://www.zhihu.com/people/fang-hui-98", 
        "https://www.zhihu.com/people/xxt-44", 
        "https://www.zhihu.com/people/wang-yu-qiang-83", 
        "https://www.zhihu.com/people/wang-chen-pku", 
        "https://www.zhihu.com/people/Ni_Guo_Chen", 
        "https://www.zhihu.com/people/nao-zi-kong-kong-wu-suo-wei", 
        "https://www.zhihu.com/people/yanghongkai", 
        "https://www.zhihu.com/people/zhang-jia-xin-1203", 
        "https://www.zhihu.com/people/da-xue-wu-hen-7", 
        "https://www.zhihu.com/people/duan-xing-99", 
        "https://www.zhihu.com/people/Rhino666", 
        "https://www.zhihu.com/people/popoblue-23", 
        "https://www.zhihu.com/people/kevin-hill", 
        "https://www.zhihu.com/people/kin-zhi", 
        "https://www.zhihu.com/people/darrenguo", 
        "https://www.zhihu.com/people/yang-60-6-83", 
        "https://www.zhihu.com/people/zhong-er-cheng-xu-yuan", 
        "https://www.zhihu.com/people/advstock", 
        "https://www.zhihu.com/people/lin-bao", 
        "https://www.zhihu.com/people/an-ran-57", 
        "https://www.zhihu.com/people/qinkang-69", 
        "https://www.zhihu.com/people/blueman", 
        "https://www.zhihu.com/people/hao-zhao-73", 
        "https://www.zhihu.com/people/alphamore", 
        "https://www.zhihu.com/people/tan-wei-81", 
        "https://www.zhihu.com/people/zhou-zhen-yi", 
        "https://www.zhihu.com/people/wen-lei-11-25", 
        "https://www.zhihu.com/people/wang-yong-hao-32", 
        "https://www.zhihu.com/people/yang-ye-79", 
        "https://www.zhihu.com/people/tenyun", 
        "https://www.zhihu.com/people/Leo_Xu06", 
        "https://www.zhihu.com/people/mess-domain", 
        "https://www.zhihu.com/people/wei-chen-55-50", 
        "https://www.zhihu.com/people/zhi-hu-19-88", 
        "https://www.zhihu.com/people/guo-yi-46-22-13", 
        "https://www.zhihu.com/people/jamie-zjm", 
        "https://www.zhihu.com/people/ma-biao-47-71", 
        "https://www.zhihu.com/people/huang-huang-bu-bu-bu-zhi-he-chu-qu", 
        "https://www.zhihu.com/people/shu-sheng-da-chai", 
        "https://www.zhihu.com/people/miller-cool", 
        "https://www.zhihu.com/people/dou-zi-44-48-38", 
        "https://www.zhihu.com/people/yeu-yang", 
        "https://www.zhihu.com/people/qinlibo_nlp", 
        "https://www.zhihu.com/people/mou-yige-chi-huo", 
        "https://www.zhihu.com/people/tang-xiao-liang-72", 
        "https://www.zhihu.com/people/matthew.he", 
        "https://www.zhihu.com/people/wei-han-88", 
        "https://www.zhihu.com/people/abelard", 
        "https://www.zhihu.com/people/hui-fei-de-cheng-xu-yuan-7", 
        "https://www.zhihu.com/people/dong-feng-zao-ji", 
        "https://www.zhihu.com/people/astarfly", 
        "https://www.zhihu.com/people/xu-yun-kun-10", 
        "https://www.zhihu.com/people/huang-huang-huang-69-15", 
        "https://www.zhihu.com/people/chen-mo-63-83", 
        "https://www.zhihu.com/people/gong-pan-94"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/43340608", 
            "userName": "房海朔", 
            "userLink": "https://www.zhihu.com/people/fdfc406067c7444363225d963f3ab320", 
            "upvote": 4, 
            "title": "记一次多分类的炼丹过程", 
            "content": "<p><b>emm...  这是一次从数据抓取到搭建模型，模型调参的全过程。</b></p><h2><b>数据</b></h2><p>数据决定了模型的上限，这话一定要记着！！，多花点时间在数据上，要比上来一顿操作调参来的强。对于多分类问题，首先要观察的是<b>数据是否干净、类别是否平衡</b>。</p><ul><li>一般需要通过上采样或者下采样的方法使样本尽可能的平衡</li><li>采用fscore 的评判标准，详细的看到每个类别的recall，precision</li></ul><p>由于数据是我们从网站上爬下来的，所以存在很多标签标错的情况，当你用CNN，RNN的模型去训练的时候，你会发现自己的模型根本学不好，这时候你开始尝试各种模型，发现还是不行，然后各种debug，开始怀疑人生，怀疑自己，连一个分类都做不好！</p><p><b>所以数据很重要</b>。针对上述问题，最简单的方法就是误差分析，分析误差的来源，认真做好数据。</p><h2><b>模型</b></h2><p>对于主题的文本分类来讲，CNN的效果已经是公认的了，可以并行、收敛速度快，效果好。</p><p>1.<b>shadow CNN</b> 也是一种ngram的特征抽取，通过卷积操作提取特征，通过maxpooling抽取关键特征，通过全连接层挖掘关键特征之间的关系，最后通过softmax进行分类。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2.<b>deep CNN</b>,2017 ACL的一篇文章<a href=\"https://link.zhihu.com/?target=http%3A//www.aclweb.org/anthology/P17-1052\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">《Deep Pyramid Convolutional Neural Networks for Text Categorization》</a></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-30da52a44930d1717f1ae868472bfcd6_b.jpg\" data-size=\"normal\" data-rawwidth=\"269\" data-rawheight=\"321\" class=\"content_image\" width=\"269\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;269&#39; height=&#39;321&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"269\" data-rawheight=\"321\" class=\"content_image lazy\" width=\"269\" data-actualsrc=\"https://pic3.zhimg.com/v2-30da52a44930d1717f1ae868472bfcd6_b.jpg\"/><figcaption>DPCNN</figcaption></figure><p>主要思想是在shallow CNN 的基础上进行关键特征之间的特征组合，使得模型不仅仅是一个ngram的关键特征模型，更能够掌握语义信息，句法信息等。并且作者提出了先做activation再做线性运算，并且加了short cut以防止较深层次的CNN在nlp方面不work,难训练的问题。原文中作者使用的固定的feature map,这样的话，<b>保证尺寸不变可以不断的压缩左右两边各(n-1)/2的特征到同一个位置</b>，然后，通过stride为2的maxpooling 不断的降维，这样期望获得全局强特征，并且可以降低训练难度。当然可以加入batch norm进去，我自己做了实验发现加入batch norm 还是有了提升。</p><p> 3.<b>RCNN</b>，RCNN也是期望通过结合CNN特征抽取优点和RNN的时序特点，希望获得对分类有益的全局特征。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5b35cc8fc4a46df81cff68cc6fe50d69_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1388\" data-rawheight=\"539\" class=\"origin_image zh-lightbox-thumb\" width=\"1388\" data-original=\"https://pic2.zhimg.com/v2-5b35cc8fc4a46df81cff68cc6fe50d69_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1388&#39; height=&#39;539&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1388\" data-rawheight=\"539\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1388\" data-original=\"https://pic2.zhimg.com/v2-5b35cc8fc4a46df81cff68cc6fe50d69_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5b35cc8fc4a46df81cff68cc6fe50d69_b.jpg\"/></figure><p>这边的卷积层或者叫做特征提取层是以recurrent的形式，拼接当前的embedding，并通过全连接进行的。这个方法进行了RNN的操作，如果文本很长时，收敛较慢。</p><p>4.<b>BiLSTM+attention</b>，RNN模型对于主题文本分类这种情况来说，效果不会差也不会很好，对于一些时序信息较强的任务，例如分词、实体识别，RNN还是可以胜任的。</p><h2><b>模型训练及调参</b></h2><ul><li>对于不同的激活函数选择不同的参数初始化的方式，比较重要的一点就是方差设置的不能太大，也不能太小，比如Tanh，如果方差设置太大会造成梯度饱和，更新缓慢。</li><li>优化方法的选择：</li></ul><p>     一般没有什么把握的选择自适应的Adam，但我在rcnn上测试，用SGD配合lr 从1.0或者0.1开始衰减，会达到比Adam较好的结果。</p><ul><li>针对于RNN的模型可以尝试加入gradient_clip，会有较好的效果，对于CNN的模型加入batch norm，实测有好的效果。</li><li>还有一些对于embedding_size,hidden_size,batch_size的调参可以去尝试，在可训练的embedding中，可以去比较200dim、100dim对效果的影响。</li><li>针对过拟合问题，这个问题大家都很懂了，可以设置min_count,l2正则,dropout，停用词和标点符号是否需要去除得看具体问题，复杂模型可以换成简单模型。</li><li>针对到底是使用字还是词，个人感觉如果分词器分不准的话还不如用字，端到端的还是可以学习到。</li><li><b>最重要的一点，模型的复杂度要和数据匹配！！！，有的时候只需要一层cnn，甚至激活函数都不要，再加个分类层，一个简单的线性模型，就可以达到很多复杂模型达不到的效果。</b></li></ul><p></p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/42686533", 
            "userName": "房海朔", 
            "userLink": "https://www.zhihu.com/people/fdfc406067c7444363225d963f3ab320", 
            "upvote": 57, 
            "title": "从HMM推导CRF", 
            "content": "<blockquote>本文主要是从HMM到CRF的推导，对李宏毅老师的课作个备忘。</blockquote><p><b>HMM的二个假设：</b></p><ul><li><b>齐次马尔科夫性假设：   </b></li></ul><p><img src=\"https://www.zhihu.com/equation?tex=P%28i_%7Bt%7D%7Ci_%7Bt-1%7D%2Co_%7Bt-1%7D...%2Ci_%7B1%7D%2Co_%7B1%7D%29%3DP%28i_%7Bt%7D%7Ci_%7Bt-1%7D%29%2C+t%3D1%2C2...T\" alt=\"P(i_{t}|i_{t-1},o_{t-1}...,i_{1},o_{1})=P(i_{t}|i_{t-1}), t=1,2...T\" eeimg=\"1\"/> </p><p>      即t时刻的状态只依赖于前一时刻的状态，与其他时刻的状态和观测无关，并且与时刻t无关。</p><ul><li><b>观测独立性假设</b>：</li></ul><p><img src=\"https://www.zhihu.com/equation?tex=P%28o_%7Bt%7D%7Ci_%7BT%7D%2Co_%7BT%7D%2Ci_%7BT-1%7D%2Co_%7BT-1%7D%2C...%2Ci_%7B1%7D%2Co_%7B1%7D%29%3DP%28o_%7Bt%7D%7Ci_%7Bt%7D%29\" alt=\"P(o_{t}|i_{T},o_{T},i_{T-1},o_{T-1},...,i_{1},o_{1})=P(o_{t}|i_{t})\" eeimg=\"1\"/> </p><p>即t时刻的观测仅与t时刻的状态。</p><p><b>HMM 是生成式模型，所以是对P(x,y)联合概率分布进行建模</b>，针对具体问题：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-2ac964b395c40b245f6d323b55aa9025_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"378\" data-rawheight=\"121\" class=\"content_image\" width=\"378\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;378&#39; height=&#39;121&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"378\" data-rawheight=\"121\" class=\"content_image lazy\" width=\"378\" data-actualsrc=\"https://pic2.zhimg.com/v2-2ac964b395c40b245f6d323b55aa9025_b.jpg\"/></figure><p>其中，y是状态变量，x是观测变量，根据联合概率的公式可以写成P(x,y)=P(y)P(x|y)。</p><p>根据二个假设可以展开写成：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-fe715540ebca63a4d68164366a359550_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"412\" data-rawheight=\"112\" class=\"content_image\" width=\"412\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;412&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"412\" data-rawheight=\"112\" class=\"content_image lazy\" width=\"412\" data-actualsrc=\"https://pic1.zhimg.com/v2-fe715540ebca63a4d68164366a359550_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d7d6aaba4f2318e0f9ee42d60a19e20e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"480\" data-rawheight=\"85\" class=\"origin_image zh-lightbox-thumb\" width=\"480\" data-original=\"https://pic3.zhimg.com/v2-d7d6aaba4f2318e0f9ee42d60a19e20e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;480&#39; height=&#39;85&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"480\" data-rawheight=\"85\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"480\" data-original=\"https://pic3.zhimg.com/v2-d7d6aaba4f2318e0f9ee42d60a19e20e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d7d6aaba4f2318e0f9ee42d60a19e20e_b.jpg\"/></figure><p>蓝色下划线代表的状态转移的那一部分的概率，红色部分表示的是发射概率，<b>可以看出HMM的状态转移和发射部分是分开建模的，crf是有特征函数，将这两个合在一起建模的，这是crf的好处。</b></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>从HMM到CRF</b></h2><p>对P(x,y)取对数：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-39bbdb8e9b75a8d6382e392d07d59ebf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"491\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb\" width=\"491\" data-original=\"https://pic4.zhimg.com/v2-39bbdb8e9b75a8d6382e392d07d59ebf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;491&#39; height=&#39;126&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"491\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"491\" data-original=\"https://pic4.zhimg.com/v2-39bbdb8e9b75a8d6382e392d07d59ebf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-39bbdb8e9b75a8d6382e392d07d59ebf_b.jpg\"/></figure><p>其中可以进行改写：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5da0a80cc7f637b81e74e35c3b979d65_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"358\" data-rawheight=\"99\" class=\"content_image\" width=\"358\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;358&#39; height=&#39;99&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"358\" data-rawheight=\"99\" class=\"content_image lazy\" width=\"358\" data-actualsrc=\"https://pic2.zhimg.com/v2-5da0a80cc7f637b81e74e35c3b979d65_b.jpg\"/></figure><p>P(t|s)表示标注s 生成word t的<b>概率</b>，N(s,t)表示标注 s生成word t的<b>次数</b>，已经很像crf中特征函数和权重的相乘了。</p><p>此处是一个例子：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4f934b3442d3dd7cc8d15ffb37f49fdb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"502\" data-rawheight=\"269\" class=\"origin_image zh-lightbox-thumb\" width=\"502\" data-original=\"https://pic4.zhimg.com/v2-4f934b3442d3dd7cc8d15ffb37f49fdb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;502&#39; height=&#39;269&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"502\" data-rawheight=\"269\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"502\" data-original=\"https://pic4.zhimg.com/v2-4f934b3442d3dd7cc8d15ffb37f49fdb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4f934b3442d3dd7cc8d15ffb37f49fdb_b.jpg\"/></figure><p>对每一项都进行这样的改写，可以得到：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ca2a9777ac122e90375d6dfa3fc8591c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"537\" data-rawheight=\"322\" class=\"origin_image zh-lightbox-thumb\" width=\"537\" data-original=\"https://pic1.zhimg.com/v2-ca2a9777ac122e90375d6dfa3fc8591c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;537&#39; height=&#39;322&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"537\" data-rawheight=\"322\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"537\" data-original=\"https://pic1.zhimg.com/v2-ca2a9777ac122e90375d6dfa3fc8591c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ca2a9777ac122e90375d6dfa3fc8591c_b.jpg\"/></figure><p>这样，此处的<b>对数概率</b>就变成了<b>权重</b>， <img src=\"https://www.zhihu.com/equation?tex=N_%7Bs%2Ct%7D%28x%2Cy%29\" alt=\"N_{s,t}(x,y)\" eeimg=\"1\"/> 表示成了<b>特征函数</b>，<b>对于crf来说可以不仅仅是由动词后面接名词这种特征，它可以考虑ngram的特征，还有一些其他自己定义的特征，然后去求解对应的权重。</b></p><p>这样P(x,y)写成下面的式子：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f09e78ce00c3da7cff297f9ed546c0cc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"257\" data-rawheight=\"48\" class=\"content_image\" width=\"257\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;257&#39; height=&#39;48&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"257\" data-rawheight=\"48\" class=\"content_image lazy\" width=\"257\" data-actualsrc=\"https://pic1.zhimg.com/v2-f09e78ce00c3da7cff297f9ed546c0cc_b.jpg\"/></figure><p><b>crf的公式就可以表示出来：</b></p><p><img src=\"https://www.zhihu.com/equation?tex=P%28y%7Cx%29+%3D+%5Cfrac%7BP%28x%2Cy%29%7D%7BP%28x%29%7D+%3D+%5Cfrac%7Bw.%5Cphi%28x%2Cy%29%7D%7BZ%7D\" alt=\"P(y|x) = \\frac{P(x,y)}{P(x)} = \\frac{w.\\phi(x,y)}{Z}\" eeimg=\"1\"/> </p><p>将 <img src=\"https://www.zhihu.com/equation?tex=w.%5Cphi%28x%2Cy%29\" alt=\"w.\\phi(x,y)\" eeimg=\"1\"/> 展开可以写成标准的crf公式：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0a9e286e594768ddd4b74a4d82f54cec_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"541\" data-rawheight=\"88\" class=\"origin_image zh-lightbox-thumb\" width=\"541\" data-original=\"https://pic1.zhimg.com/v2-0a9e286e594768ddd4b74a4d82f54cec_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;541&#39; height=&#39;88&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"541\" data-rawheight=\"88\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"541\" data-original=\"https://pic1.zhimg.com/v2-0a9e286e594768ddd4b74a4d82f54cec_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0a9e286e594768ddd4b74a4d82f54cec_b.jpg\"/></figure><p>其中 <img src=\"https://www.zhihu.com/equation?tex=t_%7Bk%7D\" alt=\"t_{k}\" eeimg=\"1\"/> , <img src=\"https://www.zhihu.com/equation?tex=s_%7Bl%7D\" alt=\"s_{l}\" eeimg=\"1\"/> 是特征函数， <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bk%7D\" alt=\"\\theta_{k}\" eeimg=\"1\"/> , <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bl%7D\" alt=\"\\theta_{l}\" eeimg=\"1\"/> 是权重。</p><p><b>这样由HMM导出了CRF</b>。</p><p>对于crf的训练，自然是更新每种特征的权重，使得表现最好，使得P(y|x)最大,</p><p>最终求导得到的公式如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-87e04d3128e45b0a03c9d4a2259ec090_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"441\" data-rawheight=\"84\" class=\"origin_image zh-lightbox-thumb\" width=\"441\" data-original=\"https://pic1.zhimg.com/v2-87e04d3128e45b0a03c9d4a2259ec090_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;441&#39; height=&#39;84&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"441\" data-rawheight=\"84\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"441\" data-original=\"https://pic1.zhimg.com/v2-87e04d3128e45b0a03c9d4a2259ec090_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-87e04d3128e45b0a03c9d4a2259ec090_b.jpg\"/></figure><p>可以利用此导数对不同的特征进行权重的更新。</p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "CRFs", 
                    "tagLink": "https://api.zhihu.com/topics/19591743"
                }
            ], 
            "comments": [
                {
                    "userName": "一苇", 
                    "userLink": "https://www.zhihu.com/people/e97b1691d0623246f4f6dbb3708220eb", 
                    "content": "<p>请问有李宏毅老师的这个课程的视频与PPT资料吗？在哪儿找呢</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "房海朔", 
                            "userLink": "https://www.zhihu.com/people/fdfc406067c7444363225d963f3ab320", 
                            "content": "b站这个神奇的地方你不知道嘛😄", 
                            "likes": 0, 
                            "replyToAuthor": "一苇"
                        }, 
                        {
                            "userName": "一苇", 
                            "userLink": "https://www.zhihu.com/people/e97b1691d0623246f4f6dbb3708220eb", 
                            "content": "<p>我之前在b站看了李宏毅老师的深度学习和机器学习，还真没看过他的概率图模型，不知道能不能指一条明路，感激不尽</p>", 
                            "likes": 0, 
                            "replyToAuthor": "房海朔"
                        }
                    ]
                }, 
                {
                    "userName": "tyhj123", 
                    "userLink": "https://www.zhihu.com/people/89a094b70df28e323e76fea69e42ae64", 
                    "content": "<p>原来是这么过来的，学习了，谢谢</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "suenpun", 
                    "userLink": "https://www.zhihu.com/people/bab0cac0c0be62028671ed895d6bd3af", 
                    "content": "<p>牛逼呀 ~~ </p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/40402713", 
            "userName": "房海朔", 
            "userLink": "https://www.zhihu.com/people/fdfc406067c7444363225d963f3ab320", 
            "upvote": 21, 
            "title": "GAN、WGAN通俗基本原理", 
            "content": "<h2><b>一、基本的GAN</b></h2><p><b>GAN 的主要思想：通过神经网络去模拟现有数据的分布（极大似然估计），从而达到生成的目的。优化方法：通过判别器不断优化。</b></p><p>给定某分布 <img src=\"https://www.zhihu.com/equation?tex=P_%7Bdata%7D\" alt=\"P_{data}\" eeimg=\"1\"/> ,利用 <img src=\"https://www.zhihu.com/equation?tex=P_%7BG%7D%28x%3B%5Ctheta%29\" alt=\"P_{G}(x;\\theta)\" eeimg=\"1\"/> 模拟 <img src=\"https://www.zhihu.com/equation?tex=P_%7Bdata%7D\" alt=\"P_{data}\" eeimg=\"1\"/> 分布。</p><p>从<img src=\"https://www.zhihu.com/equation?tex=P_%7Bdata%7D\" alt=\"P_{data}\" eeimg=\"1\"/>中sample出 <img src=\"https://www.zhihu.com/equation?tex=x%5E%7B1%7D%2Cx%5E%7B2%7D%2Cx%5E%7B3%7D....x%5E%7Bn%7D\" alt=\"x^{1},x^{2},x^{3}....x^{n}\" eeimg=\"1\"/> 数据，优化 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 使得L最大：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4bd74f75c7dbbf46e23c1023ccff82a5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"209\" data-rawheight=\"77\" class=\"content_image\" width=\"209\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;209&#39; height=&#39;77&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"209\" data-rawheight=\"77\" class=\"content_image lazy\" width=\"209\" data-actualsrc=\"https://pic2.zhimg.com/v2-4bd74f75c7dbbf46e23c1023ccff82a5_b.jpg\"/></figure><p>对等式两边取log,取完对数后变成了相加，由于x从Pdata中Sample出来的，所以就可以写成积分的形式，写成求期望的形式就是下图中的约等式,然后再加上<b>Pdata的信息熵</b>（下式中为减去，是因为把符号提到了前面，变成了减号，<b>这一项和 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 无关，不影响结果</b>），这一项看起来有点像KL散度了，或者叫相对熵。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-117ce91c3f7e63b6d21886d915157781_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"984\" data-rawheight=\"453\" class=\"origin_image zh-lightbox-thumb\" width=\"984\" data-original=\"https://pic2.zhimg.com/v2-117ce91c3f7e63b6d21886d915157781_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;984&#39; height=&#39;453&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"984\" data-rawheight=\"453\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"984\" data-original=\"https://pic2.zhimg.com/v2-117ce91c3f7e63b6d21886d915157781_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-117ce91c3f7e63b6d21886d915157781_b.jpg\"/></figure><p>上式来说极大似然函数很难去优化，怎么样去优化这个 <img src=\"https://www.zhihu.com/equation?tex=P_%7BG%7D\" alt=\"P_{G}\" eeimg=\"1\"/> ,GAN解决了这个问题。<b>利用一个判别器去衡量PG生成的数据和真实数据之间的差距，然后不断优化判别器和生成器，直至判别器判别不出来。</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d0f9d1a83bed68a57ba3f3664fc98aec_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"521\" data-rawheight=\"383\" class=\"origin_image zh-lightbox-thumb\" width=\"521\" data-original=\"https://pic1.zhimg.com/v2-d0f9d1a83bed68a57ba3f3664fc98aec_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;521&#39; height=&#39;383&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"521\" data-rawheight=\"383\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"521\" data-original=\"https://pic1.zhimg.com/v2-d0f9d1a83bed68a57ba3f3664fc98aec_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d0f9d1a83bed68a57ba3f3664fc98aec_b.jpg\"/></figure><p>其中V(G,D)为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-438728607953a2a9c25acab55ab59a85_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"467\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb\" width=\"467\" data-original=\"https://pic2.zhimg.com/v2-438728607953a2a9c25acab55ab59a85_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;467&#39; height=&#39;92&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"467\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"467\" data-original=\"https://pic2.zhimg.com/v2-438728607953a2a9c25acab55ab59a85_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-438728607953a2a9c25acab55ab59a85_b.jpg\"/></figure><p>V(G,D)到达最大的意思就是说，把来自真实分布的数据和来自生成器生成的数据分辨开，式子中的对应关系为，前一项表示来自Pdata的数据，D(x)的值要大，后一项表示来自Pg的数据，D(x)要小，这样才能到达最大，maxV(G,D)是用来优化判别器的。而对于给定的不同的G(生成器)，每个G都会有一个maxV(G,D)，然后再在不同的G中，选择一个使得maxV(G,D)最小的那个G,这一步用来优化生成器，这是上图G = arg min maxV(G,D)的含义。理解了的话，下图G1,G2,G3应该选择第三个。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7ffc48d71ac2eb2d7182f526223a6f58_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"541\" data-rawheight=\"188\" class=\"origin_image zh-lightbox-thumb\" width=\"541\" data-original=\"https://pic1.zhimg.com/v2-7ffc48d71ac2eb2d7182f526223a6f58_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;541&#39; height=&#39;188&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"541\" data-rawheight=\"188\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"541\" data-original=\"https://pic1.zhimg.com/v2-7ffc48d71ac2eb2d7182f526223a6f58_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7ffc48d71ac2eb2d7182f526223a6f58_b.jpg\"/></figure><p>根据上面的思想，优化就分为三步 <b>1.优化判别器，2.优化生成器，3.repeat</b></p><p><b>1.优化判别器</b>：对于给定生成器下，求解最优的D，即求解maxV(G,D)</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-dc41752ca7b2bba444de88a9369ef46e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"459\" data-rawheight=\"66\" class=\"origin_image zh-lightbox-thumb\" width=\"459\" data-original=\"https://pic3.zhimg.com/v2-dc41752ca7b2bba444de88a9369ef46e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;459&#39; height=&#39;66&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"459\" data-rawheight=\"66\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"459\" data-original=\"https://pic3.zhimg.com/v2-dc41752ca7b2bba444de88a9369ef46e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-dc41752ca7b2bba444de88a9369ef46e_b.jpg\"/></figure><p>上式，只有一个未知数D，上式对D求导，可得最大时，D为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ef9cab41e9a493bf3342618980df1632_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"282\" data-rawheight=\"78\" class=\"content_image\" width=\"282\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;282&#39; height=&#39;78&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"282\" data-rawheight=\"78\" class=\"content_image lazy\" width=\"282\" data-actualsrc=\"https://pic3.zhimg.com/v2-ef9cab41e9a493bf3342618980df1632_b.jpg\"/></figure><p>再把D带回V(G,D)得到maxV(G,D)，下面就是关于G的式子:</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-61048e3815ce0d102823d8431813b4d1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"584\" data-rawheight=\"407\" class=\"origin_image zh-lightbox-thumb\" width=\"584\" data-original=\"https://pic2.zhimg.com/v2-61048e3815ce0d102823d8431813b4d1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;584&#39; height=&#39;407&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"584\" data-rawheight=\"407\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"584\" data-original=\"https://pic2.zhimg.com/v2-61048e3815ce0d102823d8431813b4d1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-61048e3815ce0d102823d8431813b4d1_b.jpg\"/></figure><p>整理，得到关于KL散度的式子：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ef56af031588fbd7b7ffa4b4d9577b0a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"573\" data-rawheight=\"254\" class=\"origin_image zh-lightbox-thumb\" width=\"573\" data-original=\"https://pic3.zhimg.com/v2-ef56af031588fbd7b7ffa4b4d9577b0a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;573&#39; height=&#39;254&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"573\" data-rawheight=\"254\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"573\" data-original=\"https://pic3.zhimg.com/v2-ef56af031588fbd7b7ffa4b4d9577b0a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ef56af031588fbd7b7ffa4b4d9577b0a_b.jpg\"/></figure><p>由于JS散度的定义是P，Q和两者平均值的散度，所以上式可以化成</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8016da7c8c9c69e99edc2e979f7f1b07_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"312\" data-rawheight=\"101\" class=\"content_image\" width=\"312\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;312&#39; height=&#39;101&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"312\" data-rawheight=\"101\" class=\"content_image lazy\" width=\"312\" data-actualsrc=\"https://pic4.zhimg.com/v2-8016da7c8c9c69e99edc2e979f7f1b07_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-258cd8a5b9f64c881598b3d4bc3462e0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"547\" data-rawheight=\"64\" class=\"origin_image zh-lightbox-thumb\" width=\"547\" data-original=\"https://pic1.zhimg.com/v2-258cd8a5b9f64c881598b3d4bc3462e0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;547&#39; height=&#39;64&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"547\" data-rawheight=\"64\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"547\" data-original=\"https://pic1.zhimg.com/v2-258cd8a5b9f64c881598b3d4bc3462e0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-258cd8a5b9f64c881598b3d4bc3462e0_b.jpg\"/></figure><p>JS散度的取值范围在（0，log2）之间，所以V（ <img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> ,<img src=\"https://www.zhihu.com/equation?tex=D%5E%7B%2A%7D\" alt=\"D^{*}\" eeimg=\"1\"/> ）取值在（-2log2,0）之间。</p><p><b>2.优化生成器</b></p><p>上步求解出了判别器最优的值，并带回原式，<b>接下来V（ <img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> ,<img src=\"https://www.zhihu.com/equation?tex=D%5E%7B%2A%7D\" alt=\"D^{*}\" eeimg=\"1\"/> ）只和生成器有关</b>，<b>优化生成器，也就是求解使得真实分布和生成的分布差异最小的生成器。</b></p><p><b>因为V（ <img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> ,<img src=\"https://www.zhihu.com/equation?tex=D%5E%7B%2A%7D\" alt=\"D^{*}\" eeimg=\"1\"/> ）关于G的函数，求最小值，用梯度下降法：</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-679a3c34324e7863f7bb2f31b2665eec_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"97\" class=\"origin_image zh-lightbox-thumb\" width=\"458\" data-original=\"https://pic1.zhimg.com/v2-679a3c34324e7863f7bb2f31b2665eec_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;458&#39; height=&#39;97&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"97\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"458\" data-original=\"https://pic1.zhimg.com/v2-679a3c34324e7863f7bb2f31b2665eec_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-679a3c34324e7863f7bb2f31b2665eec_b.jpg\"/></figure><p>其中L(G)为V（ <img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> ,<img src=\"https://www.zhihu.com/equation?tex=D%5E%7B%2A%7D\" alt=\"D^{*}\" eeimg=\"1\"/> ）。V（ <img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> ,<img src=\"https://www.zhihu.com/equation?tex=D%5E%7B%2A%7D\" alt=\"D^{*}\" eeimg=\"1\"/> ）也就是js散度,更新G去减小js散度。</p><p><b>在实际操作中</b>：</p><p>(1).初始化 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bd%7D\" alt=\"\\theta_{d}\" eeimg=\"1\"/> ,<img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bg%7D\" alt=\"\\theta_{g}\" eeimg=\"1\"/></p><p>(2).每次迭代时：</p><p>            从 <img src=\"https://www.zhihu.com/equation?tex=P_%7Bdata%7D%28x%29\" alt=\"P_{data}(x)\" eeimg=\"1\"/> 中sample出m个样本｛<img src=\"https://www.zhihu.com/equation?tex=x%5E%7B1%7D%2Cx%5E%7B2%7D%2Cx%5E%7B3%7D....x%5E%7Bm%7D\" alt=\"x^{1},x^{2},x^{3}....x^{m}\" eeimg=\"1\"/>｝，从<img src=\"https://www.zhihu.com/equation?tex=P_%7Bprior%7D%28z%29\" alt=\"P_{prior}(z)\" eeimg=\"1\"/>中sample出m个noise样本，输入生成器中获得｛<img src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7Bx%7D%5E%7B1%7D%2C%5Ctilde%7Bx%7D%5E%7B2%7D%2C%5Ctilde%7Bx%7D%5E%7B3%7D+....+%5Ctilde%7Bx%7D%5E%7Bm%7D\" alt=\"\\tilde{x}^{1},\\tilde{x}^{2},\\tilde{x}^{3} .... \\tilde{x}^{m}\" eeimg=\"1\"/>｝， <img src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7Bx%7D+%3D+G%28z%3B%5Ctheta%29\" alt=\"\\tilde{x} = G(z;\\theta)\" eeimg=\"1\"/> </p><p>     更新判别器的参数<img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bd%7D\" alt=\"\\theta_{d}\" eeimg=\"1\"/>,使判别结果最大化：</p><p>               V= <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%7Blog%28D%28x%5Ei%29%29%7D%2B%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%7Blog%281-D%28%5Ctilde%7Bx%7D%5Ei%29%29%7D\" alt=\"\\frac{1}{m}\\sum_{i=1}^{m}{log(D(x^i))}+\\frac{1}{m}\\sum_{i=1}^{m}{log(1-D(\\tilde{x}^i))}\" eeimg=\"1\"/> </p><p>对上式求导，利用梯度上升更新<img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bd%7D\" alt=\"\\theta_{d}\" eeimg=\"1\"/>。<b>上述步骤是为了更新判别器</b>，使判别器效果更好。<b>需要重复k次</b>，为了可以使得V收敛，这样可以保证找到maxV(G,D),。</p><p><b>更新生成器</b>：</p><p>  V= <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%7Blog%28D%28x%5Ei%29%29%7D%2B%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%7Blog%281-D%28G%28z%5Ei%29%29%7D\" alt=\"\\frac{1}{m}\\sum_{i=1}^{m}{log(D(x^i))}+\\frac{1}{m}\\sum_{i=1}^{m}{log(1-D(G(z^i))}\" eeimg=\"1\"/></p><p>更新生成器和前面一项没有关系，只和后面一项有关，<b>后面一项log(1-x)的图像在x接近0的时候，趋近于0。不利于开始的更新，所以换成-log(x)，图像的趋势都是一致的，在x接近0时，梯度很大，有利于更新。所以优化目标变成了下面的式子：</b></p><p>V = <img src=\"https://www.zhihu.com/equation?tex=-%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%7Blog%28D%28G%28z%5Ei%29%29%7D\" alt=\"-\\frac{1}{m}\\sum_{i=1}^{m}{log(D(G(z^i))}\" eeimg=\"1\"/> </p><p>对上式求导，利用梯度下降去更新 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bg%7D\" alt=\"\\theta_{g}\" eeimg=\"1\"/>。</p><p><b>基本GAN存在的问题</b>：</p><p><b>1.两个分布很远时JS散度都是log2，没有渐变的过程，不利于梯度更新</b></p><p><b>2.判别器训练的太强，会使得JS散度都是log2，参照第一条，不利于更新，可能需要加点噪声进去才可以。</b></p><p><b>3.只能学到一部分的分布，不能学到全部的分布。</b></p><hr/><h2><b>二、WGAN</b></h2><p><b>WGAN的优化，没有使用JS散度</b></p><p><b>WGAN 的优化公式为：</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ccfe3c5dda668d869b6d2e2693ce41a9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"566\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb\" width=\"566\" data-original=\"https://pic2.zhimg.com/v2-ccfe3c5dda668d869b6d2e2693ce41a9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;566&#39; height=&#39;102&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"566\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"566\" data-original=\"https://pic2.zhimg.com/v2-ccfe3c5dda668d869b6d2e2693ce41a9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ccfe3c5dda668d869b6d2e2693ce41a9_b.jpg\"/></figure><p><b>限定判别器的范围（若是不给上界的话，优化判别器时会使得来自 <img src=\"https://www.zhihu.com/equation?tex=P_%7Bdata%7D%E5%92%8CP_%7BG%7D\" alt=\"P_{data}和P_{G}\" eeimg=\"1\"/> 的数据分的很开，不利于优化。）</b></p><p><b>判别器要符合1-Lipschitz函数</b></p><p><b>Lipschitz Function:</b></p><p><b>    ||f(x1)-f(x2)||</b> <img src=\"https://www.zhihu.com/equation?tex=%5Cleq\" alt=\"\\leq\" eeimg=\"1\"/> K||x1-x2||,k=1为1-Lipschitz</p><p>就是输出的变化给了一个上界。</p><p>实际操作是加上weight clip限值权重的范围在（-c,c）之间。</p><p><b>相比较于基本的GAN 更新的地方在去掉了所有的log，做了cilp</b>：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-61ba6fd44962cbbefa6cee739d057360_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"586\" data-rawheight=\"418\" class=\"origin_image zh-lightbox-thumb\" width=\"586\" data-original=\"https://pic1.zhimg.com/v2-61ba6fd44962cbbefa6cee739d057360_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;586&#39; height=&#39;418&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"586\" data-rawheight=\"418\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"586\" data-original=\"https://pic1.zhimg.com/v2-61ba6fd44962cbbefa6cee739d057360_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-61ba6fd44962cbbefa6cee739d057360_b.jpg\"/></figure><hr/><h2><b>三、Improve GAN</b></h2><p><b>improve GAN 对WGAN 进行了优化，WGAN的weight clip会出现方正边角的问题，就像1正则。</b></p><p><b>Improve GAN 的优化目标为：</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-62a98afd1d8150a6d23334b1ba5a49ca_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"570\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb\" width=\"570\" data-original=\"https://pic3.zhimg.com/v2-62a98afd1d8150a6d23334b1ba5a49ca_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;570&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"570\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"570\" data-original=\"https://pic3.zhimg.com/v2-62a98afd1d8150a6d23334b1ba5a49ca_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-62a98afd1d8150a6d23334b1ba5a49ca_b.jpg\"/></figure><p><b>将1-Lipschitz变成了惩罚项，</b>会使得D(x)关于所有x的斜率绝对值之和都会接近1，这样才能保证优化时，惩罚项为0，优化目标最大。而其中x-penalty在Pdata和Pg的连线之间。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-eb0eaf1e289857c6456848aadb37b09d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"423\" data-rawheight=\"127\" class=\"origin_image zh-lightbox-thumb\" width=\"423\" data-original=\"https://pic2.zhimg.com/v2-eb0eaf1e289857c6456848aadb37b09d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;423&#39; height=&#39;127&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"423\" data-rawheight=\"127\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"423\" data-original=\"https://pic2.zhimg.com/v2-eb0eaf1e289857c6456848aadb37b09d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-eb0eaf1e289857c6456848aadb37b09d_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "生成对抗网络（GAN）", 
                    "tagLink": "https://api.zhihu.com/topics/20070859"
                }
            ], 
            "comments": [
                {
                    "userName": "管天气的小小熊", 
                    "userLink": "https://www.zhihu.com/people/f83f1aec2b9fab9845119282f58923c0", 
                    "content": "看到极大似然估计后，直接跳到最后评论了😓数学不适合我", 
                    "likes": 1, 
                    "childComments": [
                        {
                            "userName": "房海朔", 
                            "userLink": "https://www.zhihu.com/people/fdfc406067c7444363225d963f3ab320", 
                            "content": "<p>23333.....概率统计里面的,其实没关系，主要能看懂他需要解决的问题、思路和流程就好了</p>", 
                            "likes": 0, 
                            "replyToAuthor": "管天气的小小熊"
                        }
                    ]
                }, 
                {
                    "userName": "AlexZhao", 
                    "userLink": "https://www.zhihu.com/people/0f28fad8de5317441c8911fbfc541e8c", 
                    "content": "看完：干！我干！这都是啥？", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/39892787", 
            "userName": "房海朔", 
            "userLink": "https://www.zhihu.com/people/fdfc406067c7444363225d963f3ab320", 
            "upvote": 2, 
            "title": "word2vec 备忘录", 
            "content": "<p>word2vec有两种模型 : CBOW和Skipgram,加速方式也有两种 : Hierarchical Softmax和Negative Smaple。</p><p><b>1.CBOW+Hierarchical Softmax</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-397460da3f1f4689b7d3a26fa7db3e95_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1036\" data-rawheight=\"655\" class=\"origin_image zh-lightbox-thumb\" width=\"1036\" data-original=\"https://pic2.zhimg.com/v2-397460da3f1f4689b7d3a26fa7db3e95_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1036&#39; height=&#39;655&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1036\" data-rawheight=\"655\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1036\" data-original=\"https://pic2.zhimg.com/v2-397460da3f1f4689b7d3a26fa7db3e95_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-397460da3f1f4689b7d3a26fa7db3e95_b.jpg\"/></figure><p>流程：</p><p>1.统计词频，过滤频次出现少的，建立哈夫曼树；</p><p>2.初始化窗口词内的词向量前后各c个（上下文的词），经过投影层相加，根据哈夫曼树的编码寻找路径，每个分叉口可以视为二分类，采用逻辑回归进行分类：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b68561bea876df81926384fb705d71fe_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"561\" data-rawheight=\"113\" class=\"origin_image zh-lightbox-thumb\" width=\"561\" data-original=\"https://pic3.zhimg.com/v2-b68561bea876df81926384fb705d71fe_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;561&#39; height=&#39;113&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"561\" data-rawheight=\"113\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"561\" data-original=\"https://pic3.zhimg.com/v2-b68561bea876df81926384fb705d71fe_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b68561bea876df81926384fb705d71fe_b.jpg\"/></figure><p>条件概率转化为二分类的链式相乘，其中 <img src=\"https://www.zhihu.com/equation?tex=x_%7Bw%7D\" alt=\"x_{w}\" eeimg=\"1\"/> 代表窗口词的相加的词向量，即输入。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bj-1%7D%5E%7Bw%7D\" alt=\"\\theta_{j-1}^{w}\" eeimg=\"1\"/> 代表j上一个节点的参数。逻辑回归的表达式：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-76549e4c2f57b4f29a2f7a36bb318c68_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"689\" data-rawheight=\"90\" class=\"origin_image zh-lightbox-thumb\" width=\"689\" data-original=\"https://pic1.zhimg.com/v2-76549e4c2f57b4f29a2f7a36bb318c68_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;689&#39; height=&#39;90&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"689\" data-rawheight=\"90\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"689\" data-original=\"https://pic1.zhimg.com/v2-76549e4c2f57b4f29a2f7a36bb318c68_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-76549e4c2f57b4f29a2f7a36bb318c68_b.jpg\"/></figure><p>带入上式，取对数就可以得到：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1a75e32c7fb08dd3cedb26e6d736f71e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"880\" data-rawheight=\"185\" class=\"origin_image zh-lightbox-thumb\" width=\"880\" data-original=\"https://pic3.zhimg.com/v2-1a75e32c7fb08dd3cedb26e6d736f71e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;880&#39; height=&#39;185&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"880\" data-rawheight=\"185\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"880\" data-original=\"https://pic3.zhimg.com/v2-1a75e32c7fb08dd3cedb26e6d736f71e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1a75e32c7fb08dd3cedb26e6d736f71e_b.jpg\"/></figure><p>分别对 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bj-1%7D%5E%7Bw%7D\" alt=\"\\theta_{j-1}^{w}\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=x_%7Bw%7D\" alt=\"x_{w}\" eeimg=\"1\"/> 进行求导，利用sgd进行更新。</p><p><b>2.CBOW + negative sample</b></p><p><b>negative sample和层次softmax的相同点在于都是做二分逻辑回归，negative sample针对词袋太过于庞大，而负采样了N个样本，和一个需要预测的样本进行更新，一次只更新n+1个向量对应的参数。</b></p><p>设需要预测的词为正样本，其他的就是负样本，根据一定的词频的采样方法，采取N个负样本，目标函数并没有变化如下:</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d4ebe98305e319825e6f656388f5ee21_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"494\" data-rawheight=\"94\" class=\"origin_image zh-lightbox-thumb\" width=\"494\" data-original=\"https://pic2.zhimg.com/v2-d4ebe98305e319825e6f656388f5ee21_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;494&#39; height=&#39;94&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"494\" data-rawheight=\"94\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"494\" data-original=\"https://pic2.zhimg.com/v2-d4ebe98305e319825e6f656388f5ee21_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d4ebe98305e319825e6f656388f5ee21_b.jpg\"/></figure><p>对于上式的目标函数，因为只有一个正样本，所以可以变为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-618b496723f8e70c25d9859ffda1030d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"429\" data-rawheight=\"87\" class=\"origin_image zh-lightbox-thumb\" width=\"429\" data-original=\"https://pic2.zhimg.com/v2-618b496723f8e70c25d9859ffda1030d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;429&#39; height=&#39;87&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"429\" data-rawheight=\"87\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"429\" data-original=\"https://pic2.zhimg.com/v2-618b496723f8e70c25d9859ffda1030d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-618b496723f8e70c25d9859ffda1030d_b.jpg\"/></figure><p>这样对于整个语料库：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-409633a380fdf3a073ff165614d63fff_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"177\" data-rawheight=\"71\" class=\"content_image\" width=\"177\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;177&#39; height=&#39;71&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"177\" data-rawheight=\"71\" class=\"content_image lazy\" width=\"177\" data-actualsrc=\"https://pic4.zhimg.com/v2-409633a380fdf3a073ff165614d63fff_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-11222e92ffdcaa355405d3e1e08e7573_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"907\" data-rawheight=\"337\" class=\"origin_image zh-lightbox-thumb\" width=\"907\" data-original=\"https://pic4.zhimg.com/v2-11222e92ffdcaa355405d3e1e08e7573_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;907&#39; height=&#39;337&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"907\" data-rawheight=\"337\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"907\" data-original=\"https://pic4.zhimg.com/v2-11222e92ffdcaa355405d3e1e08e7573_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-11222e92ffdcaa355405d3e1e08e7573_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-aa6d6345edd4689f1eb3ee48a3f61c9e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"886\" data-rawheight=\"113\" class=\"origin_image zh-lightbox-thumb\" width=\"886\" data-original=\"https://pic3.zhimg.com/v2-aa6d6345edd4689f1eb3ee48a3f61c9e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;886&#39; height=&#39;113&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"886\" data-rawheight=\"113\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"886\" data-original=\"https://pic3.zhimg.com/v2-aa6d6345edd4689f1eb3ee48a3f61c9e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-aa6d6345edd4689f1eb3ee48a3f61c9e_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ddea508241489bf9824e3aeff69de681_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"989\" data-rawheight=\"560\" class=\"origin_image zh-lightbox-thumb\" width=\"989\" data-original=\"https://pic2.zhimg.com/v2-ddea508241489bf9824e3aeff69de681_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;989&#39; height=&#39;560&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"989\" data-rawheight=\"560\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"989\" data-original=\"https://pic2.zhimg.com/v2-ddea508241489bf9824e3aeff69de681_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ddea508241489bf9824e3aeff69de681_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1e2d768acda3d460e2d40ec21257571e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"479\" data-rawheight=\"433\" class=\"origin_image zh-lightbox-thumb\" width=\"479\" data-original=\"https://pic3.zhimg.com/v2-1e2d768acda3d460e2d40ec21257571e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;479&#39; height=&#39;433&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"479\" data-rawheight=\"433\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"479\" data-original=\"https://pic3.zhimg.com/v2-1e2d768acda3d460e2d40ec21257571e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1e2d768acda3d460e2d40ec21257571e_b.jpg\"/></figure><p><b>上述采用的是二分类,极大似然估计的思想，似乎和神经网络没有半毛钱关系，下面是神经网络版的，loss函数就是交叉熵函数，做一个二分类，用softmax将每个词当做一个类别，这样类别数量会很庞大，所以采用负采样，只更新n+1个。</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8110bb68fc2cf2d265dc08441a9ef120_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"426\" data-rawheight=\"302\" class=\"origin_image zh-lightbox-thumb\" width=\"426\" data-original=\"https://pic1.zhimg.com/v2-8110bb68fc2cf2d265dc08441a9ef120_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;426&#39; height=&#39;302&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"426\" data-rawheight=\"302\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"426\" data-original=\"https://pic1.zhimg.com/v2-8110bb68fc2cf2d265dc08441a9ef120_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8110bb68fc2cf2d265dc08441a9ef120_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "word2vec", 
                    "tagLink": "https://api.zhihu.com/topics/19886836"
                }, 
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/39461254", 
            "userName": "房海朔", 
            "userLink": "https://www.zhihu.com/people/fdfc406067c7444363225d963f3ab320", 
            "upvote": 7, 
            "title": "一种无监督的新词发现算法", 
            "content": "<p>监督学习的方法需要大量的标注数据，对于层出不穷的网络新词，显得力不从心。这是就需要无监督的算法去发现新词、行业词等。本文根据这篇<a href=\"https://link.zhihu.com/?target=https%3A//spaces.ac.cn/archives/4256/comment-page-1%23comments\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">新词发现算法</a>做一个梳理和自我的总结。</p><p>上面文章做新词发现的主要思想就是通过字之间的组合，正向匹配，从相关性最低处切开，保证词内部的相关性最高，或者称之为凝固度最高。</p><p>以三个字的词为例，相关性（凝固度）的公式： </p><blockquote><img src=\"https://www.zhihu.com/equation?tex=min+%5Cleft%5C%7B+%5Cfrac%7BP%28ABC%29%7D%7BP%28A%29P%28BC%29%7D%2C%5Cfrac%7BP%28ABC%29%7D%7BP%28AB%29P%28C%29%7D+%5Cright%5C%7D+\" alt=\"min \\left\\{ \\frac{P(ABC)}{P(A)P(BC)},\\frac{P(ABC)}{P(AB)P(C)} \\right\\} \" eeimg=\"1\"/> </blockquote><p>上面公式可以这样理解,以第一个式子为例，假设A、BC为相互独立的事件，则ABC发生的概率为 <b>P&#39;(ABC) = P(A)P(BC)</b>，而<b>P(ABC)表示ABC统计而得发生的真实的概率</b>。上式可以理解为 P(ABC)/P&#39;(ABC)，<b>比值越大</b>代表ABC越不独立，<b>相关性很高</b>，<b>越有可能同时出现</b>，也就是词的内部凝固度很高。按照<b>互信息</b>的概念来讲，就是 A 和 BC <u><a href=\"https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%25E8%2581%2594%25E5%2590%2588%25E5%2588%2586%25E5%25B8%2583\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">联合分布</a></u>相对于假定 A和 BC独立情况下的联合分布之间的<b>内在依赖性。</b></p><blockquote>算法分为以下步骤：</blockquote><ol><li>统计1，2，3...ngrams的词频并根据词频设定的阈值过滤小于阈值的部分</li></ol><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-69c5bbab5a2408cbcec988e8d720c057_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"914\" data-rawheight=\"292\" class=\"origin_image zh-lightbox-thumb\" width=\"914\" data-original=\"https://pic4.zhimg.com/v2-69c5bbab5a2408cbcec988e8d720c057_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;914&#39; height=&#39;292&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"914\" data-rawheight=\"292\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"914\" data-original=\"https://pic4.zhimg.com/v2-69c5bbab5a2408cbcec988e8d720c057_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-69c5bbab5a2408cbcec988e8d720c057_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>2.根据凝固度公式计算2grams以上词语的凝固度，并过滤小于凝固度阈值的词语，针对不同长度的词语，凝固度的阈值设置不一样。按文中作者给出的经验阈值，以字典的形式给出，长度为2的为5，长度为3的为25，长度为4的为125。min_proba={2:5,3:25,4:125}。其中，凝固度的计算按照每种切分可能都进行计算，<b>取最小的最为其凝固度</b>。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6bed9a0fbbd6d4e87caa97a9a3752759_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1522\" data-rawheight=\"239\" class=\"origin_image zh-lightbox-thumb\" width=\"1522\" data-original=\"https://pic2.zhimg.com/v2-6bed9a0fbbd6d4e87caa97a9a3752759_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1522&#39; height=&#39;239&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1522\" data-rawheight=\"239\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1522\" data-original=\"https://pic2.zhimg.com/v2-6bed9a0fbbd6d4e87caa97a9a3752759_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6bed9a0fbbd6d4e87caa97a9a3752759_b.jpg\"/></figure><p>并<b>根据凝固度</b>过滤掉第一步统计出来的ngrams中小于凝固度的词语。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b27922f718871ebe5d30d053e92b34e8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"972\" data-rawheight=\"115\" class=\"origin_image zh-lightbox-thumb\" width=\"972\" data-original=\"https://pic1.zhimg.com/v2-b27922f718871ebe5d30d053e92b34e8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;972&#39; height=&#39;115&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"972\" data-rawheight=\"115\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"972\" data-original=\"https://pic1.zhimg.com/v2-b27922f718871ebe5d30d053e92b34e8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b27922f718871ebe5d30d053e92b34e8_b.jpg\"/></figure><p>3.根据前两步筛选出来的词语对句子进行分词。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-19f750c757f7984b7ca3657d3114d8f9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"707\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb\" width=\"707\" data-original=\"https://pic2.zhimg.com/v2-19f750c757f7984b7ca3657d3114d8f9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;707&#39; height=&#39;356&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"707\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"707\" data-original=\"https://pic2.zhimg.com/v2-19f750c757f7984b7ca3657d3114d8f9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-19f750c757f7984b7ca3657d3114d8f9_b.jpg\"/></figure><p>其中，对句子分词时设置了一个掩码，用于标记句子中出现在ngrams中的词语，例如“<b>中华人民”</b>，<b>除去句首外</b>，<b>各个位置设置掩码为[0,0,0]</b>,从句首开始扫描，如果&#34;<b>中华</b>&#34;在ngrams里面，则<b>掩码变成[1,0,0]</b>,再向前扫描，&#39;<b>中华人</b>&#39;不在ngrams里面，<b>掩码依然为[1,0,0]</b>,&#39;<b>中华人民&#39;在ngrams里面</b>，掩码全部加一，<b>变成[2,1,1],</b>从掩码等于0的地方断开，这样就可以进行粗略的分词。</p><p>4.上面的分词结果可能会存在一个问题，比如说&#34;各项目&#34;，是由于&#34;各项&#34;,&#34;项目&#34;都存在于ngrams，所以&#34;<b>各项目</b>&#34;才会保留下来,这时可以扫描&#34;各项目&#34;是否在3grams里面，如果在其中，将其保留，不在就删除。其实，不进行这一步的筛选，我认为也是可以的,主要看你的需求,例如&#34;<b>大字号打印机</b>&#34;，可以切成{大字号，打印机}，也可以保留成一个。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7d271919b1e6da973bbc8afc45c8f8d0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"733\" data-rawheight=\"248\" class=\"origin_image zh-lightbox-thumb\" width=\"733\" data-original=\"https://pic1.zhimg.com/v2-7d271919b1e6da973bbc8afc45c8f8d0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;733&#39; height=&#39;248&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"733\" data-rawheight=\"248\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"733\" data-original=\"https://pic1.zhimg.com/v2-7d271919b1e6da973bbc8afc45c8f8d0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7d271919b1e6da973bbc8afc45c8f8d0_b.jpg\"/></figure><blockquote><b>算法实践</b></blockquote><p>利用新闻语料测试一下新词发现的结果：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ad169d66066104a013de7b1fdb99618a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"804\" data-rawheight=\"210\" class=\"origin_image zh-lightbox-thumb\" width=\"804\" data-original=\"https://pic3.zhimg.com/v2-ad169d66066104a013de7b1fdb99618a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;804&#39; height=&#39;210&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"804\" data-rawheight=\"210\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"804\" data-original=\"https://pic3.zhimg.com/v2-ad169d66066104a013de7b1fdb99618a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ad169d66066104a013de7b1fdb99618a_b.jpg\"/></figure><p>处理结果：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b165661c6c62b8d187d6893ceddc4e1f_b.jpg\" data-size=\"normal\" data-rawwidth=\"1219\" data-rawheight=\"234\" class=\"origin_image zh-lightbox-thumb\" width=\"1219\" data-original=\"https://pic4.zhimg.com/v2-b165661c6c62b8d187d6893ceddc4e1f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1219&#39; height=&#39;234&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1219\" data-rawheight=\"234\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1219\" data-original=\"https://pic4.zhimg.com/v2-b165661c6c62b8d187d6893ceddc4e1f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b165661c6c62b8d187d6893ceddc4e1f_b.jpg\"/><figcaption>每句对应的分词结果</figcaption></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c32363b556a9bb14c91d17470aa87372_b.jpg\" data-size=\"normal\" data-rawwidth=\"224\" data-rawheight=\"443\" class=\"content_image\" width=\"224\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;224&#39; height=&#39;443&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"224\" data-rawheight=\"443\" class=\"content_image lazy\" width=\"224\" data-actualsrc=\"https://pic3.zhimg.com/v2-c32363b556a9bb14c91d17470aa87372_b.jpg\"/><figcaption>发现的新词</figcaption></figure><p>可以看出很多词都是和金融股票有关的，&#34;分红派息&#34;，&#34;现金红利&#34;也都能从文中识别出来。此算法还是具有一定的效果。</p><p>整体的代码和数据上传在github上：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/HaishuoFang/Find_New_token\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">HaishuoFang/Find_New_token</a></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "无监督学习", 
                    "tagLink": "https://api.zhihu.com/topics/19590194"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_197298959"
}
