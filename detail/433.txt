{
    "title": "计算机视觉之目标跟踪", 
    "description": "本专栏将目前最新最前沿的目标跟踪算法通过最直接的图解形式来说明其原理以及应用价值", 
    "followers": [
        "https://www.zhihu.com/people/a2108870", 
        "https://www.zhihu.com/people/17774851010", 
        "https://www.zhihu.com/people/kang-jing-yi-49", 
        "https://www.zhihu.com/people/andresforever", 
        "https://www.zhihu.com/people/zhang-bin-34-59", 
        "https://www.zhihu.com/people/bu-dao-weng-82-30", 
        "https://www.zhihu.com/people/neko-44-39", 
        "https://www.zhihu.com/people/xiao-xiao-zi-596", 
        "https://www.zhihu.com/people/zai-jian-feng-yun-9", 
        "https://www.zhihu.com/people/gu-ji-gu-ji-99-51", 
        "https://www.zhihu.com/people/mmcgg", 
        "https://www.zhihu.com/people/kevin-lee-44-18", 
        "https://www.zhihu.com/people/ning-a-89-11", 
        "https://www.zhihu.com/people/linhengjie", 
        "https://www.zhihu.com/people/bai-cai-75-58", 
        "https://www.zhihu.com/people/zhang-lei-92-87", 
        "https://www.zhihu.com/people/fan-qiang-70", 
        "https://www.zhihu.com/people/shen-yuan-71-87", 
        "https://www.zhihu.com/people/shan-yang-pa-shan-12", 
        "https://www.zhihu.com/people/su-nan-59-49", 
        "https://www.zhihu.com/people/dreamer-michael", 
        "https://www.zhihu.com/people/whjxnyzh", 
        "https://www.zhihu.com/people/leo-lee-58-57", 
        "https://www.zhihu.com/people/dou-xian-er-77", 
        "https://www.zhihu.com/people/jiang-lei-24-59", 
        "https://www.zhihu.com/people/hugof", 
        "https://www.zhihu.com/people/shuai-hua-57", 
        "https://www.zhihu.com/people/randy-74", 
        "https://www.zhihu.com/people/shao-lin-xiao-zi-24", 
        "https://www.zhihu.com/people/peng-zheng-62-65", 
        "https://www.zhihu.com/people/guang-yu-61-56", 
        "https://www.zhihu.com/people/fei-yu-81-62", 
        "https://www.zhihu.com/people/yang-aim613", 
        "https://www.zhihu.com/people/xue-lian-jie-37", 
        "https://www.zhihu.com/people/wu-qi-72-92", 
        "https://www.zhihu.com/people/freak-11-12", 
        "https://www.zhihu.com/people/xue-wen-dong-17", 
        "https://www.zhihu.com/people/tao-hong-zhi-33", 
        "https://www.zhihu.com/people/mm12345n6789", 
        "https://www.zhihu.com/people/cnlog-cheng", 
        "https://www.zhihu.com/people/hao-ren-79-93", 
        "https://www.zhihu.com/people/hkustqq", 
        "https://www.zhihu.com/people/zhang-bo-ya-73", 
        "https://www.zhihu.com/people/lao-guo-2-44", 
        "https://www.zhihu.com/people/li-kai-38-99", 
        "https://www.zhihu.com/people/Qlinjun", 
        "https://www.zhihu.com/people/zbqhc", 
        "https://www.zhihu.com/people/yeu-yang", 
        "https://www.zhihu.com/people/yue-nan-yue-ai-ni", 
        "https://www.zhihu.com/people/da-jun-jun-95", 
        "https://www.zhihu.com/people/wen-li-men-wai", 
        "https://www.zhihu.com/people/Nutlet", 
        "https://www.zhihu.com/people/daidai-94-94", 
        "https://www.zhihu.com/people/li_de_sheng_2017", 
        "https://www.zhihu.com/people/sissi-91-39-10", 
        "https://www.zhihu.com/people/captainboy", 
        "https://www.zhihu.com/people/bale-65", 
        "https://www.zhihu.com/people/yu-feng-74-69", 
        "https://www.zhihu.com/people/zhong-tao-65-23", 
        "https://www.zhihu.com/people/he-bin-78-97", 
        "https://www.zhihu.com/people/xu-hao-jie-86-28", 
        "https://www.zhihu.com/people/chen-jia-14-73-1", 
        "https://www.zhihu.com/people/zhu-41-38", 
        "https://www.zhihu.com/people/angel-85-60-62", 
        "https://www.zhihu.com/people/yue-hao-en", 
        "https://www.zhihu.com/people/zhang-yan-tian-14", 
        "https://www.zhihu.com/people/xiu-zhi-fu-58-49", 
        "https://www.zhihu.com/people/pdwpnavvt", 
        "https://www.zhihu.com/people/HarlanHW", 
        "https://www.zhihu.com/people/mao-xiao-ye-21", 
        "https://www.zhihu.com/people/ke-la-65", 
        "https://www.zhihu.com/people/meng-shen", 
        "https://www.zhihu.com/people/yang-troy-89", 
        "https://www.zhihu.com/people/gan-ma-13-61", 
        "https://www.zhihu.com/people/jiang-cheng-cheng-73-33", 
        "https://www.zhihu.com/people/hejinrong1984", 
        "https://www.zhihu.com/people/zhong-jie-96-36", 
        "https://www.zhihu.com/people/wang-peng-cheng-39-36", 
        "https://www.zhihu.com/people/zhou-liang-33-35", 
        "https://www.zhihu.com/people/xue-shi-jian-57", 
        "https://www.zhihu.com/people/gao-pan-92-43", 
        "https://www.zhihu.com/people/alan-30-62-40", 
        "https://www.zhihu.com/people/gao-bai-qi-qiu-gei-ni", 
        "https://www.zhihu.com/people/nan-feng-jian-nuan-85", 
        "https://www.zhihu.com/people/ke-wang-tui-xiu-tan", 
        "https://www.zhihu.com/people/sephrioth-82", 
        "https://www.zhihu.com/people/kang-da-90-63", 
        "https://www.zhihu.com/people/zhu-xuan-yuan", 
        "https://www.zhihu.com/people/sunny-8-3-30", 
        "https://www.zhihu.com/people/liu-li-jie-50", 
        "https://www.zhihu.com/people/zhang-zhen-57-33", 
        "https://www.zhihu.com/people/liu-hao-77-3", 
        "https://www.zhihu.com/people/wei-yu-teng", 
        "https://www.zhihu.com/people/zg9uagfv", 
        "https://www.zhihu.com/people/si-ye-88-10", 
        "https://www.zhihu.com/people/lu-xiao-bu-15-9", 
        "https://www.zhihu.com/people/nuo-wei-si-ji-kou-qiao-dan", 
        "https://www.zhihu.com/people/zhang-wu-di-41-16", 
        "https://www.zhihu.com/people/wang-yang-qing-77", 
        "https://www.zhihu.com/people/yao-yao-20-76-85", 
        "https://www.zhihu.com/people/feng-xing-long-5", 
        "https://www.zhihu.com/people/money-29-17", 
        "https://www.zhihu.com/people/zoeyunj", 
        "https://www.zhihu.com/people/skysword-70", 
        "https://www.zhihu.com/people/orackxudan", 
        "https://www.zhihu.com/people/bai-jin-zhou", 
        "https://www.zhihu.com/people/chen-bo-80-55", 
        "https://www.zhihu.com/people/xu-yan-51-66", 
        "https://www.zhihu.com/people/ke-yan-da-lao", 
        "https://www.zhihu.com/people/shu-mei-jiu-58", 
        "https://www.zhihu.com/people/cyber007", 
        "https://www.zhihu.com/people/123xing-fu-de-si-xie-cao", 
        "https://www.zhihu.com/people/wang-yan-shu-26-4", 
        "https://www.zhihu.com/people/imankou", 
        "https://www.zhihu.com/people/cai-cai-18-19-90", 
        "https://www.zhihu.com/people/kunahe", 
        "https://www.zhihu.com/people/ricky90deng", 
        "https://www.zhihu.com/people/victorld", 
        "https://www.zhihu.com/people/wang-wan-wei-34", 
        "https://www.zhihu.com/people/bu-yao-xie-zhen-ming-liao", 
        "https://www.zhihu.com/people/jiu-ye-20-63", 
        "https://www.zhihu.com/people/qing-zhi-19-46", 
        "https://www.zhihu.com/people/yuan-yan-lin-56"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/31217365", 
            "userName": "Xchen", 
            "userLink": "https://www.zhihu.com/people/9fd266f5fe4043e14105f5281a49017b", 
            "upvote": 29, 
            "title": "相关滤波之基础框架——MOSSE", 
            "content": "<h2>专栏前言</h2><p>本专栏主要用来分享视觉目标跟踪领域的相关算法和具体实现，整个专栏的分享顺序是先基于相关滤波框架后基于深度学习框架，这两个方向也是从2010年开始至今研究最火热也是跟踪效果最好的两个框架。先从涉及到相关滤波框架中的基础框架的三个算法MOSSE、CSK、KCF开始本专栏的分享，本文是相关滤波的开山之作MOSSE的理解，那就开始我们的分享之旅吧</p><hr/><h2>前言</h2><p>作者：<b>David S. Bolme</b>  J. Ross Beveridge  Bruce A. Draper  Yui Man Lui</p><p>主页：<a href=\"https://link.zhihu.com/?target=http%3A//www.cs.colostate.edu/~vision/ocof_toolset_2012/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">cs.colostate.edu/~visio</span><span class=\"invisible\">n/ocof_toolset_2012/</span><span class=\"ellipsis\"></span></a></p><p>出处：2010年CVPR</p><p>目标跟踪被广泛的应用。目标跟踪定义：在首帧得到目标的位置，在接下来的每帧中跟踪上目标。当跟踪上每一帧目标后可以得到很多信息，可以用来鉴定目标和分析目标的行为。因为跟踪比检测更加容易，跟踪算法可以在消耗较少资源的情况下实时的运行。</p><p>最近，目标跟踪受到了更多的关注。有很多跟踪方法能适应目标的外观，并且能够在目标发生运动时跟踪上目标，比如：IVT、FragTrack、GBDL、MILTrack。这些方法很高效，但是并不简单，它们往往包含复杂的外观模型或者繁琐的优化算法，速度上只能勉强在25到30FPS。</p><p>在2010年前，视觉目标跟踪使用的方法多数是生成式方法，包括meanshift、camshift、粒子滤波和kalman滤波。生成式方法的思路是对目标建模，需要使用表现优异的特征表示，同时建立的模型相对复杂，存在如下一些问题：对目标的建模过程比较费时，无法做到较好的实时性，同时只对目标进行建模，没有考虑到背景的信息，跟踪性能无法保证。我们知道之后出现的判别式方法可以很好的解决上述提到的问题，所以目前判别式方法在跟踪领域的表现相对优于生成式方法。判别式方法把目标跟踪问题看作是目标和背景的分类问题，只要在每帧图像中区分出目标和背景，然后不断更新分类器就可以实现目标跟踪，在保持实时性的同时对目标的跟踪较为准确，同时考虑了背景信息更加的鲁棒。而判别式方法中的翘楚是2010年出现的相关滤波算法，2010年MOSSE的横空出世宣告相关滤波至少7年以上的研究热潮。</p><p>这篇文章就是对CF类算法的开山之作MOSSE的介绍，介绍其带来的速度和性能上的提升后也会分析其存在的问题，毕竟是开山之作并不是CF类的巅峰之作，还是存在一些问题。</p><h2>创新点</h2><p>在2010年，相关滤波还没有被广泛应用，这篇论文则是开创性的将相关滤波技术引入到目标跟踪领域，这类算法可以很好的适应于复杂目标的旋转、被遮挡以及一些其他的干扰，并且跟踪速度是其他优秀算法的20倍。过去的相关滤波方法只是使用简单的模板，通常在跟踪中会应用失败。2010年以前一些更好的相关滤波方法，比如：ASEF和UMACE也并不适合于目标跟踪。视觉跟踪需要一个鲁棒性很强的滤波器，是通过首帧的目标训练而得的，同时能够自适应于目标的变化。</p><p>这篇文章主要提出一种误差平方和最小的滤波器MOSSE（Minimum Output Sum of<br/>Squared Error filter）跟踪算法，是通过首帧目标训练而得。基于这样滤波器的跟踪算法能够对目标的光照、尺度、姿态变化和非刚性形变有很好的鲁棒性，同时实现极快的跟踪速度，在作者的计算机上可以达到669FPS。当目标发生遮挡时，可以根据PSR值来判断目标跟踪是否失败来决定是否更新滤波器参数，当目标再次出现在视野中时，可以再次跟上目标。</p><h2>算法流程</h2><p>算法的整体流程如下图所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-cd90cca6c69fd7d6a676f7a8ae31f2c6_b.jpg\" data-size=\"normal\" data-rawwidth=\"560\" data-rawheight=\"372\" class=\"origin_image zh-lightbox-thumb\" width=\"560\" data-original=\"https://pic3.zhimg.com/v2-cd90cca6c69fd7d6a676f7a8ae31f2c6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;560&#39; height=&#39;372&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"560\" data-rawheight=\"372\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"560\" data-original=\"https://pic3.zhimg.com/v2-cd90cca6c69fd7d6a676f7a8ae31f2c6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-cd90cca6c69fd7d6a676f7a8ae31f2c6_b.jpg\"/><figcaption>算法的整体流程图</figcaption></figure><p>该算法将相关滤波概念应用到目标跟踪的理论依据是基于训练样本的最小二乘误差：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-124c650516160161ddabf35eb46f2dc3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"122\" data-rawheight=\"30\" class=\"content_image\" width=\"122\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;122&#39; height=&#39;30&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"122\" data-rawheight=\"30\" class=\"content_image lazy\" width=\"122\" data-actualsrc=\"https://pic4.zhimg.com/v2-124c650516160161ddabf35eb46f2dc3_b.jpg\"/></figure><p>所以上述流程中基于首帧图像的目标仿射变换得到的多个样本和对应各样本的期望输出都是用于求解和训练滤波器参数的，上式的求解则是在频域中的求导后置0可以求到闭式解：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1cfd68a7d90d1918c22c0103fbd8916c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"75\" data-rawheight=\"33\" class=\"content_image\" width=\"75\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;75&#39; height=&#39;33&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"75\" data-rawheight=\"33\" class=\"content_image lazy\" width=\"75\" data-actualsrc=\"https://pic1.zhimg.com/v2-1cfd68a7d90d1918c22c0103fbd8916c_b.jpg\"/></figure><p>在下一帧图像到来之后，根据训练得到的滤波器模板和基于上帧目标位置的采集样本可以获得最后的响应输出，选取响应最大的位置为这一帧中目标的位置，之后再更新滤波器和图像样本反复循环的执行以上过程可以持续的跟踪住目标。</p><h2>算法细节</h2><p>预处理</p><p>对图像和滤波器进行卷积操作会将其映射到拓扑空间中，形成圆环性，即左边与右边相连，上边与下边相连。卷积时图像发生旋转，目标位移会不准确，这样就加入了人为的影响。需要用预处理步骤来克服这一点，首先对原始灰度值进行对数变换，降低光照影响，图像灰度值被归一化到均值为0，然后将图像与一个余弦窗相乘，让图像边缘的值接近0，更加的突出中间目标的权重。</p><p>失败检测</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-78f0703d8981d74572e1f8b8c4821a56_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"81\" data-rawheight=\"27\" class=\"content_image\" width=\"81\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;81&#39; height=&#39;27&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"81\" data-rawheight=\"27\" class=\"content_image lazy\" width=\"81\" data-actualsrc=\"https://pic3.zhimg.com/v2-78f0703d8981d74572e1f8b8c4821a56_b.jpg\"/></figure><p>使用PSR的值来检测跟踪失败，在实验中PSR值在20到60之间被认为是跟踪效果较好，当PSR值低于7就可以判断为跟踪失败，不更新模板。</p><h2>图解算法</h2><p>为了方便展示算法运行过程中的效果，以OTB数据集中的跟踪序列Bolt为例：</p><figure data-size=\"small\"><noscript><img src=\"https://pic3.zhimg.com/v2-b0fcf30b19b371dbe3687f55a049cc02_b.jpg\" data-size=\"small\" data-rawwidth=\"640\" data-rawheight=\"360\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-b0fcf30b19b371dbe3687f55a049cc02_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;360&#39;&gt;&lt;/svg&gt;\" data-size=\"small\" data-rawwidth=\"640\" data-rawheight=\"360\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-b0fcf30b19b371dbe3687f55a049cc02_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b0fcf30b19b371dbe3687f55a049cc02_b.jpg\"/><figcaption>OTB数据集中的跟踪序列Bolt</figcaption></figure><p>根据目标中心位置和尺度获得滤波器与样本卷积后的期望响应：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-77af92630f81325eab2ac22625cdb0b6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1920\" data-rawheight=\"943\" class=\"origin_image zh-lightbox-thumb\" width=\"1920\" data-original=\"https://pic3.zhimg.com/v2-77af92630f81325eab2ac22625cdb0b6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1920&#39; height=&#39;943&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1920\" data-rawheight=\"943\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1920\" data-original=\"https://pic3.zhimg.com/v2-77af92630f81325eab2ac22625cdb0b6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-77af92630f81325eab2ac22625cdb0b6_b.jpg\"/></figure><p>截取目标大小的期望响应图如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4670d2dac66cb65729d1cc82183695a0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"479\" data-rawheight=\"240\" class=\"origin_image zh-lightbox-thumb\" width=\"479\" data-original=\"https://pic1.zhimg.com/v2-4670d2dac66cb65729d1cc82183695a0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;479&#39; height=&#39;240&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"479\" data-rawheight=\"240\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"479\" data-original=\"https://pic1.zhimg.com/v2-4670d2dac66cb65729d1cc82183695a0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-4670d2dac66cb65729d1cc82183695a0_b.jpg\"/></figure><p>对目标图像进行预处理前后的对比图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5394f1a13b7d6f60ce8adddc829f9011_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"70\" data-rawheight=\"111\" class=\"content_image\" width=\"70\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;70&#39; height=&#39;111&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"70\" data-rawheight=\"111\" class=\"content_image lazy\" width=\"70\" data-actualsrc=\"https://pic2.zhimg.com/v2-5394f1a13b7d6f60ce8adddc829f9011_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c65686bf74f1c653d11bffedae4744c1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"74\" data-rawheight=\"110\" class=\"content_image\" width=\"74\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;74&#39; height=&#39;110&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"74\" data-rawheight=\"110\" class=\"content_image lazy\" width=\"74\" data-actualsrc=\"https://pic2.zhimg.com/v2-c65686bf74f1c653d11bffedae4744c1_b.jpg\"/></figure><p>对目标进行仿射变换得到的图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-115208320c3f555d29357fbc6cf42d1b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"73\" data-rawheight=\"111\" class=\"content_image\" width=\"73\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;73&#39; height=&#39;111&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"73\" data-rawheight=\"111\" class=\"content_image lazy\" width=\"73\" data-actualsrc=\"https://pic4.zhimg.com/v2-115208320c3f555d29357fbc6cf42d1b_b.jpg\"/></figure><p>那么第一帧图像是这样的：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-18982b3678e43dd0e185cf9adcf7aef3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"554\" data-rawheight=\"304\" class=\"origin_image zh-lightbox-thumb\" width=\"554\" data-original=\"https://pic4.zhimg.com/v2-18982b3678e43dd0e185cf9adcf7aef3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;554&#39; height=&#39;304&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"554\" data-rawheight=\"304\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"554\" data-original=\"https://pic4.zhimg.com/v2-18982b3678e43dd0e185cf9adcf7aef3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-18982b3678e43dd0e185cf9adcf7aef3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>第二帧中的滤波器响应如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-19cedf06d37ff23348c1e519f4f14886_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"554\" data-rawheight=\"272\" class=\"origin_image zh-lightbox-thumb\" width=\"554\" data-original=\"https://pic3.zhimg.com/v2-19cedf06d37ff23348c1e519f4f14886_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;554&#39; height=&#39;272&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"554\" data-rawheight=\"272\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"554\" data-original=\"https://pic3.zhimg.com/v2-19cedf06d37ff23348c1e519f4f14886_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-19cedf06d37ff23348c1e519f4f14886_b.jpg\"/></figure><p>选取其中响应值最大的点作为这一帧中目标的位置</p><h2>优缺点总结</h2><p>MOSSE算法的训练样本时通过目标的仿射变换得到的，每个样本的输出为各自的一个高斯分布，然后根据误差平方和最小的原则计算这一帧的滤波器模板。接着用训练得到的滤波器在下一帧中找到目标的位置，最后再更新滤波器。滤波器训练采用的特征是最原始的灰度特征，特征表示能力较弱，后续有算法对其进行改进，好的特征表示可以提高跟踪算法的鲁棒性。整体算法都够适应小尺度的目标变化，但是不能适应大尺度的目标变化。最后总结起来，MOSSE算法存在如下缺点：</p><p>1、MOSSE算法的样本采样仍是一种稀疏采样，训练效果一般；</p><p>2、采样的是线性滤波器（最小二乘法），分类性能一般；</p><p>3、采用的特征是单通道的灰度特征，表征目标的能力有限。</p><p><br/><br/><br/>_    </p><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/xchen"
}
