{
    "title": "运维实践", 
    "description": "", 
    "followers": [
        "https://www.zhihu.com/people/ta-ge-xing-60-59", 
        "https://www.zhihu.com/people/conan-qzhong", 
        "https://www.zhihu.com/people/sailfishc", 
        "https://www.zhihu.com/people/xu-xin-yu-14-25", 
        "https://www.zhihu.com/people/song-wen-30", 
        "https://www.zhihu.com/people/quan-bo-mian", 
        "https://www.zhihu.com/people/zhou-peng-21-71", 
        "https://www.zhihu.com/people/qu-shi-qiang-74", 
        "https://www.zhihu.com/people/xiao-jie-32", 
        "https://www.zhihu.com/people/hlhl", 
        "https://www.zhihu.com/people/neowu0", 
        "https://www.zhihu.com/people/ying-zi-40-52", 
        "https://www.zhihu.com/people/huang-bo-20-36", 
        "https://www.zhihu.com/people/zi-mo-heng", 
        "https://www.zhihu.com/people/dong-bin-75", 
        "https://www.zhihu.com/people/du-xiao-ya-18", 
        "https://www.zhihu.com/people/ming-yu-69-3", 
        "https://www.zhihu.com/people/pop-sz-49", 
        "https://www.zhihu.com/people/yan-huaming", 
        "https://www.zhihu.com/people/fei-wo-fu-kua", 
        "https://www.zhihu.com/people/cnxobo", 
        "https://www.zhihu.com/people/jiang-fei-80", 
        "https://www.zhihu.com/people/harry-shen-3", 
        "https://www.zhihu.com/people/zacharyzhao", 
        "https://www.zhihu.com/people/marsgt", 
        "https://www.zhihu.com/people/xiaohinata", 
        "https://www.zhihu.com/people/fanyu83", 
        "https://www.zhihu.com/people/ly-lyrids", 
        "https://www.zhihu.com/people/mikoto-misaka-3", 
        "https://www.zhihu.com/people/yi-wen-53-56-60", 
        "https://www.zhihu.com/people/li-xiao-xian-er-34-30", 
        "https://www.zhihu.com/people/kome", 
        "https://www.zhihu.com/people/amazing_cat", 
        "https://www.zhihu.com/people/yetone", 
        "https://www.zhihu.com/people/ampxe", 
        "https://www.zhihu.com/people/jia-chen-83-19", 
        "https://www.zhihu.com/people/haitialdy", 
        "https://www.zhihu.com/people/programmer_song", 
        "https://www.zhihu.com/people/peter-wang-18", 
        "https://www.zhihu.com/people/rainmanx", 
        "https://www.zhihu.com/people/yimeng.ch", 
        "https://www.zhihu.com/people/lizhenyu", 
        "https://www.zhihu.com/people/xiao-xiao-zi-64-63", 
        "https://www.zhihu.com/people/veryl", 
        "https://www.zhihu.com/people/megabyte875", 
        "https://www.zhihu.com/people/shen-tu-hai-san", 
        "https://www.zhihu.com/people/lihaoquan", 
        "https://www.zhihu.com/people/xingshao-84", 
        "https://www.zhihu.com/people/shen-yu-bao", 
        "https://www.zhihu.com/people/zheng-zhen-yi", 
        "https://www.zhihu.com/people/luo-xiao-qi-39", 
        "https://www.zhihu.com/people/lane-97", 
        "https://www.zhihu.com/people/bu-yi-mi-ya", 
        "https://www.zhihu.com/people/cai-long-67", 
        "https://www.zhihu.com/people/coffeeb", 
        "https://www.zhihu.com/people/huan-xiang-shu", 
        "https://www.zhihu.com/people/wei-zhu-16", 
        "https://www.zhihu.com/people/ling-89-78", 
        "https://www.zhihu.com/people/gu-yu-98", 
        "https://www.zhihu.com/people/pandadefish", 
        "https://www.zhihu.com/people/bu-ke-zhi-82", 
        "https://www.zhihu.com/people/jiong-te-man", 
        "https://www.zhihu.com/people/tiaonmmn-33", 
        "https://www.zhihu.com/people/mx1014-95", 
        "https://www.zhihu.com/people/yu-sheng-87-52", 
        "https://www.zhihu.com/people/pw-young", 
        "https://www.zhihu.com/people/zhao-qing-50", 
        "https://www.zhihu.com/people/tianren", 
        "https://www.zhihu.com/people/shuitianyise", 
        "https://www.zhihu.com/people/TAT_hanxiao", 
        "https://www.zhihu.com/people/wang-dong-shi-90", 
        "https://www.zhihu.com/people/yang-ze-kai-25", 
        "https://www.zhihu.com/people/leon-more", 
        "https://www.zhihu.com/people/wang-xin-68-33", 
        "https://www.zhihu.com/people/wang-liao-52", 
        "https://www.zhihu.com/people/bo-zhang-51", 
        "https://www.zhihu.com/people/cdr1", 
        "https://www.zhihu.com/people/feng-wan-qing", 
        "https://www.zhihu.com/people/LuckyZpc", 
        "https://www.zhihu.com/people/liao-ming-73-60", 
        "https://www.zhihu.com/people/xu-xiao-sheng-18", 
        "https://www.zhihu.com/people/banbu_lee", 
        "https://www.zhihu.com/people/missing_r", 
        "https://www.zhihu.com/people/lightbluer", 
        "https://www.zhihu.com/people/oliver-76-84", 
        "https://www.zhihu.com/people/seven-cool", 
        "https://www.zhihu.com/people/li-de-jian-97", 
        "https://www.zhihu.com/people/zhang-lin-60-11", 
        "https://www.zhihu.com/people/psmortal", 
        "https://www.zhihu.com/people/yao-hao-23", 
        "https://www.zhihu.com/people/bu-cha-dian-81-98", 
        "https://www.zhihu.com/people/wang-yu-ming-24-81", 
        "https://www.zhihu.com/people/li-cheng-seven", 
        "https://www.zhihu.com/people/gu-yi-ji-hua", 
        "https://www.zhihu.com/people/ou-yang-kang-87", 
        "https://www.zhihu.com/people/halfer53", 
        "https://www.zhihu.com/people/ai-bo-te-3-8", 
        "https://www.zhihu.com/people/jia-wen-quan-53", 
        "https://www.zhihu.com/people/altkatz", 
        "https://www.zhihu.com/people/fan-fan-7-39", 
        "https://www.zhihu.com/people/fgsoap", 
        "https://www.zhihu.com/people/xie-yun-peng", 
        "https://www.zhihu.com/people/yu-zhe-47-84", 
        "https://www.zhihu.com/people/vroach", 
        "https://www.zhihu.com/people/wang-yang-8-54", 
        "https://www.zhihu.com/people/patrick-simon-78", 
        "https://www.zhihu.com/people/zhang-xuan-63-65", 
        "https://www.zhihu.com/people/yang-li-1", 
        "https://www.zhihu.com/people/web-13", 
        "https://www.zhihu.com/people/fen-di-73", 
        "https://www.zhihu.com/people/jee-23-34", 
        "https://www.zhihu.com/people/saber-5-66-87", 
        "https://www.zhihu.com/people/lei-zi-78-60", 
        "https://www.zhihu.com/people/yin-jiang-56-52", 
        "https://www.zhihu.com/people/halfbloodrock", 
        "https://www.zhihu.com/people/rongyangxu", 
        "https://www.zhihu.com/people/tongsu", 
        "https://www.zhihu.com/people/tim-yang", 
        "https://www.zhihu.com/people/9527chu", 
        "https://www.zhihu.com/people/rubix-lai", 
        "https://www.zhihu.com/people/cui-xiao-chen-82", 
        "https://www.zhihu.com/people/ji-kang", 
        "https://www.zhihu.com/people/lan-lan-lan-lan-96-82", 
        "https://www.zhihu.com/people/fanngyuan", 
        "https://www.zhihu.com/people/mwhi", 
        "https://www.zhihu.com/people/ning-meng-xin-73-58", 
        "https://www.zhihu.com/people/kikipedia", 
        "https://www.zhihu.com/people/zhihugexingyuming2", 
        "https://www.zhihu.com/people/SWAYing", 
        "https://www.zhihu.com/people/chybeta", 
        "https://www.zhihu.com/people/hurryport-11", 
        "https://www.zhihu.com/people/rikuh", 
        "https://www.zhihu.com/people/er-wa-13-7-20", 
        "https://www.zhihu.com/people/D0n9", 
        "https://www.zhihu.com/people/newcfg", 
        "https://www.zhihu.com/people/ivon-lee", 
        "https://www.zhihu.com/people/xia-hou-feng", 
        "https://www.zhihu.com/people/lou-kai-56-98", 
        "https://www.zhihu.com/people/ma-zei-lao", 
        "https://www.zhihu.com/people/cao-xin-58-59", 
        "https://www.zhihu.com/people/long-deng-long", 
        "https://www.zhihu.com/people/xiao-tai-yang-81-18", 
        "https://www.zhihu.com/people/linuxcpp", 
        "https://www.zhihu.com/people/zhuzhu-zhuzhu", 
        "https://www.zhihu.com/people/kaizener-74", 
        "https://www.zhihu.com/people/qian-yun-qing-40", 
        "https://www.zhihu.com/people/reader2018", 
        "https://www.zhihu.com/people/gangbj-gangbj", 
        "https://www.zhihu.com/people/yue-guo-shuai-68", 
        "https://www.zhihu.com/people/deng-liu-yan-76", 
        "https://www.zhihu.com/people/liu-xiao-yao-12", 
        "https://www.zhihu.com/people/tranch", 
        "https://www.zhihu.com/people/riskers", 
        "https://www.zhihu.com/people/jiang-ye-15", 
        "https://www.zhihu.com/people/goodlooks-com-cn", 
        "https://www.zhihu.com/people/rick-10", 
        "https://www.zhihu.com/people/feng-kai-yu-98", 
        "https://www.zhihu.com/people/vtan-89", 
        "https://www.zhihu.com/people/vl22swF-SUK_-K0crFui", 
        "https://www.zhihu.com/people/zhang-yifeng", 
        "https://www.zhihu.com/people/eta-100a"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/75450799", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 2, 
            "title": "魔改了下 MariaDB 支持 JSON binary", 
            "content": "<p>Oracle MySQL 的 JSON 数据类型在存储时用的紧凑的二进制格式，MariaDB 则傲娇的直接存原始的 JSON 文本串，从 <a href=\"https://link.zhihu.com/?target=https%3A//mariadb.com/kb/en/library/mariadb-vs-mysql-compatibility/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">MariaDB 10.2 开始兼容  MySQL 5.7</a>，如今 MariaDB 10.4 稳定版都发布了，依然还是不支持 Oracle MySQL 的 JSON 类型：</p><ul><li>使用 row based replication 时，不支持从 MySQL 5.7 复制包含 JSON 列的 binlog</li><li>不支持就地读取包含 MySQL JSON 列的数据文件</li></ul><p>MariaDB 从  10.2 开始大爆发：</p><ul><li><a href=\"https://link.zhihu.com/?target=https%3A//mariadb.com/kb/en/library/incompatibilities-and-feature-differences-between-mariadb-102-and-mysql-57/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">10.2</a>:   </li><ul><li>Window function</li><li>Recursive Common Table Expressions</li><li>WITH statement</li><li>CHECK CONSTRAINT</li><li>DML-only flashback</li></ul><li><a href=\"https://link.zhihu.com/?target=https%3A//mariadb.com/kb/en/library/changes-improvements-in-mariadb-103/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">10.3</a>:</li><ul><li>System-versioned tables (也被称为 Temporal table)</li><li>Sequence</li></ul><li><a href=\"https://link.zhihu.com/?target=https%3A//mariadb.com/kb/en/library/changes-improvements-in-mariadb-104/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">10.4</a>:</li><ul><li>System-versioned table 支持 application-time period</li><li>Galera 4</li><li>静态链接 WolfSSL (废了 OpenSSL 真好😄）</li></ul><li>10.5 (还未正式发布):</li><ul><li>S3 storage engine</li></ul></ul><p>相比之下 MySQL 8.0 的步子真是太慢了，但是两家对于 JSON 数据类型的实现，恐怕是不指望双方主动做兼容层了。</p><p>由于非常馋 MariaDB 的 <a href=\"https://link.zhihu.com/?target=https%3A//mariadb.com/kb/en/library/temporal-data-tables/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">system-versioned table</a> 功能以支持审计、调试、数据回滚、时序分析，因此魔改了下 MariaDB 10.4 分支，让其可以解析 MySQL 5.7 binlog 里的 MYSQL_TYPE_JSON 类型，这步依葫芦画瓢还是比较容易的，就是第一次看 MariaDB 代码，晕了好一阵子，还好有 GDB、LLDB 调试器在手，以及在 Wine 里跑 Source Insight 4(感谢这么好的源码阅读软件！），跌跌绊绊看明白了。</p><p>这一步搞定后，忽然想为啥不支持下就地读取包含 JSON 列的 MySQL 数据文件呢？ 操起调试器大法，试验了几个小时，发现搞定了，还挺简单。。。然后又贪心不足，能不能支持就地写包含 JSON 列的 MySQL 数据文件了？花了十分钟 hack 了下，居然也可以了😂</p><p>MariaDB 代码里不少地方有一些疑问的注释，看的出来后来的代码维护者已经不知道为啥前人那么写了😓，但总的来说，MariaDB 代码做了些抽象和重构，比 MySQL 的源代码还是要规整些。</p><p>在 macOS 下很容易编译 MariaDB 的代码：</p><div class=\"highlight\"><pre><code class=\"language-text\">mkdir builddir &amp;&amp;\n    cd builddir &amp;&amp;\n    cmake -DCMAKE_BUILD_TYPE=RelWithDebInfo -DWITH_SSL=bundled -DWITH_PCRE=bundled \\\n          -DWITH_UNIT_TESTS=OFF -DDEFAULT_CHARSET=utf8mb4  -DDEFAULT_COLLATION=utf8mb4_general_ci \\\n          -DPLUGIN_TOKUDB=NO .. &amp;&amp;\n    make &amp;&amp;\n    make install DESTDIR=$HOME/tmp/mariadb</code></pre></div><p>然后运行 MariaDB:</p><div class=\"highlight\"><pre><code class=\"language-text\">cd $HOME/tmp/mariadb/usr/local/mysql\nbin/mysqld_safe --datadir=./data</code></pre></div><p>macOS 下 clang 有个奇怪的行为，如果一次性的编译出可执行文件，那么 clang 会自动生产  xxx.dSYM 调试符号，而如果是先编译出 .o 再链接成可执行文件，则需要手动的搞一下：</p><div class=\"highlight\"><pre><code class=\"language-bash\">dsymutil <span class=\"nv\">$HOME</span>/tmp/mariadb/usr/local/mysql/bin/mysqld</code></pre></div><p>然后就可以 lldb 调试了：</p><div class=\"highlight\"><pre><code class=\"language-text\">cd sources/mariadb-server\nlldb -p PID\n(lldb) breakpoint set -n FUNCTION\n(lldb) continue</code></pre></div><p>MySQL 8.0 加入了 JSON 的 partial update，默认没有开启，如果开启后，可能 binlog 格式有点变化，目前这个魔改的 MariaDB 版本没考虑这种情况，所以，MySQL 5.7 大法好！MariaDB 大法好！—— 哦，Percona 是谁啊？😄</p>", 
            "topic": [
                {
                    "tag": "MySQL", 
                    "tagLink": "https://api.zhihu.com/topics/19554128"
                }, 
                {
                    "tag": "MariaDB", 
                    "tagLink": "https://api.zhihu.com/topics/19997537"
                }, 
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/60247018", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 4, 
            "title": "ProxySQL 整数溢出", 
            "content": "<p>最近学习了下 <a href=\"https://link.zhihu.com/?target=https%3A//proxysql.com/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ProxySQL</a>，在非生产环境大致体验了下，感觉不错，于是小流量上了生产环境，然而居然发现有个奇怪的现象，客户端大概率连不上 MySQL 服务器，报告如下错误：</p><div class=\"highlight\"><pre><code class=\"language-text\">Max connect timeout reached while reaching hostgroup 3034 after 10000ms</code></pre></div><p>非常郁闷，网上搜索了下，跟我遇到的情况都不符合，我确认了 <code>runtime_mysql_servers</code> 表中所有 server 都是 ONLINE 状态，在 <code>monitor.mysql_server_connect_log</code>, <code>monitor.mysql_server_ping_log</code> 和 <code>monitor.mysql_server_replication_lag_log</code> 几个表中的检查都非常正常，直连 MySQL 也非常快。</p><p>试验了增加下面几个参数，也没效果：</p><div class=\"highlight\"><pre><code class=\"language-text\">SET mysql-connect_retries_delay = 500;  -- default 1ms\nSET mysql-connect_timeout_server = 5000;\t -- default 3000ms\nSET mysql-connect_timeout_server_max = 35000; -- default 10000ms\n\nSET mysql-connection_delay_multiplex_ms = 100; -- default 0ms\n\n-- result in &#34;Aborted connection&#34; warnings\nSET mysql-connection_max_age_ms = 900000; -- default 0ms</code></pre></div><p>没办法，只能去看代码，根据错误信息，很容易找到了<a href=\"https://link.zhihu.com/?target=https%3A//github.com/sysown/proxysql/blob/6da427f55873d19b61ac29a2a36a54dfb1364448/lib/MySQL_HostGroups_Manager.cpp%23L1289\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">代码位置</a>：</p><div class=\"highlight\"><pre><code class=\"language-text\">MySrvC *MyHGC::get_random_MySrvC() {\n  MySrvC *mysrvc=NULL;\n\tunsigned int j;\n\tunsigned int sum=0;\n\tunsigned int TotalUsedConn=0;\n\tunsigned int l=mysrvs-&gt;cnt();\n\n  ...\n\n    if ((len * sum) &lt;= (TotalUsedConn * mysrvc-&gt;weight * 1.5 + 1)) {\n \n  ...\n}</code></pre></div><p>这里 <code>len</code> 表示当前 host group 遍历到的一个 MySQL server 上面有多少正在使用的TCP链接，<code>sum</code> 是所有 ONLINE 状态的 MySQL server 的权重之和，<code>TotalUsedConn</code> 是此 host group 一共有多少正在使用的 TCP 链接，<code>mysrvc-&gt;weight</code> 是当前遍历到的这个 MySQL server 的权重，这四个变量类型都是 <code>unsigned int</code>，这个默认是四字节，相乘非常容易整数溢出，很值得怀疑。</p><p>查了下监控数据，故障发生时，现场是这样的：</p><div class=\"highlight\"><pre><code class=\"language-text\">db-1  weight=99999999 conn=22\ndb-2  weight=99999999 conn=20\ndb-3  weight=1        conn=1</code></pre></div><p>这里 db-3 是主库，放进 readonly host group 里权重为 1，目的是在从库复制延迟超过限制，被踢出 host group 之后，还有个备用的主库，而平时从库正常时，主库尽可能少的承担读压力。</p><p>仿照着 ProxySQL 写了段代码验证：</p><div class=\"highlight\"><pre><code class=\"language-text\">#include &lt;cstdio&gt;\n#include &lt;cstdlib&gt;\n\n__thread unsigned int g_seed;\n\ninline int fastrand() {\n    g_seed = (214013*g_seed+2531011);\n    return (g_seed&gt;&gt;16)&amp;0x7FFF;\n}\n\nint main(int argc, char** argv) {\n    unsigned int usedConns[] = { 22,       20,       1};\n    unsigned int weights[]   = { 99999999, 99999999, 1};\n\n    unsigned int sum = 0;\n    unsigned int TotalUsedConn = 0;\n    unsigned int l = 3;\n    unsigned int j;\n\n    for (j = 0; j &lt; l; j++) {\n        sum += weights[j];\n        TotalUsedConn += usedConns[j];\n        printf(&#34;j=%u weight=%u usedConn=%u sum=%u TotalUsedConn=%u\\n&#34;, j, weights[j], usedConns[j], sum, TotalUsedConn);\n    }\n    printf(&#34;\\n&#34;);\n\n    unsigned int New_sum=0;\n    unsigned int New_TotalUsedConn=0;\n\n    for (j = 0; j &lt; l; j++) {\n        unsigned int len = usedConns[j];\n        unsigned int weight = weights[j];\n        printf(&#34;\\nj=%u len=%u weight=%u TotalUsedConn=%u sum=%u\\n&#34;, j, len, weight, TotalUsedConn, sum);\n        printf(&#34;        len*sum=%u TotalUsedConn*weight=%u TotalUsedConn*weight*1.5+1=%lf\\n&#34;,\n                len * sum, TotalUsedConn * weight, TotalUsedConn * weight * 1.5 + 1);\n\n        if ((len * sum) &lt;= (TotalUsedConn * weight * 1.5 + 1)) {\n            printf(&#34;j=%u old New_sum=%u New_TotalUsedConn=%u\\n&#34;, j, New_sum, New_TotalUsedConn);\n            New_sum += weight;\n            New_TotalUsedConn += len;\n            printf(&#34;j=%u now New_sum=%u New_TotalUsedConn=%u\\n&#34;, j, New_sum, New_TotalUsedConn);\n        } else {\n            printf(&#34; !!! NOT\\n&#34;);\n        }\n    }\n\n    printf(&#34;\\nNew_sum=%u New_TotalUsedConn=%u\\n&#34;, New_sum, New_TotalUsedConn);\n    if (New_sum == 0) {\n        printf(&#34;ERROR\\n&#34;);\n        return 0;\n    }\n\n    unsigned int k;\n    if (New_sum &gt; 32768) {\n        k = rand() % New_sum;\n    } else {\n        k = fastrand() % New_sum;\n    }\n\n    New_sum = 0;\n    for (j = 0; j &lt; l; j++) {\n        unsigned int len = usedConns[j];\n        unsigned int weight = weights[j];\n\n        printf(&#34;\\nj=%u len=%u weight=%u k=%u\\n&#34;, j, len, weight, k);\n        if ((len * sum) &lt;= (TotalUsedConn * weight * 1.5 + 1)) {\n            New_sum += weight;\n            if (k &lt;= New_sum) {\n                printf(&#34;got %u\\n because k(%u) &lt; New_sum(%u)!!!&#34;, j, k, New_sum);\n                break;\n            } else {\n                printf(&#34; !!! NOT ENOUGH\\n&#34;);\n            }\n        } else {\n            printf(&#34; !!! NOT\\n&#34;);\n        }\n    }\n\n    return 0;\n}</code></pre></div><p>运行之，果不其然，整数溢出了，导致判断失误，三个 server 都被排除掉了：</p><div class=\"highlight\"><pre><code class=\"language-text\">$ clang -o a a.cpp\n$ ./a\nj=0 weight=99999999 usedConn=22 sum=99999999 TotalUsedConn=22\nj=1 weight=99999999 usedConn=20 sum=199999998 TotalUsedConn=42\nj=2 weight=1 usedConn=1 sum=199999999 TotalUsedConn=43\n\n\nj=0 len=22 weight=99999999 TotalUsedConn=43 sum=199999999\n        len*sum=105032682 TotalUsedConn*weight=5032661 TotalUsedConn*weight*1.5+1=7548992.500000\n !!! NOT\n\nj=1 len=20 weight=99999999 TotalUsedConn=43 sum=199999999\n        len*sum=3999999980 TotalUsedConn*weight=5032661 TotalUsedConn*weight*1.5+1=7548992.500000\n !!! NOT\n\nj=2 len=1 weight=1 TotalUsedConn=43 sum=199999999\n        len*sum=199999999 TotalUsedConn*weight=43 TotalUsedConn*weight*1.5+1=65.500000\n !!! NOT\n\nNew_sum=0 New_TotalUsedConn=0\nERROR</code></pre></div><p>可怜的 C/C++ 语言！可怜的 C/C++ 程序员！可怜的持有性能第一、正确第二观念的语言设计者、程序编写者！</p><p>在 ProxySQL 官方代码修正这个问题前，设置多大的 weight 才是安全的呢？上面的两个乘法运算，一个是 <code>connUsed * total_weight</code>，一个是 <code>totalConnUsed * weight</code>，假定单个MySQL server 持有最多 1w 链接，100 台 MySQL server，那么:</p><div class=\"highlight\"><pre><code class=\"language-text\">connUsed*total_weight=10000 * (100 * weight) &lt;= 2^32-1\n totalConnUsed * weight = (10000 * 100) * weight &lt;= 2^32-1</code></pre></div><p>那么 weight 的上限在 4000 多一点，考虑到我们的环境没有那么多 MySQL 服务器，所以可以设置到 10000，也可以保守点设置到 1000，比起主库的 <code>weight=1</code>，也足够让主库承担尽量少的读压力了。</p>", 
            "topic": [
                {
                    "tag": "MySQL", 
                    "tagLink": "https://api.zhihu.com/topics/19554128"
                }, 
                {
                    "tag": "高可用", 
                    "tagLink": "https://api.zhihu.com/topics/19671201"
                }, 
                {
                    "tag": "负载均衡", 
                    "tagLink": "https://api.zhihu.com/topics/19596611"
                }
            ], 
            "comments": [
                {
                    "userName": "lyrids", 
                    "userLink": "https://www.zhihu.com/people/317a02085c9a1228a23242220c82a248", 
                    "content": "深坑啊 也说明 C/C++代码想写安全了有多么困难", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/59230042", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 5, 
            "title": "减少 InfluxDB 内存占用", 
            "content": "<p>Kubernetes 1.12 <a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes-retired/heapster/blob/master/docs/deprecation.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">以前</a>使用 Heapster 做集群监控，以提供数据给 <code>kubectl top</code> 命令以及 kubernetes-dashboard 的 Web UI 查看。Heapster 默认使用 InfluxDB，而 InfluxDB 默认设置的 retention policy 是七天一个 shard，无限保留，使用内存索引，所以时间久了后 InfluxDB 会消耗接近 10GB 内存。</p><p>InfluxDB 这个 default retention policy 无法在环境变量和配置文件里修改，只能在 <code>CRETE DATABASE WITH DURATION m REPLICATION 1 SHARD DURATION n</code> 或者创建数据库后用 <code>ALTER RETENTION POLICY autogen ON db DURATION m REPLICATION 1 SHARD DURATION n</code>修改。其员工 <a href=\"https://link.zhihu.com/?target=https%3A//www.linkedin.com/in/michael-desa-b3ab9823\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Michael Desa</a> 在 stackoverflow 上<a href=\"https://link.zhihu.com/?target=https%3A//stackoverflow.com/questions/41620595/how-to-set-default-retention-policy-and-duration-for-influxdb-via-configuration\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">解释</a>了下:</p><blockquote>Unfortunately there is no way to set the default retention policy via the configuration. The reason for this is that typically retention policy duration is defined during database creation.<br/>If users were allowed to set a default retention duration via the configuration, the results of the command <code>CREATE DATABASE mydb</code> would vary from instance to instance. While this isn’t necessarily problematic, it isn’t ideal either.</blockquote><p>个人觉得这个解释很牵强，可以类似 InfluxDB 的 <a href=\"https://link.zhihu.com/?target=https%3A//docs.influxdata.com/influxdb/v1.7/administration/config/%23configuration-settings\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Graphite listener</a> 那样，提供多个环境变量比如 <code>INFLUXDB_RETENTION_POLICY_&lt;db&gt;_&lt;policy&gt;=&#34;DURATION 30d REPLICATION 1 SHARD DURATION 30m&#34;</code> 这样配置嘛，其中 <code>INFLUXDB_RETENTION_POLICY_DEFAULT</code> 是特殊的默认设置。</p><p>不过不管怎样，不改代码的话只能事后去修改 RETENTION POLICY 了：</p><div class=\"highlight\"><pre><code class=\"language-text\"># kubectl exec -it 进入一个带 curl 命令的 pod\n\ncurl -s monitoring-influxdb.kube-system:8086/query?pretty=true --data-urlencode &#39;q=alter retention policy default on k8s duration 1d replication 1 shard duration 30m&#39;\n\ncurl -s monitoring-influxdb.kube-system:8086/query?pretty=true --data-urlencode &#39;q=alter retention policy monitor on _internal duration 1d replication 1 shard duration 30m&#39;\n\n\ncurl -s monitoring-influxdb.kube-system:8086/query?pretty=true --data-urlencode &#39;q=show retention policies on k8s&#39;\n\ncurl -s monitoring-influxdb.kube-system:8086/query?pretty=true --data-urlencode &#39;q=show retention policies on _internal&#39;\n\ncurl -s monitoring-influxdb.kube-system:8086/query?pretty=true --data-urlencode &#39;q=show shards&#39;\n\n# 这里 N 是 shard id，可以从上一条命令中解析得到并批量处理\ncurl -s monitoring-influxdb.kube-system:8086/query?pretty=true --data-urlencode &#39;q=drop shard N&#39;\n\n# 然后 kubectl exec -it 进入 kube-system namespace 的 monitoring-influxdb-xxxxx 容器\n# 执行 kill 1 让 pod 重启\n\n# 使用上面的 show shards 和 show retention polices 命令查看生效与否，这是为了避免 K8S 把\n# influxdb 调度到另外一台机器上，重新初始化了 /data/ 目录。</code></pre></div><p>除此之外，还可以 <code>kubectl edit -n kube-system deploy/monitoring-influxdb</code> 加几个环境变量以限制 InfluxDB 的资源消耗：</p><div class=\"highlight\"><pre><code class=\"language-text\">GOMAXPROCS=2\nINFLUXDB_DATA_INDEX_VERSION=tsi1\nINFLUXDB_DATA_MAX_CONCURRENT_COMPACTIONS=1</code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "Kubernetes", 
                    "tagLink": "https://api.zhihu.com/topics/20018384"
                }, 
                {
                    "tag": "InfluxDB", 
                    "tagLink": "https://api.zhihu.com/topics/20062289"
                }
            ], 
            "comments": [
                {
                    "userName": "lyrids", 
                    "userLink": "https://www.zhihu.com/people/317a02085c9a1228a23242220c82a248", 
                    "content": "<p>赞! 正是这些点点滴滴的体会才形成真正\"吃透\"一个复杂的系统, 也就所谓的 Know How -- 如果没有这些实际的遇到问题思考并解决的不断迭代, 对一个产品/系统的理解可能只能停留在PPT/产品说明/简单Demo的层面, 无法深入下去. 可能和潜水一个道理, 在水面和岸上只能听别人介绍水下有多美, 但只有自己潜下去才能真正体会到美, 以及美之外的坑坑坎坎.</p>", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "Ryan", 
                    "userLink": "https://www.zhihu.com/people/080f9a8b1394da17594f1d63ed6184c6", 
                    "content": "<p>影响InfluxDB内存占用的因素很多，活跃的shard数量、每个shard的cache的内存上下限、查询的限制等等都有直接关系。</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/52622856", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 165, 
            "title": "阿里云安全组之静默丢包", 
            "content": "<h2>一、引子：Redis client library 连接 Redis server 超时</h2><p>差不多一两年前，在阿里云上遇到一个奇怪的 Redis 连接问题，每隔十来分钟，服务里的 Redis client 库就报告连接 Redis server 超时，当时花了很大功夫，发现是阿里云会断开长时间闲置的 TCP 连接，不给两头发 FIN or RST 包，而当时我们的 Redis server 没有打开 tcp_keepalive 选项，于是 Redis server 侧那个连接还存在于 Linux conntrack table 里，而 Redis client 侧由于连接池重用连接进行 get、set 发现连接坏掉就关闭了，所以 client 侧的对应 local port 回收了，当接下来 Redis 重用这个 local port 向 Redis server 发起连接时，由于 Redis server 侧的 conntrack table 里 &lt;client_ip, client_port,  redis-server, 6379&gt; 四元组对应状态是 ESTABLISHED，所以自然客户端发来的 TCP SYN packet 被丢弃，Redis client 看到的现象就是连接超时。</p><p>解决这个问题很简单，打开 Redis server 的  tcp_keepalive  选项就行。 <b>然而当时没想到，这个问题深层次的原因影响很重大，后果很严重！</b></p><hr/><h2>二、孽债：&#34;SELECT 1&#34; 触发的 jdbc4.CommunicationsException</h2><p>最近生产环境的 Java 服务几乎每分钟都报告类似下面这种错误：</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"o\">[</span>main<span class=\"o\">]</span> ERROR jdbc.audit - <span class=\"m\">2</span>. Statement.execute<span class=\"o\">(</span><span class=\"k\">select</span> <span class=\"m\">1</span><span class=\"o\">)</span> <span class=\"k\">select</span> <span class=\"m\">1</span> \n\ncom.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure\n\nThe last packet successfully received from the server was <span class=\"m\">576</span>,539 milliseconds ago.  The last packet sent successfully to the server was <span class=\"m\">5</span> milliseconds ago.\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0<span class=\"o\">(</span>Native Method<span class=\"o\">)</span>\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance<span class=\"o\">(</span>NativeConstructorAccessorImpl.java:62<span class=\"o\">)</span>\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance<span class=\"o\">(</span>DelegatingConstructorAccessorImpl.java:45<span class=\"o\">)</span>\n        at java.lang.reflect.Constructor.newInstance<span class=\"o\">(</span>Constructor.java:423<span class=\"o\">)</span>\n        at com.mysql.jdbc.Util.handleNewInstance<span class=\"o\">(</span>Util.java:425<span class=\"o\">)</span>\n        at com.mysql.jdbc.SQLError.createCommunicationsException<span class=\"o\">(</span>SQLError.java:990<span class=\"o\">)</span>\n        at com.mysql.jdbc.MysqlIO.reuseAndReadPacket<span class=\"o\">(</span>MysqlIO.java:3559<span class=\"o\">)</span>\n        at com.mysql.jdbc.MysqlIO.reuseAndReadPacket<span class=\"o\">(</span>MysqlIO.java:3459<span class=\"o\">)</span>\n        at com.mysql.jdbc.MysqlIO.checkErrorPacket<span class=\"o\">(</span>MysqlIO.java:3900<span class=\"o\">)</span>\n        at com.mysql.jdbc.MysqlIO.sendCommand<span class=\"o\">(</span>MysqlIO.java:2527<span class=\"o\">)</span>\n        at com.mysql.jdbc.MysqlIO.sqlQueryDirect<span class=\"o\">(</span>MysqlIO.java:2680<span class=\"o\">)</span>\n        at com.mysql.jdbc.ConnectionImpl.execSQL<span class=\"o\">(</span>ConnectionImpl.java:2480<span class=\"o\">)</span>\n        at com.mysql.jdbc.ConnectionImpl.execSQL<span class=\"o\">(</span>ConnectionImpl.java:2438<span class=\"o\">)</span>\n        at com.mysql.jdbc.StatementImpl.executeInternal<span class=\"o\">(</span>StatementImpl.java:845<span class=\"o\">)</span>\n        at com.mysql.jdbc.StatementImpl.execute<span class=\"o\">(</span>StatementImpl.java:745<span class=\"o\">)</span>\n        at net.sf.log4jdbc.StatementSpy.execute<span class=\"o\">(</span>StatementSpy.java:842<span class=\"o\">)</span>\n        at com.zaxxer.hikari.pool.PoolBase.isConnectionAlive<span class=\"o\">(</span>PoolBase.java:158<span class=\"o\">)</span>\n        at com.zaxxer.hikari.pool.HikariPool.getConnection<span class=\"o\">(</span>HikariPool.java:172<span class=\"o\">)</span>\n        at com.zaxxer.hikari.pool.HikariPool.getConnection<span class=\"o\">(</span>HikariPool.java:148<span class=\"o\">)</span>\n        at com.zaxxer.hikari.HikariDataSource.getConnection<span class=\"o\">(</span>HikariDataSource.java:100<span class=\"o\">)</span>\n        at cn.yingmi.hikari.Main.main<span class=\"o\">(</span>Main.java:30<span class=\"o\">)</span>\nCaused by: java.io.EOFException: Can not <span class=\"nb\">read</span> response from server. Expected to <span class=\"nb\">read</span> <span class=\"m\">4</span> bytes, <span class=\"nb\">read</span> <span class=\"m\">0</span> bytes before connection was unexpectedly lost.\n        at com.mysql.jdbc.MysqlIO.readFully<span class=\"o\">(</span>MysqlIO.java:3011<span class=\"o\">)</span>\n        at com.mysql.jdbc.MysqlIO.reuseAndReadPacket<span class=\"o\">(</span>MysqlIO.java:3469<span class=\"o\">)</span>\n        ... <span class=\"m\">14</span> more</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>由于有之前调查 Redis 连接被阿里云异常中断的先例，所以怀疑是类似问题，花了大量时间比对客户端和服务端的 conntrack table，然而并没有引子中描述的问题，然后又去比对多个 MySQL 服务器的 sysctl 设置，研究 iptables TRACE，研究 tcpdump 抓到的报文，试验 tw_reuse, tw_recyle 等参数，调整 Aliyun 负载均衡器后面挂载的 MySQL 服务器个数，都没效果， 反而意外发现一个<b>**新问题**</b>，在用如下命令<b>不经过</b>阿里云 SLB <b>直接连接</b>数据库时，有的数据库可以在 600s 时返回，有的则客户端一直挂着，半个多小时了都退不出来，按 ctrl-c 中断都不行。</p><div class=\"highlight\"><pre><code class=\"language-bash\">mysql -h mysql-server-ip -u mysql-user -p -e <span class=\"s1\">&#39;SELECT sleep(1000)&#39;</span></code></pre></div><p>当时检查了一个正常的数据库和一个不正常的数据库，发现两者的 wait_timeout 和 interactive_timeout 都是 600s，思索良苦，没明白怎么回事，然后偶然发现另外一个数据库的 wait_timeout=<b>60</b>s，却一下子明白了原始的 &#34;select 1&#34; 问题怎么回事。</p><p>我们的服务使用了  <a href=\"https://link.zhihu.com/?target=https%3A//github.com/brettwooldridge/HikariCP\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Hikari JDBC 连接池</a>，它的 idleTimeout 默认是 600s, maxLifetime 默认是 1800s，前者表示 idle JDBC connection 数量超过 minimumIdle 数目并且闲置时间超过 idleTimeout 则关闭此 idle connection，后者表示连接池里的 connection 其生存时间不能超过 maxLifetime，到点了会被关掉。</p><p>在发现 &#34;select 1&#34; 问题后，我们以为是这俩参数比数据库的 wait_timeout=600s 大的缘故，所以把这两个参数缩小了，idleTimeout=570s,  maxLifetime=585s，并且设置了 minimumIdle=5。但这两个时间设置依然大于其中一个数据库<b>失误</b>设置的 wait_timeout=60s，所以闲置连接在 60s 后被 MySQL server 主动关闭，而 JDBC 并没有什么事件触发回调机制去关闭 JDBC connection，时间上也不够 Hikari 触发 idleTimeout 和 maxLifetime 清理逻辑，所以 Hikari 拿着这个“已经关闭”的连接，发了 &#34;select 1&#34; SQL 给服务器检查连接有效性，于是触发了上面的异常。</p><p>解决办法很简单，把那个错误配置的数据库里 wait_timeout 从 60s 修正成 600s 就行了。 <b>下面继续讲述 &#34;SELECT  sleep(1000)&#34; 会挂住退不出来的问题。</b></p><hr/><h2>三、缘起：阿里云安全组与 TCP KeepAlive</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>最近看了一点佛教常识，对”诸法由因缘而起“的缘起论很是感慨，在调查 &#34;SELECT sleep(1000)&#34; 问题中，真实感受到了“由因缘而起” 的意思😄</p><p>首先解释下，为什么有的数据库服务器对 &#34;SELECT sleep(1000)&#34; 可以返回，有的却挂着退不出来。 其实 wait_timeout 和 interactive_timeout 两个参数只对 “闲置” 的数据库连接，也即没有 SQL 正在执行的连接生效，对于 &#34;SELECT sleep(1000)&#34;，这是有一个正在执行的 SQL，其最大执行时间受 MySQL Server 的 max_execution_time 限制，这个参数在我司一般设置为 600s，这就是 “正常的数据库&#34; 在 600s 时 &#34;SELECT sleep(1000)&#34; 中断执行而退出了。</p><p>但不走运的是（可以说又是个失误配置😓），我们有的数据库 max_execution_time 是 6000s，所以 &#34;SELECT sleep(1000)&#34; 在 MySQL server 服务端会在 1000s 时正常执行结束——但问题是，通过二分查找以及 tcpdump、iptables TRACE，发现<b>阿里云会”静默“丢弃 &gt;=910s idle TCP connection，不给客户端、服务端发送 FIN or RST 以强行断掉连接</b>，于是 MySQL server 在 1000s 结束时发给客户端的 ACK+PSH TCP packet 到达不了客户端，然后再过 wait_timeout=600s，MySQL server 就断开了这个闲置连接——可怜的是，mysql client 这个命令行程序还一无所知，它很执着的等待 MySQL server 返回，Linux 内核的 conntrack table 显示这个连接一直是 ESTABLISHED，哪怕 MySQL server 端已经关闭对应的连接了，只是这个关闭动作的 FIN TCP packet 到不了客户端！</p><p>下面是 iptables TRACE 日志对这个问题的实锤证明。</p><p>mysql 命令行所在机器的 iptables TRACE 日志表明，mysql client 在 23:58:25 连接上了 mysql server，开始执行 SELECT sleep(1000)，然后一直收不到服务器消息，最后在 00:41:20 的时候我手动 kill 了 mysql 客户端命令行进程，mysql 客户端给 mysql server 发 FIN 包但收不到响应（此时 mysql server 端早关闭连接了）。</p><div class=\"highlight\"><pre><code class=\"language-text\">Dec 14 23:58:25 client-host kernel: [3156060.500323] TRACE: raw:OUTPUT:policy:3 IN= OUT=eth0 SRC=10.66.94.67 DST=10.31.76.36 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=43069 DF PROTO=TCP SPT=38870 DPT=3306 SEQ=2419961697 ACK=0 WINDOW=29200 RES=0x00 SYN URGP=0 OPT (020405B40402080A2F06075C0000000001030307) UID=0 GID=0 \nDec 14 23:58:25 client-host kernel: [3156060.500334] TRACE: nat:OUTPUT:rule:1 IN= OUT=eth0 SRC=10.66.94.67 DST=10.31.76.36 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=43069 DF PROTO=TCP SPT=38870 DPT=3306 SEQ=2419961697 ACK=0 WINDOW=29200 RES=0x00 SYN URGP=0 OPT (020405B40402080A2F06075C0000000001030307) UID=0 GID=0 \nDec 14 23:58:25 client-host kernel: [3156060.500419] TRACE: nat:KUBE-SERVICES:return:86 IN= OUT=eth0 SRC=10.66.94.67 DST=10.31.76.36 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=43069 DF PROTO=TCP SPT=38870 DPT=3306 SEQ=2419961697 ACK=0 WINDOW=29200 RES=0x00 SYN URGP=0 OPT (020405B40402080A2F06075C0000000001030307) UID=0 GID=0 \n....\nDec 14 23:58:25 client-host kernel: [3156060.539844] TRACE: filter:KUBE-FIREWALL:return:2 IN=eth0 OUT= MAC=00:16:3e:12:09:f0:44:6a:2e:94:ef:00:08:00 SRC=10.31.76.36 DST=10.66.94.67 LEN=52 TOS=0x00 PREC=0x00 TTL=61 ID=9283 DF PROTO=TCP SPT=3306 DPT=38870 SEQ=2025462702 ACK=2419961951 WINDOW=235 RES=0x00 ACK URGP=0 OPT (0101080AFF6B3EAF2F06075D) UID=0 GID=0 \nDec 14 23:58:25 client-host kernel: [3156060.539849] TRACE: filter:INPUT:policy:3 IN=eth0 OUT= MAC=00:16:3e:12:09:f0:44:6a:2e:94:ef:00:08:00 SRC=10.31.76.36 DST=10.66.94.67 LEN=52 TOS=0x00 PREC=0x00 TTL=61 ID=9283 DF PROTO=TCP SPT=3306 DPT=38870 SEQ=2025462702 ACK=2419961951 WINDOW=235 RES=0x00 ACK URGP=0 OPT (0101080AFF6B3EAF2F06075D) UID=0 GID=0 \n\nDec 15 00:41:20 client-host kernel: [3158634.985792] TRACE: raw:OUTPUT:policy:3 IN= OUT=eth0 SRC=10.66.94.67 DST=10.31.76.36 LEN=52 TOS=0x00 PREC=0x00 TTL=64 ID=43075 DF PROTO=TCP SPT=38870 DPT=3306 SEQ=2419961951 ACK=2025462702 WINDOW=237 RES=0x00 ACK FIN URGP=0 OPT (0101080A2F0FD975FF6B3EAF) UID=0 GID=0 \nDec 15 00:41:20 client-host kernel: [3158634.985805] TRACE: filter:OUTPUT:rule:1 IN= OUT=eth0 SRC=10.66.94.67 DST=10.31.76.36 LEN=52 TOS=0x00 PREC=0x00 TTL=64 ID=43075 DF PROTO=TCP SPT=38870 DPT=3306 SEQ=2419961951 ACK=2025462702 WINDOW=237 RES=0x00 ACK FIN URGP=0 OPT (0101080A2F0FD975FF6B3EAF) UID=0 GID=0 \nDec 15 00:41:20 client-host kernel: [3158634.985812] TRACE: filter:KUBE-SERVICES:return:5 IN= OUT=eth0 SRC=10.66.94.67 DST=10.31.76.36 LEN=52 TOS=0x00 PREC=0x00 TTL=64 ID=43075 DF PROTO=TCP SPT=38870 DPT=3306 SEQ=2419961951 ACK=2025462702 WINDOW=237 RES=0x00 ACK FIN URGP=0 OPT (0101080A2F0FD975FF6B3EAF) UID=0 GID=0 \n....\nDec 15 00:42:13 client-host kernel: [3158688.341777] TRACE: filter:KUBE-FIREWALL:return:2 IN= OUT=eth0 SRC=10.66.94.67 DST=10.31.76.36 LEN=52 TOS=0x00 PREC=0x00 TTL=64 ID=43084 DF PROTO=TCP SPT=38870 DPT=3306 SEQ=2419961951 ACK=2025462702 WINDOW=237 RES=0x00 ACK FIN URGP=0 OPT (0101080A2F100D90FF6B3EAF) \nDec 15 00:42:13 client-host kernel: [3158688.341782] TRACE: filter:OUTPUT:policy:3 IN= OUT=eth0 SRC=10.66.94.67 DST=10.31.76.36 LEN=52 TOS=0x00 PREC=0x00 TTL=64 ID=43084 DF PROTO=TCP SPT=38870 DPT=3306 SEQ=2419961951 ACK=2025462702 WINDOW=237 RES=0x00 ACK FIN URGP=0 OPT (0101080A2F100D90FF6B3EAF) \n\n</code></pre></div><p>MySQL server 在 00:15:05 时执行 SELECT sleep(1000) 结束，给 mysql 客户端回送结果，但 mysql 客户端无响应（被阿里云丢包了，mysql 客户端压根收不到），在 00:25:05 时，由于 wait_timeout=600s，所以 MySQL server 给 mysql 客户端发 FIN 包以断开连接，自然，mysql 客户端收不到，所以也没有回应，结局是 MySQL server 一侧的 Linux 内核反正自行关闭 TCP 连接了，mysql client 一侧的 Linux 内核还在傻乎乎的在 conntrack table 维持着 ESTABLISHED 状态的 TCP 连接，mysql client 命令行还在傻乎乎的 recv() 等着服务端返回或者关闭链接。</p><div class=\"highlight\"><pre><code class=\"language-text\">Dec 15 00:15:05 mysql-server kernel: [34321950.128215] TRACE: raw:OUTPUT:policy:3 IN= OUT=eth0 SRC=10.31.76.36 DST=10.66.94.67 LEN=111 TOS=0x00 PREC=0x00 TTL=64 ID=9284 DF PROTO=TCP SPT=3306 DPT=38870 SEQ=2025462702 ACK=2419961951 WINDOW=235 RES=0x00 ACK PSH URGP=0 OPT (0101080AFF6F0F362F06075D) UID=999 GID=999 \nDec 15 00:15:05 mysql-server kernel: [34321950.128225] TRACE: filter:OUTPUT:policy:1 IN= OUT=eth0 SRC=10.31.76.36 DST=10.66.94.67 LEN=111 TOS=0x00 PREC=0x00 TTL=64 ID=9284 DF PROTO=TCP SPT=3306 DPT=38870 SEQ=2025462702 ACK=2419961951 WINDOW=235 RES=0x00 ACK PSH URGP=0 OPT (0101080AFF6F0F362F06075D) UID=999 GID=999 \n....\nDec 15 00:24:36 mysql-server kernel: [34322520.278151] TRACE: raw:OUTPUT:policy:3 IN= OUT=eth0 SRC=10.31.76.36 DST=10.66.94.67 LEN=111 TOS=0x00 PREC=0x00 TTL=64 ID=9298 DF PROTO=TCP SPT=3306 DPT=38870 SEQ=2025462702 ACK=2419961951 WINDOW=235 RES=0x00 ACK PSH URGP=0 OPT (0101080AFF713C002F06075D) UID=999 GID=999 \nDec 15 00:24:36 mysql-server kernel: [34322520.278188] TRACE: filter:OUTPUT:policy:1 IN= OUT=eth0 SRC=10.31.76.36 DST=10.66.94.67 LEN=111 TOS=0x00 PREC=0x00 TTL=64 ID=9298 DF PROTO=TCP SPT=3306 DPT=38870 SEQ=2025462702 ACK=2419961951 WINDOW=235 RES=0x00 ACK PSH URGP=0 OPT (0101080AFF713C002F06075D) UID=999 GID=999 \n\nDec 15 00:25:05 mysql-server kernel: [34322550.135731] TRACE: raw:OUTPUT:policy:3 IN= OUT=eth0 SRC=10.31.76.36 DST=10.66.94.67 LEN=52 TOS=0x00 PREC=0x00 TTL=64 ID=9299 DF PROTO=TCP SPT=3306 DPT=38870 SEQ=2025462761 ACK=2419961951 WINDOW=235 RES=0x00 ACK FIN URGP=0 OPT (0101080AFF7159282F06075D) UID=999 GID=999 \nDec 15 00:25:05 mysql-server kernel: [34322550.135742] TRACE: filter:OUTPUT:policy:1 IN= OUT=eth0 SRC=10.31.76.36 DST=10.66.94.67 LEN=52 TOS=0x00 PREC=0x00 TTL=64 ID=9299 DF PROTO=TCP SPT=3306 DPT=38870 SEQ=2025462761 ACK=2419961951 WINDOW=235 RES=0x00 ACK FIN URGP=0 OPT (0101080AFF7159282F06075D) UID=999 GID=999 </code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>Ok，现在知道是阿里云对 &gt;= 910s 没有发生 TCP packet 传输的<b>虚拟机之间直连</b>闲置 TCP 连接会“静默”丢包，那么是任意虚拟机之间吗？是任意端口吗？要求服务器挂到负载均衡器后面吗？要求对应端口的并发连接到一定数目吗？</p><p>在阿里云提交工单询问后，没得到什么有价值信息，在经过艰苦卓绝的试验后——每一次试验要等近二十分钟啊——终于功夫不负有心人，我发现一个稳定复现问题的规律了：</p><ol><li>两台虚拟机分别处于不同安全组，没有共同安全组；</li><li>服务端的安全组开放端口 P 允许客户端的安全组连接，客户端不开放端口给服务端（按照一般<b>有状态</b>防火墙的配置规则，都是只开服务端端口，不用开客户端端口）；</li><li>客户端和服务端连接上后，闲置 &gt;= 910s，不传输任何数据，也不传输有 keep alive 用途的 ack 包；</li><li>然后服务端在此长连接上发给客户端的 TCP 包会在网络上丢弃，到不了客户端；</li><li>但如果客户端此时给服务端发点数据，那么会重新“激活”这条长链接，但此时还是<b>单工</b>状态，客户端能给服务端发包，服务端的包还到不了客户端（大概是在服务端 OS 内核里重试中）；</li><li>激活后，服务端再给客户端发数据时，之前发送不出去的数据（如果还在内核里的 TCP/IP  协议栈重试中），加上新发的数据，会一起到达客户端，此后这条长连接进入正常的<b>双工</b>工作状态；</li></ol><p>下图是用 nc 试验的结果。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-be139525f4cbede2cec1491f75521f89_b.jpg\" data-size=\"normal\" data-rawwidth=\"834\" data-rawheight=\"406\" class=\"origin_image zh-lightbox-thumb\" width=\"834\" data-original=\"https://pic2.zhimg.com/v2-be139525f4cbede2cec1491f75521f89_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;834&#39; height=&#39;406&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"834\" data-rawheight=\"406\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"834\" data-original=\"https://pic2.zhimg.com/v2-be139525f4cbede2cec1491f75521f89_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-be139525f4cbede2cec1491f75521f89_b.jpg\"/><figcaption>服务端</figcaption></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-22c69e339f290933bdb7451ba57a96b8_b.jpg\" data-size=\"normal\" data-rawwidth=\"870\" data-rawheight=\"380\" class=\"origin_image zh-lightbox-thumb\" width=\"870\" data-original=\"https://pic1.zhimg.com/v2-22c69e339f290933bdb7451ba57a96b8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;870&#39; height=&#39;380&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"870\" data-rawheight=\"380\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"870\" data-original=\"https://pic1.zhimg.com/v2-22c69e339f290933bdb7451ba57a96b8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-22c69e339f290933bdb7451ba57a96b8_b.jpg\"/><figcaption>客户端</figcaption></figure><p>在跟网友讨论后，认识到这应该是阿里云安全组基于“集中式防火墙”实现导致的，因为集中式防火墙处于网络中心枢纽，它要应付海量连接，因此其内存里的 conntrack table 需要比较短的 idle timeout（目前是 910s），以把长时间没活跃的 conntrack record 清理掉节约内存，所以上面问题的根源就清晰了：</p><ol><li>client 连接 server，安全组（其实是防火墙）发现规则允许，于是加入一个记录到 conntrack table；</li><li>client 和 server 到了 910s 还没数据往来，所以安全组把 conntrack 里那条记录去掉了；</li><li>server 在 910s 之后给 client 发数据，数据包到了安全组那里，它一看 conntrack table 里没记录，而 client 侧安全组又不允许这个端口的包通过，所以丢包了，于是 server -&gt; client 不通；</li><li>client 在同一个长连接上给 server 发点数据，安全组一看规则允许，于是加入 conntrack table 里；</li><li>server 重试的数据包，或者新数据包，通过安全组时，由于已经有 conntrack record 了，所以放行，于是能到达客户端了。</li></ol><hr/><p>原因知道了，怎么绕过这个问题呢？阿里云给了我两个无法接受的 workaround：</p><ol><li>把 server、client 放进同一个安全组；</li><li>修改 client 所在安全组，开放所有端口给 server 所在安全组；</li></ol><p>再琢磨下，通过 netstat -o 发现我们的 Java 服务使用的 Jedis 库和 mysql JDBC 库都对 socket 文件句柄打开了 <a href=\"https://link.zhihu.com/?target=http%3A//www.tldp.org/HOWTO/TCP-Keepalive-HOWTO/usingkeepalive.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">SO_KEEPALIVE 选项</a>：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-dd488310b20a1bad8e1c5fbb4b53ab68_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2566\" data-rawheight=\"390\" class=\"origin_image zh-lightbox-thumb\" width=\"2566\" data-original=\"https://pic1.zhimg.com/v2-dd488310b20a1bad8e1c5fbb4b53ab68_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2566&#39; height=&#39;390&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2566\" data-rawheight=\"390\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2566\" data-original=\"https://pic1.zhimg.com/v2-dd488310b20a1bad8e1c5fbb4b53ab68_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-dd488310b20a1bad8e1c5fbb4b53ab68_b.jpg\"/></figure><p>而 MySQL server 也对其打开的 socket 文件句柄打开了 SO_KEEPALIVE 选项，所以我只用修改下服务端和客户端至少其中一侧的对应 sysctl 选项即可，下面是我司服务端的默认配置，表示 TCP 连接闲置 1800s 后，每隔 30s 给对方发一个 ACK 包，最多发 3 次，如果在此期间对方回复了，则计时器重置，再等 1800s 闲置条件，如果发了 3 次后对方没反应，那么会给对端发 RST 包同时关闭本地的 socket 文件句柄，也即关闭这条长连接。</p><div class=\"highlight\"><pre><code class=\"language-text\">net.ipv4.tcp_keepalive_intvl = 30\nnet.ipv4.tcp_keepalive_probes = 3\nnet.ipv4.tcp_keepalive_time = 1800</code></pre></div><p>由于阿里云跨安全组的 910s idle timeout 限制，所以需要把 net.ipv4.tcp_keepalive_time 设置成小于 910s，比如 300s。</p><p>默认的 tcp_keepalive_time 特别大，这也解释了为什么当初 Redis client 设置了 SO_KEEPALIVE 选项后还是被阿里云静默断开。</p><p>如果某些网络库封装之后没有提供 setsockopt() 调用的机会，那么需要用 LD_PRELOAD 之类的黑科技强行设置了，<b>只有打开了 socket 文件句柄的 SO_KEEPALIVE 选项，上面三个 sysctl 才对这个 socket 文件句柄生效</b>，当然，代码里可以用 setsockopt() 函数进一步设置 keep_alive_intvl 和 keepalive_probes，不用 Linux 内核的全局默认设置。</p><p>最后，除了 Java 家对 SO_KEEPALIVE 处理的很好，利用 netstat -o 检查得知，对门的 NodeJS 家，其著名 Redis client library 开了 SO_KEEPALIVE 但其著名 mysql client library <b>并没有开</b>，而 Go lang 家则严谨多了，两个库都开了 SO_KEEPALIVE。 <b>为什么引子里说这个问题很严重呢？因为但凡服务端处理的慢点，比如 OLAP 场景，不经过阿里云 SLB 直连服务端在 910s 之内没返回数据的话，就有可能没机会返回数据给客户端了啊，这个问题查死人有没有！ </b>你可能问我为啥不通过阿里云 SLB 中转，SLB 不会静默丢包啊——但它的 idle timeout 上限是 900s 啊！！！</p>", 
            "topic": [
                {
                    "tag": "阿里云", 
                    "tagLink": "https://api.zhihu.com/topics/19560108"
                }, 
                {
                    "tag": "MySQL", 
                    "tagLink": "https://api.zhihu.com/topics/19554128"
                }, 
                {
                    "tag": "Java", 
                    "tagLink": "https://api.zhihu.com/topics/19561132"
                }
            ], 
            "comments": [
                {
                    "userName": "君枫", 
                    "userLink": "https://www.zhihu.com/people/ea8ebdafe5ac562d0a4a1c0698117b9f", 
                    "content": "<p>赞，虽不是阿里云，但也遇到了一样的问题，在我们应用和数据之间有防火墙，一定时间内无数据传输会自动断开连接，但是应用层无感知，造成页面卡死。</p><p></p>", 
                    "likes": 5, 
                    "childComments": []
                }, 
                {
                    "userName": "少年怒了", 
                    "userLink": "https://www.zhihu.com/people/8bbd171219c75100290bfa08c68c86a7", 
                    "content": "<p>特意跑到知乎来感谢您的刨坑，我们也遇到了，但是只当是灵异问题，没细究。</p>", 
                    "likes": 2, 
                    "childComments": []
                }, 
                {
                    "userName": "嵇康", 
                    "userLink": "https://www.zhihu.com/people/962e31a50c18f3124b02037307e161e0", 
                    "content": "<p>文章中，最后结尾说的 mysql client library 是啥？ mysq命令行客户端么?</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "知乎用户", 
                            "userLink": "https://www.zhihu.com/people/0", 
                            "content": "指各个编程语言的mysql客户端库，比如mysql-connector-java。", 
                            "likes": 0, 
                            "replyToAuthor": "嵇康"
                        }
                    ]
                }, 
                {
                    "userName": "哐噹哐噹", 
                    "userLink": "https://www.zhihu.com/people/588c1e0e9f4aa4ee9965171499b2fc16", 
                    "content": "分析的太好了  不赏一个对不起以后采的坑", 
                    "likes": 1, 
                    "childComments": [
                        {
                            "userName": "知乎用户", 
                            "userLink": "https://www.zhihu.com/people/0", 
                            "content": "感谢！🙏", 
                            "likes": 0, 
                            "replyToAuthor": "哐噹哐噹"
                        }
                    ]
                }, 
                {
                    "userName": "吴昊", 
                    "userLink": "https://www.zhihu.com/people/fdd8d8414b276f736d7561cf853f27fb", 
                    "content": "这个问题排查不容易，得对tcp协议非常了解才行，赞一个", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>这你都能查出来</p><a class=\"comment_sticker\" href=\"https://pic4.zhimg.com/v2-db92f653a2ec17ea3ff309d6d56e8507.gif\" data-width=\"\" data-height=\"\">[吃瓜]</a>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "jacarrichan", 
                    "userLink": "https://www.zhihu.com/people/db963c1bb377f7ed7839dc77ec19c184", 
                    "content": "<p>我这代码回滚的时候出现这样的问题 ，还是在抢购的时候，真是烦死了。</p><p><br></p><p><br></p><p>com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Communications link failure during rollback(). Transaction resolution unknown.</p><p><br></p><p>Caused by: java.io.EOFException: Can not read response from server. Expected to read 4 bytes, read 0 bytes before connection was unexpectedly lost.</p><p>        at com.mysql.jdbc.MysqlIO.readFully(<a href=\"http://link.zhihu.com/?target=http%3A//MysqlIO.java%3A3161\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">MysqlIO.java:3161</a>)</p><p>        at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(<a href=\"http://link.zhihu.com/?target=http%3A//MysqlIO.java%3A3615\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">MysqlIO.java:3615</a>)</p><p>        ... 89 common frames omitted</p><p></p><p></p>", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "吕晓阳", 
                    "userLink": "https://www.zhihu.com/people/b8cae6bbcb447719c9981eaaded9e68d", 
                    "content": "<p>服务器连接数据库，是典型的安全组内网互通场景。规规矩矩的用安全组互通，避免不必要的麻烦。Pls do not fuck yourself, OK?</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "知乎用户", 
                            "userLink": "https://www.zhihu.com/people/0", 
                            "content": "请问客户端连接数据库的3306端口时，客户端用的什么端口？如果在安全组打开客户端这个端口，那么进入这个客户端端口的SYN包要放行吗？", 
                            "likes": 1, 
                            "replyToAuthor": "吕晓阳"
                        }
                    ]
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "阿里云那层应该是在IP，不发fin是对的。", 
                    "likes": 1, 
                    "childComments": [
                        {
                            "userName": "知乎用户", 
                            "userLink": "https://www.zhihu.com/people/0", 
                            "content": "不是想让它发FIN，而是别静默丢包。", 
                            "likes": 0, 
                            "replyToAuthor": "知乎用户"
                        }, 
                        {
                            "userName": "知乎用户", 
                            "userLink": "https://www.zhihu.com/people/0", 
                            "content": "<p>不是静默丢包, 只是失效了之前的nat转发记录.云那边本身没有问题.应用层的问题.</p>", 
                            "likes": 0, 
                            "replyToAuthor": "知乎用户"
                        }
                    ]
                }, 
                {
                    "userName": "jacarrichan", 
                    "userLink": "https://www.zhihu.com/people/db963c1bb377f7ed7839dc77ec19c184", 
                    "content": "<p>【mysql 命令行所在机器的 iptables TRACE 日志表明】  请问LZ的iptable raw表是怎么设置的啊？  我也想得到那个TRACE日志。</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "知乎用户", 
                            "userLink": "https://www.zhihu.com/people/0", 
                            "content": "上Bing搜索下iptables trace噻……", 
                            "likes": 0, 
                            "replyToAuthor": "jacarrichan"
                        }
                    ]
                }, 
                {
                    "userName": "吴天龙", 
                    "userLink": "https://www.zhihu.com/people/1813ba49e9eac9046893bb6bde305b48", 
                    "content": "不只是安全组，client-server中间的诸多网络设备都有可能会丢弃长时间没有活动的连接。如果有这种长时间的通信，乖乖开keepalive就好了。", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "幻影火枪手", 
                    "userLink": "https://www.zhihu.com/people/d49a7145330e0d3f0d7e549892833a05", 
                    "content": "<p>中间任何防火墙和NAT、网络设备都有可能丢弃连接，要保证连接稳定，sever端开启keepalive，调整下内核参数就好了。</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50811605", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 4, 
            "title": "对 SaltStack 失望了", 
            "content": "<p>【迁移】对 SaltStack 失望了</p><p>Puppet 的DSL语法像是编程，Chef 更胜，直接 Ruby  编程了，都是复杂的一坨坨，真想不懂运维写代码累不累放心不放心！SaltStack 和 Ansible  都是声明式配置而非命令式，个人喜欢这个路数，最近花了两天功夫耐心仔细看了下 SaltStack  手册，其手册的教程部分真是杂乱，拿了好多篇凑在一起。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>SaltStack 在 CentOS 6.x 上安装很容易，在 EPEL 源里有，配置文件的语法也很简单一致，但仔细看文档后发现 SaltStack 还是很复杂的，概念很多，各种细微差别的指令也很多：</p><ul><li><a href=\"https://link.zhihu.com/?target=https%3A//docs.saltstack.com/en/getstarted/overview.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">docs.saltstack.com/en/g</span><span class=\"invisible\">etstarted/overview.html</span><span class=\"ellipsis\"></span></a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//docs.saltstack.com/en/latest/ref/states/requisites.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">docs.saltstack.com/en/l</span><span class=\"invisible\">atest/ref/states/requisites.html</span><span class=\"ellipsis\"></span></a></li></ul><p>SaltStack  最早是鼓吹 master-minion 架构，用 ZeroMQ 做长连接因此执行效率比批量 ssh 快的多，但现在也加入了  salt-ssh（跟 Ansible 类似，不需要 minion 端预先装 ansible），或者在 minion 端用 salt-minion  服务读取本地 /srv/salt 从而不依赖 salt-master，或者在 minion 端用 cron + salt-call  --local 从而不依赖 salt-minion 也不用跑 salt-minion 服务。</p><p>跟 Puppet 类似，这些脚本语言写的工具耗内存一点都不吝啬，salt-minion 基本空跑能消耗二三十兆 RSS，500兆 VSS，salt-master 嘛耗了三四十兆 RSS，500M ~ 1GB VSS 了，看着很是不爽。</p><p>SaltStack 宣称的快速连接真的是杀手特性吗？配置管理工具主要有三种场合下使用：</p><ol><li>做部署。部署需要考虑不同子系统的先后顺序，以及部署的并发度，以避免对线上服务影响太大，Salt 有个 <a href=\"https://link.zhihu.com/?target=https%3A//docs.saltstack.com/en/latest/topics/tutorials/states_pt5.html%23orchestrate-runner\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">orchestrator</a> 功能达到这个效果，这个东西本质就是个批处理脚本。部署时花在连接上的时间跟 state.apply 的时间相比可以忽略不计。</li><li>做持续的配置收敛。这种事一般都是在被管理机器上 cron job 定期执行，也没啥连接慢的问题。</li><li>ad hoc 的批量命令。一般操作的机器数量不会特别多，也不频繁，一般用 pdsh, parallel-ssh 什么的搞搞完事，ssh 连接慢不是太大问题。</li></ol><p>所以个人觉得 SaltStack 号称速度快没太大意思。</p><p>上面这些都是小事，最让我不能忍的是  SaltStack 不直接支持 role 的概念，官方文档推荐用 grain，但 grain 配置以 minion  机器上的设置为准，这就恶搞了啊，minion 可以随便声称自己的角色，那岂不是乱套了！ 另一个办法是把 role 定义用 pillar  管理，pillar 是在 salt-master 上配置的，但 pillar 本身是要用 targeting 比如 grain, IP 之类的跟  minion 关联的，这就成了鸡生蛋蛋生鸡的矛盾了。 或许有人提到 nodegroup 功能很接近，但这个功能要在  /etc/salt/master 文件里配置，每次改要重启 salt-master，这太二逼了！</p><p>另外 SaltStack 的 minion 和 master 之间容易出现性能问题，有人汇报过，<a href=\"https://link.zhihu.com/?target=https%3A//docs.saltstack.com/en/latest/topics/tutorials/intro_scale.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">解决办法虽然有</a>，但一个工具都要调优这种事很讨人厌啊！<br/></p><p>虽然花了两天时间，配置了一小段 salt，最后决定还是狠下心来抛弃 SaltStack 了，皈依 Ansible 吧，唉，本来还有点眼馋 SaltStack 开源的那个 web UI halite，还没玩过呢。</p>", 
            "topic": [
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }, 
                {
                    "tag": "saltstack", 
                    "tagLink": "https://api.zhihu.com/topics/19841482"
                }
            ], 
            "comments": [
                {
                    "userName": "willww64", 
                    "userLink": "https://www.zhihu.com/people/0294d2d0861a3c92ed20f0abeb90ebab", 
                    "content": "<p>ansible 用了快 3 年了，也算是有经验的老手了。感觉这种用 yaml 声明式的东西，虽然一开始简单，但写些复杂的东西还是很复杂的不直观容易出错，一开始不熟悉的话需要各种翻文档各种改，各种 workaround 式的 feature。熟悉的话也不觉得写起来简单。loop / when 什么的还不如直接用 for / if 简单直观。我倒是想学学 Chef 这种，奈何天天一大堆事情一大堆要学的。大佬熟悉 ansible 后可以来分享一下 ansible 的优缺点。</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "718097281", 
                    "userLink": "https://www.zhihu.com/people/425be0e7391e5898bb42b1e846e3e1b4", 
                    "content": "<p>可能你对salt有什么误解</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "增草剂", 
                    "userLink": "https://www.zhihu.com/people/57e69a27a15e95b594e114a638517230", 
                    "content": "salt可以在master端更具主机名自定义grains 把role写在这里面就完事了", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50811580", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 0, 
            "title": "企业内部信息聚合", 
            "content": "<p>【迁移】企业内部信息聚合</p><p>信息聚合是企业内部非常需要的东西，基本所有公司里头 wiki、ms word、gdoc、email, chat, bugs,<br/>file sharing，各种信息散乱四处。下面简单罗列几个公司里头常用的信息管理系统。</p><p>Seafile 跟 Dropbox 类似，共享文档，此外还可以群组讨论。</p><p>Jive  类似于融合了社交元素以及 twiki 的 multi site 特性的 Drupal，参与者可以 follow 一个主题，follow  一个人，大家可以提问，写文档，写博客，投票等等，所见即所得的编辑方式，可以用 &#34;@&#34; 骚扰别人。没有信息聚合功能。个人觉得使用体验不怎么样。</p><p>Yammer 是 Jive 的竞争对手，被微软收购了。功能上跟 Jive 也很像，也没做信息聚合，它指望用户只用<br/>它一个系统。</p><p>HipChat 是一个 IM 工具，单聊，群聊，@ 提醒，附件，视频聊天等等，最大卖点是聊天记录长久保存，可以搜索。</p><p>OwnCloud 是个文件共享的聚合工具，文件可以存放在任何地方，比如 Jive, SharePoint, NFS, SMB, Dropbox, FTP。</p><p>Slack 的原型是 IRC，用户把各种信息post到 IRC 的聊天室里，这就达到信息聚合的目的。</p>", 
            "topic": [
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50811563", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 12, 
            "title": "关于企业级无线网络部署方案", 
            "content": "<p>【迁移】关于企业级无线网络部署方案</p><p>以前只亲手玩过家用无线路由，对公司里无线怎么部署的毫不知情，这两天田春冰河同学在微博上公开了其 2010 年底<a href=\"https://link.zhihu.com/?target=http%3A//vdisk.weibo.com/s/qFP9Ntw_UVuC/1398398218\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">为网易调研无线部署方案的报告</a>，由此发现这里头道道很多，完全颠覆了我那点浅薄的家用无线路由部署知识。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>无线网络里最基本的硬件设备是 Wireless Access Point，简称 AP，AP 就像网桥，把无线网、有线网或者无线网、无线网连接起来，AP 分两种：</p><ul><li>胖AP(fat AP, standalone AP, autonomous AP)</li><li>瘦AP(fit AP, lightweight AP, controller-based AP)</li></ul><p>其实说胖瘦并不合适，从硬件以及软件上后者也可以很丰富，只是在部署上有区别，术语  standalone  or autonomous AP vs.  controller-based AP  才是最恰切的，从名称上很清楚，前者指多个 AP 之间没有关联，后者指多个 AP 受一个单独的硬件的或者软件的 Wireless LAN  Controller 统一配置和管理，有 WLC 后 controller-based AP  可以只做射频信号的收发，把认证、信道加密解密和转发全交给 WLC，但也可以AP自己做认证（当无法联通 WLC  时）、信道加密解密和转发（减轻WLC压力，也避免WLC和AP之间跨地区时无谓的中转流量，这个特性叫 local  switching）。顺带说一句，家用无线路由是在 AP 的基础上集合了集线器、路由甚至应用层的 DHCP、FTP、VOD  等众多服务，并不是纯粹的 AP，在家庭无线网里一般只需要无线路由内置的一个 AP，所以也不需要关心无线控制器了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在企业环境里，办公面积一般很大，单个 AP 的信号覆盖不了，所以需要多个 AP 一起提供服务，这样就带来两个问题：</p><ul><li>多个 AP 如何统一配置</li><ul><li>手动登录每个 AP 的Web管理界面逐一配置</li><li>用一个管理软件配置好然后分发到多个  AP 上，这跟 PC 上使用 Puppet/Chef 之类的配置管理工具很类似。要集中管理 AP 自然需要管理端和 AP  之间有标准协议，而不是 hack 各家的 Web 管理界面，发些 http  请求模拟网页交互。遵循业界惯例，标准的最大好处是不只一个标准，这里的管理协议最常见的有两种：</li><ul><li>LWAPP: Lightweight Access Point Protocol</li><li>CAPWAP: Control And Provisioning of Wireless Access Points Protocol<br/></li></ul></ul><li>移动设备在多个 AP 之间怎么漫游</li></ul><p>Cisco  的东西我相信确实是一分钱一分货，但我还是有点怀疑采购二十万人民币的硬件 Cisco WLC 5508的必要性。在 WLC 和 AP  之间有两种流量：控制流量，指各种配置、控制、状态报告流量；数据流量，指移动设备让AP代为转发的用户流量。我相信早期的瘦AP方案是让 WLC  做认证和信道加密而AP活儿很简单，只是收发无线信号，表现的像天线一样，这种搞法缺点是很明显的，WLC 压力太大，需要高配置的定制硬件，当 WLC  和 AP 不在一个城市时，比如WLC在杭州，AP  在北京，那么北京办公室内部的流量还要去杭州绕一圈，这显然是不划算的。所以实际一点的瘦AP方案是 WLC 只做统一配置、控制、认证，AP  自己做信道加密解密，考虑到WLC和AP之间有时连接断掉，AP 本身也要有认证功能，这时，叫瘦AP已经不合适了，Cisco 文档里称之为  controller-based AP，这种方案下 WLC 的的活儿很轻松，用软件 WLC 即可，Cisco 也确实提供<a href=\"https://link.zhihu.com/?target=http%3A//www.cisco.com/c/en/us/products/wireless/virtual-wireless-controller/index.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">纯软件版的 WLC</a>(注明适用于<a href=\"https://link.zhihu.com/?target=http%3A//www.cisco.com/c/en/us/products/wireless/buyers-guide.html%23~controllers\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">中小企业</a>)，另一个很火的无线解决方案 <a href=\"https://link.zhihu.com/?target=http%3A//www.ubnt.com/unifi\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">UniFi</a> 就压根没有硬件版的 WLC。<br/></p><p>另一个问题是无缝漫游(seamless roaming, zero handoff roaming)，所谓无缝漫游是指移动设备在两个有重叠的AP信号覆盖区间移动时不会因切换AP而丢包甚至断网。</p><p>需要明确的是，漫游总是被移动设备触发的，移动设备在发现当前 AP 的信号低到一定程度、丢包率到一定程度后就会开始扫描新的无线网络并切换过去，这个过程需要重新认证并与新的AP建立绑定，在链路层会花 50~500ms，在网络层会导致五秒左右的停顿。</p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.hometoys.com/article/2012/04/demystifying-wi-fi-roaming-what-you-need-to-know-to-avoid-costly-mistakes/1812/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Demystifying Wi-Fi Roaming: What You Need to Know to Avoid Costly Mistakes</a> 这篇文章讲解的很清晰。AP 之间迁移有两种模式：</p><ul><li>多个 AP 使用相同 SSID，相同认证方式，不同信道，不同MAC地址。这种方式是网上资料描述最多的一种，漫游被移动设备触发，扫描SSID并重新认证。Cisco 的无线解决方案支持各种 <a href=\"https://link.zhihu.com/?target=http%3A//www.cisco.com/c/en/us/support/docs/wireless-mobility/wireless-lan-wlan/116493-technote-technology-00.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">fast-secure roaming</a> 技术以加快这个重新认证过程: CCKM(在 WLC 上缓存会话秘钥，直接跳过 EAP 认证和 WPA2 4-way handshake) ，PMKID Caching(在<b>连接过</b>的AP上缓存会话秘钥),  Proactive Key Caching(WLC为所有AP缓存会话秘钥), 802.11r(Fast BSS  Transition)，这些都需要移动设备的支持。但这么做只是加快漫游，并没有消除漫游，由于漫游过程是被移动设备控制，所花时间跟移动设备的无线管理程序、网络中  SSID 个数有关，这个过程总是会导致链路层数据包丢失，从而传输层的数据包丢失：UDP 丢包，TCP  重传数据包。如果没有认证信息缓存技术，这种漫游会明显影响无线语音、视频应用，严格来说谈不上无缝漫游。</li><li>多个 AP 使用相同  SSID，相同认证方式，相同信道，相同MAC地址。所有 AP  表现的像一个信号覆盖范围很广的单一AP，这被称为无线网虚拟化。物理AP之间的迁移是被WLC触发的，AP 不断的向 WLC  汇报某个移动设备的信号强度，WLC  控制在什么时刻选择某个AP为这个移动设备服务，这个过程自然也需要WLC分发认证信息到各个AP，但关键的是不需要移动设备做AP切换，实际上移动设备没有触发漫游，能感受到的只是信号弱了点然后又恢复了，链路层上没有数据包丢失，这才是真正的  seamless roaming, zero handoff roaming，zero handoff  是指从移动设备端看来没有发生AP切换。无线网虚拟化看起来很美好，但也是有其缺陷的，由于所有AP使用相同信道，会导致信号覆盖重叠区域的带宽打折扣。Unifi  Wireless Controller Software 3.x 支持 zero handoff roaming,  Cisco  的产品没有这个特性。<br/></li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>BTW, 上面提到 WLC  做认证信息缓存、调度AP切换，这些事情也可以在AP端做，多个AP之间选举一个 &#34;domain controller AP&#34; 或者 P2P  方式，此时 WLC 除了集中配置、监管就没啥其它事了，AP运作时甚至可以关掉 WLC，貌似 UniFi 就这么搞的(不清楚是 domain  controller AP 方式还是 P2P 方式)。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>稍聊一下企业级无线网络里的认证问题。WPA 分为 WPA  Personal(WPA-PSK, pre-shared key) 和 WPA Enterprise(一般也简称为  WPA)，前者是家用无线路由上常用的方式，所有人用一个共享秘钥做认证，问题是无法容易的区分到底是哪个用户在使用无线网。WPA  Enterprise 使用 EAP 认证框架协议做认证，真正的认证过程一般会用 RADIUS 协议交给 WLC 或者 FreeRADIUS  服务。EAP 认证框架协议包含很多 EAP method，好比 SASL 框架容纳多种认证协议似的。有的 EAP method  没有考虑传输加密问题，所以需要一个封装协议安全的传输认证报文。常见的 EAP method:</p><ul><li>EAP-TLS:  使用服务端和客户端 X509 证书做互相认证，由于需要客户端证书，所以部署比较麻烦。另一个问题是客户端证书传给服务器时没有加密，会暴露用户ID，解决办法是结合 EAP-TTLS 或者 PEAP 使用。<br/></li><li>EAP-TTLS  + PAP: EAP-TTLS 使用服务端X509证书认证服务端，双方建立 TLS  连接后再进一步认证客户端，这个第二阶段认证可以用任何认证方法，包括非 EAP method。PAP 是 Password  Authentication Protocol的缩写，传明文密码到服务端。</li><li>PEAP + EAP-MSCHAPv2: PEAP 跟  EAP-TTLS 类似，也是建立一个 TLS 连接再认证客户端，区别在于第二阶段的协议不一样，PEAP 只允许使用 EAP  method。这个组合是被各种移动设备、操作系统最广泛支持的，缺陷是 MSCHAPv2 要求密码在服务端以不那么安全的 ntlm_hash  方式保存。<br/></li><li>PEAP + EAP-GTC:  Cisco 创建用来替代 PEAP + EAP-MSCHAPv2，更灵活，允许密码在服务端以多种形式存储：<a href=\"https://link.zhihu.com/?target=http%3A//deployingradius.com/documents/protocols/compatibility.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">deployingradius.com/doc</span><span class=\"invisible\">uments/protocols/compatibility.html</span><span class=\"ellipsis\"></span></a></li><li>EAP-FAST: Cisco 创建用来替代 LEAP</li><li>LEAP: 修改版的 MSCHAP，属于旧协议，不推荐使用。<br/></li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>最后列几个无线方案提供商，说不定哪天用的上：</p><ul><li>Cisco</li><li>Aruba</li><li>Ruckus</li><li>HP<br/></li><li>NetGear</li><li>UniFi<br/></li><li>Aerohive</li></ul>", 
            "topic": [
                {
                    "tag": "无线网络", 
                    "tagLink": "https://api.zhihu.com/topics/19554922"
                }, 
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50811513", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 5, 
            "title": "《淘宝技术这十年》读后感", 
            "content": "<p>【迁移】《淘宝技术这十年》读后感</p><p>花了两天时间扫了下，后面的列传没仔细看，整个的文风就是个 BBS 八卦体，写的很有趣味，对互联网从业人员也很有启发性，是本好书。下面记录下一些乱七八糟的思绪。</p><p>淘宝一开始创业的技术并不高明，虽然有很多牛人，但感觉也只是很勤奋而已（个人觉得甚至有点矬，比如那个重启  sql relay 的活儿，哥啊，你们真的没整个自动监测并重启的脚本？另一个例子是没搞定 php  的数据库连接池），没搞出啥“纯技术”上的大名堂。成功关键还在于马云的商业头脑，抓住商机。这并不是说“技术顶个球”，技术虽然不是第一，但也不是倒数第一。</p><p>淘宝开始的很早，很多开源技术还没出现或者不被人所知，如果今天开始做的话，可能有很多东西会直接拿来用或者改进了，比如 memcache，redis, voldemort, kafka, storm, thrift + twitter finagle.</p><p>一开始如果能多考虑点可扩展的架构，日后一旦壮大，重构会不那么痛苦。具体实现视情况打折扣，理想实现是即使在单机上也按照分布式应用思路做，但肯定没办法这么理想，进度所迫，产品设计总在变化，需求总在变。无论如何，一开始完全不考虑一味求糙快猛是愚蠢的，忽视业界经验，尤其是这本书讲到的经验，有可能日后中道崩殂。</p><p>技术对了，产品设计对了，未必能成功，因为时机可能不对，用户一时接受不了。</p><p>好的设计是磨练出来的，再好的架构师也没法一锤定音，一方面是流量上来后各种未预料到的问题，另一方面是没有完全一模一样的需求，各种未预料到的需求。即使如此，互联网行业的系统架构还是有粗略套路可循的，因为同一行业内要解决的事情总有类似的。</p><p>可供参考的技术，不完全是书中提到的。</p><ol><li>load balancing + high availability<br/></li></ol><ul><li>DNS server 根据客户端的 IP 对应的地理位置对同一域名解析出不同的 IP 地址，以把用户请求导向不同的机房；</li><li>在页面里嵌入一段 JS 脚本，同时请求多个机房的某相同资源，服务端可以记录下请求日志，然后经由后台的日志分析系统分析出这个客户端请求哪个机房最快，这个信息可以用来改进上面的第一种办法；</li><li>在域名解析完成后，客户端的请求被发送到指定  IP 上，这个 IP 上部署 LVS(工作在 OSI 网络模型第四层)，LVS 后面部署 HAProxy(工作在 OSI  网络模型第七层)，两层一起做 load balancer，综合 LVS 的高效以及 HAProxy 的丰富特性。</li><ul><li>Apache 和 Nginx 也有 proxy 功能，但术业有专攻，流量特别大的时候效率肯定还是比不上 HAProxy<br/></li><li>各位看官，貌似不要 LVS 也行吧? HAProxy 加上 keepalived or pacemaker + corosync/heartbeat 也能避免单点故障。</li></ul></ul><li>edge cache: Apache Traffic Server, Squid, Varnish，用来缓存静态内容或者一段时间内不变的动态内容</li><li>http server:  Nginx, Apache, Tomcat, Jetty</li><ul><li>在展现层上免不了走两条路子：服务端模板技术，在服务端全渲染好；AJAX 方式。虽然两者并不排斥，但个人觉得 AJAX 方式优雅的多，一开始就把 service 和 UI 分离开了，开发也容易，前端工程师和后端工程师不用在模板文件上打架。<br/></li></ul><li>session persistence</li><ul><li>session ID<br/></li><ul><li>cookie</li><li>url parameter，比如 JSESSIONID=xxx</li></ul><li>session 的存储</li><ul><li>直接把信息放在 cookie 里，QPS 高时带宽浪费严重</li><li>http server 本地磁盘，需要 load balancer 支持 sticky session，单个 http server 崩溃会丢失 session 数据<br/></li><li>global session storage: redis, rdbms + memcache</li></ul></ul><li>RPC system，切分各个服务后，服务之间需要统一而且高效的 RPC 机制<br/></li><ul><li>service registry，这是每个创业公司一开始都会忽视的东西，而一旦壮大后都必需得做，尤其是把各种功能分割到不同子系统成为服务之后。</li><li>RPC framework:  thrift, twitter finagle</li></ul><li>cache:  memcache, redis, <a href=\"https://link.zhihu.com/?target=http%3A//code.taobao.org/p/tair/src/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tair</a>, <a href=\"https://link.zhihu.com/?target=http%3A//www.project-voldemort.com/voldemort/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">voldemort</a></li><li>storage: 分布式存储应该是整个技术栈里最难的部分了。<br/></li><ul><li>关系数据库：MySQL, PostgreSQL</li><ul><li>这俩数据库都是单机版本，要达到集群效果需要做大量工作：分库分表、join、分页、事务、failover、负载均衡，尽可能自动化的导入导出、增减实例，目测只有 Google、Facebook和淘宝玩的很溜。</li></ul><li>NoSQL: HBase, Cassandra, Riak, RethinkDB, CouchBase，<a href=\"https://link.zhihu.com/?target=http%3A//www.project-voldemort.com/voldemort/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Voldemort</a></li><li>小文件存储：<a href=\"https://link.zhihu.com/?target=http%3A//code.taobao.org/p/tfs/src/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">TFS</a>，MogileFS, FastDFS</li></ul><li>消息队列：这类产品多如牛毛，但能同时支持至少一次、至多一次、保序特性的还是不多。多提一句的是 Redis 集群（官方的抑或各种山寨的）都是 AP 系统，不是 CP 系统，作为 queue 不适合严肃场合(作为 storage 也如此)<br/></li><li>任务队列：<a href=\"https://link.zhihu.com/?target=http%3A//gearman.org/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Gearman</a>, <a href=\"https://link.zhihu.com/?target=http%3A//www.celeryproject.org/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Celery</a></li><li>系统监测：Nagios, Ganglia, <a href=\"https://link.zhihu.com/?target=http%3A//blog.163.com/guaiguai_family/blog/static/20078414520140914721687/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Graphite</a>, <a href=\"https://link.zhihu.com/?target=https%3A//github.com/etsy/statsd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">statsd</a></li><li>日志收集：Kafka, <a href=\"https://link.zhihu.com/?target=http%3A//blog.163.com/guaiguai_family/blog/static/20078414520138100562883/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Flume</a>, Fluentd, <a href=\"https://link.zhihu.com/?target=https%3A//github.com/elasticsearch/kibana\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Kibana</a>，<a href=\"https://link.zhihu.com/?target=http%3A//blog.163.com/guaiguai_family/blog/static/20078414520138911393767/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">LinkedIn 的实践</a>非常值得参考，尤其值得一提的是 schema registry 的概念，没有这个服务，很难利用收集的数据。<br/></li><li>数据分析：Hadoop, Storm, Spark, <a href=\"https://link.zhihu.com/?target=https%3A//github.com/facebook/presto\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Presto</a></li><li>自动化部署：<a href=\"https://link.zhihu.com/?target=http%3A//puppetlabs.com/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Puppet</a>, <a href=\"https://link.zhihu.com/?target=http%3A//www.getchef.com/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Chef</a>, <a href=\"https://link.zhihu.com/?target=http%3A//www.ansibleworks.com/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Ansible</a> 等。</li>", 
            "topic": [
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50811465", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 2, 
            "title": "一个有待验证的 Redis 集群架构", 
            "content": "<p>【迁移】一个有待验证的 Redis 集群架构</p><p>官方的 Redis Cluster 还不成熟，而且也不支持跨机房复制，只好自己山寨 Redis 集群了。</p><ul><li>所有机房部署的软件是一样的，但是配置不一样，master DC 的 sentinel 启动，slave DCs 的 sentinel 不启动。</li><li>master DC 接受写入，slave DC 都是只读，切换 master DC 需要手动操作。</li><li>客户端需要封装 Jedis 或者 hiredis 客户端库，对读和写做集群化处理。Twemproxy 不适合，因为伊没有 replication 的支持，而这里的场景是把 Redis 当 storage 而非 cache（反模式啊！）<br/></li><li>master DC 中每个 partition 里的多个 redis server 使用 redis sentinel 保持高可用性</li><li>slave  DC 可以选择从 master DC 镜像，也可以选择从另一个 slave DC 镜像，这个镜像功能需要一个补丁，以保证 master DC  里的 sentinel 不会调整 slave DC 里 redis server 的 slaveof 设置。</li><li>slave DC  里每个 partition 的多个 redis server 都从其它机房同步数据，而不是做成链式，让其中一台 redis server  从其它机房同步数据。坏处是跨机房数据传输翻了好几倍（大部分情况下是 partial sync，细水长流，压力不大，但一旦出现 full sync  可能会死翘），好处是 slave DC 里不用操心 chain head 那台 Redis 死掉的问题。</li><li>Redis 集群是个  AP 系统，不是 CP 系统，可能丢数据，这点无论是官方的 Redis Cluster 还是这里山寨的架构，都是如此，新浪微博那个用  ZooKeeper 加 DNS 架设的 Redis 集群应该也如此。所以使用场合应该允许数据丢失，比如后台实时流计算分析用户行为。<br/></li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1a12ed242ff16a084695a7a7e69e8bc5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"1681\" class=\"origin_image zh-lightbox-thumb\" width=\"750\" data-original=\"https://pic2.zhimg.com/v2-1a12ed242ff16a084695a7a7e69e8bc5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;750&#39; height=&#39;1681&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"1681\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"750\" data-original=\"https://pic2.zhimg.com/v2-1a12ed242ff16a084695a7a7e69e8bc5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1a12ed242ff16a084695a7a7e69e8bc5_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Redis", 
                    "tagLink": "https://api.zhihu.com/topics/19557280"
                }, 
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50811407", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 0, 
            "title": "Graphite 集群架构", 
            "content": "<p>【迁移】Graphite 集群架构</p><p>参考这篇文章以及其中提到的几个链接：<a href=\"https://link.zhihu.com/?target=http%3A//bitprophet.org/blog/2013/03/07/graphite/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">bitprophet.org/blog/201</span><span class=\"invisible\">3/03/07/graphite/</span><span class=\"ellipsis\"></span></a>，实现一个可扩展的监测基础设施也不是个容易事情。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3d83f313d259723106ec04a956700415_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"1474\" class=\"origin_image zh-lightbox-thumb\" width=\"750\" data-original=\"https://pic2.zhimg.com/v2-3d83f313d259723106ec04a956700415_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;750&#39; height=&#39;1474&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"1474\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"750\" data-original=\"https://pic2.zhimg.com/v2-3d83f313d259723106ec04a956700415_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3d83f313d259723106ec04a956700415_b.jpg\"/></figure><p>一些说明：</p><ul><li>基本是这个链接提到的架构：<a href=\"https://link.zhihu.com/?target=https%3A//answers.launchpad.net/graphite/%2Bquestion/178969\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">answers.launchpad.net/g</span><span class=\"invisible\">raphite/+question/178969</span><span class=\"ellipsis\"></span></a>，上面  bitprophet.org 提到的架构感觉复杂了点(bitprophet 用了两层 consistent hash，并且每个  graphite-web app 只与本机上的 carbon-cache 通信，而不是与所有 carbon-cache  通信)，本文提到的架构只用了一层 consistent hash，graphite-web app 会与所有 carbon-cache  通信，好处是理解起来简单，配置也简单（每个实例基本都是等价的)，坏处是:</li><ul><li>增加 carbon-cache  实例时，所有地方的 consistent hash 分布都会受干扰。当增加第二层 hash ring 上的节点时，bitprophet  方案干扰面较小，但实际中这种情况很少出现，因为性能测试就可以在一开始决定单机上部署多少个 carbon-cache 实例，所以在  bitprophet 架构里加 carbon-cache 一般会在第一层 hash ring 上，干扰就一样大了。</li><li>graphite-web app 与 carbon-cache 的 TCP 链接更多（应该问题不大）</li></ul><li>单机上部署  carbon-relay : carbon-aggregator : carbon-cache : graphite-web = 1 : M :  N : 1，relay 因为有两层，可以单机上部署两个，系统瓶颈在 carbon-aggregator 和 carbon-cache 的  CPU 运算，以及 carbon-cache 的磁盘 I/O，graphite-web 应该没必要在单机上部署多个，如果有多台机器的话已经有多个  graphite-web 实例做负载均衡了。<br/></li><li>carbon-relay 在用 rule 转发时，如果  relay-rules.conf 里头的配置支持 consistent hash 方式转发出去，则可以省略掉第二层 carbon-relay  (Graphite 作者两年前说要支持这个特性，至今貌似还没加上去)。</li><li>carbon-0.9.12 里的  bin/carbon-client.py 可以从标准输入读入 metrics，所以可以把 carbon-client 部署到  application 所在机器上，启动两个 carbon-client 一个 relay 到 carbon-aggregator, 一个  relay 到第二层 carbon-relay，这样第一层 carbon-relay 可以省略掉，但这个思路并不推荐，因为会把  monitoring infrastructure layout 暴露给 application 一侧，维护麻烦。<br/></li><li>如果 aggregation 的需求可以全部用 StatsD 做，则 carbon-aggregator 可以省略掉。</li><li>graphite-web 互相调用这个设计有点奇怪，graphite-web 充当了 frontend 和 web service 双重角色。</li><li>carbon-cache  可以从支持 AMQP 协议的 message queue 比如 ActiveMQ 获取 metrics，这个架构里没有使用，ActiveMQ  的负载均衡、性能、bug本身就是个头疼事，包装个 ActiveMQ 也隐藏了 carbon 简单的网络协议，感觉加重 application  一侧的负担，甚为可惜。</li><li>凡是标记 any 的地方以及 graphite-web 可以用 HAProxy 做 round robin 式的负载均衡。</li></ul><p><a href=\"https://link.zhihu.com/?target=http%3A//douban.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">douban.com</span><span class=\"invisible\"></span></a> 用了 Graphite，不知道他们怎么配置的，也许单机 Graphite 足够用，不用想这么复杂。。。。</p><p>另外，Graphite  的 clustering 支持是后加的，graphite-web 提供了 web service 暴露本地的 whisper  文件，基本就是自行实现了 NFS，搞的挺复杂，又不能保证 whisper 存储的可靠性（节点失败 whisper  文件不会自动被多复制几份），graphite-web 之间广播式的查询很浪费，如果把 whisper 文件放 NFS 甚至 MooseFS  上，看起来会让 clustering 支持简单很多。opentsdb 一开始就考虑了集群，设计的很简单，但是伊的 UI 弱爆了。</p>", 
            "topic": [
                {
                    "tag": "软件架构", 
                    "tagLink": "https://api.zhihu.com/topics/19556273"
                }, 
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50811381", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 0, 
            "title": "不要用 Scribe 收集日志", 
            "content": "<p>【迁移】不要用 Scribe 收集日志</p><p>Scribe 曾经名声大噪，可能很大原因是 Facebook 开源出来的，在 Hadoop in China 2011 大会的时候，听 Facebook 的邵铮讲 Scribe 代码质量低下，Facebook 内部用 Java 重写了一份，今天仔细研究了下，发现这东西确实不能用。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>且不说代码年久失修依赖老旧编译麻烦了，其一个严重问题在于  bucket store 的设计，按照设计，在 random bucket 的模式下，日志会随机分配给下游的某些 Scribe  机器(实际测试发现这个“随机”相当不均匀），但如果某一台下游 Scribe 崩溃后，上游 Scribe  还是会把日志分配到它头上，当然，这里会发送失败，于是上游 Scribe 在 buffer store 的配置下会缓存起来。个人觉得这里既然是  random bucket，就说明使用者压根不关心日志切分，要的就是个均匀分布下游 Scribe server  的压力罢了，所以不应该缓存，而是任意选择一台下游 Scribe server 继续发送。也许有人想到用 load balancer，这样可以用  network store 替换 bucket store，但因为这里的网络传输模式是 request 远远大于 response，load  balancer 用在这个场合下就成了性能瓶颈。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>对比 Kafka 的 partition 和 replication  设计，Scribe 太业余了，bucket store 设计很不方便运维人员随意增减下游 Scribe 集群的机器。另外 Scribe 的  HDFS store 据说太老旧，不可用了，不折腾也罢，它也不像后来的日志收集系统一样自带一大堆的 source 插件，连 tail -f  这么常见的需求都得自己实现（scribe_cat  读一行输入发送出去然后就进程退出）。修改这几个问题看起来不是太难，但考虑到烂泥终究扶不上墙，还是不如重写个或者索性找个其它日志收集工具，比如  Fluentd 和 FlumeNG，甚或用大杀器 Kafka。</p>", 
            "topic": [
                {
                    "tag": "日志分析", 
                    "tagLink": "https://api.zhihu.com/topics/19615655"
                }, 
                {
                    "tag": "日志", 
                    "tagLink": "https://api.zhihu.com/topics/19718384"
                }, 
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50811353", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 0, 
            "title": "Java Monitoring Library", 
            "content": "<p>【迁移】Java Monitoring Library </p><p>公司里有个牛逼的 monitoring 库，C/C++/Java/Perl/PHP 通吃，但是伊有 JNI 的，不方便在非 RHEL  机器上跑，所以找了下开源的 Java monitoring library，倒是颇有几个，但是感觉还是公司那个 API  设计的好，整体架构高端大气。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>基本的 monitoring API 要求：</p><ul><li>set(label, value):  单纯的记录下数值，比如 memUsed<br/></li><li>add(label, increment): 记录增量，比如 QPS</li><li>start(label), stop(label): 记录过程花费的时间，比如 latency</li><li>每次 metric 记录需要保存下时间戳，因为画图以及统计需要用到</li><li>每次 metric 记录需要支持有 dimension 信息，比如登录与否、哪个地区的用户，维度信息对于 metrics 分析很重要</li><li>支持对 metric 定义 histogram，比如分 10 个区间，从 0 到 200ms，每个区间 20ms</li><li>最好能支持多种输出后端，比如到本地文件，送给 Ganglia 等。<br/></li></ul><p>参照<a href=\"https://link.zhihu.com/?target=http%3A//day-to-day-stuff.blogspot.com/2009/01/runtime-monitoring-libraries-for-java.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">这个链接</a>找了好几个开源的库，总结如下：</p><ul><li>Jamon: <a href=\"https://link.zhihu.com/?target=http%3A//jamonapi.sourceforge.net/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">jamonapi.sourceforge.net</span><span class=\"invisible\">/</span><span class=\"ellipsis\"></span></a>，我的神，居然能把类库整这么庞大，只有 start/stop/add，没有 set 以及自定义直方图，不支持输出到文件或者送给 Ganglia。<br/></li><li>Java Simon: <a href=\"https://link.zhihu.com/?target=http%3A//code.google.com/p/javasimon/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">code.google.com/p/javas</span><span class=\"invisible\">imon/</span><span class=\"ellipsis\"></span></a>，跟 Jamon 类似，nano sec 的改良貌似 Jamon 也有的，类似 log4j 的层次化 metric name 没感觉有多大好处，API 跟 Jamon 一般的矬。</li><li>usemon: <a href=\"https://link.zhihu.com/?target=http%3A//code.google.com/p/usemon/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">code.google.com/p/usemo</span><span class=\"invisible\">n/</span><span class=\"ellipsis\"></span></a>，看起来很企业级，但是 2008 年后代码就没更新了，没兴趣细看了。</li><li>MoSKito: <a href=\"https://link.zhihu.com/?target=http%3A//www.moskito.org/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">moskito.org/</span><span class=\"invisible\"></span></a>，官网广告视频太恶搞了，半夜工程师奋斗时被英雄  MoSKito 所救，moskito  是蚊子啊啊啊。。。文档确实含糊不清，看架势是挺有商业味道收服务费的感觉，短暂扫描文档的结果是，这东西好像是用 AOP  测量指定类的某些方法执行的时间，可以说是个 online profiling 工具，跟 monitoring 还是有差别的。</li><li>commons-monitoring: <a href=\"https://link.zhihu.com/?target=http%3A//commons.apache.org/sandbox/monitoring/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">commons.apache.org/sand</span><span class=\"invisible\">box/monitoring/</span><span class=\"ellipsis\"></span></a>，API 不够漂亮，开发倒是挺活跃。</li><li>JETM:<a href=\"https://link.zhihu.com/?target=http%3A//jetm.void.fm/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"> http://jetm.void.fm/</a>，这是做 profiling 的，统计类的方法执行多长时间。</li><li>Yammer Metrics: <a href=\"https://link.zhihu.com/?target=http%3A//metrics.codahale.com/getting-started/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">metrics.codahale.com/ge</span><span class=\"invisible\">tting-started/</span><span class=\"ellipsis\"></span></a>，API  设计是比较矬，把 metric 记录跟统计类型绑定在一起了，比如对某个 metric 我一会想 Histogram，一会想  Counter，一会又想 Meter，伊的 API 设计就要求使用者修改源码。功能挺丰富的，支持输出到 Ganglia, Graphite  等，包装包装应该还不错。似乎 Histogram 不支持自定义切分。</li><li>Netflix servo: <a href=\"https://link.zhihu.com/?target=https%3A//github.com/Netflix/servo/wiki\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/Netflix/serv</span><span class=\"invisible\">o/wiki</span><span class=\"ellipsis\"></span></a>，API  稍嫌复杂，javadoc 看不了，从代码示例来看，伊似乎只支持 pull 模式获取 metrics，原理是把一些打上 @Monitor  标记的对象注册上，开个后台线程定期查询这些对象的成员变量或者方法，得到 metrics 的值，但是这种搞法怎么应付临时开一个 http  链接获取个远程资源并关闭的情况？这个过程短到几十毫秒量级，靠 metrics collector 轮询的收集是不行的，需要 API 可以主动把  metrics 缓存到某个地方。<br/></li><li>Jinspired: <a href=\"https://link.zhihu.com/?target=http%3A//www.jinspired.com/solutions/open-api\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">jinspired.com/solutions</span><span class=\"invisible\">/open-api</span><span class=\"ellipsis\"></span></a>，伊把  Yammer 的 metrics library 以及 Netflix 的 servo 骂的狗血淋头，确实伊说的有道理，但是个人觉得一个  monitoring 的东西把架子搭的过于华丽了，没有 open api  不要紧，自己封装下够用就行。而且这个东西是商业产品，价格还不低，所以嘛。。。。</li></ul><p>总结，在 API 设计上还真不敢恭维。Yammer metrics 的功能倒还丰富，可以在之上封装一层再用。</p>", 
            "topic": [
                {
                    "tag": "Java 编程", 
                    "tagLink": "https://api.zhihu.com/topics/19582744"
                }, 
                {
                    "tag": "Java", 
                    "tagLink": "https://api.zhihu.com/topics/19561132"
                }, 
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50795801", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 1, 
            "title": "Parallel SSH", 
            "content": "<p>【迁移】Parallel SSH</p><p>集群管理中常常要在大批远程机器上执行一些命令，最直接的办法是整个 bash 的 for  循环以调用ssh或者scp一台台机器的操作，这个方式有两个问题，一是对不同机器的操作是串行的，效率很低，而如果给 ssh 或者 scp 命令添加  &#34;&amp;&#34; 让其后台执行，虽然并发执行了，但又没能控制并发度；二是这些批量的 ssh  命令的输出没有汇总成方便查看的样子，全混在一起，糙办法是输出到以主机名命名的文件里，最后再汇总，但每次这么搞总是不大方便。</p><p>这个场景很普遍，所以对应的解决方案也是相当多，前段时间个个浅试了下，总结如下，其中  csshx 以及 parallel-ssh、pdsh 是推荐使用的。想要更高级但又轻量级的配置管理工具，Python 众可以考虑  Fabric，Ruby 众可以考虑 Capistrano，Perl 众可以考虑 Rex，这三个工具对各自的编程语言能力要求较高，或者也可以看看  Ansible，使用 YAML 语法声明配置。</p><ol><li><a href=\"https://link.zhihu.com/?target=http%3A//code.google.com/p/csshx/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">code.google.com/p/csshx</span><span class=\"invisible\">/</span><span class=\"ellipsis\"></span></a>，Mac  OS X 应用，以 tiling window  方式同时操作多台机器并显示其输出，如果本机屏幕够大而且操作的远程机器在几十台到一两百台，还是相当好用的，比如 grep  下集群中所有机器的日志文件，随便查找点什么东西。这个应用还可以切换模式只操作其中一台远程机器，本机是 Mac OS X  的同学强烈建议试用下，个人觉得是同类应用中做的最好的一个。</li><li><a href=\"https://link.zhihu.com/?target=http%3A//code.google.com/p/parallel-ssh/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">code.google.com/p/paral</span><span class=\"invisible\">lel-ssh/</span><span class=\"ellipsis\"></span></a> ，命令行模式的 parallel ssh 工具集，使用 Python 编写，包含 pssh, pscp, prsync, pnuke, pslurp，支持并发度控制，省得自己山寨了。</li><li><a href=\"https://link.zhihu.com/?target=https%3A//code.google.com/p/pdsh/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">code.google.com/p/pdsh/</span><span class=\"invisible\"></span></a>，命令行模式的 parallel ssh 工具集，使用 C 编写，包含：</li></ol><ul><li>pdsh: 批量并发的在多台远程机器上执行命令</li><li>dshbak: pdsh 在每行输出前面加上远程主机名作为前缀，如果远程命令输出很多行的话，来自不同主机的输出会交错出现，dshbak 用来分析 pdsh 的输出，把相同前缀的日志汇总到一起</li><li>pdcp 和 rpdcp: 从本地复制文件到多台远程主机上，以及反过来复制文件。</li></ul><li><a href=\"https://link.zhihu.com/?target=http%3A//www.ansibleworks.com/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">ansibleworks.com/</span><span class=\"invisible\"></span></a>,  ansible 是一个配置管理工具，使用 Python 编写，类似 <a href=\"https://link.zhihu.com/?target=https%3A//puppetlabs.com/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">puppet</a>、<a href=\"https://link.zhihu.com/?target=http%3A//www.opscode.com/chef\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">chef</a>，ansible  的最大特点是其不需要 server、agent 什么的，直接用 ssh  执行命令来配置目标机器，同时它也可以用来并发的直接在一堆远程机器上执行命令，比如 ansible all -i my-hosts -c ssh  -f 20 -a uptime。</li><li><a href=\"https://link.zhihu.com/?target=http%3A//packages.debian.org/sid/mssh\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">packages.debian.org/sid</span><span class=\"invisible\">/mssh</span><span class=\"ellipsis\"></span></a> 使用 GTK+ 库编写界面，跟 csshx 类似，但要差点，凑合用。</li><li><a href=\"https://link.zhihu.com/?target=http%3A//www.heiho.net/pconsole/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">heiho.net/pconsole/</span><span class=\"invisible\"></span></a>  Xwindow 应用，tiling windows 同时操作多个远程主机，不怎么好用。</li><li><a href=\"https://link.zhihu.com/?target=http%3A//sourceforge.net/apps/mediawiki/clusterssh/index.php%3Ftitle%3DMain_Page\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">sourceforge.net/apps/me</span><span class=\"invisible\">diawiki/clusterssh/index.php?title=Main_Page</span><span class=\"ellipsis\"></span></a>, Perl/Tk 应用，tiling windows 同时操作多个远程主机，不怎么好用。</li><li><a href=\"https://link.zhihu.com/?target=http%3A//clusterit.sourceforge.net/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">clusterit.sourceforge.net</span><span class=\"invisible\">/</span><span class=\"ellipsis\"></span></a>,   这个软件包包含很多工具，dvt 是 tiling window 式的交互的同时操作多台远程机器，不大好用，dsh 和 dshbak 相当于  pdsh 软件包里的 pdsh 和 dshbak，pcp 相当于 pdsh 里的 pdcp，另外 clusterit  还有一些工具用于挑选某台远程机器执行某个命令。</li><li>若干类似工具，没仔细研究，浅试了下觉得不堪用：</li><ul><li><a href=\"https://link.zhihu.com/?target=http%3A//pussh.sourceforge.net/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">pussh.sourceforge.net/</span><span class=\"invisible\"></span></a></li><li><a href=\"https://link.zhihu.com/?target=http%3A//mussh.sourceforge.net/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">mussh.sourceforge.net/</span><span class=\"invisible\"></span></a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//github.com/ndenev/mpssh\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/ndenev/mpssh</span><span class=\"invisible\"></span></a></li><li><a href=\"https://link.zhihu.com/?target=http%3A//sourceforge.net/projects/mpssh\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">sourceforge.net/project</span><span class=\"invisible\">s/mpssh</span><span class=\"ellipsis\"></span></a></li><li><a href=\"https://link.zhihu.com/?target=http%3A//sourceforge.net/projects/mssh/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">sourceforge.net/project</span><span class=\"invisible\">s/mssh/</span><span class=\"ellipsis\"></span></a></li><li><a href=\"https://link.zhihu.com/?target=http%3A//packages.debian.org/sid/dsh\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">packages.debian.org/sid</span><span class=\"invisible\">/dsh</span><span class=\"ellipsis\"></span></a></li><li><a href=\"https://link.zhihu.com/?target=http%3A//www.csm.ornl.gov/torc/C3/index.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">csm.ornl.gov/torc/C3/in</span><span class=\"invisible\">dex.html</span><span class=\"ellipsis\"></span></a></li><li><a href=\"https://link.zhihu.com/?target=http%3A//freeshell.de/~drimiks/gnu/dish.shtml\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">freeshell.de/~drimiks/g</span><span class=\"invisible\">nu/dish.shtml</span><span class=\"ellipsis\"></span></a></li><li><a href=\"https://link.zhihu.com/?target=http%3A//taktuk.gforge.inria.fr/kanif/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">taktuk.gforge.inria.fr/</span><span class=\"invisible\">kanif/</span><span class=\"ellipsis\"></span></a></li></ul><li><a href=\"https://link.zhihu.com/?target=http%3A//capistranorb.com/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">capistranorb.com/</span><span class=\"invisible\"></span></a>，Capistrano  是个很有意思的工具，尤其对 Ruby 粉丝应该很有吸引力，它有个 cap shell 的用法，在命令行下利用持续的 ssh  连接多次发送命令到一批远程主机上，UI 形式很有创意。Capistrano 可以直接用来远程执行任意命令：cap COMMAND=uptime  HOSTS=localhost,purplebattle-lm.local  invoke。可惜，这个工具不支持并发度控制，如果目标主机数目太多，这个工具会因为遇到 max open file limit  而报错，个人觉得作者脑袋被门夹了：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/capistrano/capistrano/issues/270\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/capistrano/c</span><span class=\"invisible\">apistrano/issues/270</span><span class=\"ellipsis\"></span></a>，幸好最新的 Capistrano v3 支持并发度控制了：<a href=\"https://link.zhihu.com/?target=http%3A//www.capistranorb.com/2013/06/01/release-announcement.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">capistranorb.com/2013/0</span><span class=\"invisible\">6/01/release-announcement.html</span><span class=\"ellipsis\"></span></a></li><li><a href=\"https://link.zhihu.com/?target=http%3A//fabfile.org/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">fabfile.org/</span><span class=\"invisible\"></span></a>，跟  Capistrano 类似的工具，不知道谁模仿谁，Fabric 使用 Python 编写，个人觉得其设计比 Capistrano  更简单纯粹，Fabric 的控制脚本直接就是很普通的 Python 脚本，Capistrano 基于 Ruby 弄了个 DSL(貌似这是  Ruby 社区对 Ruby 语言最自豪的地方），到了 v3 又转向 Rakefile 了，而且到了 v3 才支持并发度控制。用 Fabric  直接远程执行任意命令：fab -H host1,host2 -z 32 -- uptime，参考 <a href=\"https://link.zhihu.com/?target=http%3A//docs.fabfile.org/en/1.6/usage/fab.html%23arbitrary-remote-shell-commands\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">docs.fabfile.org/en/1.6</span><span class=\"invisible\">/usage/fab.html#arbitrary-remote-shell-commands</span><span class=\"ellipsis\"></span></a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//fedorahosted.org/func/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">fedorahosted.org/func/</span><span class=\"invisible\"></span></a>，跟  Fabric 类似，基本上是用 Python  实现了一套远程管理的函数库，然后写个命令式的控制脚本调用这些函数来达到配置系统的目的，感觉设计不如 Fabric 优雅简单，而且 Func  需要在 master server 上装一个 cert manager 服务，在所有被管理机器上装 funcd 服务，虽然比  puppet、chef 要轻量点，但跟 ansible、fabric、capsitrano 相比还是笨重了些：实际中基本所有远程机器都会有  ssh 服务，为啥不重复利用这个现成的基础设施呢？</li><li><a href=\"https://link.zhihu.com/?target=http%3A//rexify.org/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">rexify.org/</span><span class=\"invisible\"></span></a>，Perl 版的 Capistrano，作为一个 Perl fan，其基于 Perl 设计的 DSL 看着相当亲切:-)</li><li><a href=\"https://link.zhihu.com/?target=http%3A//docs.saltstack.com/index.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">http://docs.saltstack.com/index.html, </a>Salt 是一个配置管理工具，使用 Python 编写，比起 puppet、chef 安装要简单许多。Salt 需要某台机器运行 salt-master 服务，所有被管理机器上运行 salt-minion 服务。配置定义使用 YAML 标记语言。</li>", 
            "topic": [
                {
                    "tag": "SSH(Secure Shell)", 
                    "tagLink": "https://api.zhihu.com/topics/19557973"
                }, 
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": [
                {
                    "userName": "豆瓣酱", 
                    "userLink": "https://www.zhihu.com/people/41d872e07eec8a4380620fd206e8adbc", 
                    "content": "额，大杂烩吗", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50795760", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 4, 
            "title": "备份系统", 
            "content": "<p>【迁移】备份系统</p><p>天有不测风云，虽然如今云计算风生水起，大家都把分布式啦、负载均衡啦、高可用啦、横着切竖着切斜着切啦挂在嘴边，悲剧的是 IT  基础设施往往没这个待遇，堆在角落积灰的服务器，ad hoc的几个 rsync  脚本，跑的好的时候大家玩命干活，崩溃了大家就闲着聊天等管理员恢复系统。站在被剥削阶级考虑，IT  基础设施崩了真没啥，正好休息下，对于剥削阶级就不是利好了：驴儿闲着也是白耗粮食啊。而处于被剥削阶级又正好处在管理员位置上的可怜虫来说，没比这时更蛋疼的时候了。</p><p>调侃牢骚完毕，俺其实大多数时候是那个休息的一员，偶尔担惊一下也是略惊无险，哈哈。回归到技术本身，我个人觉得 IT 基础设施中的备份系统应该具备如下几个特性：</p><ul><li>C/S 架构：备份是每台机器都要做的事情，极为需要集中控制, 不支持 C/S 架构的工具可以用 rsh/ssh/samba 等包装下凑合模拟成 C/S 架构；</li><li>跨 OS 支持：工作用 PC 也是需要备份的，不只是服务器;</li><li>快速定位备份所在位置，尤其能妥善处理分卷的情况，备份的一大窘境是找不到需要的文件放哪里了;</li><li>支持不同种类数据的备份，如文件系统、数据库、代码库，不是所有情况都能 rsync、tar 搞定的，应该可以容易的自定义插件以应对特定的数据。 这个需求可以写独立脚本做，导出完毕后再开始备份，但如果备份系统有直接支持会更一致更方便;</li><li>稳妥应对备份存储可用空间耗尽的情况;</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>有则更佳的特性：</p><ul><li>支持多种存储设备：硬盘、磁带、光盘，这几种介质本身的特性决定了在写入时不能等同看待; 不支持的话需要额外写脚本，比如从硬盘转储到磁带、光盘;</li><li>支持分卷存储：备份的存储总有耗尽的时候，需要能够容易的切换存储位置继续写入;</li><li>支持增量备份或者差异备份，加快备份速度;</li><li>能定制策略决定完整、增量备份的频率和顺序，能定制备份轮转策略，或者直接</li><li>支持 GFS 轮转策略，不支持的话也可以写脚本包装下;</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>提一句增量备份和差异备份的区别：incremental backup 指只复制上次备份以来更改的文件，differential backup 指只复制自上次完整备份以来更改的文件。</p><p>备份的存储格式上，有一些流行的做法：</p><ul><li>保存一份快照以及一系列补丁，这种做法对频繁修改的大文件很有利，比如 Outlook 的邮箱文件</li><li>保存多份快照，快照之间用硬链接共享相同内容的文件</li><li>打包存放，一般是完整备份加上多个增量备份或者差异备份</li></ul><p>有一些工具按数据块存放文件，以更大程度的避免重复数据，如果备份工具既没有按数据块去重，也没有使用补丁，那么需要特别考虑频繁修改的大文件如何转换成“备份友好”的，一般是将大文件导出成多个片段，大部分片段是不变的，比如按日期切分邮箱。  另外快照式备份虽然恢复文件时很方便，但会消耗大量 inode。</p><p>对远程备份的实现，有两种办法，一种是  pull，备份服务器主动去其他服务器打包文件并传到备份存储里，一般用 rsh/ssh/samba  实现，好处是被备份的机器不需要装特别的东西，坏处是被备份的机器需要开一个特权帐号给备份服务器用，如果被备份机器是个人工作  PC，我觉得怪怪的，像被人安了后门，对于其他需要备份的服务器，这也是个安全隐患，一旦备份服务器被攻陷，其它服务器就都废了。</p><p>另一种远程备份方式是 push，需要备份的服务器装一个代理服务程序，它收集文件传给备份存储服务器集中存放。我比较偏好这种方式，虽然部署要麻烦点。</p><p>往往备份工具可以同时支持这两种模式，因为如果能 rsh/ssh/samba 访问被备份系统的文件，适当配置的话就可以让被备份机器往中心存储服务器上传备份。</p><p>BTW，图形界面的桌面 PC 备份工具不在讨论范围内，这个主题是为系统管理员设定的。</p><p><b>备份策略</b></p><p>备份看起来是个很简单的事情，实际操作时是有讲究的，比如这个简单的增量备份策略：每周日做一次完整备份，随后每天备份前一天修改的文件。看起来很节约磁盘，但是它有两个问题：</p><ul><li>这一个星期的备份，任意一次损坏，都会导致损坏的备份之后的恢复不完整</li><li>越往后恢复越麻烦，因为需要这一星期里之前的所有增量备份(想想如果你的备份是分卷存储的)</li></ul><p>为了缓解这两个问题，需要定制稍微复杂点的备份策略。先介绍一个术语&#34;备份级别&#34;(backup  level): level 0 表示完整备份(full backup)，level n 表示备份上次 level n-1  备份以来修改过的文件，如果之前在 level 0 之后没有 level n-1 备份，则是 level n-2 备份以来，一直递归到level 0  以来。</p><p>上面的简单策略就是周日 level 0 备份，周一 level 1，周二 level 2，周三 level  3，...，周六 level 6。 在 O&#39;Reilly 的《Backup &amp;  Recovery》第二章中专门讲述了这个问题，这是其中推荐的两种策略：</p><ul><li>周日做一次完整备份，周一到周六每天做一次 level-1 备份，也就是差异备份</li><li>Hanoi 调度策略: 周日 level 0，接下来 level 3/2/5/4/7/6.</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>Hanoi 调度策略的结果(假设所有备份在午夜十二点开始)：</p><ul><li>周日 level 0;</li><li>周一 level 3，只备份周一修改的文件;</li><li>周二 level 2，因为 level 3 &gt; level 2，所以用 level 0 参照，备份周一、周二修改的文件;</li><li>周三 level 5，以 level 2 参照(我有点奇怪为啥不参照 level 3)，备份周三修改的文件;</li><li>周四 level 4，参照 level 2 (同奇怪)，备份周三、周四修改的文件;</li><li>周五 level 7，参照 level 4，备份周五修改的文件;</li><li>周六 level 6，参照 level 4，备份周五、周六修改的文件;</li></ul><p>大部分时候文件备份了两次，比较可靠，大部分时候恢复时也不需要所有备份，比较快速方便，但不完美的是周二和周四修改的文件都只备份了一次。《Backup  &amp; Recovery》一书推荐了扩展到每月的 Hanoi 备份级别策略，就是把每月的第二、三、四周周日 level 0改成 level  1，我个人觉得如果时间和存储空间允许，还是每周周日一次 level 0 安全点。</p><p>除了备份的级别有策略，备份存储空间有限需要重用的话，备份轮转(backup rotation) 也是需要考虑策略的，有三种常见做法(<a href=\"https://link.zhihu.com/?target=http%3A//en.wikipedia.org/wiki/Backup_rotation_scheme\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">en.wikipedia.org/wiki/B</span><span class=\"invisible\">ackup_rotation_scheme</span><span class=\"ellipsis\"></span></a>)：</p><ol><li>FIFO(First-In-First-Out)，也就是新的备份总是占用最旧的备份所占空间。这种做法并不是很恰当的，比如保留七次备份，每天备份一次，那么如果一个备份错误一礼拜之内没发现，正确的备份就找不回来了。这种策略的问题是保留的备份历史太短。</li><li>GFS(Grandfather-father-son)，可能是最常用的备份轮转方案。这是一个分层次的  FIFO，比如三个层次的备份：daily(son), weekly(father), monthly(grandfather), daily  backups 每天轮转一次，到周日时被挤出去那个备份升级到 weekly backups 里(注意：这个新的 weekly backup  应该是一个 full backup，或者相对某个旧的 weekly backup 的 incremental/differential  backup，而不是相对某个 daily backup 的，因为一旦 daily backups 发生轮转，这个参照就无效了），weekly  backups 每周轮转一次，到月底时被挤出去那个备份升级到 monthly backups 里。monthly backups  每月轮转一次。如果对备份时间要求更长，还可以建立 quaterly, biannual, annual backups 或者将  grandfather 层挤出的备份转储以长期保存。这种方案兼备了磁盘占用少以及备份周期长的优势，而且很容易理解。</li><li>Towers  of Hanoi，这个更复杂了，没有备份程序辅助，手工很容易出错。这种策略在限制 n  个备份位置的条件下，每天备份一次，那么第一个位置每两天重用一次，第二个位置每四天重用一次，第三个位置每八天重用一次，在最后一个备份位置被重写前，能找回最早  2^(n-1) 天前的备份。</li></ol><p><br/><b>备份工具</b></p><p>罗列下 Debian 打包的备份工具，没耐心的同学可以直接跳到本文最末尾的总结，精力有限，没有亲自折腾 Amanda, Bacula, BackupPC。</p><p>* rdiff-backup</p><p>Python 编写，使用了 librsync，基本相当于一个支持多版本备份的、不支持 rsync standalone server 的 rsync。</p><p>rdiff-backup  对 Linux 文件系统的各种属性都支持很好，使用也很简单，定期执行 rdiff-backup dir dir.backup 就可以了。  rdiff-backup 的备份存储很简单，首先它把修改过的源文件复制到备份目录，同时做一个反向 diff 保存到目标目录下  rdiff-backup-data/increments/ 下对应路径，比如 a/b.txt 的修改保存到  rdiff-backup-data/increments/a/b.txt-yyyy-mm-ddThh:mm:ss+tz.diff.gz，这种方式带来如下特点：</p><ol><li>最近的一次备份可以很容易查看，直接 less、cp 即可;</li><li>源目录不能有 rdiff-backup-data/ 目录;</li><li>找一个文件的历史版本很容易，因为  increments/ 下备份按照原始路径存放，查看每一次备份的文件列表也很容易：rdiff-backup -l dir.backup，然后  rdiff-backup --list-at-time &#39;yyyy-mm-ddThh:mm:ss+tz&#39; dir.backup;</li><li>备份不能存储在 Windows 上，因为文件名有冒号（没测试是否在 Windows 下有转义);</li><li>文件名不能太长，因为文件名包含时间戳(不是问题，一般文件名允许 256 字节);</li><li>由于 increments/ 下文件数目越来越多，对文件系统压力会比较大;</li></ol><p>貌似 rdiff-backup 在保存反向 diff 时会判断 diff 和源文件大小，如果 diff 太大则直接保存一份此文件。这一定程度上减少了最新备份损坏导致旧的反向 diff 失去意义的风险。</p><p>总结： rdiff-backup 很适合文件备份，我自己把它放在定时任务里，两年下来备份所占磁盘空间依然很少，运行很稳定。如果担心持续增量备份而基准版本损坏，可以定期的恢复某个版本出来，打包单独存放。</p><p>* rsnapshot</p><p>Perl  编写，调用了 rsync， 这个工具在 《Backup and Recovery》、《BSD Hacks》中都提到过。rsnapshot  的创意很巧妙，它在第一次备份时把源目录复制一份，比如 hourly.0/，然后以后备份时，如果文件没改变，则为这个文件创建一个硬链接，比如在  hourly.1/ 里，共享之前的文件备份内容；如果改变了就复制源文件。这样每一个备份看起来都是一个完整的拷贝。</p><p>这样做的结果是查找备份极为容易，占用磁盘也很少，文件名也没有额外的字符，但是跟  rdiff-backup 一样不支持备份级别，没有定期的完整备份，某个文件损坏的话它的所有硬链接指向的文件都失效。rsnapshot  没有考虑文件扩展属性比如  acl/xattr，备份所在文件系统不支持这些特殊属性就会丢失信息，而普通文件属性直接依赖文件系统，比如抵抗不了意外的chmod -R 之类的。</p><p>rsnapshot 支持 GFS 备份轮转，但所有备份都是包含硬链接的快照，需要注意基准版本失效的风险。 rsnapshot 还支持触发外部脚本，比如一个导出 postgresql 数据库的脚本，然后对导出的数据文件做备份。</p><p>总结：  rsnapshot 适合文件 *内容* 备份，其最大的特点是备份的快照存储格式就像复制了一份目录树，可以用 find、grep、cp  等直接操作，不过rdiff-backup 有个 rdiff-backup-fs 工具为 rdiff-backup 创建一个  fuse文件系统，达到类似 rsnapshot 的效果。</p><p>如果在乎文件的属性，而且有频繁修改的大文件，推荐用 rdiff-backup。</p><p>* dirvish</p><p>跟 rsnapshot 思路一样，比 rsnapshot 知名度小的多。</p><p>* Flexbackup</p><p>Perl 脚本，自 2003 年已经没更新了，陆续有一些增强性能的补丁被发行版收录，比如 Gentoo(<a href=\"https://link.zhihu.com/?target=http%3A//sources.gentoo.org/cgi-bin/viewvc.cgi/gentoo-x86/app-backup/flexbackup/files/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">sources.gentoo.org/cgi-</span><span class=\"invisible\">bin/viewvc.cgi/gentoo-x86/app-backup/flexbackup/files/</span><span class=\"ellipsis\"></span></a>), Debian(<a href=\"https://link.zhihu.com/?target=http%3A//patch-tracker.debian.org/package/flexbackup/1.2.1-6\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">patch-tracker.debian.org</span><span class=\"invisible\">/package/flexbackup/1.2.1-6</span><span class=\"ellipsis\"></span></a>)。<br/>Flexbackup 整个程序只有一个脚本，很便携，使用也非常简单，就一个配置文件，可以备份本地文件，也可以通过rsh/ssh 备份远程服务器上的文件，输出可以是磁带、硬盘、文件夹，支持备份级别，每一次备份默认以 tar ball 格式保存。</p><p>Flexbackup  没有特别记录文件在哪一个归档里，只记录了每个归档的备份级别以及备份日期，如果忘记归档文件名的意义，那么找文件只能逐个备份的flexbackup  -list 了，其实就是 tar tvf 列出归档里的文件列表，在 tar ball 很大时会很慢，因此可能需要额外包装下  Flexbackup 把每个归档里的文件列表提取出来存放以加快查询速度。</p><p>在把玩时发现一个有意思的细节，Flexbackup  在备份结束时 touch 了一个时间戳，时间戳文件的 atime 和 mtime 都是备份开始时精确到分钟的时间，比如备份是 22:05:33  开始的，那么时间戳文件的 atime 和 mtime 都是22:05:00。这个策略会导致在 22:05:00 至 22:05:33  之间修改完毕的文件在随后的增量备份里又会被备份一次，更别提在  22:05:33开始到备份结束期间修改完毕的文件了。后一种重复备份显然是必要的，前一种重复备份很可能是多余的，但考虑到 atime/mtime  更新有一定误差，为了避免竞态条件，将时间戳提前一点就保险多了（疑问：如果刚好在整秒开始备份呢？我觉得时间戳提前一分钟更保险)。</p><p>细追究下来，不由感叹一下，有没有不经思索的管理员在备份结束后 touch 一个文件作为时间戳了事呢？ 自己临时写的糙快猛的备份小脚本看起来容易，其实往往暗藏陷阱。</p><p>Flexbackup 支持备份级别，因此自身就避免快照类备份工具隐含的唯一基准版本损坏的风险。</p><p>总结：Flexbackup  很适合替换自己临时拼凑的备份小脚本，Flexbackup 的做法传统、直白、可靠，但对于数据库、版本库等需要用其它工具导出下再让  Flexbackup 备份，文件的扩展属性如 acl、xattr 需要额外处理。另外难得的是 Flexbackup  考虑了磁带设备，后来的备份工具往往假定了备份存储是硬盘，需要管理员额外处理硬盘到磁带的转储。</p><p>* backup2l</p><p>backup2l  就像是 Flexbackup 的傻瓜版，Flexbackup  的功能是比较低级的，提供了做多级备份的功能，但多级备份时机、备份轮转策略都需要额外写脚本以及 cron job，而 backup2l  直接提供了这两个特性(当然就不能随意的制定多级备份策略了)，另一个优点是 backup2l  生成归档时顺带生成了一份文件列表，这样查找文件会快很多，不用每次都去 tar tf 归档文件查看了。backup2l 还生成了文件的 md5  摘要，可以用来校验备份是否损坏。</p><p>backup2l 只考虑了备份到磁盘，没专门考虑磁带的情况，更需要注意的是 backup2l 采用的 level 0, level 1, level 2 这样的逐级增量备份，前面讲过这其实是不大好的做法。</p><p>总结：backup2l 牺牲了一点灵活性和可靠性，引入了更多特性，并且使用方便。</p><p>* boxbackup</p><p>C/S 架构的备份工具，号称支持 OpenBSD/Linux/NetBSD/FreeBSD/MacOS X/Windows/Solaris, 在 <a href=\"https://link.zhihu.com/?target=http%3A//www.boxbackup.org/wiki/Installation\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">boxbackup.org/wiki/Inst</span><span class=\"invisible\">allation</span><span class=\"ellipsis\"></span></a> 提到 client 端删除旧文件时，服务端也会很快的清除这个文件的备份，我没大看明白这是在说什么，貌似是boxbackup 实现的局限导致可能丢老数据，需要用户手动绕过这个问题。</p><p>boxbackup-client  有两种模式，一种是作为服务持续运行，可以快速持续的探测修改并备份到服务器上，另一种是作为定时任务运行，得到客户端被备份文件的快照。client  和 server 之间用 ssl 认证和通讯，客户端备份的文件可以被加密后再传给服务器。</p><p>boxbackup 的安装过程官方文档在密钥管理那块语焉不详，dpkg-reconfigure 又报错，还好 /usr/share/doc/boxbackup-server/READ.Debian.gz 说的比较清楚。 下面是安装过程：</p><p>Server:<br/>    # aptitude install boxbackup-server<br/>    # mkdir /backup<br/>    # chown bbstored:bbstored /backup</p><p>    ### &#34;dpkg-reconfigure boxbackup-server&#34; fails, have to configure it manually.<br/>    # raidfile-config /etc/boxbackup 4096 /backup<br/>    # bbstored-config /etc/boxbackup http://gold.corp.example.com bbstored<br/>    # bbstored-certs /etc/boxbackup/ca init<br/>    # bbstored-certs /etc/boxbackup/ca sign-server /etc/boxbackup/bbstored/gold.corp.example.com-csr.pem<br/>    # cp -a /etc/boxbackup/ca/servers/gold.corp.example.com-cert.pem /etc/boxbackup/bbstored<br/>    # cp -a /etc/boxbackup/ca/roots/clientCA.pem /etc/boxbackup/bbstored<br/>    # service boxbackup-server restart<br/>    # bstoreaccounts create 1 0 1024M 1250M</p><p>上面创建了帐号 1，帐号可以是八位十六进数。</p><p>Client:<br/>    # aptitude install boxbackup-client<br/>    # bbackupd-config /etc/boxbackup lazy 1 http://gold.corp.example.com /var/lib/bbackupd /home</p><p>    如果 lazy 改成 snapshot，那么需要把 /etc/cron.d/boxbackup-client 中的注释打开。</p><p>    ### On server side:<br/>    # bbstored-certs /etc/boxbackup/ca sign /etc/boxbackup/bbackupd/1-csr.pem<br/>    ### in this case, server and client are in same box<br/>    # cp -a /etc/boxbackup/ca/clients/1-cert.pem /etc/boxbackup/bbackupd/<br/>    # cp -a /etc/boxbackup/ca/roots/serverCA.pem /etc/boxbackup/bbackupd/<br/>    # service boxbackup-client restart</p><p>boxbackup  的这套 ssl 证书管理看起来挺赞的，很符合企业环境的需求。备份在服务端的存储有点像 GIT 的 object  database，boxbackup 有自己的 RAID5 软件实现，提交文件存储的可靠性，也可以直接使用硬件 RAID 或者 Linux 的  device mapper 支持。 客户端的 bbackupquery 有交互模式，像是个 ftp  客户端，可以枚举文件、比较备份和本地的区别、恢复文件等等。</p><p><b>但是</b>，在我修改一个本地文件后，备份里的这个文件<b>居然消失</b>了，bbackupctl force-sync后依旧，太不稳定了！ box-backup server 的文件存储格式看起来比较复杂，没有 tar ball或者分散单文件存储可靠。</p><p>总结：看起来特性很不错，可是有 bug。。。。</p><p>* BackupPC</p><p>在《Backup  &amp; Recovery》中谈到三个开源备份工具，Amanda, BackupPC, Bacula，但 BackupPC  的名气小的多，可能是BackupPC 这个名字太弱了，怎么看都无法联想到企业级备份软件。BackupPC 对 Windows  支持不是很好，据说将来会有支持Windows VSS 的 BackupPC windows 客户端。</p><p>BackupPC 使用  Perl 编写，最大的特色一是文件去重，二是方便的 Web 界面。BackupPC以文件长度以及文件部分内容计算 MD5  摘要作为备份文件的文件名，所以可以跨目录树跨多次备份去重。 Web  界面可以修改配置选项，浏览各个被备份主机的情况，浏览备份内容，发起备份，恢复文件等等，不愧是企业级的派头。</p><p>BackupPC 是  pull 模式的，可以使用 tar over rsh/ssh、rsync over rsh/ssh、rsyncd和 smbclient  备份文件，推荐使用 rsync or rsyncd，因为 tar 和 smbclient 方式使用文件的 mtime 判断是否需要备份，没考虑  ctime 以及其它文件属性，所以在 *增量备份*时可能会漏备份东西(关键是 mtime 是可以修改的)，比如 tar xf 出来的文件有很早的  mtime，还有删除和改名的文件。可以用 Filesys::SmbClient 替换 smbclient 程序判断 mtime之外的文件属性。</p><p>BackupPC  对 Windows 支持不是太好，不能备份被锁定的文件，不能备份非 POSIX 的文件属性。对于文件锁定问题，BackupPC  会发邮件提醒用户某个文件长时间没被备份，提示用户退出锁定此文件的程序，并使用邮件里提到的链接触发备份。 BackupPC 备份  Windows主要用 samba 获取文件，在公司里可以由管理员建立一个域帐号，加入每台 Windows 机器的Backup operators  组里。另一个办法是用 BackupPC 作者打包的 cygwin-rsyncd(<a href=\"https://link.zhihu.com/?target=http%3A//sourceforge.net/projects/backuppc/files/cygwin-rsyncd/2.6.8_0/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">sourceforge.net/project</span><span class=\"invisible\">s/backuppc/files/cygwin-rsyncd/2.6.8_0/</span><span class=\"ellipsis\"></span></a>)，不过有点老了，可以自己打包最新的 cygwin 里的 rsync。</p><p>总结： 不想鼓捣复杂的 Amanda、Bacula 的话，BackupPC 是很值得尝试的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>* Amanda</p><p>Amanda   是老牌开源备份软件了，现在依然在发布新版本，专门针对备份存储介质为磁带设计的，连备份到磁盘时也是写入虚拟磁带。。。我好像在电影里见过自动换光盘的机器，但还从来没见过自动换磁带的机器，估摸着如果是用磁带备份文件的话，Amanda还是相当强大的。在如今这个硬盘涨价、容量增加但速度停滞不前的时代，磁带作为数据备份是个可以考虑的选择，参见<a href=\"https://link.zhihu.com/?target=http%3A//hardware.solidot.org/article.pl%3Fsid%3D12/05/25/0647205\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">hardware.solidot.org/ar</span><span class=\"invisible\">ticle.pl?sid=12/05/25/0647205</span><span class=\"ellipsis\"></span></a> 和 <a href=\"https://link.zhihu.com/?target=http%3A//hardware.solidot.org/article.pl%3Fsid%3D12/03/30/0515204\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">hardware.solidot.org/ar</span><span class=\"invisible\">ticle.pl?sid=12/03/30/0515204</span><span class=\"ellipsis\"></span></a> 。</p><p>Amanda 的 Windows 客户端比 Burp 要更“Windows”点，但也没好哪里去，基本就是个 GUI 化的配置编辑器。为了记录备份了什么文件，Amanda 的windows 客户端打包了一个 MySQL 服务器，感觉有点夸张。</p><p>* Bacula</p><p>Bacula 比 Amanda 年轻，但感觉已然赶超了，据 Bacula 网站上说法，从<a href=\"https://link.zhihu.com/?target=http%3A//sf.net\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">sf.net</span><span class=\"invisible\"></span></a> 下载统计看，Bacula 是最受欢迎的备份工具。 没深入研究这东西，光从 Windows client 看，确实是这一堆里看起来最企业级的。</p><p>Bacula 跟 Amanda 一样，也是一开始针对备份存储为磁带的情形设计的，虽然后来也支持备份到磁盘，但是也基本是把文件看成磁带管理的，早期版本连随机定位磁盘文件都不行。我没有深入研究 Bacula 在这方面的缺点，Burp 作者倒是批评了这个。</p><p>Amanda  貌似没有文件去重一说，Bacula 有极为基本的文件去重：第一次备份可以指定为 base  backup，之后备份中如果有文件属性、内容没改变的文件包含在 base backup 中，则会跳过此文件的备份。  这种做法可以使用于备份操作系统，因为操作系统一旦安装后系统文件大部分不会更改，但个人觉得这种去重的做法局限性太大了，伊不是按照文件 md5 or  sha1 摘要去重的。</p><p>Bacula 的配置相当复杂，参数、概念暴多。</p><p>* Burp</p><p>Burp 的作者在其主页数落了不少 Bacula 的缺点：<a href=\"https://link.zhihu.com/?target=http%3A//burp.grke.net/why.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">burp.grke.net/why.html</span><span class=\"invisible\"></span></a> 大致翻译过来，Bacula：</p><ul><li>配置太复杂了，Bacula 分成四个独立服务，每个有自己的配置文件;</li><li>代码很复杂，难维护</li><li>主要为磁带备份设计，不适合磁盘存储</li><li>编目（某个文件在哪个归档中）和归档分开存放，导致维护麻烦</li><li>总是复制整个文件，哪怕只改了几个字节</li><li>非常依赖时钟精度</li><li>笔记本上备份难以调度（因为笔记本可能随时中止备份？)</li><li>不支持断点续备</li><li>备份轮转策略难以配置合理</li><li>不支持 Windows EFS 文件</li></ul><p>在 <a href=\"https://link.zhihu.com/?target=http%3A//burp.grke.net/faq.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">burp.grke.net/faq.html</span><span class=\"invisible\"></span></a>  中有 Burp 详细的特性清单，看起来相当诱人。在 Debian 上浅试了下，体验还是很愉悦的，安装完 burp 包后修改  /etc/default/burp启用它，然后 service burp start 启动之，这就完事了。默认  /etc/burp/burp.conf会备份 /home，此目录内容巨大的同学记得修改下再测试。/etc/cron.d/burp  这个客户端的定时任务默认是注释掉了，手动执行 burp -a b 会做一次完整备份。burp 服务端的备份默认放在  /var/spool/burp 下，按客户端的 cname 分目录存放，被备份的文件被 gzip  压缩后按照原来的目录布局存放，每一个备份是一个快照，可以选择是否用硬链接节约磁盘，另外每一个备份额外存放了文件清单、相对下一次备份的文件内容差异等信息。由于每个文件的信息是单独存放的，所以会消耗大量  inode。</p><p>我觉得 burp 比较爽的地方：</p><ul><li>客户端和服务端是同一个程序，部署简单;</li><li>服务端和客户端之间用 ssl 证书验证，而且证书是自动生成的;</li><li>不依赖数据库;</li><li>每一个备份是一个快照，方便查看;</li><li>查看备份列表、备份里的文件列表很方便，还支持文件路径的过滤;</li><li>客户端执行 burp -a t 时由服务端决定是否到时间点需要备份了;</li><li>执行时的日志输出挺清楚的，代码简单扫了下，整洁朴实;</li><li>支持 GFS 日志轮转策略;</li><li>服务端可以在 /var/spool/burp/xxx/ 下 touch 一个 backup 文件，这会让客户端 xxx 下次 burp -a t 时立马触发备份，间接达到了温和的 pull 备份效果;</li><li>保存了 sha1 摘要，可以校验备份;</li><li>支持 Windows</li><li>断点续备</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>从 2011 年 1 月到几天前(2012-5-28) 发布 burp-1.3.6，虽然历史不久，但作者卯足了劲跟 Bacula 唱对台戏似的，相当活跃。</p><p>Burp 不支持多级别，每一份快照默认不使用硬链接节约空间（猜测是传输时只传差异部分，写入磁盘时把旧备份拷贝一份再应用补丁，兼顾传输高效以及存储可靠)，如果使用硬链接的话又有基准版本损坏导致后续备份失效的风险，需要定期恢复一份出来单独存储。</p><p>Burp 的 Windows 客户端挺土，就一个 windows 定时任务，没图形界面，备份没问题，恢复得上命令行了，不过倒是非常的轻量级。Burp 服务端可以为客户端设置配置，并且能主动升级客户端，对管理员是挺友好的。</p><p>总结： 值得评估试用。</p><p>* backupninja</p><p>backupninja  基本是一个 shell 写成的备份框架，依靠插件（称为 handler)做具体的数据库导出、版本库导出、数据传输等操作，比如 mysql  导出、postgresql导出、rsync 传输、rdiff-backup 传输、duplicity 传输，backupninja  把它们的配置集中起来统一格式，然后调用各个插件。对于 tar 插件，备份每次都是完整备份，对于  rsync/rdiff-backup/duplicity，备份的存储是快照式的，貌似会使用硬链接以节约磁盘。backupninja 提供了一个  curses 的 ninjahelper生成配置文件，还挺好上手。</p><p>个人觉得 backupninja 这层框架太鸡肋了，统一的配置文件格式虽好，但可能新增问题。可怜其作者从 2004 年开始持续开发这么个东西，今年(2012) 年马上就要发布正式的 1.0 了，汗！</p><p>* obnam</p><p>Python 编写，很新的备份工具，一些特点：</p><ul><li>快照似的备份，每一个备份看起来都像是完整备份</li><li>跨备份、跨文件的去重</li><li>使用 GnuPG 做加密备份</li><li>支持 pull 和 push 模式的远程备份</li><li>断点续备</li></ul><p>安装使用了下，居然第一次备份都做不了，抛异常。。。看来相当不稳定。看错误信息，似乎存储格式用了什么 B-tree，再加上快照、去重，看起来备份出错导致数据丢失的可能性很大，作者有点玩弄技巧似的。</p><p>总结：不稳定，不推荐。</p><p>* duplicity</p><p>跟  rdiff-backup 很像，Python 编写，也是基于 librsync。不同的是存储时用单个 tar 文件保存归档，而不是  rdiff-backup 那样分散在各自目录里，另外 duplicity 用 GnuPG  加密了文件列表以及归档内容，可以防止备份所在服务器查看或者修改归档。</p><p>总结：使用没有 rdiff-backup 方便，没有特殊考虑文件的 acl 和 xattr 属性。</p><p>* hdup</p><p>hdup 作者声称不再维护这个工具了，他转向了 rdup。</p><p>总结：忽略。</p><p>* rdup</p><p>rdup 只生成需要备份的文件列表，并不真的做备份，需要其它工具配合，以遵循小工具合作的哲学。</p><p>总结：忽略。</p><p>* vbackup</p><p>一套 shell 脚本，很简陋的插件机制。</p><p>总结：忽略之。</p><p>* slbackup</p><p>对 rdiff-backup 的简单封装，给一个基于 Debian 的教育用途发行版定制的定时备份任务。</p><p>总结：忽略之。</p><p>* backup-manager</p><p>backup-manager  做的事情很简单，定期用 tar 做完整备份，或者做完整备份然后做增量备份，将生成的 tar ball 通过 ssh/ftp/rsync  等上传到备份服务器。backup-manager 支持导出 mysql 数据库以及 svn 代码库，以及用 GnuPG 加密 tar ball。</p><p>总结： 压根没考虑恢复这回事，不直接支持查询备份文件所在归档，基本就是个糙快猛的打包、上传集中存储脚本，远远不满足一个备份系统的需求。</p><p>* chiark-backup</p><p><a href=\"https://link.zhihu.com/?target=http%3A//chiark.greenend.org.uk\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">chiark.greenend.org.uk</span><span class=\"invisible\"></span></a> 网站使用的备份脚本。</p><p><b>总结</b></p><p>简单点，用 rdiff-backup (快照式) 或者 Flexbackup、backup2l(多级别备份)，高级点，用 Burp(push 模式) or BackupPC(pull 模式)，更企业级点，用 Amanda or Bacula。</p><p>Flexbackup、Bacula、Amanda 直接支持磁带备份，后两者能自动管理磁带。</p><p>Amanda、Bacula、Burp 支持 Windows 的VSS(Volume Shadow copy Service)，可以备份正被打开的文件。</p><p>如果有频繁重复的文件，只有  BackupPC 做的最好，可以跨目录树跨备份去重，其它基本都是两次备份之间同一文件名去重，最常见的是用硬链接。Amanda和 Bacula  这两个企业级备份软件基本可以说没有去重特性(磁带的去重天生就不容易搞，顺序读取加多磁带倒腾，会死人的)。</p><p>快照式备份最好定期转储一份完整备份出来，以免因基准版本损坏或者硬链接共同指向的文件内容损坏，哪怕使用硬件 RAID、软件 RAID，还是有可能文件系统损坏。</p><p>只针对磁盘设计的备份工具，可以用自定义脚本定期转储到磁带、光盘上。</p><p>rdiff-backup:  快照式的持续增量备份，最好加辅助脚本定期导出某个完整版本，避免基准版本损坏。这个导出的备份需要额外脚本做 GFS  备份轮转。如果不重视文件扩展属性，也没有频繁修改的大文件，可以用 rsnapshot。两个都不支持多级别备份，都是针对以磁盘作为备份存储设计的。</p><p>Flexbackup: 多级别备份，支持磁带，需要额外编写备份轮转脚本，非常灵活。</p><p>Backup2l: 多级别备份，针对磁盘设计，支持备份轮转策略，不够灵活但简便。</p><p>Burp: 快照式备份，针对磁盘设计。</p><p>BackupPC: 快照式备份，针对磁盘设计。</p><p>Bacula: 多级别备份，针对磁带设计。</p><p>Amanda: 多级别备份，针对磁带设计。</p>", 
            "topic": [
                {
                    "tag": "备份", 
                    "tagLink": "https://api.zhihu.com/topics/19551843"
                }, 
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50795633", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 0, 
            "title": "代码评审系统 ReviewBoard 和 Gerrit (下)", 
            "content": "<p>【迁移】代码评审系统 ReviewBoard 和 Gerrit (下)</p><p>接下来是把玩下 Gerrit，这厮的文档写的也很赞（开源的东西文档写的好的真不多见），安装是很简单了，早期的 Gerrit 据说是用  Python 写的，在 GIT 主力开发者以及 jgit 项目发起人 Shawn O. Pearce 加入 Google 后就改用 Java  写了，编译好的 Gerrit 就是一个 war 包，可以放入 Servlet 容器里运行，也可以java -jar gerrit.war  直接用内置的 Jetty，太贴心了。Shawn 是个很勤奋的人，用 Java 重新实现了 GIT 核心功能，Gerrit 内置 Web  server、SSH server，还有一个 Prolog 语言解释器。。。。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Gerrit 里评审流程分三个阶段，可以分别让不同角色执行：</p><ul><li>review: 人肉扫描代码有无问题</li><li>verify: 编译、测试，可以用 Jenkins 的 Gerrit Trigger 插件自动出发</li><li>submit: 提交代码到正式分支上，貌似由人触发，Gerrit 来执行</li></ul><p>在使用 Gerrit 时，不要忘记把 commit-msg hook 装上：</p><p><a href=\"https://link.zhihu.com/?target=http%3A//gerrit-documentation.googlecode.com/svn/Documentation/2.3/user-changeid.html%23_creation\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">gerrit-documentation.googlecode.com</span><span class=\"invisible\">/svn/Documentation/2.3/user-changeid.html#_creation</span><span class=\"ellipsis\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><p>由于我想把安装过程自动化，所以在执行  java -jar gerrit.war init 时磕绊了下，这个 init 命令有个 --batch 选项，表示在非交互状态下时  init会用默认配置创建一个 Gerrit site，比如使用 H2 数据库，这不是我期望的，我希望它用 PostgreSQL  数据库，虽然创建完 site 后可以修改SITE_DIR/etc/gerrit.config，但我担心 init 时会初始化数据库什么的，不是在  gerrit.config 简单改下配置就能切换数据库的。 于是我把 init 交互式运行时的答案写入文件里，想通过管道传给 java  -jar gerrit.war init，没想到伊判断了输入是否终端，发现不是终端就直接走 --batch 模式了，真是自做聪明。。。折腾了会  empty (<a href=\"https://link.zhihu.com/?target=http%3A//empty.sf.net\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">empty.sf.net</span><span class=\"invisible\"></span></a>)、expect、socat 后放弃了，还是老实交互式安装吧，反正不会频繁重装。  下面是我输入的问题答案，@@...@@ 标记处需要替换成真的密码，在执行 init 之前要先创建好gerrit 数据库以及 gerrit  系统账户、gerrit 邮件账户：</p><div class=\"highlight\"><pre><code class=\"language-text\">### Gerrit Code Review 2.3\n# Create &#39;/srv/gerrit/site&#39;      [Y/n]?\ny\n\n### Git Repositories\n# Location of Git repositories   [git]:\ngit\n\n### SQL Database\n# Database server type           [H2/?]:\npostgresql\n\n# Server hostname                [localhost]:\nlocalhost\n\n# Server port                    [(POSTGRESQL default)]:\n5432\n\n# Database name                  [reviewdb]:\ngerrit\n\n# Database username              [gerrit]:\ngerrit\n\n# gerrit&#39;s password              :\n@@GERRIT_DB_PASSWORD@@\n\n#               confirm password :\n@@GERRIT_DB_PASSWORD@@\n\n### User Authentication\n# Authentication method          [OPENID/?]:\nhttp\n\n# Get username from custom HTTP header [y/N]?\ny\n\n# Username HTTP header           [SM_USER]:\nX-Forwarded-User\n\n# SSO logout URL                 :\nhttps://sso.corp.example.com/logout\n\n### Email Delivery\n# SMTP server hostname           [localhost]:\nsmtp.corp.example.com\n\n# SMTP server port               [(default)]:\n25\n\n# SMTP encryption                [NONE/?]:\ntls\n\n# SMTP username                  [gerrit]:\ngerrit@corp.example.com\n\n# gerrit&#39;s password              :\n@@GERRIT_SMTP_PASSWORD@@\n\n#               confirm password :\n@@GERRIT_SMTP_PASSWORD@@\n\n### Container Process\n# Run as                         [gerrit]:\ngerrit\n\n# Java runtime                   [/usr/lib/jvm/java-6-openjdk-i386/jre]:\n/usr/lib/jvm/default-java/jre\n\n# Copy gerrit.war to /srv/gerrit/site/bin/gerrit.war [Y/n]?\ny\n\n### SSH Daemon\n# Listen on address              [*]:\n*\n# Listen on port                 [29418]\n2022\n\n# Download and install it now [Y/n]?\ny\n\n### HTTP Daemon\n# Behind reverse proxy           [y/N]\ny\n\n# Proxy uses SSL (https://)      [y/N]?\nn\n\n# Subdirectory on proxy server   [/]:\n/\n\n# Listen on address              [*]:\n127.0.0.1\n\n# Listen on port                 [8081]:\n2080\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>在这个回答里有几个地方是比较特殊的，一是用户认证方式，由于我是把Gerrit  放在 Apache 后面，Apache 使用 mod_auth_kerb 做用户认证，所以这里我给 Gerrit 选择了 http  认证方式，默认情况下 Gerrit http认证方式会使用前端 Web 服务器传过来的 Authorization HTTP 头部，比如  &#34;Authorization: Basic xxxxxx&#34; 或者 &#34;Authorization: Digest xxxx&#34;，可惜的是  Gerrit 代码没有处理 &#34;Authorization: Negotiate xxxx&#34; 的情况，所以需要在 Apache 里用  mod_rewrite 把 REMOTE_USER 变量作为X-Forwarded-User 头部传给 Gerrit，这个名字可以随便取，但根据  Gerrit文档说法，不要重用 Authorization 头部。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>第二个特殊的地方是 SMTP encryption，ssl  表示直接以 ssl 方式连接，tls 表示先以非 ssl 方式连接，然后用 STARTTLS 升级为 ssl 连接，后一种方式是现在 ssl  用法里推荐的。使用哪一种取决于你的 smtp 服务器配置，一般 ssl 会用独立端口，tls 的话直接用标准 SMTP 25 端口。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>第三个特殊的地方是反向代理，因为我要配置 Kerberos 统一登录，所以Gerrit 前面有个 Apache 做反向代理，这俩我配置在同一台机器上，所以不用 https。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在 java -jar gerrit.war init -d /srv/gerrit/site 执行完之后，它会提示你访问 <a href=\"https://link.zhihu.com/?target=http%3A//127.0.0.1%3A2080/%23/admin/projects/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">127.0.0.1:2080/#</span><span class=\"invisible\">/admin/projects/</span><span class=\"ellipsis\"></span></a>，但你应该访问</p><p>http://gerrit.corp.example.com/#/admin/projects/，这里http://gerrit.corp.example.com  是我给 gerrit 所在机器设置的 CNAME，这个请求会被 Apache 的 gerrit virtual host 截获，做完 HTTP  Negotiate认证后转发给后台的 Gerrit，也就是 <a href=\"https://link.zhihu.com/?target=http%3A//127.0.0.1%3A2080/..\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">127.0.0.1:2080/..</span><span class=\"invisible\"></span></a>.，直接请求 2080  端口这个地址的话，Gerrit 会报错说没有 Authorization 头部。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>下面是我的 Apache gerrit 虚拟主机配置：</p><div class=\"highlight\"><pre><code class=\"language-text\">&lt;VirtualHost *:80&gt;\n    ServerName gerrit.corp.example.com\n    ServerAdmin webmaster@corp.example.com\n    DocumentRoot /nonexistent\n\n    ErrorLog ${APACHE_LOG_DIR}/gerrit-error.log\n\n    # Possible values include: debug, info, notice, warn, error, crit,\n    # alert, emerg.\n    LogLevel warn\n\n    CustomLog ${APACHE_LOG_DIR}/gerrit-access.log combined\n\n    ProxyRequests Off\n    ProxyVia Off\n    ProxyPreserveHost On\n\n    &lt;Proxy *&gt;\n        Order deny,allow\n        Allow from all\n    &lt;/Proxy&gt;\n\n    &lt;Location /login/&gt;\n        AuthType Kerberos\n        Require valid-user\n\n        Order allow,deny\n        Allow from all\n\n        RewriteEngine On\n        RewriteCond %{REMOTE_USER} (.+)\n        RewriteRule .* - [E=RU:%1]\n        RequestHeader set X-Forwarded-User %{RU}e\n    &lt;/Location&gt;\n\n    ProxyPass   / http://127.0.0.1:2080/\n    ProxyPassReverse  /  http://127.0.0.1:2080/\n&lt;/VirtualHost&gt;\n</code></pre></div><p>在请求  http://gerrit.corp.example.com/#/admin/projects/ 时，Gerrit可能报错说找不到  All-Projects，原因不明，解决办法是把 PostgreSQL里的 gerrit 数据库删除重建，再重新 java -jar  gerrit.war init。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>没出其它问题的话，Gerrit 的 Web 界面就展现在你面前，它要求为当前用户注册一个 email 帐号，第一个登录的用户自动成为管理员，后续登录的其它用户是普通权限的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如果你用的  SMTP 服务器的 SSL 证书是自签名的，并且跟我一样 Gerrit使用 tls 方式连接 SMTP  服务器，到这里会卡壳一下。第一个问题是site/etc/gerrit.config 里默认没有  sendemail.sslverify，它的值默认是 true，这会导致 javax.net.ssl 检查 SMTP 服务器的 SSL  证书是否是 trusted 的，答案当然是否，于是 Gerrit 抛异常了：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>sun.security.validator.ValidatorException:  PKIX path building failed:  sun.security.provider.certpath.SunCertPathBuilderException: unable to  find valid certification path to requested target</p><p class=\"ztext-empty-paragraph\"><br/></p><p>解决办法有如下这些：</p><ol><li>在 /srv/gerrit/site/etc/gerrit.config 里 [sendemail] 里添加 sslverify = false，虽然使用 openssl 的 client 大多是这个德行，但我觉得不大舒服，所以没用这个办法。</li><li>把 SMTP 服务器的 SSL 证书导入 Java 的 truststore 里供 javax.net.ssl 使用。</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><p>truststore  是只包含公钥的 keystore, keystore 是 Java 安全框架里用来保存证书、私钥等等的东西，最常用的是 JKS 格式的  keystore 文件，比如 $JAVA_HOME/jre/lib/security/cacerts,  $HOME/.keystore。truststore 被 Java 类库里的 TrustManager 使用，keystore 被 Java  类库里的 KeyManager 使用，当然 TrustManager 也能用 keystore。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在这个证书验证问题上，需要给 TrustManager 指定一个 truststore 或者keystore，有三种办法：</p><p class=\"ztext-empty-paragraph\"><br/></p><ol><li>如果  javax.net.ssl.trustStore 系统属性指定了，就使用这个系统属性指定的那个文件当作  truststore，truststore  的密码从javax.net.ssl.trustStorePassword系统属性获取。网上有不少文章还提到  javax.net.ssl.keyStore 和 javax.net.ssl.keyStorePassword，这个只在 ssl server  端或者 ssl client 端使用client cert 向 ssl server认证的情况下才需要。</li><li>如果 javax.net.ssl.trustStore 属性没指定或者文件没找到，则使用 $JAVA_HOME/jre/lib/security/jssecacerts</li><li>如果 jssecacerts 没找到，则使用 $JAVA_HOME/jre/lib/security/cacerts。</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><p>网上有不少文章都说把证书直接加入  jssecacerts 或者 cacerts 中，我是半安全偏执狂，觉得把一个自己玩的证书加进去不太靠谱，另外担心 Debian  的软件包升级会自动更新 cacerts（伊其实是符号链接到  /etc/ssl/certs/java/cacerts了，ca-certificates 包的  /usr/sbin/update-ca-certificates 会通过ca-certificates-java 包的  /etc/ca-certificates/update.d/jks-keystore更新它，文档有提到 local cert  会依然保留，我没试验）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>通过分析 /srv/gerrit/site/bin/gerrit.sh 启动脚本，伊使用了JAVA_OPTIONS 变量，并且读取 /etc/default/gerritcodereview 文件，所以</p><p>可以在 /etc/default/gerritcodereview 里写入：</p><div class=\"highlight\"><pre><code class=\"language-text\">JAVA_OPTIONS=&#34;-Djavax.net.ssl.trustStore=/srv/gerrit/truststore \\\n  -Djavax.net.ssl.trustStorePassword=changeit&#34;\n</code></pre></div><p>然后  /srv/gerrit/site/bin/gerrit.sh stop 再 start 重启 gerrit。(也可以把这个选项放在  /srv/gerrit/site/etc/gerrit.config 的container.javaOptions 里：</p><p><a href=\"https://link.zhihu.com/?target=http%3A//gerrit-documentation.googlecode.com/svn/Documentation/2.3/config-gerrit.html%23_a_id_container_a_section_container\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">gerrit-documentation.googlecode.com</span><span class=\"invisible\">/svn/Documentation/2.3/config-gerrit.html#_a_id_container_a_section_container</span><span class=\"ellipsis\"></span></a></p><p>)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>/srv/gerrit/truststore 是这么生成的：</p><div class=\"highlight\"><pre><code class=\"language-text\">gerrit$ keytool -importcert -alias exim -file /etc/exim4/exim.crt \\\n            -keystore /srv/gerrit/truststore -storepass changeit\n</code></pre></div><p>这个 truststore 的密码是无所谓的，因为它里头没有私钥。 exim.crt 是用/usr/share/doc/exim4/examples/exim-gencert 生成的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>重启  Gerrit 后，证书问题解决了，第一次登录要求注册邮箱的对话框也没了，这时可以点击右上角的 settings 链接，在 contact  information 那一栏里。可惜的是问题没完，输入邮箱点击&#34;Register New Email...&#34;后，Gerrit 一直  Loading，/var/log/exim4/mainlog 以及 /srv/gerrit/site/logs/error_log  里没有错误信息，Gerrit Web 页面就那么一直挂着，直到 exim4 报告连接超时，把 Gerrit发起的 smtp 链接断掉。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>花费了三百脑细胞后，俺终于找到原因，是 Gerrit 的 AuthSMTPClient.startTLS()实现跟 SMTP 服务器配合有问题，这是一个 SMTP STARTTLS 会话：</p><div class=\"highlight\"><pre><code class=\"language-text\">$ gsasl --smtp smtp.corp.example.com\nTrying `gold.corp.example.com&#39;...\n220 gold.corp.example.com ESMTP Exim 4.77 Mon, 21 May 2012 14:46:43 +0800\nEHLO [127.0.0.1]\n250-gold.corp.example.com Hello localhost [127.0.0.1]\n250-SIZE 10485760\n250-PIPELINING\n250-AUTH GSSAPI\n250-STARTTLS\n250 HELP\nSTARTTLS\n220 TLS go ahead\nEHLO [127.0.0.1]\n250-gold.corp.example.com Hello localhost [127.0.0.1]\n250-SIZE 10485760\n250-PIPELINING\n250-AUTH GSSAPI DIGEST-MD5 CRAM-MD5\n250 HELP\nAUTH GSSAPI\n....\n</code></pre></div><p>可以看到在 STARTTLS 之后，Exim 不会再次发送 banner 了：</p><p>220 http://gold.corp.example.com ESMTP Exim 4.77 Mon, 21 May 2012 14:46:43 +0800</p><p class=\"ztext-empty-paragraph\"><br/></p><p>下面是 Gerrit AuthSMTPClient 的代码，伊在 org.apache 的命名空间了插了个 AuthSMTPClient 类，试图给 apache commons-net 2.2 的 SMTPClient增加 STARTTLS 支持：</p><p class=\"ztext-empty-paragraph\"><br/></p><p><a href=\"https://link.zhihu.com/?target=http%3A//code.google.com/p/gerrit/source/browse/gerrit-patch-commonsnet/src/main/java/org/apache/commons/net/smtp/AuthSMTPClient.java%3Fname%3Dstable-2.3\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">code.google.com/p/gerri</span><span class=\"invisible\">t/source/browse/gerrit-patch-commonsnet/src/main/java/org/apache/commons/net/smtp/AuthSMTPClient.java?name=stable-2.3</span><span class=\"ellipsis\"></span></a></p><div class=\"highlight\"><pre><code class=\"language-text\">  public boolean startTLS(final String hostname, final int port, final boolean verify)\n      throws SocketException, IOException {\n    if (sendCommand(&#34;STARTTLS&#34;) != 220) {\n      return false;\n    }\n\n    _socket_ = sslFactory(verify).createSocket(_socket_, hostname, port, true);\n    _connectAction_();\n    return true;\n  }\n</code></pre></div><p>事情坏在  _connectAction_() 里，这个在 AuthSMTPClient 的父类 SMTPClient的父类 SMTP 里会在去读取  SMTP 服务器的 banner 信息，于是 startTLS() 就挂在这个地方傻等直到 SMTP  服务器踢开它。。。。人生不如意事十之八九啊。。。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.rfc-editor.org/rfc/rfc2487.txt\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">rfc-editor.org/rfc/rfc2</span><span class=\"invisible\">487.txt</span><span class=\"ellipsis\"></span></a></p><p>5.2 Result of the STARTTLS Command</p><blockquote>Upon completion of the TLS handshake, the SMTP protocol is reset to<br/>the initial state (the state in SMTP after a server issues a 220<br/>service ready greeting). </blockquote><p class=\"ztext-empty-paragraph\"><br/></p><p>从这个描述看，SMTP server 是不应该再发一次 banner 的。在 AuthSMTPClient.startTLS()搭个补丁后(<a href=\"https://link.zhihu.com/?target=http%3A//code.google.com/p/gerrit/issues/detail%3Fid%3D1397\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">code.google.com/p/gerri</span><span class=\"invisible\">t/issues/detail?id=1397</span><span class=\"ellipsis\"></span></a>)，STARTTLS  顺利完成，开始 SMTP 认证，注意 apache commons-net 2.2 只支持 CRAM-SHA1, CRAM-MD5,  LOGIN, PLAIN这几种，不支持 DIGEST-MD5，还好我前面为了ReviewBoard打开了 Exim4 的 CRAM-MD5  认证支持。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这块代码感觉比较龌龊，不知道是不是实现有缺陷，所以 apache commons-net网站上把 2.x 系列从下载页面删除了，只有 1.x 和 3.x 系列。。。不知道Google 那帮人为什么没转向标准的 JavaMail API。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如果  Gitweb 已经安装了，那么 Gerrit 自动集成 Gitweb，标准安装情况下啥都不用配置，在 Gerrit Web UI  上每个补丁旁边有 gitweb 的链接，相当相当的好用。Gerrit 文档还声称能跟 cgit 集成，我没实验过。由于 Gerrit  会动态的在/srv/gerrit/.gerritcodereview/tmp/gerrit..../ 下生成  gitweb_config.perl，有这个文件后 /usr/lib/cgi-bin/gitweb.cgi 就不会读取  /etc/gitweb.conf 了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Gerrit 还能通过 commentlink 和 trackingid 跟外部的 Bug 跟踪系统集成，参考：<a href=\"https://link.zhihu.com/?target=http%3A//gerrit-documentation.googlecode.com/svn/Documentation/2.3/config-gerrit.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">gerrit-documentation.googlecode.com</span><span class=\"invisible\">/svn/Documentation/2.3/config-gerrit.html</span><span class=\"ellipsis\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><p>Gerrit 直接使用 jgit 库直接管理代码库，如果有外部的 GIT 库，比如被Gitolite 管理的，有两个办法让提交到 Gerrit 的修改也散播到外部 GIT 库里：</p><ol><li>使用 Gerrit 的 Git replication 特性，Gerrit 背地里把修改 git push 到外部库里，这个办法会有延迟。 参考：<a href=\"https://link.zhihu.com/?target=http%3A//gerrit-documentation.googlecode.com/svn/Documentation/2.3/config-replication.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">gerrit-documentation.googlecode.com</span><span class=\"invisible\">/svn/Documentation/2.3/config-replication.html</span><span class=\"ellipsis\"></span></a></li><li>创建外部库的符号链接到 /srv/gerrit/site/git/ 里，比如 ln -s /srv/git/repositories/testing.git /srv/gerrit/site/git/testing.git，需要注意文件权限。 参考：<a href=\"https://link.zhihu.com/?target=http%3A//gerrit-documentation.googlecode.com/svn/Documentation/2.3/project-setup.html%23_manual_creation\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">gerrit-documentation.googlecode.com</span><span class=\"invisible\">/svn/Documentation/2.3/project-setup.html#_manual_creation</span><span class=\"ellipsis\"></span></a>，这个文档不是说创建符号链接的，我只是猜测可行。</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><p>一番配置、读文档下来，感觉 Gerrit 真是 GIT 用户居家办公必备良品，这么好的玩意居然是开源的，太赞了！</p>", 
            "topic": [
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50795526", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 0, 
            "title": "代码评审系统 ReviewBoard 和 Gerrit (上)", 
            "content": "<p>【迁移】代码评审系统 ReviewBoard 和 Gerrit (上) </p><p>// 本来是打算把 ReviewBoard 和 Gerrit 放在一起写的，但是太长了，所以分成上下两篇。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>不论是公司里还是公司外，正经的多人合作开发最好要经常做代码评审，其必要性不用我多说，但是如何做代码评审确实个头大的事情，我个人是非常反对拉一票人去会议室开投影仪一行行讲的，太浪费资源和时间了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>抛开评审的积极性不谈，我觉得代码评审应是可以随时发起随时结束的，邮件是个很不错的载体，这在开源界已经印证了。但是邮件里发补丁确实不够正式，需要众人极高的热情和自觉，另外邮件的时延比较大，纯文本diff 格式很多人接受不了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>我以前整了一套龌龊的脚本，解决临时库和正式库的自动提交问题：</p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">          auto fetch       manually push\n正式库  --------&gt; 临时库 &lt;--------+\n |      &lt;--------                 |\n |       auto push                |\n |                                |\n +---------------&gt;  开发人员------+\n    manually fetch\n\n</code></pre></div><p>开发人员往临时库上自己的独立分支  push，触发 GIT hook  发送邮件通知大家，邮件里内嵌了提交信息，然后另外一个人回复通知邮件，在邮件开头添加[COMMIT]，临时库所在机器上有个定时任务收取邮件，遇到标题以[COMMIT]  开头的邮件就去邮件内容里找提交信息，并自动 push 到正式库里。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这么搞的目的一是让正式库安全点，因为大家 GIT  用的不是很熟练，二是增加点提交延时，虽然大家并不真的评审，但至少每一个修改会有两个人“关注”（其实是牵涉而已，很少有人真的看修改内容）。这个东西太过于理想化了，被人抱怨的最大问题是提交延时（其实也就半小时左右）。当然，也有几次确实避免了错误的提交。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>但在公司里邮件往往太多，处理很低效，还是有个  Web 界面更友好更实时点，这方面开源的独立可运行产品最为知名的该属 ReviewBoard 和Gerrit 了。还有个  Rietveld，算是 Gerrit 的前辈，Python 之父针对 Subversion 做的，使用了 Google App Engine  的服务，有人给它打了补丁以支持在本地单机运行，但终是在 Google 之外使用不广。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>ReviewBoard  的界面简洁漂亮，我挺喜欢的，但我司用的 1.5 的版本频繁挂掉，不知道新的 1.6 好点没，界面上倒是又有改进，特别是可以在 comment  里开 issue 的做法很有创意。注意这个并不是真的在 BUG 管理系统里开一个 BUG，而是在 ReviewBoard 里做一个类似FIXME  的标记。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>ReviewBoard 的安装很容易，官方文档做的挺好，我在邮件配置里卡壳了下，我的 Exim 4 只给配置了  GSSAPI 和 DIGEST-MD5 认证方式，前者给用户，后者给需要发邮件的服务，比如  RoundCube、ReviewBoard、Gerrit，但是悲催的是 ReviewBoard 使用的 Python smtplib 库只支持  CRAM-MD5、LOGIN，俺折腾了好几天，最后在qunshan@newsmth 的大力帮助下，终于搞出一个凑合可用的支持DIGEST-MD5  的 smtplib.py，代码见 <a href=\"https://link.zhihu.com/?target=https%3A//gist.github.com/2679719\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">gist.github.com/2679719</span><span class=\"invisible\"></span></a> ，但最后我还是决定让 Exim 4 支持  CRAM-MD5 认证得了，反正有 SSL 保护。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这个过程中有个小笑话，折腾 ReviewBoard  邮件发送时，我发现有时候可以成功，很是惊讶，最后发现 Web 界面保存 Email 设置时 smtp 密码有时保存为空，也就是 smtp  密码没指定时可以发送成功，原来此时 ReviewBoard 压根不会向 Exim 发送 AUTH指令做认证，而 Exim4  居然也乐呵呵的同意发送了，惊了我半身冷汗！</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Exim 这么做可能是 SMTP 协议历史上很开放，以及局域网内比较安全的缘故，但是我还是偏执的担心某个访客进公司后插个网线就可以利用Exim 狂发邮件，所以就做了限制，必需 STARTTLS 并且认证了才能发邮件：</p><div class=\"highlight\"><pre><code class=\"language-text\">$ cat /etc/exim4/conf.d/acl/30_exim4-config_check_mail\n\n### acl/30_exim4-config_check_mail\n#################################\n\n# This access control list is used for every MAIL command in an incoming\n# SMTP message. The tests are run in order until the address is either\n# accepted or denied.\n#\nacl_check_mail:\n  require\n    message = no AUTH given before MAIL command\n    authenticated = *\n    message = no STARTTLS given before MAIL command\n    encrypted = *\n\n  .ifdef CHECK_MAIL_HELO_ISSUED\n  deny\n    message = no HELO given before MAIL command\n    condition = ${if def:sender_helo_name {no}{yes}}\n  .endif\n\n  accept\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>ReviewBoard  另一个问题是不支持 HTTP 认证，原因是ReviewBoard 使用的 Django框架的旧版本对这个支持不力，新版 Django  貌似支持了，但 ReviewBoard 还需要做额外工作才能配合，比如自动创建 ReviewBoard  里的用户，比如去掉登录、注册、注销链接等等，有人提了补丁出来，还没收录，我也不清楚是不是改的完备，我不懂 Python，暂且不折腾了。</p>", 
            "topic": [
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50795404", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 1, 
            "title": "Gitolite 3 的配置", 
            "content": "<p>【迁移】Gitolite 3 的配置 </p><p>本来以为这厮很容易配置的，因为两三年前简单用了下 Gitolite 2，只用 ssh 方式访问，相当简单，没想到折腾 ssh + http + gitweb 还是很费了番力气。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Gitolite  是干嘛的就不宣传了，只说一句，Gitosis 现在是废弃的东西，关于这俩的介绍请各位参考《Pro Git》一书。之前有在水木上吹水  Gitolite 宣称与 Gitweb 集成度高，有人问了句怎么就集成度高了，俺当时没细折腾，答不具体，这次可以说详细了：</p><ul><li>Gitolite  自带的 git update hook 会根据 gitolite-admin/conf/gitolite.conf 中的权限配置更新  ~git/projects.list 文件，这个文件内容被 Gitweb 当作代码库列表。Gitolite 会根据某个 repo 是否可被  gitweb 读取或者是否有 gitweb 相关的 git config 决定要不要加入 projects.list 文件中，同时根据是否可被  daemon 用户读取决定要不要生成 REPO.git/git-daemon-export-ok 标记文件。</li><li>由于 Gitolite 是 Perl 编写的，所以其权限判断函数很容易被同是 Perl 写的 Gitweb 使用。</li></ul><p>众多开源版本控制工具里，本人对 GIT 最有好感（虽然伊也不是百分百让人满意），本着推广 Git 的精神，下面破例说说具体配置过程，各位看官参考的时候留神这篇小文写作的时间，以免信息过时被误导。另外这里是全新安装，从 2.x 升级的同学一定记得看官方文档，<b>不能自动平滑升级的！</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>Git 代码库的 Web 浏览，除了官方的 Gitweb，还有 cgit 被用的比较多，比如 <a href=\"https://link.zhihu.com/?target=http%3A//cgit.freedesktop.org/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">cgit.freedesktop.org/</span><span class=\"invisible\"></span></a>，其特点是用  C 编写，链接了 Git 官方的 libgit.a 库，不用 fork 进程去执行 git 命令，所以效率很高，但是伊就不方便用  Gitolite 的权限控制功能了。另外还有个 git-browser 使用 JavaScript 在浏览器里绘制类似 gitk  展示的版本图，效果很炫，但是太费资源了，个人觉得也没必要，gitk 已经非常好用了，又跨平台，想围观的同学可以看看 <a href=\"https://link.zhihu.com/?target=http%3A//repo.or.cz/git-browser/by-commit.html%3Fr%3Dgit/repo.git\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">repo.or.cz/git-browser/</span><span class=\"invisible\">by-commit.html?r=git/repo.git</span><span class=\"ellipsis\"></span></a> ，<b>警告，你的浏览器甚至整个桌面可能冻结住！</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>一、安装 Gitolite、Gitweb 等。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Gitolite  作者又闲又追求完美，重写了个 Gitolite 3 出来，使用上跟 2.x 很类似，据说内部代码整理的更干净高效了，目前 Debian  上还没引入 Gitolite 3.x，估计短期也不会引入，3.0 才出来一个月，而且 2.x 到 3.x  不能平滑升级。从源码安装相当简单的，git clone 把源码抓下来，git describe &gt; src/VERSION，然后把  src/ 目录复制为 /usr/local/share/gitolite，然后 </p><div class=\"highlight\"><pre><code class=\"language-text\"># ln -s /usr/local/share/gitolite/gitolite /usr/local/bin</code></pre></div><p> 就搞定了，clone 下来那份源码可以删掉。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>安装 gitweb 以及 gitweb 用来高亮代码的 highlight:</p><div class=\"highlight\"><pre><code class=\"language-text\"># aptitude install gitweb highlight\n</code></pre></div><p>由于我想让  gitolite 也支持 http 方式 push，但让 www-data 用户可以写入 git 代码库总觉得不安全，所以我 apache 的  suexec 特性，用 apache2-suexec-custom 包是因为不想把 gitolite-shell-wrapper  放到  /var/www/ 这个垃圾场里，这个地方我保留给未受 Kerberos 保护的 default virtual host  了。但是这里有个意外的问题，Apache 的 SuexecUserGroup 指令是针对整个 virtualhost 生效的，而我配置的  gitolite http 访问和 gitweb http 访问都用 http://git.corp.example.com 域名，所以 gitweb 也得以  git 用户运行，如果 gitweb 有漏洞可以写入 /srv/git，那么事情就乱套了。理想状态是 gitweb 以 gitweb  用户运行，并在 git 组里所以得以访问 /srv/git/repositories。</p><div class=\"highlight\"><pre><code class=\"language-text\"># aptitude install apache2-suexec-custom</code></pre></div><p>让  Gitolite 支持不那么正统的 http 访问方式是因为 Kerberos 统一认证，以及 Windows 下用 ssh  公钥什么的可能比较麻烦，不过还不清楚 msysgit 编译时用的 libcurl 支持 Kerberos 不，汗！ 让 Gitolite 支持  ssh 一是可以应付 Kerberos 服务当机的情况，二是一些需要访问 git 代码库的服务不用提供 Kerberos ticket  了（这个是需要用 k5start 定时刷新的），用 ssh-agent + ssh key 就行了，比如 ReviewBoard 就显式支持  ssh 方式。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>二、配置 Gitolite、Gitweb</p><p class=\"ztext-empty-paragraph\"><br/></p><p>接下来创建 git 系统用户以及 git 系统组：</p><div class=\"highlight\"><pre><code class=\"language-text\"># addgroup --system git# adduser --system --home /srv/git --shell /bin/bash --ingroup git \\\n  --disable-password --disable-login --gecos &#34;git repository hosting&#34; git\n# chmod 700 /srv/git\n</code></pre></div><p>初始化 Gitolite：</p><div class=\"highlight\"><pre><code class=\"language-text\"># ssh-keygen -t ecdsa -b 521 -f admin\n# cp admin.pub /srv/git\n$ su git\n$ gitolite setup --pubkey admin.pub\n$ rm admin.pub\n</code></pre></div><p>ssh-keygen 生成的 admin 和 admin.pub 需要复制到自己本机的 ~/.ssh 下，并设置 ~/.ssh/config:</p><div class=\"highlight\"><pre><code class=\"language-text\">Host git.corp.example.com\n    User git\n    IdentifyFile ~/.ssh/admin\n</code></pre></div><p>这时可以用 ssh 方式访问 Gitolite 了：</p><div class=\"highlight\"><pre><code class=\"language-text\">...检出 gitolite-admin 库，需要事先在 /etc/hosts 里设置 git.corp.example.com 和 IP 的对应关系...\n\n$ git clone git.corp.example.com:gitolite-admin\n...查看自己能访问哪些代码库\n\n$ ssh git.corp.example.com info\n</code></pre></div><p>布置  suexec 执行的  CGI 程序：（没有用 mod_perl 是因为 mod_perl 技术以及 git  本身都很耗内存，我是针对小站点，没必要很高效，另外 gitweb 的 FastCGI 方式不是很成熟，Gitolite 不清楚是否支持  FastCGI）</p><div class=\"highlight\"><pre><code class=\"language-text\"># mkdir -m 755 -p /srv/www/git\n# cp gitolite-shell-wrapper /srv/www/git/gitolite-shell-wrapper\n# cp /usr/share/gitweb/gitweb.cgi /srv/www/git/gitweb.cgi\n# chown -R git git /srv/www/git\n# chmod 755 /srv/www/git\n# chmod 700 /srv/www/git/{gitweb.cgi,gitolite-shell-wrapper}\n</code></pre></div><p>比较悲催的是为了安全起见 suexec 不允许符号链接，所以这里只好拷贝文件了，属主、权限都必需按照上面的设置，否则 suexec 检查会失败。</p><p>/srv/www/git/gitolite-shell-wrapper 的内容如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">#!/bin/sh\n#\n# Suexec wrapper for gitolite-shell, suexec doesn&#39;t allow symlink,\n# and it will clear those GIT or Gitolite environment variables\n# before executing this script.\n#\n\nexport GIT_PROJECT_ROOT=${GIT_PROJECT_ROOT:-/srv/git/repositories}\nexport GITOLITE_HTTP_HOME=${GITOLITE_HTTP_HOME:-/srv/git}\nexport GIT_HTTP_EXPORT_ALL=1\n\nexec /usr/local/share/gitolite/gitolite-shell\n</code></pre></div><p>/etc/gitweb.conf（如果同一个机器上你想安装多份 gitweb，可以把这个内容复制到 /srv/www/git/gitweb_config.perl，参见 man gitweb.conf）:</p><div class=\"highlight\"><pre><code class=\"language-text\"># path to git projects (&lt;project&gt;.git)\n# DON&#39;T include trailing slash, it breaks $export_auth_hook below!\n$projectroot = &#34;/srv/git/repositories&#34;;\n\n# directory to use for temp files\n$git_temp = &#34;/tmp&#34;;\n\n# target of the home link on top of all pages\n#$home_link = $my_uri || &#34;/&#34;;\n\n# html text to include at home page\n#$home_text = &#34;indextext.html&#34;;\n\n# file with project list; by default, simply scan the projectroot dir.\n$projects_list = &#34;/srv/git/projects.list&#34;;\n\n# stylesheet to use\n@stylesheets = (&#34;/static/gitweb.css&#34;);\n\n# javascript code for gitweb\n$javascript = &#34;/static/gitweb.js&#34;;\n\n# logo to use\n$logo = &#34;/static/git-logo.png&#34;;\n\n# the &#39;favicon&#39;\n$favicon = &#34;/static/git-favicon.png&#34;;\n\n# git-diff-tree(1) options to use for generated patches\n#@diff_opts = (&#34;-M&#34;);\n\n$prevent_xss = 1;\n$highlight_bin = &#34;/usr/bin/highlight&#34;;\n$feature{&#39;highlight&#39;}{&#39;default&#39;} = [1];\n$feature{&#39;timed&#39;}{&#39;default&#39;} = [1];\n\n$strict_export = 1;\n$export_ok = &#34;git-daemon-export-ok&#34;;\n\nBEGIN { $ENV{HOME} = &#34;/srv/git&#34;; }\nuse lib &#39;/usr/local/share/gitolite/lib&#39;;\nuse Gitolite::Easy ();\n$export_auth_hook = sub {\n    return unless $ENV{REMOTE_USER};\n    my $repo_path = $_[0];\n    my $repo_name = substr($repo_path, length($projectroot) + 1);\n\n    $ENV{GL_USER} = $ENV{REMOTE_USER};\n    $repo_name =~ s/\\.git$//;\n    Gitolite::Easy::can_read($repo_name);\n};\n</code></pre></div><p>上面结尾的一段是调用 Gitolite 3 的权限判断函数，can_read() 貌似只能判断到库一级，更底层的函数 access() 貌似能判断更细，但是由于 GIT 的代码树不可分割以及这是为企业内部考虑，读权限粒度粗放一点一般也不是大问题。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>需要强调一下的是  $strict_export 和 $export_ok，默认两个是没定义的，在没有定义 $export_auth_hook 时，会导致  gitweb 首页没有列出的 project 在 URL 里直接输入是可以访问的，比如  http://git.corp.example.com/gitweb?p=gitolite-admin.git;a=log，我很晕这个默认设置，太不安全了，我甚至怀疑扫描互联网上公开的  gitweb，或许能扫出一片 gitolite-admin.git，从 conf/gitolite.conf  知道所有代码库列表，从而看到隐藏代码库的内容。。。。</p><p>$strict_ok 的意思是不在 projects.list  中的代码库不显示在代码库列表里，也不能通过 URL 直接访问，这个选项需要 Gitolite 里给代码库权限加入  R =  gitweb，或者这个代码库有 <a href=\"https://link.zhihu.com/?target=http%3A//gitweb.xxx\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">gitweb.xxx</span><span class=\"invisible\"></span></a> 的 git 配置(Gitolite 支持在 conf/gitolite.conf  里为每个代码库设置 git config 的配置）。</p><p>$export_ok 的意思是只有  REPO.git/git-daemon-export-ok 存在时才放入 projects.list 并可以通过 URL 直接访问，这个选项需要  Gitolite 里给代码库加入 R = daemon 权限，加上 gitweb 的，就是 R = daemon gitweb 了，比如：</p><div class=\"highlight\"><pre><code class=\"language-text\">...in working copy of gitolite-admin...\n$ cat conf/gitolite.conf\n...\nrepo myserver\n    RW+ = dieken\n    R   = daemon gitweb\n$ git comit -a -m &#34;blah blah&#34;\n$ git push  # will trigger git update hook, /srv/git/projects.list will be generated from scratch\n</code></pre></div><p>最后一道工序，配置 Apache:</p><div class=\"highlight\"><pre><code class=\"language-text\">$ cat /etc/apache2/suexec/www-data\n/srv/www\npublic_html/cgi-bin\n$ cat /etc/apache2/sites-enabled/git\n&lt;VirtualHost *:80&gt;\n    ServerName git.corp.example.com\n    ServerAdmin webmaster@corp.example.com\n    DocumentRoot /nonexistent\n\n    ErrorLog ${APACHE_LOG_DIR}/git-error.log\n\n    # Possible values include: debug, info, notice, warn, error, crit,\n    # alert, emerg.\n    LogLevel warn\n\n    CustomLog ${APACHE_LOG_DIR}/git-access.log combined\n\n    SuexecUserGroup git git\n\n    # Actually, suexec will clear these environment variables...\n    SetEnv GIT_PROJECT_ROOT         /srv/git/repositories\n    SetEnv GITOLITE_HTTP_HOME       /srv/git\n    SetEnv GIT_HTTP_EXPORT_ALL\n\n    ScriptAlias /git/ /srv/www/git/gitolite-shell-wrapper/\n    ScriptAlias /gitweb /srv/www/git/gitweb.cgi\n    Alias /static /usr/share/gitweb/static\n\n    &lt;Directory /usr/share/gitweb/static&gt;\n        AllowOverride None\n        Order allow,deny\n        Allow from all\n    &lt;/Directory&gt;\n\n    &lt;Location /&gt;\n        AuthType Kerberos\n        Require  valid-user\n\n        Order allow,deny\n        Allow from all\n    &lt;/Location&gt;\n\n    &lt;Location /git&gt;\n        AuthType Kerberos\n        Require  valid-user\n\n        Order allow,deny\n        Allow from all\n    &lt;/Location&gt;\n\n    &lt;Location /gitweb&gt;\n        AuthType Kerberos\n        Require  valid-user\n\n        Order allow,deny\n        Allow from all\n    &lt;/Location&gt;\n\n    RedirectMatch   ^/$     /gitweb\n\n&lt;/VirtualHost&gt;\n\n</code></pre></div><p><b>没有给 Apache 配置 Kerberos 认证的同学可以用 AuthType Basic，参考 <a href=\"https://link.zhihu.com/?target=http%3A//sitaramc.github.com/gitolite/http.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">sitaramc.github.com/git</span><span class=\"invisible\">olite/http.html</span><span class=\"ellipsis\"></span></a> 。</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>HTTP 方式访问 Gitolite:</p><div class=\"highlight\"><pre><code class=\"language-text\">$ kinit\n\n$ curl --negotiate -u : http://git.corp.example.com/git/info\n$ git clone http://dieken@git.corp.example.com/git/gitolite-admin\n</code></pre></div><p>&#34;git  clone&#34; 命令里需要指定 &#34;dieken@&#34; 部分，否则 git 会提问密码，但是在 Kerberos  认证时，这两个问题都可以填入任意字符，如同上面那条 curl  命令似的，&#34;-u :&#34; 指定用户名为冒号，这是 curl 实现的问题，虽然  Kerberos ticket 提供了用户名，但是还是需要指定下。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>访问 Gitweb：http://git.corp.example.com/ 。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Gitolite 这个奇技淫巧的东西还有许多特性有待钻研，以后再慢慢琢磨了。</p>", 
            "topic": [
                {
                    "tag": "Git", 
                    "tagLink": "https://api.zhihu.com/topics/19557710"
                }, 
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": [
                {
                    "userName": "ktw361", 
                    "userLink": "https://www.zhihu.com/people/9f87a7a6f2071508a2036844a4cbc175", 
                    "content": "伊？兄弟福建人？", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50795346", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 0, 
            "title": "Web 会议系统", 
            "content": "<p>【迁移】Web 会议系统</p><p>有时候地理位置不在同一地方的几个人想开会，讨论个设计，开个讲座啥的，有个 Web 会议系统会很方便，客户端有浏览器就行，装个 JRE 或者 Flash 插件。<a href=\"https://link.zhihu.com/?target=http%3A//en.wikipedia.org/wiki/Web_conferencing\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Web conferencing</a> 系统的几个基本需求：</p><ul><li>客户端只需要 Web 浏览器，一般都会用 Flash 或者 JRE 插件，Google 搞的 WebRTC 基于 HTML 5，很有意思。</li><li>会议室的权限控制：谁可以参加，谁可以得到主持权力，谁可以邀请别人参加，等等。</li><li>共享白板用来画示意图</li><li>共享屏幕用来展示一些文档什么的</li><li>共享桌面控制，允许别人暂时操作下自己的机器</li><li>音频、视频通讯，声音是不用说了，视频可以用来看看说话的人是谁（我经常分辨不出电话会议里说话的人是谁。。。）</li><li>会议过程的录制，供事后查阅</li><li>跟日历系统集成，或者有自己的日历系统，查看有哪些会议即将发生，过去开过什么会议</li><li>会议的进行跟某一个参与者是否在线没有耦合，比如主持人掉线了，会议不能自动结束，会议录制不能中断</li><li>为了避免泄漏商业机密，Web 会议系统不能是第三方公司管理的<br/></li></ul><p>商业 Web 会议系统很多，我用过 Adobe Connect，听说过 Lync 和 WebEx。满足<i><b>部分</b></i>需求的产品也颇有一些，比如  NetMeeting(可惜 Ekiga 不支持白板、程序共享、桌面共享）、一些 IM 工具。开源的正经 Web  会议系统却是很少，很可能是因为涉及到音频、视频、流媒体、客户端 JS、Flash 等多方面技术，不那么容易实现。从网上打听到的开源 Web  会议系统有这些：</p><ul><li>OpenMeetings：原来代码在 google code 上，现在转移到 Apache  组织下孵化了，这是我唯一试用过的，运行很方便，开箱即用，外部依赖极少，功能比较全面，基本可用，小 bug 比较多。客户端用的 Flash  做界面，Java WebStart 技术做屏幕共享，服务端使用 Java 技术，尤其是 Red5 流媒体服务器。<br/></li><li>BigBlueButton：开发状态不错，但我看到安装文档就退却了，安装还真罗嗦，依赖暴多，看文档似乎是混杂了 Ruby/Java/Flash。看 demo 的界面貌似功能不如 OpenMeetings 丰富。</li><li>WebHuddle:  这厮号称是开源的，其在 <a href=\"https://link.zhihu.com/?target=http%3A//sourceforge.net\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">sourceforge.net</span><span class=\"invisible\"></span></a> 上的源码看起来很久没更新了，下载里有 <a href=\"https://link.zhihu.com/?target=http%3A//sourceforge.net/projects/webhuddle/files/webhuddle/WebHuddle-0.4.9/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">0.4.9 Windows</a> 版，2007 年上传的。从这个台湾兄弟的<a href=\"https://link.zhihu.com/?target=http%3A//163.13.177.120/blog/index.php%3Fload%3Dread%26id%3D11\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">教学片</a>看，这套系统服务端用了 jboss，客户端是用 Java Applet。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>总结下：如果企业有这方面的需求，还是找个商业系统吧，实在没钱又不想盗版，可以试试 OpenMeetings。</p>", 
            "topic": [
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50795289", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 0, 
            "title": "居然将近五个月过去了", 
            "content": "<p>【迁移】居然将近五个月过去了</p><p>从去年十二月开始写系统管理系列，断断续续罗哩罗嗦自说自划居然将近五个月了！ 这好像是我有生以来全凭兴趣业余坚持的最长的一件事情，汗。。。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>想想还是比较骄傲的，虽然大多写的不深入，但毕竟绝大多数都是亲身实践了一次，我也不是（也没打算将来成为）专业系统管理员，另外持续维护了五个月的自动配置脚本，所以我可以在很短时间内复现这一段经历，这点尤其自豪。剩下还有一些东西折腾下就暂告一段落了：</p><ul><li>Web 会议系统：地理位置上不在一起办公，偶尔会有需求开网络会议，能共享桌面、白板啥的；</li><li>Code review 系统：ReviewBoard, Gerrit。Gitolite 作者正在重写新版本，可惜 Debian 还没收录。</li><li>备份系统：Bacula/Amanda/Burp/BakcupPC/obnam...</li></ul><p>这几个比较好玩，其它几个也挺基本，但不打算折腾了：</p><ul><li>FAI：由于是 IT 基础设施，重装的概率很小，而且有自动配置脚本，所以对自动安装系统没太大需求；</li><li>Jenkins：CI 系统是必不可少的，但这个东西没啥好研究的，用就是了；</li><li>OpenGrok: 这个我一直在用，相当好使，也属于基本没讲头的东西，用 Apache 挡在前面防止匿名访问好了，麻烦的是基于路径的访问权限设置，这个恐怕要改 OpenGrok 代码，我是没精力折腾到 OpenGrok 代码里头了；</li><li>Search：用 Apache solr 搭一个内部搜索网站，这个工作量太大，不是简单配置下就可以的。</li></ul><p>另外监控也可以做的更细，比如用 monit 监控服务进程，用 logcheck 检测日志，用 fail2ban 避免暴力破解；</p><p>Kerberos 如何引入双因子或者一次性密码认证也是个有意思的话题，有空了再看看，可以参考 google-authenticator 和 Barada 。</p><p>系统级的安全防护对于至关重要的  IT 基础设施能提供更高的保险，我是没指望各个服务自身是没有漏洞了，只能希望一个服务被攻击了影响的面越小越好，这方面 GrSecurity,  AppArmor, SELinux, SMACK, TOMOYO  之流应该很有帮助，但恐怕是没精力一一折腾了，偏执归偏执，暂且认为小企业内部员工是相当值得信任的吧，并且所有服务都在内网里头，好比鸵鸟埋头于沙中，安全感那是相当强啊，哈哈。</p><p>还有一些细节问题没处理，比如邮件地址簿，我没折腾  LDAP 地址簿，RoundCube 里也没配置这个，滑稽的是这恐怕是 LDAP 最原始的用途了，安慰自己“无伤大雅”了，还好  Kerberos 单点登录、各系统之间配合的想法得到贯彻，整个系列配置下来还是基本达到目标的。</p><p>人生在于折腾，生命不息，折腾不止，等上面三个大致琢磨一遍后再玩玩 Common Lisp，写个 Common Lisp 学习笔记系列？</p>", 
            "topic": [
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50795225", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 2, 
            "title": "监控系统", 
            "content": "<p>【迁移】监控系统</p><p>浅尝辄止的一点笔记。在正文之前的总结：</p><p>监控一个系统，无非是两种需求： 持续的监视各个组件的表现(performance)，比如<br/>QPS、latency、CPU、memory、disk、network 等等，得知系统运行是否顺畅，资源<br/>利用是否充分；另一个重要需求是在性能超出预定范围时及时通知管理员，比如<br/>内存不足、CPU 占用太高、磁盘剩余空间不够。</p><p>    * 警报(alerting): Nagios 或者 Icinga 无疑是这方面事实上的王者。<br/>    * 性能图表(performance graph)：<br/>        * 有了 Nagios/Icinga 后可以重用它们收集的性能数据，自行搭配Nagios的各种<br/>          Web UI 以及 Cacti，或者使用 OpsView、isyVmon 那样的集成包；<br/>        * 或者再单独用其它系统收集性能数据并展现，这方面推荐 Ganglia，其实<br/>          Ganglia 收集的数据也可以送给 Nagios 来判断是否发出警报，这样就<br/>          不用 Nagios 收集数据了。</p><p>对于系统管理员来说，警报比起性能图表有用的多，谁有精力整天盯着漂亮的图表看？<br/>这也就解释了 Nagios 在 <a href=\"https://link.zhihu.com/?target=http%3A//LinuxQuestions.org\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">LinuxQuestions.org</span><span class=\"invisible\"></span></a> Members Choice 中占据了约 65% 的<br/>票数：<br/><a href=\"https://link.zhihu.com/?target=http%3A//www.linuxquestions.org/questions/2011-linuxquestions-org-members-choice-awards-95/network-monitoring-application-of-the-year-919908/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">linuxquestions.org/ques</span><span class=\"invisible\">tions/2011-linuxquestions-org-members-choice-awards-95/network-monitoring-application-of-the-year-919908/</span><span class=\"ellipsis\"></span></a><br/><a href=\"https://link.zhihu.com/?target=http%3A//www.linuxquestions.org/questions/2011mca.php\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">linuxquestions.org/ques</span><span class=\"invisible\">tions/2011mca.php</span><span class=\"ellipsis\"></span></a></p><p>想尝试商业支持力度大的非 Nagios 衍生的开源产品，可以试试 Pandora FMS、Zabbix、<br/>NetXMS、Zenoss，注意它们各自的收费版本相比其开源版本，功能更强大。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><a href=\"https://link.zhihu.com/?target=http%3A//en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">en.wikipedia.org/wiki/C</span><span class=\"invisible\">omparison_of_network_monitoring_systems</span><span class=\"ellipsis\"></span></a></p><p>监控系统必不可少的三个部分：<br/>    * 数据采集：如 SNMP 方式，TCP/IP 主动探测服务方式，采集系统内存、负载信息；<br/>    * 数据存储：使用 rrdtool 保存到 rrd 数据文件里，或者保存到关系数据库里；<br/>    * 数据展现：常用 rrdtool 绘制时间序列图。</p><p>数据采集部分如果能支持自动服务发现会方便很多，不用一项项基础服务都手动配置一遍；<br/>数据采集完后送给数据存储时，可以根据预定义的条件触发警报；<br/>在数据展现层面可以做一些数据分析，如服务状态的预测。</p><p>* Nagios Core</p><p>标配不支持服务自动发现，配置繁琐。长于数据收集和预警，预警的逻辑考虑周全。<br/>标配的数据展现非常弱。</p><p>  Remote check:<br/>    * check_by_ssh<br/>    * NRPE<br/>    * nsclient<br/>    * check_snmp<br/>    * nrdp: replacement of nsca<br/>    * nsca<br/>    * check-mk<br/>    * nagios-statd-server</p><p>  Web UI:<br/>    * Nagios CGI<br/>    * Centreon<br/>    * Ninja<br/>    * Nagvis<br/>    * Thruk<br/>    * Check-MK Multisite</p><p>* Icinga</p><p>Nagios 的派生版本，改进了 Web UI，使用了一些 HTML 5 特性。<br/><a href=\"https://link.zhihu.com/?target=https%3A//www.icinga.org/nagios/feature-comparison/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">https://www.icinga.org/nagios/feature-comparison/ </a>并没有很重大的改进。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//robertogaloppini.net/2010/11/25/sos-open-source-reports-open-source-monitoring-icinga-vs-nagios/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">robertogaloppini.net/20</span><span class=\"invisible\">10/11/25/sos-open-source-reports-open-source-monitoring-icinga-vs-nagios/</span><span class=\"ellipsis\"></span></a><br/>The &#34;community&#34; behind Icinga is the company Netways. Their business model was<br/>to hijack the Nagios brand until Ethan Galstad told them to stop selling<br/>Nagios as if it were their own.</p><p>比较怀疑是一部分开发者被踢出局没分到好处。这个 netways 公司很奇怪，其<br/>主页上的产品列表把 Nagios/JasperReport/Puppet/MuleESB/RT/Bacula/TWiki<br/>全列上去(<a href=\"https://link.zhihu.com/?target=http%3A//www.netways.de/en/de/produkte/products/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">netways.de/en/de/produk</span><span class=\"invisible\">te/products/</span><span class=\"ellipsis\"></span></a>)，产品图标<br/>换了，又没有明显链接到各自官方网站，确实颇有欺世盗名之嫌。</p><p>* Shinken</p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.shinken-monitoring.org/what-is-in-shinken-not-in-nagios-and-vice-versa/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">shinken-monitoring.org/</span><span class=\"invisible\">what-is-in-shinken-not-in-nagios-and-vice-versa/</span><span class=\"ellipsis\"></span></a></p><p>Python 编写，基本是重新实现 Nagios，兼容 Nagios 的插件，独立执行的组件之间分工很明确。<br/>喜欢 Python 的兄弟不妨试试。</p><p>* OpsView</p><p>打包 Nagios + NagVis + NagiosGraph + MRTG + NMIS.</p><p>* isyVmon (<a href=\"https://link.zhihu.com/?target=http%3A//www.isyvmon.com/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">isyvmon.com/</span><span class=\"invisible\"></span></a>)</p><p>打包 Nagios + Centreon.</p><p>* Munin</p><p>数据收集、存储和展现，后两者都依赖 rrdtool。Perl 编写，数据采集插件丰富，<br/>尤其插件自动激活很好用，达到服务自动发现的效果，很省事。可能是因为 Perl<br/>编写的缘故，以及 munin 没有一个为 web ui 提供数据的持续运行的 server，<br/>每次展现都要重新生成图表以及读取 rrd 文件，导致 munin CPU 消耗较大，我<br/>试用的结果是频繁有百分之十五到二十的 CPU 占用率。</p><p>munin 从 monitor agent 收集信息存入 rrd 文件以及生成 html 页面供<br/>web 界面使用，这有两种方式实现，默认是用每五分钟执行的 cron 任务，<br/>这个任务以 munin 用户身份执行，写入 /var/cache/munin 和 /var/lib/munin<br/>下，另一种方式是让 munin cgi 程序被调用时实时生成，这种方式耗费资源更多，<br/>但实时性强。注意第二种方式是以 apache 用户身份执行，也就是 www-data<br/>用户，切换执行方式时需要注意上面提到的两个目录里文件权限，权限不对<br/>会导致 munin 无法更新 rrd 文件，各种性能图不会更新。</p><p>Munin 的最大亮点是其丰富而且自动配置的插件，munin 也支持基本的报警，但<br/>太嫩，推荐通过 Nagios 报警:<br/><a href=\"https://link.zhihu.com/?target=http%3A//munin-monitoring.org/wiki/HowToContact\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">munin-monitoring.org/wi</span><span class=\"invisible\">ki/HowToContact</span><span class=\"ellipsis\"></span></a><br/><a href=\"https://link.zhihu.com/?target=http%3A//waste.mandragor.org/munin_tutorial/munin.html%23alerting\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">waste.mandragor.org/mun</span><span class=\"invisible\">in_tutorial/munin.html#alerting</span><span class=\"ellipsis\"></span></a></p><p>* Ganglia</p><p>收集信息并使用 rrdtools 存储、画图，漂亮，高效，扩展大多用 C 编写，<br/>以动态库形式载入 Ganglia monitor 服务进程，虽然高效，但感觉耦合太大，<br/>崩溃的风险比较大。</p><p>不知道是不是错觉，虽然也是用 rrdtools 画图，我觉得 Ganglia 画出来<br/>的图表就是要漂亮的多，貌似跟用色、线条有关系。</p><p>Ganglia 默认使用 UDP 多播通信，可能需要路由以及防火墙的额外配置，如果没<br/>这个条件的话可以用 UDP 单播。</p><p>* Collectd</p><p>收集 cpu/ram/disk/network/irq/processes 信息写入 rrd 文件，没有直接支持<br/>画图功能，报警特性还处于初级阶段。</p><p>* Cacti</p><p>专注于数据展现，基本是 RRDTool 的 Web 前端，用于展示各种性能图表，依赖 MySQL 数据库。</p><p>* Zabbix</p><p>依赖 SQL 数据库， 服务进程会派生很多子进程，网页响应很慢，比较重量级。<br/>支持性能监控和报警，文档和功能都比较丰富，有企业级派头。zabbix proxy 的<br/>设计很适合企业监管地理隔离的多个机房。</p><p>zabbix 的 web 界面支持 HTTP 认证，但是是用 PHP 的内置 http 认证特性做的，不支持<br/>web 服务器的 http 认证，糙快猛的修改很容易，grep -nr ZBX_AUTH_HTTP /usr/share/zabbix，<br/>给 HTTP 认证加上对 REMOTE_USER 的判断就差不多了。</p><p>Debian 对 Zabbix 的打包有两个坑人的地方，zabbix_agentd.conf 里 Server<br/>选项官方设置的是 127.0.0.1，Debian 改成 localhost 了，可能是新版<br/>zabbix 不再解析主机名或者域名了，导致 zabbix-agent 阻止了来自 127.0.0.1<br/>的连接请求，zabbix web 界面的 Graphs 部分显示不了图形，这个问题在<br/>zabbix_agentd.log 里显示：<br/>Listener error: Connection from [127.0.0.1] rejected. Allowed server is [localhost]</p><p>另一个问题是官方的 zabbix_agentd.conf 里 Hostname 是 &#34;Zabbix server&#34;,<br/>这个是跟 zabbix web UI 里的 hosts 设置对应的，zabbix-agent 靠这个名字<br/>找它的配置，但 Debian 打包的这个文件里此设置是空的，默认是主机名，跟<br/>zabbix Web UI 里的 &#34;Zabbix server&#34; 不匹配，导致 Zabbix Web UI 里<br/>报错说 Zabbix server unreachable，在 Zabbix agent 日志里显示：<br/>No active checks on server: host [gold] not found</p><p>虽然 Zabbix 的 Web UI 做的挺好，但是我有点排斥这种把配置信息记录到<br/>数据库里的做法，不便于版本跟踪（真是难伺候啊，没有 Web UI 抱怨不友好，<br/>有 Web UI 又抱怨没有控制感）。</p><p>* Zenoss</p><p>使用 Python 编写，依赖 MySQL 数据库，官方有提供给 Debian 用的 Zenoss Core<br/>deb 包。Web UI 很强大，支持部分 Nagios 插件，号称不需要 agent（猜测是用<br/>ssh 远程运行探测脚本)。</p><p>* Pandora FMS</p><p>被其 Web UI 抓图震住了，很漂亮，很强大的样子。</p><p>Pandora agent 和 server 采用 Perl 编写，使用 MySQL 存储数据，Web UI<br/>使用 PHP。</p><p>* OpenNMS</p><p>使用 Java 编写，我觉得监控的东西应该耗资源越少越好，所以一看是 Java<br/>的我就没兴趣了，伊还依赖关系数据库(PostgreSQL)，Web 服务器(Tomcat or Jetty)。</p><p>伊还是颇占了一小块市场份额，猜测是因为强大的 Web 界面以及通过 NRPE、NSClient<br/>使用 Nagios 的插件。</p><p>* NetXMS</p><p>官网 <a href=\"https://link.zhihu.com/?target=http%3A//www.netxms.org/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">netxms.org/</span><span class=\"invisible\"></span></a> 有适用于 Debian 的 deb 包，控制台原来是个<br/>Win32 GUI 做的，现在基于 Eclipse 了，看抓图很强大的样子。</p><p>NetXMS 跟很多监控系统一样，分 agent/server/console 三大块，agent 很小，<br/>运行在被监控机上；server 存储监控数据，NetXMS server 依赖关系数据库；<br/>console 用来查看监控数据。</p><p>NetXMS 可以收集数据、展现并报警，有意思的是它可以使用大部分 nagios<br/>插件，并且拥有自己的脚本语言。</p><p>我没有试用这东西，有兴趣的同学可以试试，官方文档做的不错。</p><p>* MRTG</p><p>使用 SNMP 收集信息，并使用 rrdtools 画图。</p><p>* cricket (<a href=\"https://link.zhihu.com/?target=http%3A//cricket.sourceforge.net/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">http://cricket.sourceforge.net</a>)</p><p>收集信息并使用 rrdtools 存储、画图，看项目状态还很嫩的样子。</p><p>* xymon</p><p>UI 太 geeky 了，sourceforge 项目主页上有不少好评，貌似很容易安装。粗略看<br/>起 Web UI 的输出，看起来检测都是用 shell 脚本分析的，比如分析 df、<br/>netstat 输出，连 web 的 CGI 脚本都是用 shell 写的。</p><p>* ntop</p><p>网络性能监控，依赖网卡混杂模式嗅探数据包，能检测到局域网中每台机器的流量。</p><p>* argus (argus-client, argus-server)</p><p>网络性能监控。</p>", 
            "topic": [
                {
                    "tag": "监控", 
                    "tagLink": "https://api.zhihu.com/topics/19563297"
                }, 
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50795191", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 0, 
            "title": "Drupal 浅试", 
            "content": "<p>【迁移】Drupal 浅试</p><p>理论上有 Wiki 了，是没太大必要整花哨的 CMS 的，不过，肯定有用户不愿意皈依 Wiki，哪怕有所见即所得编辑特性。以前也搭过   Drupal，界面是相当清爽，不少同事表示很喜欢，Blog、Book、Forum、Poll，界面一致易用，也很实用。Debian 打包的  Drupal 最新版是 7.12，就单单一个包，安装很容易，deb 包安装时会自动配置好数据库。一个小问题是 PHP  版本太高，对语法要求更严格了，导致在 Drupal 的界面上经常显示长长的警告信息，Drupal  官方的问题列表有人提出补丁了，貌似也进了版本库，只是没发布新版本。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>接下来就是不变的主题了：如何支持 web server 的认证机制，也即让 Drupal 7 使用 REMOTE_USER 信息识别当前用户。Drupal 7  没有内置支持，有一个 Drupal 6.x 的插件 <a href=\"https://link.zhihu.com/?target=http%3A//drupal.org/project/webserver_auth\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">drupal.org/project/webs</span><span class=\"invisible\">erver_auth</span><span class=\"ellipsis\"></span></a>  支持这个特性，我拿过来借助 Drupal 的 Coder 模块自动升级功能以及慢慢摸索，成功移植到 Drupal 7.x 上了，Drupal  官方的文档做的还是挺不错的。移植过程中增加了一点小特性，或者说改正了一个小问题，就是在用 webserver_auth  注册用户前，先判断下有没有其它机制创建了同名用户，有的话就直接登录，不注册了，否则 Drupal 会报错说插入数据库时违反了唯一性。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>移植这个模块过程中，又发现 PHP 的一个很龊的地方：</p><div class=\"highlight\"><pre><code class=\"language-text\">&lt;?php\n\nerror_reporting(E_ALL | E_STRICT);\n#echo array_pop(array(2, 3));\n$tmp = array(2, 3);\necho array_pop($tmp);\n</code></pre></div><p>注释掉的那行是不行的写法，php  会抱怨： Only variables can be passed by  reference，我没仔细看网上关于这个问题的解释，貌似是因为解释器实现的限制，会导致内存泄漏还是破坏来着。唉，怎么有这么矬的语言大家用的还挺  high。。。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>折腾完毕后测试了下，用 Kerberos 认证后，Drupal 确实给创建了个新用户，但是这个新用户不能在  Drupal 上新增任何内容，我还以为是那个 webserver_auth  模块移植的有问题呢，后来猜测是默认权限设置有问题，于是找在哪里设置权限， People、Configuration 页面都没有找到，最后发现在  Modules 页面里，真是蛋疼，对 Drupal UI 友好的感觉下降了一小截。Drupal 里用 Role 分配权限，其实 Role 就是  group，貌似是不支持直接针对 user 设置权限，非得放到某个 Role 里，这点不大方便，貌似也不能直接对某一篇文章设置权限，这在  wiki 里是很方便的事情。</p>", 
            "topic": [
                {
                    "tag": "Drupal 7", 
                    "tagLink": "https://api.zhihu.com/topics/19629064"
                }, 
                {
                    "tag": "Drupal", 
                    "tagLink": "https://api.zhihu.com/topics/19597200"
                }, 
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50795110", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 0, 
            "title": "ViewVC 的配置", 
            "content": "<p>【迁移】ViewVC 的配置</p><p>之前给 svnserve 分配了 http://svn.corp.example.com 的域名，用于 svn:// 协议，空出  http://svn.corp.example.com 没用总觉的空了一块。本来是不想鼓捣 svn 一众的，考虑到不是所有人都甘心皈依  GIT，就配置了 svnserve，所以索性就再给 http://svn.corp.example.com/ 配上 ViewVC 得了。</p><p>ViewVC 是用 Python 写的，其作者不满 Perl 写成的 CVSWeb (见 <a href=\"https://link.zhihu.com/?target=http%3A//viewvc.org/who.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">viewvc.org/who.html</span><span class=\"invisible\"></span></a>)。我大致了解下  ViewVC 的发展历史，伊其实也前后改动蛮大的，到现在恐怕也谈不上代码结构完美，这其中一方面是 VCS  工具特性各异，想抽象出统一的抽象层比较难，另一方面也是时间长了，掺和的人多了，需求多了，特性多了，必然是代码趋于混乱，颠扑不破的熵增原理。改成“优美”的语言写东西，真不见得代码就优美就易维护。BTW，本人  Perl fan。。。</p><p>ViewVC 有个极好的特性，支持 svn 的 authz  文件，这样一份基于路径的授权文件，可以同时用于 svnserve 服务以及 ViewVC，相当赞！ 但这里有个小问题，svnserve +  gssapi sasl 认证得到的用户名是不带 @REALM 后缀的，这是我期望的行为，因为我不想在 authz 文件里罗嗦的写  userA@CORP.EXAMPLE.COM、userB@CORP.EXAMPLE.COM，但是 apache2 + mod_auth_kerb  得到的 REMOTE_USER 是 user@REALM 的形式，导致 authz 里的设置 ViewVC 用不了。</p><p>解决这个问题的第一思路自然是修改  ViewVC 的代码，让它判断 authz 时把 @REALM  去掉，很容易找到修改的地方，改动也就两三行代码，改完收工，试用了下果然有效，美了不到几分钟，无意间 Google 出 mod_auth_kerb  最新版有个秘密特性 KrbLocalUserMapping，设置成 On 的话 mod_auth_kerb 就会把 REMOTE_USER  设置成 user 而非 user@REALM，这个特性在配置文档里没提，代码里的 ChangeLog 提到了，汗。。。。</p><p>于是小折腾了一番，打开这个特性，重新检查之前配置的所有  web app，中间经历莫名的 &#34;gss_acquire_cred() failed: Unspecified GSS failure.   Minor code may provide more information (, )&#34; 错误，重启机器后居然好了，最后除了 Bugzilla  要求用户名是邮件地址，其它 web app 都用了 KrbLocalUserMapping On 设置，可怜我之前还折腾 Foswiki  里如何写插件把 user@REALM 转成 user。。。还好结局是比较理想的，看着不带 @REALM 的用户名感觉还是清爽多了。</p><p>Debian  打包的 ViewVC 版本太旧了，官方的 1.1.x 系列已经初步支持 WSGI 和 FastCGI 模式运行了，我配置成 WSGI 模式，以  viewvc 用户身份运行 WSGI daemon，这个用户处在 svn 组里，这样伊就可以读取 /srv/svn 下的代码库了。</p><p>还有一个小问题是  ViewVC 的 query.cgi 脚本，在 Debian 里位于单独的 viewvc-query 软件包，这个包依赖 MySQL  数据库保存 svn 提交日志，在 ViewVC 的 bug 跟踪系统里有 PostgreSQL 的补丁，但是很暴力，直接把 MySQL  支持干掉了，所以至今 ViewVC 也没正式支持 PostgreSQL。我因为之前配置的 web app 都用的 PostgreSQL  数据库，不想再累赘添个 MySQL，而且同时装这两种数据库的话，一些 web app 的 Debian 安装脚本会优先选择  MySQL，从而破坏我那些自动安装配置脚本的逻辑，所以就放弃 query.cgi 了。貌似新版 ViewVC 里 viewvc.cgi 内置了  query 功能，viewvc.conf 里提到，我没找到对应代码什么地方，可能是指现在 viewvc 可以直接查询 svn log  了，缺少的只是搜索功能，这部分我打算用更强大的 OpenGrok 以及定制 Apache solr 代替。</p>", 
            "topic": [
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50793993", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 0, 
            "title": "Windows，KFW，SASL，GSSAPI， 迷一般的组合", 
            "content": "<p>【迁移】Windows，KFW，SASL，GSSAPI， 迷一般的组合</p><p>前几天发布 MyServer-scripts-2.1 时，发现 64 位Windows 7下 HTTP Negotiate 和 GSSAPI 认证不好使了，接下来几天很是折腾了一番，终于比较粗暴的搞定了，记录点笔记以备忘。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>问题一：  KFW (Kerberos for Windows 3.2.2)保存设置失败。</p><p>原因：  用 Sysinternals Suite 的 Process Monitor 查看了下，KFW 写入 C:\\Windows\\krb5.ini  失败，原来是 Windows 7 对 C: 盘做了保护，不让普通用户写入，这个很好解决，右键选择这个文件，属性里赋予 users  组可写权限，再保存设置就可以了，KFW 保存设置时是先删除这个文件，再写一份新的。但是奇葩得是 Windows 7  对这个写入做了魔法般的重定向： 实际是写入到  C:\\Users\\YOUR-ACCOUNT\\AppData\\Local\\VirtualStore\\Windows\\krb5.ini 下了。  KFW 的代码肯定不知道这个魔法，可见 Windows 的向后兼容是多么可怕的强大！ 从 ProcMon 的输出看，貌似  Putty/Firefox/ThunderBird 都不会去 VirtualStore/ 下找文件，稳妥起见，还是往  C:\\Windows\\  下复制一份得了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>KFW-3.2.2 只有 32 位版本，实验性的 3.2.3 倒是有 64 位版本，但 Putty  0.62、Firefox 11、ThunderBird 11 都是 32 位的，所以必须得安装 KFW-3.2.2。 Putty 和 KFW  配合良好，在  Putty 源码 putty-src\\windows\\wingss.c 中，Putty 通过注册表  HKLM\\Software\\MIT\\Kerberos\\InstallDir 得到 KFW 安装目录，默认是 C:\\Program Files  (x86)\\MIT\\Kerberos，拼接上 bin\\gssapi32.dll 并加载得到重要的几个 GSSAPI 函数入口地址。 这个地方  64bit Windows 7 又搞了个魔法， Putty 的这次注册表读取被重定向到  HKLM\\Software\\Wow6432Node\\MIT\\Kerberos 了，Putty 的代码自然对这个也是一无所知，可怕的  Windows 兼容性！</p><p class=\"ztext-empty-paragraph\"><br/></p><p>问题二： Firefox 以及 ThunderBird 均不做 HTTP Negotiate 认证，直接被 Apache 401 了。</p><p>原因： </p><ul><li>Firefox  和 ThunderBird 需要找到 gssapi32.dll， 它们的代码貌似是通过 PATH 找这个文件，没用注册表，还好  KFW-3.2.2 的 MSI 安装程序自动把 C:\\Program Files (x86)\\MIT\\Kerberos\\bin 加入 PATH  了，所以这一步是没有问题的（注意 Windows 7 自身也有 klist.exe 程序，命令行下调用时需要分清楚）。</li><li>KFW  的设置里默认 realm 没有设置成 http://CORP.EXAMPLE.COM，这会导致做 HTTP Negotiate 或者 GSSAPI  认证时非常缓慢，因为 KFW 跑去 MIT 的 kerberos 服务器上找用户了。貌似找不到后会 rollback 到  http://CORP.EXAMPLE.COM，可能是根据域名推断的。</li><li>Firefox 和 Thunderbird 的 Config  Editor 里没有把 network.auth.use-sspi 设置成 false， 只设置  network.negotiate-auth.trusted-uris 为 http://corp.example.com  是不够的。前一个选项的意思是是否使用 Windows 的 security service provider interface，由于  Windows 的 Kerberos 实现夹带了私货，不能与 MIT Kerberos 服务互操作，所以必须设置成  false，否则根本不会去调用 gssapi32.dll。 由于这个选项在 Linux 版 Firefox 和 Thunderbird  里没有，我又很少在 Windows 下测试，所以早就忘记当初在 32bit WinXP 上给 Firefox 设置过这个选项……</li></ul><p>这个问题的一些参考资料：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//hg.mozilla.org/mozilla-central/file/fd2da289a3c1/extensions/auth\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">hg.mozilla.org/mozilla-</span><span class=\"invisible\">central/file/fd2da289a3c1/extensions/auth</span><span class=\"ellipsis\"></span></a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//hg.mozilla.org/mozilla-central/annotate/fd2da289a3c1/extensions/auth/nsAuthGSSAPI.cpp\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">hg.mozilla.org/mozilla-</span><span class=\"invisible\">central/annotate/fd2da289a3c1/extensions/auth/nsAuthGSSAPI.cpp</span><span class=\"ellipsis\"></span></a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//developer.mozilla.org/En/Integrated_Authentication\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">developer.mozilla.org/E</span><span class=\"invisible\">n/Integrated_Authentication</span><span class=\"ellipsis\"></span></a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//developer.mozilla.org/en/Mozilla_Source_Code_Directory_Structure\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">developer.mozilla.org/e</span><span class=\"invisible\">n/Mozilla_Source_Code_Directory_Structure</span><span class=\"ellipsis\"></span></a></p><p>Firefox 的调试日志：</p><div class=\"highlight\"><pre><code class=\"language-text\">set NSPR_LOG_MODULES=negotiateauth:5\nset NSPR_LOG_FILE=c:\\firefox_in_domain_negotiateauth.log\n...\\firefox.exe -console\n\nset NSPR_LOG_MODULES=all:5\nset NSPR_LOG_FILE=c:\\firefox_in_domain_debug.log...\\firefox.exe -console\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>问题三： 64 bit TortoiseSVN 报告 Unable to negotiate authentication mechanism.</p><p>原因：  64bit Windows 下安装不了 32bit TortoiseSVN， 而 64bit TortoiseSVN 又需要  gssapi64.dll，所以需要去 <a href=\"https://link.zhihu.com/?target=http%3A//web.mit.edu/kerberos/dist/testing.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">web.mit.edu/kerberos/di</span><span class=\"invisible\">st/testing.html</span><span class=\"ellipsis\"></span></a> 下载  kfw-3-2-3-amd64.zip (没下载 MSI  版是因为担心干扰 32bit KFW-3.2.2)，解压缩后，把里头 amd64/  目录下所有 *64.dll 复制到 C:\\Program Files\\TortoiseSVN\\bin\\ 下， 然后就可以让  TortoiseSVN 访问 svn://http://svn.corp.example.com/test 时使用 SASL GSSAPI 认证机制了。  如果还是失败，检查 32bit KFW-3.2.2 是否已经成功获得 Kerberos ticket 并且对应 Principal 是否是默认  Principal， 以及 C:\\Windows\\krb5.ini 是否存在，gssapi*.dll 会在 dll 所在目录以及  C:\\Windows 下找寻这个文件——不过我在成功过一次后删除 C:\\Windows\\krb5.ini 伊还是可以成功认证，不解中。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这个问题的调试过程中发现一个对调试有些许帮助的技巧： <a href=\"https://link.zhihu.com/?target=http%3A//subversion.apache.org/docs/community-guide/debugging.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">subversion.apache.org/d</span><span class=\"invisible\">ocs/community-guide/debugging.html</span><span class=\"ellipsis\"></span></a></p><p> $  socat -v TCP-LISTEN:9630,reuseaddr,fork TCP4:localhost:svn ，在 svnserve  运行所在机器执行这个命令，然后客户端访问 svn://....:9630/test，socat 就会把 svnserve 和 svn  client 之间的通讯输出到屏幕上。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>问题四： 64bit TortoiseSVN 随带的 svn.exe 报告 Unable to negotiate authentication mechansim.</p><p>原因：  命令行版本的 svn client 需要知道 sasl module library 所在路径，TortoiseSVN  大概是代码里设置了，svn.exe 就没这个好运气，这个问题调试过程中，ProcMon 又帮了大忙，可以看到 svn.exe   先查注册表HKLM\\SOFTWARE\\Carnegie Mellon\\Project Cyrus\\SASL Library，然后去找  c:\\cmu\\bin，放狗一搜，立马见到答案：<a href=\"https://link.zhihu.com/?target=http%3A//svn.apache.org/repos/asf/subversion/trunk/notes/sasl.txt\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">svn.apache.org/repos/as</span><span class=\"invisible\">f/subversion/trunk/notes/sasl.txt</span><span class=\"ellipsis\"></span></a>  ，当初配置 svnserve 时就看过，可惜没注意看 Windows 相关章节。 这段文档里只有 SearchPath 对 svn  client 是需要的，在我这个场景里，就是设置成 C:\\Program Files\\TortoiseSVN\\bin。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>至此，64bit  Windows 7 下 TortoiseSVN/svn/Firefox/ThunderBird/Putty 的 GSSAPI  认证搞定，加上以前 32bit WinXP 下的试验，Windows 下使用 MIT Kerberos server 还是相当凑合的 :-)</p>", 
            "topic": [
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50793952", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 2, 
            "title": "Trac & Redmine", 
            "content": "<p>【迁移】Trac &amp; Redmine</p><p>玩了下这两个玲珑的项目管理工具，集 wiki、bts、source browser 于一身的 web 应用。</p><p>Redmine  没出来的时候，Trac 一枝独秀，颇得小众欢心，我也曾经浅用过，做管理员比较悲催，干点嘛管理操作都是得命令行里操作，不知道作者出于什么考虑，另外  Trac 官网的 wiki 比较乱，过期的信息、不推荐的做法全都混在正文里；做 Trac 的用户是挺爽的，界面清爽，wiki 里能方便的使用  ticket 号码以及 svn 版本号，集成的感觉挺好，有点贪心不足的是觉得 wiki、ticket  功能少了些，没办法，全面之后难以独专一面。比较有意思的是 Trac 不支持访问远程 Subversion 库，这个问题已经悬着八年了，见 <a href=\"https://link.zhihu.com/?target=http%3A//trac.edgewall.org/ticket/493\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">trac.edgewall.org/ticke</span><span class=\"invisible\">t/493</span><span class=\"ellipsis\"></span></a> 。</p><p>时至今日，这个小众市场上  Redmine 分了很大一杯羹，颇有好评。跟 Trac 相比，Redmine 的管理功能有 Web UI  可操作，对管理员友好了很多，直接支持多种 VCS 比如 Subversion、Mercurial、GIT、Bazaar，不像 Trac  默认只支持 Subversion。除了跟 Trac 一样支持 wiki、ticket、timeline 特性之外， Redmine 还有  forrum、calendar、gantt，后两个特性很方便看项目进展情况，看起来挺实用的。要说缺点的话，Redmine  官方的文档、论坛信息也比较过时，插件比 Trac 少很多，冷启动很慢（重启动 Apache 后第一次访问，或者闲置一段时间后再访问，似乎  FastCGI 模式下 ruby 进程会过一段时间自动退出），得花个十来秒。</p><p>给这俩配置 Kerberos 认证真是天上地下，Trac 嘛都不用管，直接支持，Redmine 就郁闷大发了，由于我是 Perl fan，不懂 Ruby 也提不起兴趣学，网上搜索了下，让 Redmine 支持 HTTP Auth 的办法倒是颇有几个：</p><ul><li><a href=\"https://link.zhihu.com/?target=http%3A//www.redmine.org/issues/1131\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">redmine.org/issues/1131</span><span class=\"invisible\"></span></a> 这个办法是三年前的，经测试，不好用</li><li><a href=\"https://link.zhihu.com/?target=http%3A//www.redmine.org/projects/redmine/wiki/Alternativecustom_authentication_HowTo\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">redmine.org/projects/re</span><span class=\"invisible\">dmine/wiki/Alternativecustom_authentication_HowTo</span><span class=\"ellipsis\"></span></a>  这个算是最靠谱的文档了，也有快两年历史了，有些地方也是需要修改的，而且这个文档说的不详细（或者我没看明白），伊没说明是不是写成 Redmine  plugin，害我钻研了下怎么写 Redmine 插件。这个文档里那段直接往 Redmine 数据库里 auth_sources  表格里插记录的方式很恶搞，丫就不能整个脚本或者 Web UI 啥的封装这个内部细节么？</li><li><a href=\"https://link.zhihu.com/?target=https%3A//github.com/AdamLantos/redmine_http_auth\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/AdamLantos/r</span><span class=\"invisible\">edmine_http_auth</span><span class=\"ellipsis\"></span></a>  看起来这个正是我要的，可惜试验过后发现不好用，毕竟也有两年没更新了，后来大致扫了下实现，伊是玩了个技巧，实际也是利用上面第一个办法的思路，只是利用 Ruby 动态语言特性覆写了一些  Redmine 的代码。</li><li><a href=\"https://link.zhihu.com/?target=https%3A//github.com/edavis10/redmine_sso_client\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/edavis10/red</span><span class=\"invisible\">mine_sso_client</span><span class=\"ellipsis\"></span></a> 没试验这玩意，伊还整了个 sso server，鬼才用呢。。。</li></ul><p>没办法，只好自己摸索着鼓捣，Ruby  的语法糖真多，看得我这个 Perl 中高段选手都一愣一愣的，好不容易摸出门道，结合上面第一个办法和第二个办法，让 Redmine 支持  HTTP auth 了，本来想给 Redmine 官方提个补丁，结果发现伊连邮件列表都没有，我懒得在它网站注册帐号，你看 Trac  的开发者就没那么二，人家就有邮件列表——你以为你 Redmine 搞个 forum 用户就纷纷来注册啊。。。</p>", 
            "topic": [
                {
                    "tag": "Redmine", 
                    "tagLink": "https://api.zhihu.com/topics/19560850"
                }, 
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50793941", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 0, 
            "title": "svnserve 配置", 
            "content": "<p>【迁移】svnserve 配置</p><p>题外话：Debian 上 Dovecot 终于升级到 2.0.18 了，CalenderServer 也升级了一把，解决了 Python 2.7 下无法启动的问题，Samba 4 居然也打包了，虽然安装时报一大堆错。。。</p><p>作为  GIT 派，本来是不想配置 Subversion 的，不过对 TortoiseSVN 还真是印象良好，加上考虑到大众需求，未必所有人都喜欢  GIT 这样的口味，所以还是配置下 svnserve 玩玩。svn 最近的发展也挺有意思，.svn 目录只放在顶级工作目录里，fsfs  库的文件存储方式增加了 shard 特性，避免一个目录下有太多文件，还增加了 svnadmin pack 命令，颇有像 GIT  看齐的意思，不得不再赞一下 Linus 的高瞻远瞩。</p><p>Subversion 的服务端有三种运行方式：<a href=\"https://link.zhihu.com/?target=http%3A//svnbook.red-bean.com/en/1.7/svn.serverconfig.choosing.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">svnbook.red-bean.com/en</span><span class=\"invisible\">/1.7/svn.serverconfig.choosing.html</span><span class=\"ellipsis\"></span></a></p><ul><li>Apache + mod_dav_svn，亮点是可以利用 Apache 的认证机制、详细的访问日志、单写多读主从代理机制以及简单的代码库浏览 Web 界面。<br/></li><li>svnserve daemon 或者  inetd</li><li>svnserve + ssh</li></ul><p>之前用  subversion 时一直用第一种方式，但在我这个 SSO 方案里，它有一些问题：代码库被 www-data 用户所有，同一个 Apache  上服务的其它 web 应用如果有漏洞，那么代码库也有暴露风险；mod_kerb 得到的用户名带有 @REALM 后缀，我估计很可能  mod_dav_svn 得到的也是这种用户名，这样写 authz 文件时比较罗嗦。</p><p>第三种方式下 svnserve  所在机器需要定制 login shell，避免用户可以登录 svnserve 所在服务器获得 shell 访问，但即使这么做，由于  svnserve 处于 tunnel 模式，以 ssh 登录用户身份运行，所以要特别注意 svn repository  的文件权限，一般需要把所有用户放入 git 组，并使用 svnwrap 来统一 umask；如果仿造 gitolite 的做法，用同一个 svn  账户，用户用 ssh 公钥认证提交，那又用不了 kerberos 认证，而且很可能 svn log 里的 author 全是 svn 账户了。</p><p>由于  svnserve 支持 SASL 认证，所以还是第二种方式最完美，不需要用户可以登录 svnserve 所在机器，可以用 Cyrus SASL  支持的 GSSAPI 认证，svnserve 的 --log-file 选项可以记录 svn 访问日志，美中不足的是 svnserve  的错误日志完全没有，比如登录失败了 svnserve 屁都不放一个，还好 Cyrus SASL 有调试日志。另一个问题是 svnserve  不支持接收到 SIGHUP 时重新打开日志文件，所以做日志 rotation 时需要重启 svnserve 服务。</p><p>Debian  打包的 Subversion 没有提供 svnserve 的 /etc/init.d/svnserve 文件，我折腾了下，仿照  /etc/init.d/skeleton 写了一个，并创建了 svn 用户以及 svn 组，让 svnserve 以 svn  用户身份运行。svnserve 的 SASL 认证配置文件可以放在 /etc/sasl2/svn.conf 里。一番折腾后，svnserve 的  GSSAPI 认证搞定，Linux 下 svn 命令行以及 Windows 下 TortoiseSVN 都可以。需要注意的一点是  Kerberos for Windows 里如果当前 principal 不是 default principal，那么  TortoiseSVN、Putty 在做 GSSAPI 认证时都会触发 Kerberos for Windows 弹出密码输入窗口。</p>", 
            "topic": [
                {
                    "tag": "SVN(Subversion)", 
                    "tagLink": "https://api.zhihu.com/topics/19607465"
                }, 
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50793909", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 0, 
            "title": "企业内部即时通讯服务", 
            "content": "<p>【迁移】企业内部即时通讯服务</p><p>公司内部同事之间基于网络的通讯方式，最常用的是邮件和即时通讯工具，比如 MSN、QQ、GTalk、Yahoo!  Messenger，这些都是面向因特网用户的，面向企业内部的即时通讯工具我用过  RTX，相当好用，不过貌似是商业收费的（不清楚公司购买了没，哈哈）。搭建企业内部即时通讯服务，跟搭建企业内部邮件系统一样，主要是为了公司机密不被泄漏（不知道奇虎  360 内部是否禁止使用 QQ，再次哈哈！），其次是内部维护更方便监管以及历史回溯，比如记录聊天日志。顺带说一嘴，GTalk  的聊天记录默认会放邮箱里，有点恐怖，一是肯定会被 Google 分析用户行为，二是一旦帐号泄漏，那就尴尬大发了。</p><p>公司内部搭建即时通讯服务只有两个选择，IRC  和 Jabber(XMPP)，前者貌似还不是现代意义的即时通讯，不支持用户之间脱离聊天室直接聊。Jabber  则特意打造为开放的即时通讯协议，GTalk 一开始就采用了 Jabber 协议，MSN 据说新版本也要皈依了。开源的 Jabberd  服务器有很多，但现在活的好的凤毛麟角，ejabberd 采用 Erlang 语言编写，对多 Jabberd  服务器组成集群支持的很好（Erlang 语言运行时天生就内置集群支持），估计是目前用的最广的 Jabberd 服务程序了，Debian  包含了它的安装包，所以在 Debian 上安装很容易，但可惜的是 ejabberd 一开始不支持 GSSAPI 认证，幸好 Mikma  同学发飙给它加入这个重要的企业级特性，在 <a href=\"https://link.zhihu.com/?target=http%3A//www.ejabberd.im/cyrsasl_gssapi\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">ejabberd.im/cyrsasl_gss</span><span class=\"invisible\">api</span><span class=\"ellipsis\"></span></a> 上提供了针对  ejabberd 多个版本的补丁，ejabberd 维护者之一 Badlop 同学把这个特性收入了 ejabberd 3.0  分支中，期待其今年早日发布并收入 Debian 软件源中，等不及的同学可以按照这个步骤尝试下：</p><ul><li>获取 Debian ejabberd 的源码包，或者 git clone 其代码库：git://git.deb.at/pkg/ejabberd.git</li><li>apt-get build-dep ejabberd 安装编译 ejabberd 所需要的软件包</li><li>aptitude install erlang-nox 安装 erlang 运行环境</li><li>安装 <a href=\"https://link.zhihu.com/?target=https%3A//support.process-one.net/doc/display/EXMPP/exmpp%252Bhome\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">exmpp</a> 和 <a href=\"https://link.zhihu.com/?target=https%3A//github.com/mikma/esasl\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">esasl</a>，注意  esasl 默认 make install 后会装到 /usr/local/lib/erlang 下，需要把它复制到  /usr/lib/erlang 的对应目录里，因为 ejabberd 不会去找 /usr/local/lib/erlang 下的 Erlang  模块；<br/></li><li>删除掉除 debian/ 目录外的所有文件，将 ejabberd-3.0 代码从其官方 git 库抓下来并覆盖过去</li><li>cd src/;  aclocal; autoconf; cd ..</li><li>删除 debian/patches/*，我懒得把这些补丁移植到 3.0 上了，有兴趣的同学可以折腾下；<br/></li><li>dpkg-buildpackage -rfakeroot -b -us -uc</li></ul><p>ejabberd  的配置挺简单的，读一遍官方 Installation and Operation Guide 就行了，Pidgin 对 GSSAPI  认证支持很好，按着 <a href=\"https://link.zhihu.com/?target=http%3A//www.ejabberd.im/cyrsasl_gssapi\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">ejabberd.im/cyrsasl_gss</span><span class=\"invisible\">api</span><span class=\"ellipsis\"></span></a> 的说明配置下来（其实我只配置了  KRB5_KTNAME 以及 sasl_realm 就行了），Pidgin GSSAPI 认证一次通过。必需提一下的是 ejabberd 的  mod_muc_log 插件可以把聊天室的聊天日志保存到按日期命名的 HTML 文件里，颇有 IRC logger 的感觉。</p><p>ejabberd  的其它功能如 pubsub 还不知道怎么用，与 MSN、ICQ、Yahoo! Messenger  的互通我也没折腾，因为我本来就想让这个服务局限在公司内部。比较滑稽的一件事情是 ejabberd 自带了个 web admin  界面，真是程序员的设计水平，简直就好比开了个 GUI 编辑器让你编辑配置文件，虚张了 web admin 的门面。。。。另外在 Pidgin  里创建聊天室时，我没搞定怎么让 logging 默认打开，在 ejabberd.cfg 里指定了没用，而且创建完后，Pidgin 没有 UI  可以重新配置聊天室，不知道是 Jabber 协议不支持还是 Pidgin 不支持。</p><p>另一个现在活的不错的开源 Jabber  服务器是使用 Java 编写的 Openfire，它背后的公司还做了个 Jabber 客户端 Spark 以及 Jabber 协议库  Smack，三个我都粗略的用过，相当的易用，Openfire 最称道的是其 web admin 界面——这才能叫“Web 管理界面”啊！  但图形配置界面做的好了，文本配置文件却没什么文档解释，不方便自动化配置。而且 Openfire 的集群支持是后加的，据说没 ejabberd  那么 scalable and stable，另外我也没发现它怎么把聊天室日志记录到 HTML 文件里，这个特性很重要啊。Openfire  官方提供了 deb 包，但是打包的太粗糙了，/etc/init.d/openfire 还是 2001 年的版本，里头判断 java  可执行程序路径的逻辑过时了，需要改改，日志文件伊输出到 /usr/share/openfire/ 里，真是让人无语。</p><p>托 Java 的福气，Openfire 的 GSSAPI 认证支持很好，<a href=\"https://link.zhihu.com/?target=http%3A//community.igniterealtime.org/docs/DOC-1060\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">community.igniterealtime.org</span><span class=\"invisible\">/docs/DOC-1060</span><span class=\"ellipsis\"></span></a> 说明的很详细，我试验了下，也是一次通过。</p><p>比起商业的企业内部即时通讯工具如  RTX，ejabberd 是太阳春了，不过要求不高的话也很凑合用了，再加上是开源产品，折腾下还是很方便的（不懂 Erlang  的兄弟可以折腾下用 Java 写的 Openfire。。。)，开放的 Jabber 协议让写点 bot 啥的完全是小菜一碟。</p>", 
            "topic": [
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": [
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "MSN啊、GTalk啊、Yahoo! Messenger啊，早都关门了，这是前年写的稿子吗？", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "知乎用户", 
                            "userLink": "https://www.zhihu.com/people/0", 
                            "content": "大约六年前写的，所以标题有迁移二字😄", 
                            "likes": 0, 
                            "replyToAuthor": "知乎用户"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50771850", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 0, 
            "title": "openldap, libnss-ldapd (续)", 
            "content": "<p>【迁移】openldap, libnss-ldapd (续) </p><p>libnss-ldapd -&gt; nslcd -&gt; slapd 的认证和授权过程真是比较绕，在 nss-pam-ldapd  邮件列表问了下，几天了都没有回应。。。大致看了下 nss-pam-ldapd 的源码，发现 rebind 只发生在发生 referral  的时候，也就是说第一个 slapd 服务器的某个目录是委托另外一个 slapd 服务的。libnss-ldapd -&gt; nslcd  -&gt; slapd 的认证和授权过程猜测如下：</p><ol><li>用户进程比如 getent passwd 调用 glibc 库  nss 函数，最终通过 libnss-ldapd 库函数请求本地服务 nslcd，这个通讯是用的 UNIX domain  socket，nslcd 通过这个 socket 可以知道用户进程的 pid、uid、gid 信息。</li><li>/etc/nslcd.conf  里配置了 sasl_mech GSSAPI 认证方式，所以 nslcd 和 slapd 之间利用 Kerberos 做认证，nslcd  的身份由 /etc/nslcd.conf 配置的 krb5_ccname 获得 authentication id (authcid)，默认是  host@`hostname -f`@REALM。SASL 认证过程还会查找 nslcd 的 authorization id  (authzid)，可以在 /etc/nslcd.conf 里配置。</li><li>slapd 允许 host@`hostname  -f`@REALM 做 sasl bind 成功，nslcd 端的 authzid 要起作用，需要在 slapd 一端配置 DN  &#34;uid=host/gold.corp.example.com,cn=gssapi,cn=auth&#34; 的 authzFrom 和 authzTo  属性，默认情况下是没有配置的，所以 host@HOSTNAME 默认配置下相当于 LDAP 普通用户，对目录项有读权限。<br/></li></ol><p class=\"ztext-empty-paragraph\"><br/></p><p>在这个过程中，libnss-ldapd  并没有用用户进程的 krb ticket 向 nslcd 和 slapd 做身份认证，对于服务器来说，可以让 nslcd 使用  host@HOSTNAME krb5 principal，nslcd 没有 admin principal 的话是没法修改 slapd  里的数据的，只是 slapd 没法细分用户权限了(nslcd 的 authzid  只能是固定的，不能根据用户身份变动)，对于普通用户机器来说，如果管理员没有给配置 host  principal(一般也不大可能让管理员配置这个，麻烦，而且不归管理员负责），那么普通用户得配置 nslcd 使用自己的 user  ticket，这也是比较麻烦的。比较方便的是让 slapd 支持匿名  bind，也就是匿名认证，但是我又不喜欢网络上的访客能随随便便的获取账户列表。安全与方便，两难啊。</p><p>由于 nslcd 使用的  krb5 principal 没有特权，所以它没法修改 slapd 上的 userPassword，不过因为我原本就打算全用 krb5  认证，压根就不让 slapd 保存用户密码摘要，所以也没问题。另一个问题是 nslcd 没有权限修改用户 shell、gecos  等信息，但这些一般都不会修改，不是个大问题，而且其实 chsh 命令貌似不认 ldap 或者是 nss-ldapd 不支持  chsh，这个命令压根就不去找 ldap，如果你 chsh 改一个 ldap 里存储的账户，chsh 会报告说在 /etc/passwd  里找不到这个用户，这个问题也在 nss-pam-ldapd 邮件列表里问了，但尚无回信。</p><p>libnss-ldapd  在鉴别用户上有上述缺陷，其前辈  libnss-ldap 也是一样的，配置文件里也是写死了 authcid 和 authzid，我原以为  libnss-ldap 没有独立的 nslcd 服务， 用户进程直接跟 slapd 交互，就可以在跟 slapd 认证时直接使用 user  krb5 ticket，实际并非如此，可能这两个东西在设计之初并没有考虑到这个问题，或者我想复杂或想歪了。</p><p>关于 pam、nss 与  slapd 的集成，除了 nss-pam-ldap 和 nss-pam-ldapd 外，还有一些其它方案。RedHat 做了一个  sssd，前几年宣传的比较火，现在似乎淡定了，开发还在继续，但不知道为什么 Debian  社区开发者没有收录，似乎其它发行版以及广大用户也没怎么采纳，估计也还是有不少问题。</p><p>sssd  要解决的一个重大问题是离线时账户处理问题，用过 Windows 的域账户（MS 的 domain controller 其实就是  Kerberos KDC，domain 就是 realm，account 就是  principal）的用户就会知道，在连不上域控制器时，用户依然可以以域账户身份登录系统，这是因为 Windows 缓存了域账户信息。在  nss-pam-ldap/nss-pam-ldapd/pam-krb5 中，用户登录凭证(credential，典型的是密码)可以用  pam-ccreds 缓存，用户账户信息(home dir, login shell, uid, gid, groups)  却没有缓存方案，nscd 可以修改配置长时间缓存数据，但这会导致它长时间不联系 ldap 服务器更新缓存，这个问题可以修正（比如 nscd  尝试连接 ldap 成功就更新本地缓存），但貌似 nscd 维护者没兴趣这么做，毕竟术业有专攻，伊设计之初就没考虑离线应用。哪怕是  pam-ccreds，如果用户信息是存放在 ldap 里的，slapd 连接不上时，login 找不到用户，依然不会允许登录，所以  nss-pam-ldap/nss-pam-ldapd + kerberos + slapd 的单点登录方案还是不甚理想，十分依赖  kerberos + slapd  都可以连接并且工作正常(slapd 的 gssapi 认证需要 kerberos 服务)。</p><p>单点故障、离线应用，痛楚啊。牺牲一点及时性，还是有绕过的办法的，那就是  nsscache 和 nss-updatedb，前者不依赖 nss-ldap 和 nss-ldapd，要配置 ldap 连接、认证等，后者依赖  nss-ldapd/nss-ldapd 获取信息，只有当 ldap 无法连接上时才会用到缓存信息，所以 nss-updatedb  及时性更好。两种方案都需要 cron 任务定期更新缓存。</p><p>pam, nss 这块真龌龊。。。。</p>", 
            "topic": [
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50771776", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 0, 
            "title": "openldap, libnss-ldapd", 
            "content": "<p>【迁移】openldap, libnss-ldapd </p><p>OpenLDAP 这个话题恐怕得写两篇，我还没搞定 libnss-ldapd  的配置。以前一直没实际玩过目录服务，初上手感觉非常别扭，术语完全不知所云，OpenLDAP 手册看了两遍还不是特别有谱。也许将来会发现折腾  OpenLDAP 是个错误，因为越来越多的服务都支持 SQL 型数据库存储用户信息。但无论如何，Kerberos + LDAP  的组合是个经典传奇，值得了解，不然 MS 也不会费力吧唧的搞个 Kerberos + LDAP 的组合拳 Active Directory 了。</p><p>这里先以我粗浅的理解说一下目录服务是怎么回事。目录服务真的是像一个目录树，只是没有普通文件(当然你也可以认为空的目录节点是普通文件节点），每个目录有一个或者多个类型(objectClass)，表明这个目录本身可以有什么样的元信息，元信息是以  key-value 对的形式保存的。如同 Linux 下文件系统的加载点(mount  point)，目录服务里的目录也是可以绑定到不同存储后端的，比如 cn=config 保存到 config 后端（在  /etc/ldap/slapd.d 下的众多 LDIF 文件里），dc=corp,dc=example,dc=com 存放在  /var/lib/ldap 下的  BerkeleyDB 数据库里。这里容易让人迷糊的是 dc= 这样的东西，在 LDAP 中，“路径”是以  DN 来指代的，Distinguished Name，dc 是 domain component，还有个 cn 是 common name，ou  是 orgnization unit，一个 DN 包含多个 以逗号分割的  components，就好像文件路径包含多个斜杠分割的目录、文件名。我觉得很不明白的是为啥要写 dc=, cn=, ou=  这样的东西，把这些放入 key-value 元信息不更简单么？</p><p>LDAP 相比 SQL 数据库，个人觉得好处如下：</p><ul><li>LDAP 为读优先设计，读取效率高；（我没实测过）</li><li>LDAP 的树状结构组织层次信息很方便，SQL 数据库则费力的多；</li><li>LDAP 的权限控制非常灵活；</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>LDAP 的实现有两个，OpenLDAP 和 389 Directory Service (<a href=\"https://link.zhihu.com/?target=http%3A//directory.fedoraproject.org/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">directory.fedoraproject.org</span><span class=\"invisible\">/</span><span class=\"ellipsis\"></span></a>，389 是默认 LDAP 端口号），后者刚从水木上听说，貌似名气很小，留作以后研究了。</p><p>OpenLDAP  服务器在 Debian 上的包名叫 slapd，为了支持 GSSAPI 认证，还需要 libsasl2-modules-gssapi-mit  这个包，SASL 是一个实现了很多认证方式的函数库，其中一种认证方式是 GSSAPI，其实也就是 Kerberos v5  了。安装时会询问服务器域名，填入 http://corp.example.com，这个会转换成 /var/lib/ldap 里数据库对应的目录树  dc=corp,dc=example,dc=com。另外一个预装的目录树是 cn=config，这个 slapd 的配置目录树，它是对老版本  slapd.conf 配置方式的改进，从而支持运行时修改 slapd 配置。</p><p>麻烦的地方在于 GSSAPI 认证，需要阅读 SASL 的系统管理员手册，设置 /etc/ldap/sasl2/slapd.conf。这里有个小插曲，开始试验 slapd 的 Kerberos 认证的时候，它老是告诉我一个莫名的错误信息：<br/>    $ ldapwhoami<br/>    SASL/GSSAPI authentication started<br/>    ldap_sasl_interactive_bind_s: Other (e.g., implementation specific) error (80)<br/>             additional info: SASL(-1): generic failure: GSSAPI Error: Unspecified  GSS failure.  Minor code may provide more information (Unknown error)</p><p>后来发现是  slapd 调用 gssapi-mit sasl module 默认读取 /etc/krb5.keytab，因为 slapd 不是以 root  身份运行，它读不了这个文件，所以 slapd 就可耻的失败了，伊的错误信息就像是入门 C 程序员打印的日志，很 magic，而 SASL  也没报告啥明显的错误信息，由此可见一斑这俩的代码质量，咳咳。。。。于是需要设置下，让 SASL 读指定的 keytab 文件，SASL  的系统管理员手册中说 keytab 选项目前是不支持的，我查了下，其实是支持的，不过是 libsasl2-modules-gssapi-mit  这个包的编译脚本有误，具体见 <a href=\"https://link.zhihu.com/?target=http%3A//bugs.debian.org/cgi-bin/bugreport.cgi%3Fbug%3D651308\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">bugs.debian.org/cgi-bin</span><span class=\"invisible\">/bugreport.cgi?bug=651308</span><span class=\"ellipsis\"></span></a> 。还好这个很容易绕过去，修改 /etc/default/slapd 设置  KRB5_KTNAME 环境变量即可，可惜这是个 Debian 打包的文件，我很不情愿修改它，因为在软件包升级时要留意打包者对它的修改。</p><p>我是个偏执狂，为了安全起见，我就去修改  cn=config 目录内容，试图达到最大的安全性，因为很多其它服务要依赖 LDAP 服务，我不希望 LDAP 服务 IP 被抢啥的出现假冒  LDAP 服务，或者 LDAP 网络传输信息被窃听，结果发现 slapd 的安全相关选项的文档严重不足，我基本是狂琢磨文档狂对半测试才找出让  slapd 不报错的安全设置。这里我故意没用 TLS 认证，因为企业自身没有 CA 资格，搞个自签名证书客户端部署又麻烦，而且要让 TLS  支持客户端验证的话，客户端也要部署自己的证书，反正已经有 Kerberos 了，再折腾 TLS、PKI 就太蛋疼了。在配置过程中发现 SASL  无法识别 GSSAPI 的加密安全强度，给死了一个 ssf 参数 56（认为 GSSAPI 的加密强度是 DES，而其实 Kerberos  现在优先用 AES 了），SASL 开发者给出的理由是 GSSAPI 还是 Kerberos API  没提供获取加密算法的函数，从而无法评估加密强度。这个问题颇有年头了，从这也可以看出 Kerberos/GSSAPI/SASL  这个生态圈不是那么健康。</p><p>这个安全配置的结果是只允许 root 用 -Y EXTERNAL -H ldapi:/// 或者  Kerberos client 用 -Y GSSAPI -H ldap:/// (注意前者是 ldapi 后者是 ldap)，并且禁止 SASL  认证算法协商（必须显式设置 /etc/ldap/ldap.conf 的 SASL_MECH 选项，要么是在命令行用 -Y  GSSAPI），原因是这个协商过程貌似是明文的，我没搞明白怎么让 slapd 的 ssf  设置允许这个通信数据开始是低安全级别然后是高安全级别。所幸我只希望它用 GSSAPI，也就无所谓了。</p><p>接下来是配置 nss  让它支持从 LDAP 获取 /etc/{passwd,group,shadow} 信息了，实际上 LDAP  也支持用来认证的，nss-pam-ldap 和 nss-pam-ldapd 是两个实现，后者是前者的改进版本，两个都同时有 pam 和 nss  模块。pam 模块支持用 LDAP 里的 shadow 密码验证用户，因为前面已经配置了 libpam-krb5，就没必要再用  pam-ldap/pam-ldapd 了，图增多处存储密码的麻烦以及密码泄漏的风险。顺带说一句，MIT Kerberos  服务默认的账户存储后端是 BerkeleyDB，它还支持 LDAP 后端，我本来想用 LDAP  后端，但这里有个蛋生鸡，鸡生蛋的问题：Kerberos 如何知道 LDAP 服务是真的呢？要达到这个效果，先要用 BerkeleyDB  后端，配置好 LDAP 的 kerberos principal，导出 keytab 文件，然后换用 LDAP   后端。这么搞有点麻烦，而且不记得那里看到的，Kerberos 开发人员不推荐这样的用法，因为增加了 Kerberos  账户数据库被攻陷的风险，而且因为依赖 LDAP 服务，Kerberos 系统发生故障的可能性更大了。</p><p>从 nss-pam-ldapd 主页上看，(<a href=\"https://link.zhihu.com/?target=http%3A//arthurdejong.org/nss-pam-ldapd/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">arthurdejong.org/nss-pa</span><span class=\"invisible\">m-ldapd/</span><span class=\"ellipsis\"></span></a>)  上看，这个实现比 nss-pam-ldap 要好。装上之后，发现这东西有点把问题搞复杂了，原来是用户登录，login 调用  nss-ldap模块（同一进程），再访问 slapd，现在是 login 调用 nss-ldapd模块（同一进程），委托本地的 nslcd  (nss-pam-ldapd 自带）去访问 slapd，这就对 slapd 做身份鉴别造成麻烦了，首先，nslcd 需要配置 kerberos  principal，默认是 host@`hostname -f`，以让其跟 slapd 互相认证，然后可能是 nslcd 告诉 slapd  说我是 userA，让 slapd 以 userA 身份进行授权。我还没搞明白 slapd 怎么知道 nslcd 声称 userA  是否合法，网上有说法是 nslcd 跟 slapd 认证成功后，会再以 userA 身份向 slapd  认证一次，如果是这样的话倒没有问题。目前还在摸索中，后文待续。</p><p>BTW，安装 libnss-ldapd 会修改  /etc/nsswitch.conf 文件，这里面 passwd/group/shadow  必须选中，因为他们是登录必需的，其它可以琢情选用，比如 ethers 存放 MAC 地址，可能能让 dhcp server 的 ldap  后端使用，netgroup 据说可以设置组权限，aliases 存放邮件地址别名，虽然 MTA 默认不会用它，但或许能配置 MTA 让它读取  LDAP 服务。</p>", 
            "topic": [
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50771734", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 0, 
            "title": "dhcp, dns, ntp, kerberos, ssh", 
            "content": "<p>【迁移】dhcp, dns, ntp, kerberos, ssh</p><p>一个局域网里头，机器正常开机最可能先接触的网络服务应该就是 DHCP 了，所以从它开刀是很自然的了，搭配 FAI 还可以 PXE  安装系统，不过在局域网里试验 DHCP 太容易冲突了，而且看 isc-dhcp-server 有个支持 ldap 的版本，dhcp 还有自动更新  dns 记录的功能，所以放到后面再鼓捣。</p><p>局域网里有个 DNS 服务器会很方便，不用记住 IP 了，如果设置 DNS 域名时把用户 ID 加上去，那就更方便网络状态分析了。另外 Kerberos 要求服务所在机器必需有 DNS 域名。DNS 服务器程序有很多，<a href=\"https://link.zhihu.com/?target=http%3A//en.wikipedia.org/wiki/Comparison_of_DNS_server_software\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">en.wikipedia.org/wiki/C</span><span class=\"invisible\">omparison_of_DNS_server_software</span><span class=\"ellipsis\"></span></a>  有一个很赞的表格对比，对于小企业规模自用，bind9，djbdns, powerdns, dnsmasq, unbound  都很有意思。bind 9 是市场份额最大的 DNS 服务器，占据 90% 多份额。djbdns 的安全方面口碑非常好，可惜 DJB  大神已然不维护这东西，它不支持 DNSSEC 和 IPv6。PowerDNS 支持很多存储后端，最吸引人的是 MySQL/PostgreSQL  后端支持了，<br/>但网上有大量舆论说这东西效率其实很低，我估计不消耗大量内存缓存的话，来一个请求就执行一系列 SQL  语句确实很崩溃，对于企业内部，应该还不至于域名变更频繁到需要用数据库的地步。dnsmasq 很适合局域网使用，伊内置支持 DHCP 和  TFTP，不过伊不支持 DNSSEC，不支持递归查询（实在是不想受制于 ISP 的 DNS forwarder 啊），貌似支持的 DNS  资源记录类型也很有限，当然也不支持 master、slave 模式了，个人觉得它很适合网吧、十来人的小企业。unbound 完美支持  IPv6、DNSSEC，不过伊对权威记录支持有限，不支持动态域名，我在自己的 PC 上就用 unbound，图它的 DNSSEC  支持，你懂的，防小人。。。。顺带提一句 DNSSEC，感兴趣的同学可以参考  <a href=\"https://link.zhihu.com/?target=http%3A//wiki.debian.org/DNSSEC\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">wiki.debian.org/DNSSEC</span><span class=\"invisible\"></span></a>，有点点老，unbound  现在不用手动配置 root trust anchor，但还是很有参考价值，尤其是教你如何识别你是否处在 DNSSEC  保护中，很有意思。再顺带提一句，自己本机搭个 DNS server 比如 unbound、bind9，可以屏蔽 ISP  的域名错误广告页面哦。。。</p><p>于是选择了牛刀  bind9.  《Linux Administration Handbook》的  DNS 一章非常给力，看完后对 DNS 的认识拔高了十几个台阶（原来认识太差了。。。），配置了下  bind9，其实很容易的。感兴趣的同学可以参考我放在 github 上的配置文件 shell/etc/bind/db.{corp,10}，我用的  10.x.y.z 私有 IP 地址空间，http://gold.corp.example.com 占用了 10.0.0.10，各位试验的时候如有 IP  地址差异记得改这两个文件。不过由于我这里全是单机配置，/etc/hosts 已然够用了，配置 bind9 只是为了全面点，以及为其它客户机服务。</p><p>配置好 bind9 后不要忘记修改 etc/{shorewall,shorewall6}/rules 打开 53 端口。</p><p>ntp  服务器是 kerberos 运行所需要的，因为 Kerberos 协议对时间比较敏感，默认配置下服务器和客户机两边时差超过 5  分钟，Kerberos 协议会拒绝工作。ntp  服务器程序也有好几个，ntp，openntpd，chrony，第一个是名门正统，第二个实现的特性不完全，好像不支持修正时钟频率，第三个适合桌面机以及笔记本等不是经常联网的机器，以及虚拟机这种时差漂移严重的环境，因为名门  ntp 实现的机制比较慢腾腾，笔记本等联网、运行、关机这么几个小时还不够 ntp 服务器同步完时间的（ntp  是逐步微调时间的，避免因时间跨度太大影响某些对时间特别敏感的应用）。Chrony 特意考虑了桌面用户，Anacron 也是，怪不得最新的  Federa 16 把  Chrony 设为默认的 NTP 服务器程序了：<a href=\"https://link.zhihu.com/?target=https%3A//fedoraproject.org/wiki/Features/ChronyDefaultNTP%23Current_status\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">fedoraproject.org/wiki/</span><span class=\"invisible\">Features/ChronyDefaultNTP#Current_status</span><span class=\"ellipsis\"></span></a></p><p>看过 MIT Kerberos 主页上的管理员和用户手册后，配置 Kerberos 服务器和客户端是很简单的事情，我就不罗嗦重复了，偷懒的同学可以去看看 kerberos.sh 脚本，或者这篇速成教程：<a href=\"https://link.zhihu.com/?target=http%3A//techpubs.spinlocksolutions.com/dklar/kerberos.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">techpubs.spinlocksolutions.com</span><span class=\"invisible\">/dklar/kerberos.html</span><span class=\"ellipsis\"></span></a>  。需要注意的一点是 Kerberos 服务器在寻找 service principal 时是按照客户端连接的服务端 IP  地址反向解析域名判断的，比如你用 ssh localhost 或者 ssh 127.0.0.1，那么 支持 kerberos 的 ssh  服务器会去找  host@localhost，而不是我期望的 host@gold.corp.example.com，因为找不到这个  principal 而服务端报错。</p><p>配置完 Kerberos 后，同样也不要忘记修改 Shorewall  配置文件开放端口，Kerberos 这厮开的端口比较多。客户机极为容易配置，把 libpam-krb5 和 krb5-user  装上就搞定，最好也把 libpam-ccreds 装上，这个模块会缓存认证信息，万一 kerberos 服务挂了，我们还是有机会登录成功。</p><p>接下来配置  ssh server，否则要是在真机上，我就得一直趴机房里操作了。自然，ssh server 实现也颇有一些，dropbear  合适嵌入式系统，lsh-server 是 GNU 出品，据说维护不是很活跃，剩下的自然是如日中天的 openssh-server  了，可惜这玩意最近几年也频繁报告安全漏洞，<a href=\"https://link.zhihu.com/?target=http%3A//solidot.org\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">solidot.org</span><span class=\"invisible\"></span></a> 都有新闻说 cracker 攻破系统后第一件事就是升级 openssh。。。。</p><p>Debian  Wheezy 里的 openssh 装完后需要配置才能开启 Kerberos 支持。/etc/ssh/sshd_config 里，把  PasswordAuthentication 和 ChallengeResponseAuthentication 都设置成  no，因为我们不直接用 pam_unix 的密码认证，也不用 S/KEY、OPIE  之类的一次一密认证方案，开着它们有风险。KerberosAuthentication 也要是 no，这个选项的意思是 ssh client  把密码传给 ssh server，ssh server 作为 kerberos client 向 kerberos server  认证，这个密码传输显然太不靠谱。GSSAPIAuthentication 设置成 yes 就能支持 kerberos v5 认证了，GSSAPI  本来是想设计为通用安全接口的，不过安全界好像不怎么鸟它，搞的它成了 Kerberos v5 的代名词。<br/>最后，UsePAM 设置成 yes，因为 pam 的 account 和 session 检查还是需要的。</p><p>在  kerberos 服务端设置好 host@gold.corp.example.com 这个 service principal 并把其加入  /etc/krb5.keytab 后，就可以试试 kinit 再 ssh http://gold.corp.example.com，不要用 ssh  localhost，原因在上面说了。SSO 的第一个无密码登录服务就诞生了！</p><p>由于 kerberos ticket 过期后要重新  kinit 申请，可能会觉得有点麻烦，另外为了避免 kerberos 服务挂掉无法登录，可以用 ssh-keygen 生成一对公钥、私钥，配置好  ssh server 的公钥认证，给自己留条后路，省的出问题了就要趴机房热身。</p>", 
            "topic": [
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50771604", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 0, 
            "title": "服务配置测试环境，locale, etckeeper，shorewall", 
            "content": "<p>【迁移】服务配置测试环境，locale, etckeeper，shorewall</p><p>配置服务需要一个环境反复安装测试，用虚拟机和 chroot 都是很方便的法子，Linux 的 container  支持看起来不错，不过还不会用。Debian 打包了一个 multistrap 程序，可以用来快速建立一个 Debian chroot  环境，使用很简单，它还有个设计理念与我心颇有戚戚焉：</p><div class=\"highlight\"><pre><code class=\"language-text\">$ man multistrap\n...\nState\n       multistrap is stateless - if the directory exists, it will simply proceed as normal and\napt will try to pick up where it left off.\n</code></pre></div><p>这不就是实现了一个稳定状态么:-)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>为了再省点事，我写个几行脚本封装了下 multistrap，让它自动 mount 必需的一些文件系统如 /proc, /sys，脚本放在 <a href=\"https://link.zhihu.com/?target=https%3A//github.com/Dieken/jff/tree/master/sysadmin\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/Dieken/jff/t</span><span class=\"invisible\">ree/master/sysadmin</span><span class=\"ellipsis\"></span></a>，使用办法：</p><div class=\"highlight\"><pre><code class=\"language-text\">$ ./bootstrap-wheezy.sh /path/to/wheezy/chroot/root</code></pre></div><p>这个命令自然也是可以在一次会话里重复调用的:-)   有兴趣尝试这个来搭建环境的同学，注意检查 multistrap-wheezy.conf 里的 arch 选项是不是要改成 amd64。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>由于这个 chroot 没有建立虚拟网络接口，所以 chroot 内外的服务共用网络端口区间，后面试验服务搭建时要小心，比如不要内外都同时启动 ssh server 和 dns server。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>为了简单起见，我会把所有服务都放在一台机器上，这台机器的主机名是 gold，内部网络的域名是 http://corp.example.com。试验的时候可以用 &#34;hostname gold&#34; 临时把主机名修改成 gold. 另外检查 chroot 里的 /etc/hostname 只包含 gold，/etc/hosts 有这样一行：</p><div class=\"highlight\"><pre><code class=\"language-text\">127.0.1.1   gold.corp.example.com gold kerberos.corp.example.com kerberos krb ldap.corp.example.com ldap www.corp.example.com www corp.example.com\n</code></pre></div><p>这个设置的用意是尽量不依赖 DNS server，以免 DNS server 失败时，这台服务器上的服务解析到外部机器出现安全问题，同时也可以提高域名解析速度，毕竟所有服务都在一台机器上。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>接下来给这个测试服务器环境做些基本设置，这些设置我已经写成脚本，放在 <a href=\"https://link.zhihu.com/?target=https%3A//github.com/Dieken/jff/tree/master/sysadmin/shell\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/Dieken/jff/t</span><span class=\"invisible\">ree/master/sysadmin/shell</span><span class=\"ellipsis\"></span></a>  下，其中 run.sh 是个汇总的脚本，其它 .sh 脚本除了 lib.sh  都是可以直接执行的，当然它们之间有依赖顺序，这个顺序记录在  run.sh 中，非常粗糙，单线程的顺序执行，所幸除了 apitude update/install，它执行的还很快。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>(1) locale，我选择系统默认 locale 是 en_US.UTF-8，选择 UTF-8 没啥好说的，Linux 下的主流，选择  en_US 而非 zh_CN 是避免一些命令行工具输出中文信息，干扰自动配置脚本解析其它程序的输出。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>(2)  etckeeper，这是个将 /etc 纳入 GIT 版本控制的方便工具，装完后无需配置，有  cron 任务以及 apt hook 自动调用  etckeeper，你能很容易看到 /etc 的变迁情况，用 etckeeper vcs CMD OPTIONS 可以直接调用底层 VCS  工具。强烈推荐桌面用户也用用这个工具。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>(3) firewall，我选择了老少咸宜居家必备的 Shorewall (<a href=\"https://link.zhihu.com/?target=http%3A//shorewall.net/GettingStarted.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">shorewall.net/GettingSt</span><span class=\"invisible\">arted.html</span><span class=\"ellipsis\"></span></a>)，我在个人  PC 上也安装这个，非常容易使用，其新版本支持 Universal 配置，对于单独主机的保护，配置起来尤为方便，我也是因为这个原因选择以  Debian testing 作为基础做试验，Debian stable 中的 shorewall 包不支持这个特性。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>简单来说，shorewall  封装 iptables 提供了高级抽象，让防火墙配置更简单更不容易出错，毕竟这是专业人士打造的，我等拿来就用好了。Shorewall  分成三部分：shorewall-init, shorewall,  shorewall6，第一个提供了在网卡配置过程中的保护，后两个是真正的防火墙设置程序。安装过程很简单，复制下 Universal  样例配置文件到 /etc/shorewall, /etc/shorewall6，在 /etc/default 下修改  shorewall-init, shorewall, shorwall6 启动它们，额外的一点修改涉及日志设置，这个在  /usr/share/doc/shorewall/README.Debian.gz  中有说明。当你花个十分钟配置过一次后，以后再配置也就是秒秒中的事情，然后你就可以观摩下 sudo iptables -L  的输出了。对端口的打开关闭可以在 /etc/shorewall*/rules 中设置。</p>", 
            "topic": [
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50771542", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 0, 
            "title": "Debian、系统配置管理工具", 
            "content": "<p>【迁移】Debian、系统配置管理工具</p><p>作为 Debian 爱好者，选择一个 OS 配置服务，自然选择 Debian Linux 了。就我所知，Debian 是社区 Linux 发行版中管理最为有条理最为规范的，软件包信息页面 <a href=\"https://link.zhihu.com/?target=http%3A//packages.debian.org/sid/vim-nox\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">packages.debian.org/sid</span><span class=\"invisible\">/vim-nox</span><span class=\"ellipsis\"></span></a> 很规整，右边 Bug Reports 和 Developer Information 是我在这个页面最喜欢的链接，另外透露一个小技巧，用 dget 命令下载页面右边的 .dsc 文件链接，可以快速的下载对应的 Debian 源码包。开发者信息页面 <a href=\"https://link.zhihu.com/?target=http%3A//packages.qa.debian.org/v/vim.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">packages.qa.debian.org/</span><span class=\"invisible\">v/vim.html</span><span class=\"ellipsis\"></span></a> 列出了打包开发过程的重要信息，我就不一一列举了，各位看客可以慢慢浏览。我时常感叹一个松散的 Debian 组织，能把开发过程整理的如此规整，透明，富含信息量，比无数商业公司都强了一个数量级。</p><p>鼓吹完  Debian  后，再聊聊系统配置管理工具。为了让配置可重复，可快速重复，精准重复，第一要义是把配置过程脚本化，以此思想提升一级，把各种配置任务、资源抽象出来，形成一个代码库和驱动程序，加上用户自定义规则，编写出简练可移植的管理自动化脚本，这种思路催生了大量系统配置管理工具。这方面比较出名的有  cfengine 和 puppet。我浅浅尝试了下 cfengine 最新改写版 cfengine 3 和  puppet，感觉都不大好。cfengine3 的 bug  现在还比较多，而最郁闷的是这东西稍微一点高级功能就放在收费版本里，让我非常恼火，开源版本非常鸡肋，另外我也很担心 cfengine  的限制版编程功能不够灵活，实现复杂逻辑太费力，经常需要调用外部脚本。所谓牛刀可杀鸡，鸡刀不可屠牛，如果一定要我选择，还是给我牛刀吧。。。。puppet   的开源做的不错，文档、社区、规则库都发展的相当好，我把其手册打印出来好好拜读了一番，但在跌跌撞撞学习了之后，我也同样觉得很郁闷。这是我之前在水木社区发的一片抱怨文：<br/><a href=\"https://link.zhihu.com/?target=http%3A//www.newsmth.net/nForum/article/LinuxApp/819076\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">newsmth.net/nForum/arti</span><span class=\"invisible\">cle/LinuxApp/819076</span><span class=\"ellipsis\"></span></a></p><p>“依赖太多了，语法不断扩展已然基本是新的编程语言了”，</p><p>“puppet master 最阳春的是用 webbrick，文档说这个性能很差，推荐 <br/> 用 mongrel 甚至更好的 mod_passenger，前者需要一个 web server， <br/> 做 proxy 因为 mongrel 不支持 ssl，后者需要 apache。 <br/> <br/> puppet master 似乎包含了一个 rails 应用，所以依赖了 rails。 <br/> 操作文件内容依赖了augeas 库，puppet agent 向  puppet master <br/> 汇报时，为了提高 puppet master 响应能力，这个 report 被异步 <br/> 处理，依赖 stompserver。 <br/> <br/> storeconfigs 特性需要 puppet master 用一个数据库，阳春的是 <br/> sqlite3，高端点要用 mysql、postgresql。 <br/> <br/> 如果用预编译配置的办法，需要 memcached。 <br/> <br/> 所以你可以看到这一大堆依赖有点恐怖吧。 <br/> <br/> 性能差可能是 ruby 解释器太慢、走 http 协议的缘故。另外配置的 <br/> 生成（不是说执行）是在 puppet master 里而非 agent，似乎 cfengine <br/> 里配置的生成是在 agent 里。 <br/> <br/> 内存占用 puppet 100 MB 左右，还不算 stompserver/mysql/memcached 这些，相比 <br/> 下 cfengine 的内存占用小一个数量级。 ”</p><p>可能抱怨的太抽象，总之，用 Puppet 已经不是一个问题变成两个问题，而是一个问题变成 N 个问题，在我投入精力折腾工作所需服务之前，我就得花好多精力配置 puppet 这个服务，伊还消耗相当多内存和 CPU 资源，太蛋疼了。</p><p>一开始我尝试写个自己的配置管理工具，粗略有三个目标，第一，它就是要支持脚本语言，我愤怒提供阉割版编程功能的东西，尤其是在发现不够灵活之后加入更多传统编程语言元素的东西；第二，类似  Make  支持并发执行配置规则；第三，规则的执行是事务性的，要么成功，改动了系统，要么失败，没有改动系统，不要留着烂摊子等下次运行再清理，因为在这个过程中系统处于不稳定状态，容易出问题。</p><p>有这种重造轮子的想法绝对不只我一个人，就我所知的，鄙公司的大牛们已然造了三代广为使用的内部系统管理工具，用来维护大量的服务器，导致一个笑话就是新人入职时看到一个工具说哇，好高级啊，好复杂啊，好好啊，然后被人打击：拜托，那玩意的下一代都已经不推荐使用了！</p><p>可惜我还是太弱而且太懒，一直没努力把轮子造出来，痛定思痛，我还是回归到我所想做的事情吧：配置一套 IT 基础设施，至于配置工具，worse is better，粗快猛是我 coder 本色，先把正经东西鼓捣出来给人 show show，laf。。。</p><p>浅尝  cfengine 还是对我有深远影响的，cfengine 极为提倡 promise，我个人理解就是 rule，一个 rule  规定了一个规则让系统收敛到期望的稳定状态，如果系统已经处在那个状态了，那么 rule  就应该没有任何实际动作。这个思想有点多线程编程的可重入的意思，多次执行同一个规则应该是安全的，不会有永远的副作用。譬如一个 rule  规定了某个文件是 0644，那么一条 chmod 是不行的，因为它达不到稳定状态，文件的 ctime  总是在变，这会影响备份程序。领会这个思想后，我就觉得用什么工具都无所谓了，只要写脚本时能注意稳定状态这个目标，兼顾 coding  一贯准则“可维护性”，就能写出符合 cfengine promise 思想的可维护的定制配置系统。</p><p>此为系列开篇，为啥我用 Debian，以及糙快猛的用 shell 脚本做配置自动化。</p>", 
            "topic": [
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50771354", 
            "userName": "纤夫张", 
            "userLink": "https://www.zhihu.com/people/f70fdf87f24b7bc7238878bf456d1b4b", 
            "upvote": 0, 
            "title": "逛书店观系统管理服务配置书籍归来", 
            "content": "<p>【迁移】逛书店观系统管理服务配置书籍归来   (2011 年的文字😄）</p><p>去亚运村图书大厦跑了趟，发现有十几本这方面的书，粗略看了下，<br/>写的稍好的只有两三本，大部分书的废话占三分之一篇幅。所有的<br/>书都没有整体感，把各个服务拆开来讲，没有一本提到 kerberos，<br/>pam-ldap/nss-ldap 单点登录的。</p><p>有两本国家规划教材啥的，其中一本还在讲安装 QQ for linux，FlashGet<br/>for Linux。 有一本铁道部 2010 年 6 月出的，dovecot、cyrus-imapd<br/>章节貌似抄袭了人邮一本 2009 年出的书，文字一样，图片一样，排版稍有差别，<br/>不知道是抄袭还是作者换了个名字一稿多投。</p><p>一直对系统管理有兴趣，起因是以前工作中经常被强迫兼任系统管理员，<br/>配置个 svn、git、samba、bugzilla、wordpress 啥啥的，经常觉得<br/>磨枪上阵，一团稀泥，没有可回溯性。 前两年跟 JulyClyde@newsmth<br/>聊过一些有关系统管理的话题，伊提出规范化管理，与我心颇有戚戚焉，<br/>可惜那只懒虫忙于发春，疏于撰文，我原还期待伊能整一本系统管理经典<br/>书籍出来。。。。写书我是没耐心的，麻烦，累，赚不了几个子，太严肃，<br/>容易被人骂，修订困难，等等等等，还是时不时码个三言两语符合我<br/>Perl coder 的优良传统：懒惰:-)</p><p>闲话少说，切入正题，先聊聊我对 IT 行业小规模基础设施管理方面的想法。</p><p>(0.1) 配置可重复性</p><p>尽量自动化配置，把配置过程记录成脚本，减少 ad hoc 的东改西改。</p><p>(0.2) 对系统管理员狠一点，尽量不提供图形界面</p><p>普通用户是应该享受图形界面的便利的，而对于系统管理，往往脚本<br/>方式更便利，更容易定制，更容易接触到核心，可以强迫人学习，而且<br/>图形界面的东西往往容易给系统管理引入安全问题。 </p><p>(1) 单点登录，账户统一管理，权限设置分开</p><p>单点登录是极为重要的，每个服务一个密码的做法太不方便了。在内部系统<br/>单点登录方面，Kerberos + LDAP 是很传统的做法，Microsoft 的 Active<br/>Directory 就是个集成了Kerberos(带私有扩展) 和目录服务的集成服务。<br/>Windows, Linux, Unix, MacOS 均对 Kerberos + LDAP 有较好支持。</p><p>通过 Kerberos + LDAP，账户可以统一管理了（很多服务都支持从 LDAP<br/>读取账户信息)，但权限管理由于各个服务权限定义方式不一样，难以<br/>集中存放，所以权限设置分开在各个服务特定的存储方式里。</p><p>单点登录在 Web 上稍微复杂点，似乎 Kerberos 用于 Web 应用不是那么<br/>自如，Web SSO 有许多独有的方案，比如 CAS, SAML, OpenID，还有用于<br/>Web app 之间授权的 OAUTH。</p><p>总之，我期望的单点登录系统要达到的效果：<br/>  * 从 PC 登录后，不用再输入密码即可访问所有授权的服务<br/>  * 可以根据用户名查询其在所有服务中的分组、授权情况<br/>  * 可以注销一个账户即可注销其之前授权的所有服务</p><p>我现在所在的公司没有用 Kerberos + LDAP，不知道出于什么考虑，<br/>Linux/Unix 主机上用账户信息同步得到统一账户效果，Windows 桌面机<br/>用 Windows 域，大家用 ssh 公钥认证 得到 Linux/Unix 多主机无密码<br/>登录效果。Web SSO 方面公司做的非常好，可能跟互联网背景有关系。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>(2) 工作涉及的 IT 基础设施越少越好，特别是文档不要用多种存储途径</p><p>在前公司曾经历过文档分散的不良体验，一开始我们用 svn 管理文档，<br/>分目录，非常清晰，但这种方式编辑体验不好，后来我们尝试 wiki、<br/>drupal、wordpress，多个系统并用，然后就乱套了，虽然大家各有所好<br/>各得其所，但找个资料太麻烦了，大家阅读时需要关注的地方太杂。</p><p>关于 bug 管理和项目跟踪，也有类似的问题，bugzilla, twiki, trac 都<br/>搞过，大家各有所好，要找个东西，或者回溯历史，头绪纷杂。</p><p>另外大家很可能同时在用 bug 跟踪系统、工作流系统、请假系统，<br/>报销系统，其实这三者完全可以放在一起，最土鳖的办法是用<br/>bugzilla，我现在所在公司就用 bugzilla 做工作流系统，现在<br/>已有五百万 tickets 了（赞 bugzilla 之强大！)。</p><p>在我所在公司里，开发人员需要频繁面对的系统很精简，都是必不可少的：<br/>svn 用于源码管理, bugzilla 用于 bug 管理以及工作流跟踪,<br/>twiki 用于文档管理, hudson 用于自动构建, package repository<br/>用于存放打包结果，这种工具链，说夸张点，已经形成了统一稳定的<br/>主流开发文化。公司也有 blog 服务，但不是主流。</p><p>(3) 信息的良好分类以及级别很重要</p><p>在 Web 2.0 时代，大家很习惯用标签、搜索来标记文章，知识库往往成了<br/>平铺的网状结构，一个垃圾场。个人非常不喜欢这点，我觉得良好的树状<br/>分类如同网站导航，必须是主要的索引办法，而标签、搜索只是辅助，<br/>树状分类尤其对新人有利。</p><p>另外，信息需要有级别之分，譬如有个打分的机制，让重要的文档自动<br/>浮现出来，不要一眼望去，密密麻麻都是文档链接。文档可以很多，但是<br/>重要的、权威的文档应当很少、很正式、很容易找到。</p><p>这点公司做的很不好，twiki 是个蕴含无数宝藏的垃圾场，幸好它的<br/>搜索支持的很好。</p><p>(4) 建立内部搜索引擎</p><p>人总是懒惰和健忘的，信息总是多的数不过来的，所以一个内部搜索引擎是很有<br/>必要的，这方面 apache solr 貌似颇堪重任。</p><p>我所在公司内部有搜索引擎，可以搜索文档、人员联系方式等等，还比较好用。</p><p>(5) 软件包命名，版本号，依赖关系的规范化</p><p>在 IT 公司，往往会内部产生大量软件包，如何有效维护它们是很头疼的问题，<br/>这方面 Linux 发行版做的很好，尤其是我喜爱的 Debian，其软件包管理维护<br/>的有条不紊。另外 APR 关于版本号记法的文章非常值得参考：</p><p>    <a href=\"https://link.zhihu.com/?target=http%3A//apr.apache.org/versioning.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">apr.apache.org/versioni</span><span class=\"invisible\">ng.html</span><span class=\"ellipsis\"></span></a></p><p>这点公司做的不好，现在大家有点觉悟，但积重难返，软件包管理还<br/>是比较混乱的，经常安装、编译出问题。</p><p>(6) 开发人员手册，一个系统上手的第一手信息，对于新人非常重要</p><p>在软件开发过程中，大家往往不注意这方面信息的积累和规范化，<br/>比如编译一个产品，东一帖西一帖，也不知道是不是与时俱进。<br/>如果新员工入职，能收到一份 developer handbook，介绍公司的<br/>IT 基础设置，基本开发环境，能收到一份所在项目的 newbie kickstart，<br/>给人的感觉是无与伦比的。</p><p>这点公司做的很好，内部非常重视文档的规范化管理，有集中的<br/>文档网站，很正式，很漂亮，开篇第一个文档就是 developer handbook，<br/>我连夜看完，很有感触。</p><p>(7) 容器、虚拟化技术，方便快速建立统一的开发、测试、运行环境</p><p>软件开发人员最常扯皮的事情是“我这里好好的啊”，如果使用容器、虚拟化<br/>技术，那么大家能非常容易的建立统一环境，做事情可重复，会让开发<br/>过程更为流畅。</p><p>我所在公司有统一的 chroot 和 vm 工具，建立环境非常快速方便。</p>", 
            "topic": [
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_1049252916510142464"
}
