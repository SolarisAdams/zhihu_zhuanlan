{
    "title": "素质云笔记", 
    "description": "Research Area：计算机视觉舆情 + 知识图谱", 
    "followers": [
        "https://www.zhihu.com/people/ni-ming-4-41", 
        "https://www.zhihu.com/people/marcus-95-55", 
        "https://www.zhihu.com/people/lin-hai-80-25", 
        "https://www.zhihu.com/people/duan-xing-99", 
        "https://www.zhihu.com/people/yuwei-39-18", 
        "https://www.zhihu.com/people/cao-ji-49-42", 
        "https://www.zhihu.com/people/han-bing-20-97", 
        "https://www.zhihu.com/people/ba-bee", 
        "https://www.zhihu.com/people/yi-xian-wei-50", 
        "https://www.zhihu.com/people/xiong-sheng-87-40", 
        "https://www.zhihu.com/people/hu-run-76", 
        "https://www.zhihu.com/people/wool-14", 
        "https://www.zhihu.com/people/yi-zhi-a-mu-mu-16", 
        "https://www.zhihu.com/people/heyang-36", 
        "https://www.zhihu.com/people/xiao-xiong-bing-gan-86-92", 
        "https://www.zhihu.com/people/tracy-always", 
        "https://www.zhihu.com/people/liu-fei-94-95", 
        "https://www.zhihu.com/people/rui-zhu-95", 
        "https://www.zhihu.com/people/tony-88-69-67", 
        "https://www.zhihu.com/people/chen-yun-shan-81", 
        "https://www.zhihu.com/people/astronstar", 
        "https://www.zhihu.com/people/wang-xue-yi-44-57", 
        "https://www.zhihu.com/people/explorercc", 
        "https://www.zhihu.com/people/ning-meng-scolcos", 
        "https://www.zhihu.com/people/gu-yiyi-77", 
        "https://www.zhihu.com/people/Rxma1805", 
        "https://www.zhihu.com/people/chen-hao-yuan-64-10", 
        "https://www.zhihu.com/people/cheng-liu-xiang-28", 
        "https://www.zhihu.com/people/xi-xi-4-12-44", 
        "https://www.zhihu.com/people/li-liang-63-9", 
        "https://www.zhihu.com/people/zhang-zhang-93-49-58", 
        "https://www.zhihu.com/people/spirit-22-32", 
        "https://www.zhihu.com/people/fp567s0626-45", 
        "https://www.zhihu.com/people/pengfei-li-5", 
        "https://www.zhihu.com/people/wang-jing-bo-27-88", 
        "https://www.zhihu.com/people/ss-jiligulu", 
        "https://www.zhihu.com/people/jin-tian-52-16", 
        "https://www.zhihu.com/people/lvxiucheng", 
        "https://www.zhihu.com/people/vocan", 
        "https://www.zhihu.com/people/dong-wen-hui-90", 
        "https://www.zhihu.com/people/afakehacker", 
        "https://www.zhihu.com/people/mu-xi-luo-chen", 
        "https://www.zhihu.com/people/jinshenhehuan", 
        "https://www.zhihu.com/people/zhang-da-xian-1973", 
        "https://www.zhihu.com/people/xia-tian-39-18", 
        "https://www.zhihu.com/people/wang-hao-18-27-92", 
        "https://www.zhihu.com/people/nlp-45", 
        "https://www.zhihu.com/people/yang-jing-lin-16", 
        "https://www.zhihu.com/people/wang-tong-78-19", 
        "https://www.zhihu.com/people/oyit", 
        "https://www.zhihu.com/people/zhou-tao-tao-66", 
        "https://www.zhihu.com/people/chncwang", 
        "https://www.zhihu.com/people/upcsjt", 
        "https://www.zhihu.com/people/bie-kai-qiang-56", 
        "https://www.zhihu.com/people/chen-jun-li-90", 
        "https://www.zhihu.com/people/zhang-hong-feng-45", 
        "https://www.zhihu.com/people/lite", 
        "https://www.zhihu.com/people/wu-jia-ming-40", 
        "https://www.zhihu.com/people/chen-yong-63-19", 
        "https://www.zhihu.com/people/tang-xiao-liang-72", 
        "https://www.zhihu.com/people/stanleyhu-59", 
        "https://www.zhihu.com/people/tu-dou-45-65", 
        "https://www.zhihu.com/people/wang-xiao-qian-93-83", 
        "https://www.zhihu.com/people/pan-da-53-40", 
        "https://www.zhihu.com/people/hh-zz-77-1", 
        "https://www.zhihu.com/people/sun-xiao-fei-75-94", 
        "https://www.zhihu.com/people/zydsole", 
        "https://www.zhihu.com/people/li-jie-95-55", 
        "https://www.zhihu.com/people/cagefreedom", 
        "https://www.zhihu.com/people/spongebob-53-24", 
        "https://www.zhihu.com/people/vicfu-81", 
        "https://www.zhihu.com/people/ma-yu-xiang-4", 
        "https://www.zhihu.com/people/xiao-shun-shi-da-ye", 
        "https://www.zhihu.com/people/zhang-xiao-hui-66-53", 
        "https://www.zhihu.com/people/xiao-chong-12-1", 
        "https://www.zhihu.com/people/zhounq", 
        "https://www.zhihu.com/people/xiao-huang-ren-nao-can-fen", 
        "https://www.zhihu.com/people/hu-jinger", 
        "https://www.zhihu.com/people/xu-qiang-29-78", 
        "https://www.zhihu.com/people/guang-ming-gmg", 
        "https://www.zhihu.com/people/yan-jian-76-38", 
        "https://www.zhihu.com/people/dong-cheng-72-7", 
        "https://www.zhihu.com/people/chen-zhao-wei-16-2", 
        "https://www.zhihu.com/people/amusi1994", 
        "https://www.zhihu.com/people/ye-cha-4", 
        "https://www.zhihu.com/people/wu-ye-nan-30", 
        "https://www.zhihu.com/people/she-liang", 
        "https://www.zhihu.com/people/qifei002", 
        "https://www.zhihu.com/people/jin-se-liang-dian-ban", 
        "https://www.zhihu.com/people/wumo", 
        "https://www.zhihu.com/people/zhang-qi-hua-98", 
        "https://www.zhihu.com/people/luozhongbin", 
        "https://www.zhihu.com/people/dai-wei-66-30", 
        "https://www.zhihu.com/people/xiao-dao-min-38", 
        "https://www.zhihu.com/people/moolighty", 
        "https://www.zhihu.com/people/jancolin", 
        "https://www.zhihu.com/people/bai-se-fan-bu-xie-ju-zi-57", 
        "https://www.zhihu.com/people/ming-ming-89-83", 
        "https://www.zhihu.com/people/ge-ge-95-69", 
        "https://www.zhihu.com/people/nuo-wei-si-ji-kou-qiao-dan", 
        "https://www.zhihu.com/people/lygwangyp", 
        "https://www.zhihu.com/people/da-shen-86-94", 
        "https://www.zhihu.com/people/yin-he-bian-yuan-zi-you-xing", 
        "https://www.zhihu.com/people/mrconch", 
        "https://www.zhihu.com/people/wu-long-69", 
        "https://www.zhihu.com/people/cheer-chen-78", 
        "https://www.zhihu.com/people/dong-fang-ying-19", 
        "https://www.zhihu.com/people/hong-alex", 
        "https://www.zhihu.com/people/sun-yan-90-29", 
        "https://www.zhihu.com/people/chao-chao-93-5", 
        "https://www.zhihu.com/people/libin-sui", 
        "https://www.zhihu.com/people/cutecy-6", 
        "https://www.zhihu.com/people/xiaojay", 
        "https://www.zhihu.com/people/zhang-li-tong-30", 
        "https://www.zhihu.com/people/jeafi", 
        "https://www.zhihu.com/people/huang-shuai-4", 
        "https://www.zhihu.com/people/wangcheny91", 
        "https://www.zhihu.com/people/hijackjave", 
        "https://www.zhihu.com/people/jia-xue-feng", 
        "https://www.zhihu.com/people/lan-lan-lan-lan-96-82", 
        "https://www.zhihu.com/people/choiyeren", 
        "https://www.zhihu.com/people/zhao-xu-15-64", 
        "https://www.zhihu.com/people/wang-bo-yang-8-25", 
        "https://www.zhihu.com/people/zhhhzhang", 
        "https://www.zhihu.com/people/wei-guan-qun-69", 
        "https://www.zhihu.com/people/haohao-7-40", 
        "https://www.zhihu.com/people/lei-xiao-yu-49", 
        "https://www.zhihu.com/people/ke-zhan-shen", 
        "https://www.zhihu.com/people/jellyzhang0909", 
        "https://www.zhihu.com/people/wei-yuan-88-25", 
        "https://www.zhihu.com/people/ni-hao-26-27", 
        "https://www.zhihu.com/people/shi-san-long", 
        "https://www.zhihu.com/people/da-jun-jun-95", 
        "https://www.zhihu.com/people/fang-hui-98", 
        "https://www.zhihu.com/people/ma-liang-83-3", 
        "https://www.zhihu.com/people/xie-zhao-yang-80-76", 
        "https://www.zhihu.com/people/bu-cheng-49-54", 
        "https://www.zhihu.com/people/wu-hen-12-25", 
        "https://www.zhihu.com/people/hanyu-48-34", 
        "https://www.zhihu.com/people/guoxiaoNY", 
        "https://www.zhihu.com/people/wan-yong-tao", 
        "https://www.zhihu.com/people/hu-gui-feng-45", 
        "https://www.zhihu.com/people/zhao-hu-41-13", 
        "https://www.zhihu.com/people/ke-nan-dao-er-bo-bo", 
        "https://www.zhihu.com/people/dong-si-jia-11", 
        "https://www.zhihu.com/people/wei-ting-yang", 
        "https://www.zhihu.com/people/fonttian", 
        "https://www.zhihu.com/people/rusty-nail", 
        "https://www.zhihu.com/people/ping-an-yiyu", 
        "https://www.zhihu.com/people/da-da-18-6-40", 
        "https://www.zhihu.com/people/ni-wei-tai-yang", 
        "https://www.zhihu.com/people/yi-liang-23-12", 
        "https://www.zhihu.com/people/alpha_chan", 
        "https://www.zhihu.com/people/sukho1", 
        "https://www.zhihu.com/people/quxiaofeng", 
        "https://www.zhihu.com/people/jeremy_liu91", 
        "https://www.zhihu.com/people/feng-yuan-s", 
        "https://www.zhihu.com/people/cly525876914", 
        "https://www.zhihu.com/people/yao-jia-qi-97-76", 
        "https://www.zhihu.com/people/qi-lin-44-44"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/84470460", 
            "userName": "悟乙己", 
            "userLink": "https://www.zhihu.com/people/428693835688c688c6c9ac1d30fc0ece", 
            "upvote": 20, 
            "title": "python | 高效统计语言模型kenlm：新词发现、分词、智能纠错", 
            "content": "<p>﻿之前看到苏神</p><a href=\"https://link.zhihu.com/?target=https%3A//spaces.ac.cn/archives/6920\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">重新写了之前的新词发现算法：更快更好的新词发现 - 科学空间|Scientific Spaces</a><p>中提到了kenlm，之前也自己玩过，没在意，现在遇到一些大规模的文本问题，模块确实好用，前几天还遇到几个差点“弃疗”的坑，解决了之后，就想，不把kenlm搞明白，对不起我浪费的两天。。</p><p><b>kenlm的优点（<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/HHTNAN/article/details/84231733\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">关于kenlm工具训练统计语言模型</a>）：</b> 训练语言模型用的是传统的“统计+平滑”的方法，使用kenlm这个工具来训练。它快速，节省内存，最重要的是，允许在开源许可下使用多核处理器。 kenlm是一个C++编写的语言模型工具，具有速度快、占用内存小的特点，也提供了Python接口。</p><p>额外需要加载的库：</p><div class=\"highlight\"><pre><code class=\"language-text\">kenlm\npypinyin</code></pre></div><p>可装可不装的库：<code>pycorrector</code> 笔者的代码可见github，只是粗略整理，欢迎大家一起改:</p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/mattzheng/py-kenlm-model\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-ebdf2143fc9dbbb5be188afd830bfe73_ipico.jpg\" data-image-width=\"200\" data-image-height=\"200\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">mattzheng/py-kenlm-model</a><p class=\"ztext-empty-paragraph\"><br/></p><h2>1 kenlm安装</h2><p>在这里面编译：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kpu/kenlm\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">kpu/kenlm</a>，下载库之后编译：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">mkdir</span> <span class=\"o\">-</span><span class=\"n\">p</span> <span class=\"n\">build</span>\n<span class=\"n\">cd</span> <span class=\"n\">build</span>\n<span class=\"n\">cmake</span> <span class=\"o\">..</span>\n<span class=\"n\">make</span> <span class=\"o\">-</span><span class=\"n\">j</span> <span class=\"mi\">4</span></code></pre></div><p>一般编译完，很多有用的文件都存在<code>build/bin</code>之中，这个后面会用到： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-551d0ddb934f9e27b1ff9767460b87fe_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"526\" data-rawheight=\"169\" class=\"origin_image zh-lightbox-thumb\" width=\"526\" data-original=\"https://pic3.zhimg.com/v2-551d0ddb934f9e27b1ff9767460b87fe_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;526&#39; height=&#39;169&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"526\" data-rawheight=\"169\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"526\" data-original=\"https://pic3.zhimg.com/v2-551d0ddb934f9e27b1ff9767460b87fe_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-551d0ddb934f9e27b1ff9767460b87fe_b.jpg\"/></figure><p> python库的安装方式：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"n\">https</span><span class=\"p\">:</span><span class=\"o\">//</span><span class=\"n\">github</span><span class=\"o\">.</span><span class=\"n\">com</span><span class=\"o\">/</span><span class=\"n\">kpu</span><span class=\"o\">/</span><span class=\"n\">kenlm</span><span class=\"o\">/</span><span class=\"n\">archive</span><span class=\"o\">/</span><span class=\"n\">master</span><span class=\"o\">.</span><span class=\"nb\">zip</span></code></pre></div><p>简单使用：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">kenlm</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">kenlm</span><span class=\"o\">.</span><span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"s1\">&#39;lm/test.arpa&#39;</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">score</span><span class=\"p\">(</span><span class=\"s1\">&#39;this is a sentence .&#39;</span><span class=\"p\">,</span> <span class=\"n\">bos</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">eos</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">))</span></code></pre></div><p>坑点来了，笔者之前装在docker之中的，之前一不小心重启docker，kenlm就不灵了。。 当时并不知道该如何重新编译，就重新：<code>cmake ..</code> + <code>make -j 4</code>，但是这样出来，运行会报很多依赖没装：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">libboost_program_options</span><span class=\"o\">.</span><span class=\"n\">so</span><span class=\"o\">.</span><span class=\"mf\">1.54</span><span class=\"o\">.</span><span class=\"mi\">0</span><span class=\"p\">:</span> <span class=\"n\">cannot</span> <span class=\"nb\">open</span> <span class=\"n\">shared</span> <span class=\"nb\">object</span> <span class=\"nb\">file</span><span class=\"p\">:</span> <span class=\"n\">No</span> <span class=\"n\">such</span> <span class=\"nb\">file</span> <span class=\"ow\">or</span> <span class=\"n\">directory</span></code></pre></div><p>笔者还假了嘛嘎的去ubuntu上拉下来装了，又报其他依赖错。。</p><p>（此处省略N多次，无效尝试。。。）</p><p>如果出现：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"o\">--</span> <span class=\"n\">Could</span> <span class=\"n\">NOT</span> <span class=\"n\">find</span> <span class=\"n\">BZip2</span> <span class=\"p\">(</span><span class=\"n\">missing</span><span class=\"p\">:</span>  <span class=\"n\">BZIP2_LIBRARIES</span> <span class=\"n\">BZIP2_INCLUDE_DIR</span><span class=\"p\">)</span> \n<span class=\"o\">--</span> <span class=\"n\">Could</span> <span class=\"n\">NOT</span> <span class=\"n\">find</span> <span class=\"n\">LibLZMA</span> <span class=\"p\">(</span><span class=\"n\">missing</span><span class=\"p\">:</span>  <span class=\"n\">LIBLZMA_INCLUDE_DIR</span> <span class=\"n\">LIBLZMA_LIBRARY</span> <span class=\"n\">LIBLZMA_HAS_AUTO_DECODER</span> <span class=\"n\">LIBLZMA_HAS_EASY_ENCODER</span> <span class=\"n\">LIBLZMA_HAS_LZMA_PRESET</span><span class=\"p\">)</span></code></pre></div><p>需安装：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">sudo</span> <span class=\"n\">apt</span> <span class=\"n\">install</span> <span class=\"n\">libbz2</span><span class=\"o\">-</span><span class=\"n\">dev</span>\n<span class=\"n\">sudo</span> <span class=\"n\">apt</span> <span class=\"n\">install</span> <span class=\"n\">liblzma</span><span class=\"o\">-</span><span class=\"n\">dev</span></code></pre></div><p>之后实验发现，把<code>build</code>文件夹删了，重新来一遍<code>cmake ..</code> + <code>make -j 4</code>即可。</p><h2>2 kenlm统计语言模型使用</h2><h2>2.1 kenlm的训练 <code>lmplz</code></h2><h3>2.1.1 两种训练方式</h3><p>训练是根据<code>build/bin/lmplz</code>来进行，一般来说有两种方式：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-766378f3b1ad6d6e6461096c9be026d2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1144\" data-rawheight=\"330\" class=\"origin_image zh-lightbox-thumb\" width=\"1144\" data-original=\"https://pic3.zhimg.com/v2-766378f3b1ad6d6e6461096c9be026d2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1144&#39; height=&#39;330&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1144\" data-rawheight=\"330\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1144\" data-original=\"https://pic3.zhimg.com/v2-766378f3b1ad6d6e6461096c9be026d2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-766378f3b1ad6d6e6461096c9be026d2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>（1）管道的方式传递</p><p>数据print的方式，苏神之前的博客【<a href=\"https://link.zhihu.com/?target=https%3A//spaces.ac.cn/archives/3956%23%25E5%25AE%259E%25E8%25B7%25B5%25EF%25BC%259A%25E8%25AE%25AD%25E7%25BB%2583\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【中文分词系列】 5. 基于语言模型的无监督分词</a>】中有提到：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">python</span> <span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">py</span><span class=\"o\">|./</span><span class=\"n\">kenlm</span><span class=\"o\">/</span><span class=\"nb\">bin</span><span class=\"o\">/</span><span class=\"n\">lmplz</span> <span class=\"o\">-</span><span class=\"n\">o</span> <span class=\"mi\">4</span> <span class=\"o\">&gt;</span> <span class=\"n\">weixin</span><span class=\"o\">.</span><span class=\"n\">arpa</span></code></pre></div><p>p.py为：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">pymongo</span>\n<span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"n\">pymongo</span><span class=\"o\">.</span><span class=\"n\">MongoClient</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">weixin</span><span class=\"o\">.</span><span class=\"n\">text_articles</span>\n\n<span class=\"k\">for</span> <span class=\"n\">text</span> <span class=\"ow\">in</span> <span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">find</span><span class=\"p\">(</span><span class=\"n\">no_cursor_timeout</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">limit</span><span class=\"p\">(</span><span class=\"mi\">500000</span><span class=\"p\">):</span>\n    <span class=\"k\">print</span> <span class=\"s1\">&#39; &#39;</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">[</span><span class=\"s1\">&#39;text&#39;</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"s1\">&#39;utf-8&#39;</span><span class=\"p\">)</span></code></pre></div><p>（2）预先生成语料文本</p><p>直接命令行，数据保存</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"nb\">bin</span><span class=\"o\">/</span><span class=\"n\">lmplz</span> <span class=\"o\">-</span><span class=\"n\">o</span> <span class=\"mi\">3</span> <span class=\"o\">--</span><span class=\"n\">verbose_header</span> <span class=\"o\">--</span><span class=\"n\">text</span> <span class=\"o\">../</span><span class=\"n\">text</span><span class=\"o\">-</span><span class=\"mi\">18</span><span class=\"o\">-</span><span class=\"mo\">03</span><span class=\"o\">/</span><span class=\"n\">text_18</span><span class=\"o\">-</span><span class=\"mo\">03</span><span class=\"o\">-</span><span class=\"n\">AU</span><span class=\"o\">.</span><span class=\"n\">txt</span> <span class=\"o\">--</span><span class=\"n\">arpa</span> <span class=\"n\">MyModel</span><span class=\"o\">/</span><span class=\"n\">log</span><span class=\"o\">.</span><span class=\"n\">arpa</span></code></pre></div><p>其中参数的大致意义：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"o\">-</span><span class=\"n\">o</span> <span class=\"n\">n</span><span class=\"p\">:</span><span class=\"err\">最高采用</span><span class=\"n\">n</span><span class=\"o\">-</span><span class=\"n\">gram语法</span>\n<span class=\"o\">-</span><span class=\"n\">verbose_header</span><span class=\"p\">:</span><span class=\"err\">在生成的文件头位置加上统计信息</span>\n<span class=\"o\">--</span><span class=\"n\">text</span> <span class=\"n\">text_file</span><span class=\"p\">:</span><span class=\"err\">指定存放预料的</span><span class=\"n\">txt文件</span>\n<span class=\"o\">--</span><span class=\"n\">arpa</span><span class=\"p\">:</span><span class=\"err\">指定输出的</span><span class=\"n\">arpa文件</span>\n<span class=\"o\">-</span><span class=\"n\">S</span> <span class=\"p\">[</span> <span class=\"o\">--</span><span class=\"n\">memory</span> <span class=\"p\">]</span> <span class=\"n\">arg</span> <span class=\"p\">(</span><span class=\"o\">=</span><span class=\"mi\">80</span><span class=\"o\">%</span><span class=\"p\">)</span>  <span class=\"n\">Sorting</span> <span class=\"n\">memory内存预占用量</span>\n<span class=\"o\">--</span><span class=\"n\">skip_symbols</span> <span class=\"p\">:</span> <span class=\"n\">Treat</span> <span class=\"o\">&lt;</span><span class=\"n\">s</span><span class=\"o\">&gt;</span><span class=\"p\">,</span> <span class=\"o\">&lt;/</span><span class=\"n\">s</span><span class=\"o\">&gt;</span><span class=\"p\">,</span> <span class=\"ow\">and</span> <span class=\"o\">&lt;</span><span class=\"n\">unk</span><span class=\"o\">&gt;</span> <span class=\"k\">as</span> <span class=\"n\">whitespace</span> <span class=\"n\">instead</span> <span class=\"n\">of</span> <span class=\"n\">throwing</span> <span class=\"n\">an</span>  <span class=\"n\">exception</span></code></pre></div><p>预先语料可以不加开头、结尾符号，其中， 需要特别介绍三个特殊字符。</p><p><code>&lt;s&gt;、&lt;/s&gt;和&lt;unk&gt;</code> <code>&lt;s&gt;</code>和<code>&lt;/s&gt;</code>结对使用，模型在计算概率时对每句话都进行了处理，将该对标记加在一句话的起始和结尾。 </p><p>这样就把开头和结尾的位置信息也考虑进来。 如<code>“我 喜欢 吃 苹果” --&gt; &#34;&lt;s&gt; 我 喜欢 吃 苹果 &lt;/s&gt;&#34;</code> <code>&lt;unk&gt;</code>表示unknown的词语，对于oov的单词可以用它的值进行替换。</p><p>可参考： 不带开头结尾：</p><div class=\"highlight\"><pre><code class=\"language-text\">W h o o   后   拱 辰 享 水   水 妍 护 肤 套 装 整 套 质 地 都 比 较 清 爽 \n 滋 润 \n 侧 重 保 湿 \n 适 合 各 种 肤 质 \n 调 节 肌 肤 水 平 衡 \n 它 还 具 有 修 复 功 效 \n 提 亮 肤 色 我 是 油 性 肤 质 用 起 来 也 一 点 也 不 觉 得 油 腻 \n 味 道 淡 淡 的 还 很 好 闻 \n 也 很 好 吸 收 \n 质 地 清 爽</code></pre></div><p>带开头结尾的：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"o\">&lt;</span><span class=\"n\">s</span><span class=\"o\">&gt;</span> <span class=\"mi\">3</span> <span class=\"err\">乙</span> <span class=\"err\">方</span> <span class=\"err\">应</span> <span class=\"err\">依</span> <span class=\"err\">据</span> <span class=\"err\">有</span> <span class=\"err\">关</span> <span class=\"err\">法</span> <span class=\"err\">律</span> <span class=\"err\">规</span> <span class=\"err\">定</span> <span class=\"o\">&lt;/</span><span class=\"n\">s</span><span class=\"o\">&gt;</span>\n<span class=\"o\">&lt;</span><span class=\"n\">s</span><span class=\"o\">&gt;</span> <span class=\"err\">对</span> <span class=\"err\">甲</span> <span class=\"err\">方</span> <span class=\"err\">为</span> <span class=\"err\">订</span> <span class=\"err\">立</span> <span class=\"err\">和</span> <span class=\"err\">履</span> <span class=\"err\">行</span> <span class=\"err\">本</span> <span class=\"err\">合</span> <span class=\"err\">同</span> <span class=\"err\">向</span> <span class=\"err\">乙</span> <span class=\"err\">方</span> <span class=\"err\">提</span> <span class=\"err\">供</span> <span class=\"err\">的</span> <span class=\"err\">有</span> <span class=\"err\">关</span> <span class=\"err\">非</span> <span class=\"err\">公</span> <span class=\"err\">开</span> <span class=\"err\">信</span> <span class=\"err\">息</span> <span class=\"err\">保</span> <span class=\"err\">密</span> <span class=\"o\">&lt;/</span><span class=\"n\">s</span><span class=\"o\">&gt;</span>\n<span class=\"o\">&lt;</span><span class=\"n\">s</span><span class=\"o\">&gt;</span> <span class=\"err\">但</span> <span class=\"err\">下</span> <span class=\"err\">列</span> <span class=\"err\">情</span> <span class=\"err\">形</span> <span class=\"err\">除</span> <span class=\"err\">外</span> <span class=\"o\">&lt;/</span><span class=\"n\">s</span><span class=\"o\">&gt;</span>\n<span class=\"o\">&lt;</span><span class=\"n\">s</span><span class=\"o\">&gt;</span> <span class=\"mi\">1</span> <span class=\"err\">贷</span> <span class=\"err\">款</span> <span class=\"err\">人</span> <span class=\"err\">有</span> <span class=\"err\">权</span> <span class=\"err\">依</span> <span class=\"err\">据</span> <span class=\"err\">有</span> <span class=\"err\">关</span> <span class=\"err\">法</span> <span class=\"err\">律</span> <span class=\"err\">法</span> <span class=\"err\">规</span> <span class=\"err\">或</span> <span class=\"err\">其</span> <span class=\"err\">他</span> <span class=\"err\">规</span> <span class=\"err\">范</span> <span class=\"err\">性</span> <span class=\"err\">文</span> <span class=\"err\">件</span> <span class=\"err\">的</span> <span class=\"err\">规</span> <span class=\"err\">定</span> <span class=\"err\">或</span> <span class=\"err\">金</span> <span class=\"err\">融</span> <span class=\"err\">监</span> <span class=\"err\">管</span> <span class=\"err\">机</span> <span class=\"err\">构</span> <span class=\"err\">的</span> <span class=\"err\">要</span> <span class=\"err\">求</span> <span class=\"o\">&lt;/</span><span class=\"n\">s</span><span class=\"o\">&gt;</span></code></pre></div><p>具体的训练过程可见该博客：<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/asrgreek/article/details/81979194\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">图解N-gram语言模型的原理--以kenlm为例</a></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f9819e22209a42d25f06ef70ff84ad91_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1820\" data-rawheight=\"696\" class=\"origin_image zh-lightbox-thumb\" width=\"1820\" data-original=\"https://pic2.zhimg.com/v2-f9819e22209a42d25f06ef70ff84ad91_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1820&#39; height=&#39;696&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1820\" data-rawheight=\"696\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1820\" data-original=\"https://pic2.zhimg.com/v2-f9819e22209a42d25f06ef70ff84ad91_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f9819e22209a42d25f06ef70ff84ad91_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h3>2.1.2 生成文件arpa的解释</h3><p>来源：<a href=\"https://link.zhihu.com/?target=https%3A//www.bbsmax.com/A/WpdKmENJVQ/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">语言模型kenlm的训练及使用</a> 其中生成的arpa文件有：</p><div class=\"highlight\"><pre><code class=\"language-python\">\\<span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">grams</span><span class=\"p\">:</span>\n    <span class=\"o\">-</span><span class=\"mf\">6.5514092</span>  <span class=\"o\">&lt;</span><span class=\"n\">unk</span><span class=\"o\">&gt;</span>   <span class=\"mi\">0</span>\n    <span class=\"mi\">0</span>   <span class=\"o\">&lt;</span><span class=\"n\">s</span><span class=\"o\">&gt;</span> <span class=\"o\">-</span><span class=\"mf\">2.9842114</span>\n    <span class=\"o\">-</span><span class=\"mf\">1.8586434</span>  <span class=\"o\">&lt;/</span><span class=\"n\">s</span><span class=\"o\">&gt;</span>    <span class=\"mi\">0</span>\n    <span class=\"o\">-</span><span class=\"mf\">2.88382</span>    <span class=\"err\">!</span>   <span class=\"o\">-</span><span class=\"mf\">2.38764</span>\n    <span class=\"o\">-</span><span class=\"mf\">2.94351</span>    <span class=\"n\">world</span>   <span class=\"o\">-</span><span class=\"mf\">0.514311</span>\n    <span class=\"o\">-</span><span class=\"mf\">2.94351</span>    <span class=\"n\">hello</span>   <span class=\"o\">-</span><span class=\"mf\">0.514311</span>\n    <span class=\"o\">-</span><span class=\"mf\">6.09691</span>    <span class=\"n\">guys</span>    <span class=\"o\">-</span><span class=\"mf\">0.15553</span>\n\n    \\<span class=\"mi\">2</span><span class=\"o\">-</span><span class=\"n\">grams</span><span class=\"p\">:</span>\n    <span class=\"o\">-</span><span class=\"mf\">3.91009</span>    <span class=\"n\">world</span> <span class=\"err\">!</span> <span class=\"o\">-</span><span class=\"mf\">0.351469</span>\n    <span class=\"o\">-</span><span class=\"mf\">3.91257</span>    <span class=\"n\">hello</span> <span class=\"n\">world</span> <span class=\"o\">-</span><span class=\"mf\">0.24</span>\n    <span class=\"o\">-</span><span class=\"mf\">3.87582</span>    <span class=\"n\">hello</span> <span class=\"n\">guys</span>  <span class=\"o\">-</span><span class=\"mf\">0.0312</span>\n\n    \\<span class=\"mi\">3</span><span class=\"o\">-</span><span class=\"n\">grams</span><span class=\"p\">:</span>\n    <span class=\"o\">-</span><span class=\"mf\">0.00108858</span> <span class=\"n\">hello</span> <span class=\"n\">world</span> <span class=\"err\">!</span>\n    <span class=\"o\">-</span><span class=\"mf\">0.000271867</span>    <span class=\"p\">,</span> <span class=\"n\">hi</span> <span class=\"n\">hello</span> <span class=\"err\">!</span>\n\n    \\<span class=\"n\">end</span>\\</code></pre></div><p>介绍该文件需要引入一个新的概念，back_pro【<a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/visionfans/article/details/50131397\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">language model</a>】 </p><p>三个字段分别是：<code>Pro,word,back_pro</code> </p><p>注：arpa文件中给出的数值都是以10为底取对数后的结果</p><h3>2.1.3 几个训练坑点解读</h3><p>划重点来了，其中<code>-s</code> 非常重要，默认是<code>80%</code>，如果机器有20%被占了，笔者当时发现，10句话训练模型也能超内存，这不是瞎胡闹：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\">#34304 what():  /mnt/mNLP/kg/kenlm/util/scoped.cc:20 in void* util::{anonymous}::InspectAddr(void*, std::size_t, const char*) threw MallocException because `!addr &amp;&amp; requested&#39;.</span>\n<span class=\"c1\">#Cannot allocate memory for 84881776616 bytes in malloc</span></code></pre></div><p>需要额外设置内存占用量！当然还有挺多可能会产生意外的参数：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6950aa5626dbd0d6b7be467730ced2a2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"758\" data-rawheight=\"359\" class=\"origin_image zh-lightbox-thumb\" width=\"758\" data-original=\"https://pic3.zhimg.com/v2-6950aa5626dbd0d6b7be467730ced2a2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;758&#39; height=&#39;359&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"758\" data-rawheight=\"359\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"758\" data-original=\"https://pic3.zhimg.com/v2-6950aa5626dbd0d6b7be467730ced2a2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6950aa5626dbd0d6b7be467730ced2a2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>还有可能会报错：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">Unigram</span> <span class=\"n\">tokens</span> <span class=\"mi\">153</span> <span class=\"n\">types</span> <span class=\"mi\">116</span>\n    <span class=\"o\">===</span> <span class=\"mi\">2</span><span class=\"o\">/</span><span class=\"mi\">5</span> <span class=\"n\">Calculating</span> <span class=\"ow\">and</span> <span class=\"n\">sorting</span> <span class=\"n\">adjusted</span> <span class=\"n\">counts</span> <span class=\"o\">===</span>\n    <span class=\"n\">Chain</span> <span class=\"n\">sizes</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"mi\">1392</span> <span class=\"mi\">2</span><span class=\"p\">:</span><span class=\"mi\">10964970496</span> <span class=\"mi\">3</span><span class=\"p\">:</span><span class=\"mi\">20559319040</span> <span class=\"mi\">4</span><span class=\"p\">:</span><span class=\"mi\">32894910464</span>\n    <span class=\"o\">/</span><span class=\"n\">mnt</span><span class=\"o\">/</span><span class=\"n\">mNLP</span><span class=\"o\">/</span><span class=\"n\">kg</span><span class=\"o\">/</span><span class=\"n\">kenlm</span><span class=\"o\">/</span><span class=\"n\">lm</span><span class=\"o\">/</span><span class=\"n\">builder</span><span class=\"o\">/</span><span class=\"n\">adjust_counts</span><span class=\"o\">.</span><span class=\"n\">cc</span><span class=\"p\">:</span><span class=\"mi\">52</span> <span class=\"ow\">in</span> <span class=\"n\">void</span> <span class=\"n\">lm</span><span class=\"p\">::</span><span class=\"n\">builder</span><span class=\"p\">::{</span><span class=\"n\">anonymous</span><span class=\"p\">}::</span><span class=\"n\">StatCollector</span><span class=\"p\">::</span><span class=\"n\">CalculateDiscounts</span><span class=\"p\">(</span><span class=\"n\">const</span> <span class=\"n\">lm</span><span class=\"p\">::</span><span class=\"n\">builder</span><span class=\"p\">::</span><span class=\"n\">DiscountConfig</span><span class=\"o\">&amp;</span><span class=\"p\">)</span> <span class=\"n\">threw</span> <span class=\"n\">BadDiscountException</span> <span class=\"n\">because</span> <span class=\"err\">`</span><span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">n</span><span class=\"p\">[</span><span class=\"n\">j</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"s1\">&#39;.</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    Could not calculate Kneser-Ney discounts for 1-grams with adjusted count 4 because we didn&#39;</span><span class=\"n\">t</span> <span class=\"n\">observe</span> <span class=\"nb\">any</span> <span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">grams</span> <span class=\"k\">with</span> <span class=\"n\">adjusted</span> <span class=\"n\">count</span> <span class=\"mi\">3</span><span class=\"p\">;</span> <span class=\"n\">Is</span> <span class=\"n\">this</span> <span class=\"n\">small</span> <span class=\"ow\">or</span> <span class=\"n\">artificial</span> <span class=\"n\">data</span><span class=\"err\">?</span>\n    <span class=\"n\">Try</span> <span class=\"n\">deduplicating</span> <span class=\"n\">the</span> <span class=\"nb\">input</span><span class=\"o\">.</span>  <span class=\"n\">To</span> <span class=\"n\">override</span> <span class=\"n\">this</span> <span class=\"n\">error</span> <span class=\"k\">for</span> <span class=\"n\">e</span><span class=\"o\">.</span><span class=\"n\">g</span><span class=\"o\">.</span> <span class=\"n\">a</span> <span class=\"n\">class</span><span class=\"o\">-</span><span class=\"n\">based</span> <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">rerun</span> <span class=\"k\">with</span> <span class=\"o\">--</span><span class=\"n\">discount_fallback</span></code></pre></div><p>报错码为:34304,主要是因为字数太少，所以训练的时候需要多加一些。</p><h2>2.2 模型压缩二进制化<code>build_binary</code></h2><p>这边生成的arpa文件，可能会比较大，可以通过二进制化缩小文件大小：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"nb\">bin</span><span class=\"o\">/</span><span class=\"n\">build_binary</span> <span class=\"o\">-</span><span class=\"n\">s</span> <span class=\"n\">lm</span><span class=\"o\">.</span><span class=\"n\">arpa</span> <span class=\"n\">lm</span><span class=\"o\">.</span><span class=\"nb\">bin</span></code></pre></div><p>将arpa文件转换为binary文件，这样可以对arpa文件进行压缩，提高后续在python中加载的速度。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6191e79f4fd60dc0b1da3d6ed5994bef_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1146\" data-rawheight=\"230\" class=\"origin_image zh-lightbox-thumb\" width=\"1146\" data-original=\"https://pic4.zhimg.com/v2-6191e79f4fd60dc0b1da3d6ed5994bef_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1146&#39; height=&#39;230&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1146\" data-rawheight=\"230\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1146\" data-original=\"https://pic4.zhimg.com/v2-6191e79f4fd60dc0b1da3d6ed5994bef_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6191e79f4fd60dc0b1da3d6ed5994bef_b.jpg\"/></figure><p> 虽然大小没有发生太大的变化，但是压缩后会大大提高Python加载的速度。</p><p>可能会报错，报错码为：256，原因如下：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">No</span> <span class=\"n\">such</span> <span class=\"nb\">file</span> <span class=\"ow\">or</span> <span class=\"n\">directory</span> <span class=\"k\">while</span> <span class=\"n\">opening</span> <span class=\"n\">output</span><span class=\"o\">/</span><span class=\"n\">test2</span><span class=\"o\">.</span><span class=\"n\">arpa</span></code></pre></div><h2>2.3 利用kenlm的<code>count_ngrams</code>计算n-grams</h2><p>苏神<a href=\"https://link.zhihu.com/?target=https%3A//spaces.ac.cn/archives/6920\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【重新写了之前的新词发现算法：更快更好的新词发现】</a>中用的是这个。 这个库存在<code>build/bin/count_ngrams</code></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># Counts n-grams from standard input.</span>\n    <span class=\"c1\"># corpus count:</span>\n    <span class=\"c1\">#   -h [ --help ]                     Show this help message</span>\n    <span class=\"c1\">#   -o [ --order ] arg                Order</span>\n    <span class=\"c1\">#   -T [ --temp_prefix ] arg (=/tmp/) Temporary file prefix</span>\n    <span class=\"c1\">#   -S [ --memory ] arg (=80%)        RAM</span>\n    <span class=\"c1\">#   --read_vocab_table arg            Vocabulary hash table to read.  This should</span>\n    <span class=\"c1\">#                                     be a probing hash table with size at the </span>\n    <span class=\"c1\">#                                     beginning.</span>\n    <span class=\"c1\">#   --write_vocab_list arg            Vocabulary list to write as null-delimited </span>\n    <span class=\"c1\">#                                     strings.</span></code></pre></div><p>其中也有该死的<code>-s</code>，要留意。 执行命令示例：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"o\">./</span><span class=\"n\">count_ngrams</span> <span class=\"o\">-</span><span class=\"n\">S</span> <span class=\"mi\">50</span><span class=\"o\">%</span> <span class=\"o\">-</span><span class=\"n\">o</span> <span class=\"mi\">4</span> <span class=\"o\">--</span><span class=\"n\">write_vocab_list</span> <span class=\"n\">output</span><span class=\"o\">/</span><span class=\"n\">test2</span><span class=\"o\">.</span><span class=\"n\">chars</span> <span class=\"o\">&lt;</span><span class=\"n\">output</span><span class=\"o\">/</span><span class=\"n\">test2</span><span class=\"o\">.</span><span class=\"n\">corpus</span> <span class=\"o\">&gt;</span><span class=\"n\">output</span><span class=\"o\">/</span><span class=\"n\">test2</span><span class=\"o\">.</span><span class=\"n\">ngrams</span></code></pre></div><p>其中，参数<code>-s</code>,<code>-o</code>与前面一样， 输入的是预生成文本<code>output/test2.corpus</code>，生成两个文件：<code>output/test2.chars</code> 和 <code>output/test2.ngrams</code>，分别是单词文件和ngrams的文件集合</p><p>其中，执行的时候，如果返回的不是0，都是有错误的，笔者自己遇到过几个：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-50ba9968e67e449ddabdcf207b3a56bf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"755\" data-rawheight=\"196\" class=\"origin_image zh-lightbox-thumb\" width=\"755\" data-original=\"https://pic4.zhimg.com/v2-50ba9968e67e449ddabdcf207b3a56bf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;755&#39; height=&#39;196&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"755\" data-rawheight=\"196\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"755\" data-original=\"https://pic4.zhimg.com/v2-50ba9968e67e449ddabdcf207b3a56bf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-50ba9968e67e449ddabdcf207b3a56bf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>3 kenlm模型的初级使用</p><p>参考文档：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kpu/kenlm/blob/master/python/example.py\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">kenlm/python/example.py</a></p><h2>3.1 model.score函数</h2><p>python已经有可以使用的库，安装教程见第1章，简单测试方式：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">kenlm</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">kenlm</span><span class=\"o\">.</span><span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"s1\">&#39;lm/test.arpa&#39;</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">score</span><span class=\"p\">(</span><span class=\"s1\">&#39;this is a sentence .&#39;</span><span class=\"p\">,</span> <span class=\"n\">bos</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">eos</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">))</span></code></pre></div><p>其中， 每个句子通过语言模型都会得到一个概率(0-1),然后对概率值取log得到分数(-\\propto ,0],得分值越接近于0越好。 score函数输出的是对数概率，即log10(p(&#39;微 信&#39;))，其中字符串可以是gbk，也可以是utf-8 <code>bos=False, eos=False</code>意思是不自动添加句首和句末标记符,得分值越接近于0越好。</p><p> 一般都要对计算的概率值做log变换，不然连乘值太小了，在程序里会出现 inf 值。</p><blockquote> @param sentence is a string (do not use boundary symbols) @param bos should kenlm add a bos state @param eos should kenlm add an eos state 来源：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kpu/kenlm/blob/master/python/kenlm.pyx\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/kpu/kenlm/bl</span><span class=\"invisible\">ob/master/python/kenlm.pyx</span><span class=\"ellipsis\"></span></a><br/> </blockquote><p>该模块，可以用来测试词条与句子的通顺度：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;再 心 每 天也 不 会 担 个 大 油 饼 到 了 下  午 顶 着 一 了 &#39;</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">score</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">,</span> <span class=\"n\">bos</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">eos</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span></code></pre></div><p>需要注意，是需要空格隔开的。</p><h2>3.2 model.full_scores函数</h2><p><code>score</code>是<code>full_scores</code>是精简版，full_scores会返回：<code>(prob, ngram length, oov)</code> 包括：概率，ngram长度，是否为oov</p><div class=\"highlight\"><pre><code class=\"language-text\"># Show scores and n-gram matches\nsentence = &#39;盘点不怕被税的海淘网站❗️海淘向来便宜又保真，比旗舰店、专柜和代购好太多！&#39;\n\nwords = [&#39;&lt;s&gt;&#39;] + parse_text(sentence).split() + [&#39;&lt;/s&gt;&#39;]\nfor i, (prob, length, oov) in enumerate(model.full_scores(sentence)):\n    print(&#39;{0} {1}: {2}&#39;.format(prob, length, &#39; &#39;.join(words[i+2-length:i+2])))\n    if oov:\n        print(&#39;\\t&#34;{0}&#34; is an OOV&#39;.format(words[i+1]))\n\n# Find out-of-vocabulary words\nfor w in words:\n    if not w in model:\n        print(&#39;&#34;{0}&#34; is an OOV&#39;.format(w))</code></pre></div><h2>3.3 kenlm.State()状态转移概率</h2><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">状态的累加\n</span><span class=\"s1\">score defaults to bos = True and eos = True.  \n</span><span class=\"s1\">Here we&#39;ll check without the endof sentence marker.  \n</span><span class=\"s1\">&#39;&#39;&#39;</span>\n<span class=\"c1\">#Stateful query</span>\n<span class=\"n\">state</span> <span class=\"o\">=</span> <span class=\"n\">kenlm</span><span class=\"o\">.</span><span class=\"n\">State</span><span class=\"p\">()</span>\n<span class=\"n\">state2</span> <span class=\"o\">=</span> <span class=\"n\">kenlm</span><span class=\"o\">.</span><span class=\"n\">State</span><span class=\"p\">()</span>\n<span class=\"c1\">#Use &lt;s&gt; as context.  If you don&#39;t want &lt;s&gt;, use model.NullContextWrite(state).</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">BeginSentenceWrite</span><span class=\"p\">(</span><span class=\"n\">state</span><span class=\"p\">)</span></code></pre></div><p>然后还有：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">accum</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>\n<span class=\"n\">accum</span> <span class=\"o\">+=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">BaseScore</span><span class=\"p\">(</span><span class=\"n\">state</span><span class=\"p\">,</span> <span class=\"s2\">&#34;海&#34;</span><span class=\"p\">,</span> <span class=\"n\">state2</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">accum</span><span class=\"p\">)</span>\n<span class=\"n\">accum</span> <span class=\"o\">+=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">BaseScore</span><span class=\"p\">(</span><span class=\"n\">state2</span><span class=\"p\">,</span> <span class=\"s2\">&#34;淘&#34;</span><span class=\"p\">,</span> <span class=\"n\">state</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">accum</span><span class=\"p\">)</span>\n<span class=\"n\">accum</span> <span class=\"o\">+=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">BaseScore</span><span class=\"p\">(</span><span class=\"n\">state</span><span class=\"p\">,</span> <span class=\"s2\">&#34;&lt;/s&gt;&#34;</span><span class=\"p\">,</span> <span class=\"n\">state2</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">accum</span><span class=\"p\">)</span>\n\n<span class=\"o\">&gt;&gt;&gt;-</span><span class=\"mf\">3.0864107608795166</span>\n<span class=\"o\">&gt;&gt;&gt;-</span><span class=\"mf\">3.6341209411621094</span>\n<span class=\"o\">&gt;&gt;&gt;-</span><span class=\"mf\">4.645392656326294</span>\n\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">score</span><span class=\"p\">(</span><span class=\"s2\">&#34;海 淘&#34;</span><span class=\"p\">,</span> <span class=\"n\">eos</span> <span class=\"o\">=</span> <span class=\"bp\">False</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"o\">-</span><span class=\"mf\">3.381103515625</span></code></pre></div><p>这个实验可以看到：state2的状态概率与score的概率差不多，该模块还有很多可以深挖，NSP任务等等。</p><h2>3.4 语句通顺度检测</h2><p>通顺度其实用score即可，只不过用整个句子，整个句子需要空格隔开。 这边有一个项目，还封装了API，可参考：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/DRUNK2013/lm-ken\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">DRUNK2013/lm-ken</a></p><h2>4 kenlm的深度使用 - 分词</h2><p>参考于：<a href=\"https://link.zhihu.com/?target=https%3A//spaces.ac.cn/archives/3956%23%25E5%25AE%259E%25E8%25B7%25B5%25EF%25BC%259A%25E8%25AE%25AD%25E7%25BB%2583\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【中文分词系列】 5. 基于语言模型的无监督分词</a> 苏神的代码模块：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">kenlm</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">kenlm</span><span class=\"o\">.</span><span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"s1\">&#39;weixin.klm&#39;</span><span class=\"p\">)</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">math</span> <span class=\"kn\">import</span> <span class=\"n\">log10</span>\n\n<span class=\"c1\">#这里的转移概率是人工总结的，总的来说，就是要降低长词的可能性。</span>\n<span class=\"n\">trans</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">&#39;bb&#39;</span><span class=\"p\">:</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">&#39;bc&#39;</span><span class=\"p\">:</span><span class=\"mf\">0.15</span><span class=\"p\">,</span> <span class=\"s1\">&#39;cb&#39;</span><span class=\"p\">:</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">&#39;cd&#39;</span><span class=\"p\">:</span><span class=\"mf\">0.01</span><span class=\"p\">,</span> <span class=\"s1\">&#39;db&#39;</span><span class=\"p\">:</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">&#39;de&#39;</span><span class=\"p\">:</span><span class=\"mf\">0.01</span><span class=\"p\">,</span> <span class=\"s1\">&#39;eb&#39;</span><span class=\"p\">:</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ee&#39;</span><span class=\"p\">:</span><span class=\"mf\">0.001</span><span class=\"p\">}</span>\n<span class=\"n\">trans</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"n\">i</span><span class=\"p\">:</span><span class=\"n\">log10</span><span class=\"p\">(</span><span class=\"n\">j</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span><span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"n\">trans</span><span class=\"o\">.</span><span class=\"n\">iteritems</span><span class=\"p\">()}</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">viterbi</span><span class=\"p\">(</span><span class=\"n\">nodes</span><span class=\"p\">):</span>\n    <span class=\"n\">paths</span> <span class=\"o\">=</span> <span class=\"n\">nodes</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">nodes</span><span class=\"p\">)):</span>\n        <span class=\"n\">paths_</span> <span class=\"o\">=</span> <span class=\"n\">paths</span>\n        <span class=\"n\">paths</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"n\">nodes</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"p\">]:</span>\n            <span class=\"n\">nows</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n            <span class=\"k\">for</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"n\">paths_</span><span class=\"p\">:</span>\n                <span class=\"k\">if</span> <span class=\"n\">j</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">+</span><span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"n\">trans</span><span class=\"p\">:</span>\n                    <span class=\"n\">nows</span><span class=\"p\">[</span><span class=\"n\">j</span><span class=\"o\">+</span><span class=\"n\">i</span><span class=\"p\">]</span><span class=\"o\">=</span> <span class=\"n\">paths_</span><span class=\"p\">[</span><span class=\"n\">j</span><span class=\"p\">]</span><span class=\"o\">+</span><span class=\"n\">nodes</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"p\">][</span><span class=\"n\">i</span><span class=\"p\">]</span><span class=\"o\">+</span><span class=\"n\">trans</span><span class=\"p\">[</span><span class=\"n\">j</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">+</span><span class=\"n\">i</span><span class=\"p\">]</span>\n            <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"n\">nows</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">index</span><span class=\"p\">(</span><span class=\"nb\">max</span><span class=\"p\">(</span><span class=\"n\">nows</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"p\">()))</span>\n            <span class=\"n\">paths</span><span class=\"p\">[</span><span class=\"n\">nows</span><span class=\"o\">.</span><span class=\"n\">keys</span><span class=\"p\">()[</span><span class=\"n\">k</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"n\">nows</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"p\">()[</span><span class=\"n\">k</span><span class=\"p\">]</span>\n    <span class=\"k\">return</span> <span class=\"n\">paths</span><span class=\"o\">.</span><span class=\"n\">keys</span><span class=\"p\">()[</span><span class=\"n\">paths</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">index</span><span class=\"p\">(</span><span class=\"nb\">max</span><span class=\"p\">(</span><span class=\"n\">paths</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"p\">()))]</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">cp</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">score</span><span class=\"p\">(</span><span class=\"s1\">&#39; &#39;</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">),</span> <span class=\"n\">bos</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">,</span> <span class=\"n\">eos</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">score</span><span class=\"p\">(</span><span class=\"s1\">&#39; &#39;</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">[:</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]),</span> <span class=\"n\">bos</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">,</span> <span class=\"n\">eos</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">))</span> <span class=\"ow\">or</span> <span class=\"o\">-</span><span class=\"mf\">100.0</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">mycut</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">):</span>\n    <span class=\"n\">nodes</span> <span class=\"o\">=</span> <span class=\"p\">[{</span><span class=\"s1\">&#39;b&#39;</span><span class=\"p\">:</span><span class=\"n\">cp</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]),</span> <span class=\"s1\">&#39;c&#39;</span><span class=\"p\">:</span><span class=\"n\">cp</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"n\">i</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">]),</span> <span class=\"s1\">&#39;d&#39;</span><span class=\"p\">:</span><span class=\"n\">cp</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">:</span><span class=\"n\">i</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">]),</span> <span class=\"s1\">&#39;e&#39;</span><span class=\"p\">:</span><span class=\"n\">cp</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"o\">-</span><span class=\"mi\">3</span><span class=\"p\">:</span><span class=\"n\">i</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">])}</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">))]</span>\n    <span class=\"n\">tags</span> <span class=\"o\">=</span> <span class=\"n\">viterbi</span><span class=\"p\">(</span><span class=\"n\">nodes</span><span class=\"p\">)</span>\n    <span class=\"n\">words</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">s</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]]</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)):</span>\n        <span class=\"k\">if</span> <span class=\"n\">tags</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;b&#39;</span><span class=\"p\">:</span>\n            <span class=\"n\">words</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">])</span>\n        <span class=\"k\">else</span><span class=\"p\">:</span>\n            <span class=\"n\">words</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"n\">s</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span>\n    <span class=\"k\">return</span> <span class=\"n\">words</span></code></pre></div><p>将分词转化为了标注问题，如果字语言模型取到4-gram，那么它相当于做了如下的字标注：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">b</span><span class=\"err\">：单字词或者多字词的首字</span>\n<span class=\"n\">c</span><span class=\"err\">：多字词的第二字</span>\n<span class=\"n\">d</span><span class=\"err\">：多字词的第三字</span>\n<span class=\"n\">e</span><span class=\"err\">：多字词的其余部分</span></code></pre></div><p>笔者基本没改动，微调至py3可用，笔者的模块可以使用的方式为：</p><div class=\"highlight\"><pre><code class=\"language-text\"># 初始化\nkm = kenlm_model()\nkm.model = km.load_model(&#39;output/test2.klm&#39;)</code></pre></div><p>查询与分词：</p><div class=\"highlight\"><pre><code class=\"language-text\">sentence = &#39;这瓶洗棉奶用着狠不错&#39;\nkm.mycut(sentence)</code></pre></div><p>当然，分词模块只是for fun的。。</p><hr/><h2>5 kenlm的深度使用 - 新词发现</h2><p>苏神<a href=\"https://link.zhihu.com/?target=https%3A//spaces.ac.cn/archives/6920\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【重新写了之前的新词发现算法：更快更好的新词发现】</a>中用的是这个。大部分与苏神一致，微调至py3已经加入分词方式的调用。这个可能需要先训练：</p><h2>5.1 训练语料</h2><p><b>第一步：模型加载</b></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">km</span> <span class=\"o\">=</span> <span class=\"n\">kenlm_model</span><span class=\"p\">(</span><span class=\"n\">save_path</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;output&#39;</span><span class=\"p\">,</span><span class=\"n\">project</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;test2&#39;</span><span class=\"p\">,</span>\\\n                 <span class=\"n\">memory</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;50%&#39;</span><span class=\"p\">,</span><span class=\"n\">min_count</span> <span class=\"o\">=</span> <span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"n\">order</span> <span class=\"o\">=</span> <span class=\"mi\">4</span><span class=\"p\">,</span>\\\n                 <span class=\"n\">skip_symbols</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;&#34;&lt;unk&gt;&#34;&#39;</span><span class=\"p\">,</span><span class=\"n\">kenlm_model_path</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;./kenlm/build/bin/&#39;</span><span class=\"p\">)</span></code></pre></div><p>其中， </p><p>- save_path， 是相关文件存储在哪，因为一次性会生成很多临时文件 </p><p>- project ，是项目编号，编译项目管理 - memory，调用时候占用的内存容量 </p><p>- min_count = 2，在筛选n-grams最小的频率</p><p> - order = 4，n-grams中的n </p><p>- skip_symbols = <code>&#39;&#34;&lt;unk&gt;&#34;&#39;</code>，Treat<code>&lt;s&gt;</code>, <code>&lt;/s&gt;</code>, and <code>&lt;unk&gt;</code> as whitespace instead of throwing an exception - kenlm_model_path = &#39;./kenlm/build/bin/&#39;，kenlm那些编译好的文件存放在的位置</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>第二步：准备训练材料</b> 训练，笔者拿了五句话来做训练（<b>实际需要多准备一些，不然文字太少会报错</b>）：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">text_list</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">&#39;Whoo 后 拱辰享水 水妍护肤套装整套质地都比较清爽，滋润，侧重保湿，适合各种肤质&#39;</span><span class=\"p\">,</span>\n <span class=\"s1\">&#39;盘点不怕被税的海淘网站❗️海淘向来便宜又保真，比旗舰店、专柜和代购好太多！还能体验海淘乐趣~外网需要双币信用卡，往往需要转运，北上地区容易被税。&#39;</span><span class=\"p\">,</span>\n <span class=\"s1\">&#39;学生用什么洗面奶好？学生党必备的这六款性价比最高的洗面奶是什么？&#39;</span><span class=\"p\">,</span>\n <span class=\"s1\">&#39;国货大宝。启初。……使用分享修复：玉泽或至本（第三代玉泽身体乳没有了麦冬根和神经酰胺）。芦荟胶（含酒精，不锁水，偶尔敷一下，皮肤会越用越干）。Swisse蜂蜜面膜（清洁鼻子，效果肉眼可见，不能常用）。&#39;</span><span class=\"p\">,</span>\n <span class=\"s1\">&#39;资生堂悦薇乳液，会回购。夏天用略油腻，冬天用刚好。真的有紧致感，28岁，眼部有笑纹，其他地方还可以。这是第二个空瓶。冬天会回购。没有美白效果。(资生堂悦薇)&#39;</span><span class=\"p\">]</span>\n\n<span class=\"n\">km</span><span class=\"o\">.</span><span class=\"n\">write_corpus</span><span class=\"p\">(</span><span class=\"n\">km</span><span class=\"o\">.</span><span class=\"n\">text_generator</span><span class=\"p\">(</span><span class=\"n\">text_list</span><span class=\"p\">,</span><span class=\"n\">jieba_cut</span> <span class=\"o\">=</span> <span class=\"bp\">False</span><span class=\"p\">),</span> <span class=\"n\">km</span><span class=\"o\">.</span><span class=\"n\">corpus_file</span><span class=\"p\">)</span> <span class=\"c1\"># 将语料转存为文本</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">success</span> <span class=\"n\">writed</span></code></pre></div><p>将文本解析为：</p><div class=\"highlight\"><pre><code class=\"language-text\">W h o o   后   拱 辰 享 水   水 妍 护 肤 套 装 整 套 质 地 都 比 较 清 爽 \n 滋 润 \n 侧 重 保 湿 \n 适 合 各 种 肤 质</code></pre></div><p>并保存在：<code>km.corpus_file</code>文件之中</p><p><b>第三步：计算模型的n-grams</b></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># 计算模型的n-grams</span>\n<span class=\"n\">km</span><span class=\"o\">.</span><span class=\"n\">count_ngrams</span><span class=\"p\">()</span> <span class=\"c1\"># 用Kenlm统计ngram</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span><span class=\"n\">success</span><span class=\"p\">,</span><span class=\"n\">code</span> <span class=\"ow\">is</span> <span class=\"p\">:</span> <span class=\"mi\">0</span> <span class=\"p\">,</span> \n <span class=\"n\">code</span> <span class=\"ow\">is</span> <span class=\"p\">:</span> <span class=\"o\">./</span><span class=\"n\">kenlm</span><span class=\"o\">/</span><span class=\"n\">build</span><span class=\"o\">/</span><span class=\"nb\">bin</span><span class=\"o\">/</span><span class=\"n\">build_binary</span> <span class=\"o\">-</span><span class=\"n\">S</span> <span class=\"mi\">50</span><span class=\"o\">%</span> <span class=\"o\">-</span><span class=\"n\">s</span> <span class=\"n\">output</span><span class=\"o\">/</span><span class=\"n\">test2</span><span class=\"o\">.</span><span class=\"n\">arpa</span> <span class=\"n\">output</span><span class=\"o\">/</span><span class=\"n\">test2</span><span class=\"o\">.</span><span class=\"n\">klm</span></code></pre></div><p>这里如果状态码不是0，就是报错了，写在py之中不好看到报错信息，笔者自己把相关执行代码也显示出来，所以自己去终端敲一下，定位问题。这步骤，根据<code>.corpus</code>文件，生成<code>.chars</code>和<code>.ngrams</code></p><h2>5.2 读入模型并使用</h2><p>这个<code>read_ngrams</code> 和 <code>filter_ngrams</code>都是苏神中的代码了</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">ngrams</span><span class=\"p\">,</span><span class=\"n\">total</span> <span class=\"o\">=</span> <span class=\"n\">km</span><span class=\"o\">.</span><span class=\"n\">read_ngrams</span><span class=\"p\">()</span>\n<span class=\"n\">ngrams_2</span> <span class=\"o\">=</span> <span class=\"n\">km</span><span class=\"o\">.</span><span class=\"n\">filter_ngrams</span><span class=\"p\">(</span><span class=\"n\">ngrams</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">,</span> <span class=\"n\">min_pmi</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">])</span>\n<span class=\"n\">ngrams_2</span></code></pre></div><p>read_ngrams是读入之前的训练文件，ngrams是有三个grams（1-gram,2-gram,3-gram）的（word freq）词与词频， filter_ngrams就是过滤ngram了，</p><p>[0, 2, 4, 6]是互信息的阈值，其中第一个0无意义，仅填充用，而2, 4, 6分别是2gram、3gram、4gram的互信息阈值，基本上单调递增比较好。 </p><p>得到这些n-grams之后的逻辑与苏神有点不一样，我的逻辑是：</p><div class=\"highlight\"><pre><code class=\"language-text\">是否能够被jieba分开\n且限定在一定的条件下：词性限定 + 个别停用字</code></pre></div><p>那么使用方式：</p><div class=\"highlight\"><pre><code class=\"language-text\">km.word_discovery(ngrams_2)\n\n&gt;{&#39;缓痘痘&#39;: 2,\n &#39;奶参考&#39;: 2,\n &#39;中文界&#39;: 5,\n &#39;文界面&#39;: 5,\n &#39;界面支&#39;: 5,\n &#39;蜂蜜面&#39;: 2,\n &#39;20英&#39;: 2,\n &#39;面奶参&#39;: 2,\n &#39;舒缓痘&#39;: 2,\n &#39;0英&#39;: 4}</code></pre></div><p>我这边是返回了词 + 词频，便于画词云。</p><hr/><h2>6 kenlm的深度使用 - 智能纠错</h2><p>部分来源： <a href=\"https://link.zhihu.com/?target=https%3A//github.com/shibing624/pycorrector\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">pycorrector</a> <a href=\"https://zhuanlan.zhihu.com/p/40806718\" class=\"internal\">中文文本纠错算法--错别字纠正的二三事</a></p><p>笔者最近在研究智能写作，对纠错还是蛮有需求的，这边有看到文章些kenlm用在纠错上，不过是a/an的简单区别，这边笔者也基于此简单使用了一些。 纠错任务一般分别两个：</p><ul><li>发现错误</li><li>改正错误</li></ul><p>这边智能纠错笔者比较推荐的库是：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/shibing624/pycorrector\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">pycorrector</a>，优点很多：</p><ul><li>一直在维护</li><li>可自定义加载自己的一些规则</li><li>有深度方案的选项</li></ul><p>当然这个库好像要预装tensorflow？ 需要安装尝试的小伙伴注意下。中文文本纠错任务，常见错误类型包括：</p><ul><li>谐音字词，如 配副眼睛-配副眼镜</li><li>混淆音字词，如 流浪织女-牛郎织女</li><li>字词顺序颠倒，如 伍迪艾伦-艾伦伍迪</li><li>字词补全，如 爱有天意-假如爱有天意</li><li>形似字错误，如 高梁-高粱</li><li>中文拼音全拼，如 xingfu-幸福</li><li>中文拼音缩写，如 sz-深圳</li><li>语法错误，如 想象难以-难以想象</li></ul><p>因为只是实验，所以，发现错误这个环节就交给pycorrector了，笔者用kenlm来改正错误。 简单的发现错误的环节，思路大概是：</p><blockquote> 错误检测部分先通过结巴中文分词器切词，由于句子中含有错别字，所以切词结果往往会有切分错误的情况，这样从字粒度和词粒度两方面检测错误，整合这两种粒度的疑似错误结果，形成疑似错误位置候选集<br/> </blockquote><p>Kenlm改正错误，有个好处就是kenlm可以定制化训练某一领域的大规模语料的语言模型。本次简单实验的改正逻辑是：</p><div class=\"highlight\"><pre><code class=\"language-text\">两个字至少有一个字，字形相似\n两个字拼音首字母一致</code></pre></div><p>所以只是上述提到错误中的拼音缩写修正。</p><h2>6.1 pypinyin拼音模块</h2><p>其中，拼音模块涉及到了<code>pypinyin</code>，用来识别汉字的拼音，还有非常多种的模式：</p><div class=\"highlight\"><pre><code class=\"language-text\">from pypinyin import lazy_pinyin, Style\n    # Python 中拼音库 PyPinyin 的用法\n    # https://blog.csdn.net/devcloud/article/details/95066038\n\ntts = [&#39;BOPOMOFO&#39;, &#39;BOPOMOFO_FIRST&#39;, &#39;CYRILLIC&#39;, &#39;CYRILLIC_FIRST&#39;, &#39;FINALS&#39;, &#39;FINALS_TONE&#39;,\n &#39;FINALS_TONE2&#39;, &#39;FINALS_TONE3&#39;, &#39;FIRST_LETTER&#39;, &#39;INITIALS&#39;, &#39;NORMAL&#39;, &#39;TONE&#39;, &#39;TONE2&#39;, &#39;TONE3&#39;]\nfor tt in tts:\n    print(tt,lazy_pinyin(&#39;聪明的小兔子吃&#39;, style=eval(&#39;Style.{}&#39;.format(tt))   ))</code></pre></div><p>其中结果为：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">BOPOMOFO</span> <span class=\"p\">[</span><span class=\"s1\">&#39;ㄘㄨㄥ&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ㄇㄧㄥˊ&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ㄉㄜ˙&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ㄒㄧㄠˇ&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ㄊㄨˋ&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ㄗ˙&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ㄔ&#39;</span><span class=\"p\">]</span>\n<span class=\"n\">BOPOMOFO_FIRST</span> <span class=\"p\">[</span><span class=\"s1\">&#39;ㄘ&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ㄇ&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ㄉ&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ㄒ&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ㄊ&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ㄗ&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ㄔ&#39;</span><span class=\"p\">]</span>\n<span class=\"n\">CYRILLIC</span> <span class=\"p\">[</span><span class=\"s1\">&#39;цун1&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;мин2&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;дэ&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;сяо3&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ту4&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;цзы&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;чи1&#39;</span><span class=\"p\">]</span>\n<span class=\"n\">CYRILLIC_FIRST</span> <span class=\"p\">[</span><span class=\"s1\">&#39;ц&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;м&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;д&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;с&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;т&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ц&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ч&#39;</span><span class=\"p\">]</span>\n<span class=\"n\">FINALS</span> <span class=\"p\">[</span><span class=\"s1\">&#39;ong&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ing&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;e&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;iao&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;u&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;i&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;i&#39;</span><span class=\"p\">]</span>\n<span class=\"n\">FINALS_TONE</span> <span class=\"p\">[</span><span class=\"s1\">&#39;ōng&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;íng&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;e&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;iǎo&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ù&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;i&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ī&#39;</span><span class=\"p\">]</span>\n<span class=\"n\">FINALS_TONE2</span> <span class=\"p\">[</span><span class=\"s1\">&#39;o1ng&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;i2ng&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;e&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ia3o&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;u4&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;i&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;i1&#39;</span><span class=\"p\">]</span>\n<span class=\"n\">FINALS_TONE3</span> <span class=\"p\">[</span><span class=\"s1\">&#39;ong1&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ing2&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;e&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;iao3&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;u4&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;i&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;i1&#39;</span><span class=\"p\">]</span>\n<span class=\"n\">FIRST_LETTER</span> <span class=\"p\">[</span><span class=\"s1\">&#39;c&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;m&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;d&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;x&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;t&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;z&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;c&#39;</span><span class=\"p\">]</span>\n<span class=\"n\">INITIALS</span> <span class=\"p\">[</span><span class=\"s1\">&#39;c&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;m&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;d&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;x&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;t&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;z&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ch&#39;</span><span class=\"p\">]</span>\n<span class=\"n\">NORMAL</span> <span class=\"p\">[</span><span class=\"s1\">&#39;cong&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ming&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;de&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;xiao&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;tu&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;zi&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;chi&#39;</span><span class=\"p\">]</span>\n<span class=\"n\">TONE</span> <span class=\"p\">[</span><span class=\"s1\">&#39;cōng&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;míng&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;de&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;xiǎo&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;tù&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;zi&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;chī&#39;</span><span class=\"p\">]</span>\n<span class=\"n\">TONE2</span> <span class=\"p\">[</span><span class=\"s1\">&#39;co1ng&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;mi2ng&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;de&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;xia3o&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;tu4&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;zi&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;chi1&#39;</span><span class=\"p\">]</span>\n<span class=\"n\">TONE3</span> <span class=\"p\">[</span><span class=\"s1\">&#39;cong1&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ming2&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;de&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;xiao3&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;tu4&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;zi&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;chi1&#39;</span><span class=\"p\">]</span></code></pre></div><p>可以看出不同的style可以得到不同拼音形式。</p><h2>6.2 pycorrector纠错模块</h2><p>pycorrector的<code>detect</code>，可以返回，错误字的信息</p><div class=\"highlight\"><pre><code class=\"language-text\">import pycorrector\nsentence = &#39;这瓶洗棉奶用着狠不错&#39;\nidx_errors = pycorrector.detect(sentence)\n&gt;&gt;&gt; [[&#39;这瓶&#39;, 0, 2, &#39;word&#39;], [&#39;棉奶&#39;, 3, 5, &#39;word&#39;]]</code></pre></div><p>correct是专门用来纠正：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">pycorrector</span><span class=\"o\">.</span><span class=\"n\">correct</span><span class=\"p\">(</span><span class=\"n\">sentence</span><span class=\"p\">)</span></code></pre></div><h2>6.3 pycorrector与kenlm纠错对比</h2><p>来对比一下pycorrector自带的纠错和本次实验的纠错：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">pycorrector</span>\n<span class=\"n\">sentence</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;这瓶洗棉奶用着狠不错&#39;</span>\n<span class=\"n\">idx_errors</span> <span class=\"o\">=</span> <span class=\"n\">pycorrector</span><span class=\"o\">.</span><span class=\"n\">detect</span><span class=\"p\">(</span><span class=\"n\">sentence</span><span class=\"p\">)</span>\n\n<span class=\"n\">correct</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"k\">for</span> <span class=\"n\">ide</span> <span class=\"ow\">in</span> <span class=\"n\">idx_errors</span><span class=\"p\">:</span>\n    <span class=\"n\">right_word</span> <span class=\"o\">=</span> <span class=\"n\">km</span><span class=\"o\">.</span><span class=\"n\">find_best_word</span><span class=\"p\">(</span><span class=\"n\">ide</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"n\">ngrams_</span><span class=\"p\">,</span><span class=\"n\">freqs</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"n\">right_word</span> <span class=\"o\">!=</span> <span class=\"n\">ide</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]:</span>\n        <span class=\"n\">correct</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">([</span><span class=\"n\">right_word</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">ide</span><span class=\"p\">)</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;错误：&#39;</span><span class=\"p\">,</span><span class=\"n\">idx_errors</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;pycorrector的结果：&#39;</span><span class=\"p\">,</span><span class=\"n\">pycorrector</span><span class=\"o\">.</span><span class=\"n\">correct</span><span class=\"p\">(</span><span class=\"n\">sentence</span><span class=\"p\">))</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;kenlm的结果：&#39;</span><span class=\"p\">,</span><span class=\"n\">correct</span><span class=\"p\">)</span>\n\n<span class=\"o\">&gt;</span> <span class=\"err\">错误：</span> <span class=\"p\">[[</span><span class=\"s1\">&#39;这瓶&#39;</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s1\">&#39;word&#39;</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">&#39;棉奶&#39;</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"s1\">&#39;word&#39;</span><span class=\"p\">]]</span>\n<span class=\"o\">&gt;</span> <span class=\"n\">pycorrector的结果</span><span class=\"err\">：</span> <span class=\"p\">(</span><span class=\"s1\">&#39;这瓶洗面奶用着狠不错&#39;</span><span class=\"p\">,</span> <span class=\"p\">[[</span><span class=\"s1\">&#39;棉奶&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;面奶&#39;</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">]])</span>\n<span class=\"o\">&gt;</span> <span class=\"n\">kenlm的结果</span><span class=\"err\">：</span> <span class=\"p\">[[</span><span class=\"s1\">&#39;面奶&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;棉奶&#39;</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"s1\">&#39;word&#39;</span><span class=\"p\">]]</span></code></pre></div><p>其他类似的案例：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">sentence</span> <span class=\"o\">=</span>  <span class=\"s1\">&#39;少先队员因该给老人让坐&#39;</span>\n\n<span class=\"o\">&gt;</span> <span class=\"err\">错误：</span> <span class=\"p\">[[</span><span class=\"s1\">&#39;因该&#39;</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"s1\">&#39;word&#39;</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">&#39;坐&#39;</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">11</span><span class=\"p\">,</span> <span class=\"s1\">&#39;char&#39;</span><span class=\"p\">]]</span>\n<span class=\"o\">&gt;</span> <span class=\"n\">pycorrector的结果</span><span class=\"err\">：</span> <span class=\"p\">(</span><span class=\"s1\">&#39;少先队员应该给老人让座&#39;</span><span class=\"p\">,</span> <span class=\"p\">[[</span><span class=\"s1\">&#39;因该&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;应该&#39;</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">&#39;坐&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;座&#39;</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">11</span><span class=\"p\">]])</span>\n<span class=\"o\">&gt;</span> <span class=\"n\">kenlm的结果</span><span class=\"err\">：</span> <span class=\"p\">[[</span><span class=\"s1\">&#39;应该&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;因该&#39;</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"s1\">&#39;word&#39;</span><span class=\"p\">]]</span></code></pre></div><p>这里笔者的简陋规则暴露问题了，只能对2个字以上的进行判定。 另一个：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">sentence</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;绿茶净华可以舒缓痘痘机肤&#39;</span>\n\n<span class=\"o\">&gt;</span> <span class=\"err\">错误：</span> <span class=\"p\">[[</span><span class=\"s1\">&#39;净华&#39;</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s1\">&#39;word&#39;</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">&#39;机肤&#39;</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">12</span><span class=\"p\">,</span> <span class=\"s1\">&#39;word&#39;</span><span class=\"p\">]]</span>\n<span class=\"o\">&gt;</span> <span class=\"n\">pycorrector的结果</span><span class=\"err\">：</span> <span class=\"p\">(</span><span class=\"s1\">&#39;绿茶净化可以舒缓痘痘肌肤&#39;</span><span class=\"p\">,</span> <span class=\"p\">[[</span><span class=\"s1\">&#39;净华&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;净化&#39;</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">&#39;机肤&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;肌肤&#39;</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">12</span><span class=\"p\">]])</span>\n<span class=\"o\">&gt;</span> <span class=\"n\">kenlm的结果</span><span class=\"err\">：</span> <span class=\"p\">[[</span><span class=\"s1\">&#39;精华&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;净华&#39;</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s1\">&#39;word&#39;</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">&#39;肌肤&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;机肤&#39;</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">12</span><span class=\"p\">,</span> <span class=\"s1\">&#39;word&#39;</span><span class=\"p\">]]</span></code></pre></div><p>因为训练的是这方面的语料，要比prcorrector好一些。</p><hr/><h2>参考文献</h2><p class=\"ztext-empty-paragraph\"><br/></p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/Nicholas_Wong/article/details/80013547\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">1 使用kenLM训练语言模型</a><p class=\"ztext-empty-paragraph\"><br/></p><a href=\"https://zhuanlan.zhihu.com/p/39722203\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\">晒月亮的孩子：使用kenlm模型判别a/an错别字</a><p class=\"ztext-empty-paragraph\"><br/></p><a href=\"https://link.zhihu.com/?target=https%3A//www.bbsmax.com/A/WpdKmENJVQ/\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">3 语言模型kenlm的训练及使用</a><a href=\"https://link.zhihu.com/?target=https%3A//github.com/DRUNK2013/lm-ken\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-c09603493027c5292f2ed23d96e42cc2_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">DRUNK2013/lm-ken</a><a href=\"https://link.zhihu.com/?target=https%3A//spaces.ac.cn/archives/6920\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">重新写了之前的新词发现算法：更快更好的新词发现 - 科学空间|Scientific Spaces</a><a href=\"https://link.zhihu.com/?target=https%3A//spaces.ac.cn/archives/3956%23%25E5%25AE%259E%25E8%25B7%25B5%25EF%25BC%259A%25E8%25AE%25AD%25E7%25BB%2583\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【中文分词系列】 5. 基于语言模型的无监督分词</a><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/sdu_hao/article/details/87101741\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">7 自然语言处理 | (13)kenLM统计语言模型构建与应用</a><p></p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "纠错", 
                    "tagLink": "https://api.zhihu.com/topics/20033650"
                }, 
                {
                    "tag": "神经网络语言模型", 
                    "tagLink": "https://api.zhihu.com/topics/19955854"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/67670119", 
            "userName": "悟乙己", 
            "userLink": "https://www.zhihu.com/people/428693835688c688c6c9ac1d30fc0ece", 
            "upvote": 2, 
            "title": "gensim-fast2vec改造、灵活使用大规模外部词向量（具备OOV查询能力）", 
            "content": "<p>﻿本篇是继 </p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/sinat_26917383/article/details/83041424\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">极简使用︱Gemsim-FastText 词向量训练以及OOV（out-of-word）问题有效解决</a><p> 之后，让之前的一些旧的&#34;word2vec&#34;具备一定的词表外查询功能。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>还有一个使用场景是很多开源出来的词向量很好用，但是很大，用gensim虽然可以直接用，如果能尽量节省一些内存且比较集中会更好，同时如果有一些OOV的功能就更好了，于是笔者就简单抛砖引玉的简单写了该模块。</p><p>github:</p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/mattzheng/gensim-fast2vec\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/mattzheng/ge</span><span class=\"invisible\">nsim-fast2vec</span><span class=\"ellipsis\"></span></a><p class=\"ztext-empty-paragraph\"><br/></p><p><b>譬如以下这些大规模词向量：</b></p><h2>1  Embedding/Chinese-Word-Vectors</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>地址：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/Embedding/Chinese-Word-Vectors\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/Embedding/Ch</span><span class=\"invisible\">inese-Word-Vectors</span><span class=\"ellipsis\"></span></a> </p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4da9b5fa0fe4655056b8778241ef76f9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"854\" data-rawheight=\"502\" class=\"origin_image zh-lightbox-thumb\" width=\"854\" data-original=\"https://pic2.zhimg.com/v2-4da9b5fa0fe4655056b8778241ef76f9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;854&#39; height=&#39;502&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"854\" data-rawheight=\"502\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"854\" data-original=\"https://pic2.zhimg.com/v2-4da9b5fa0fe4655056b8778241ef76f9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-4da9b5fa0fe4655056b8778241ef76f9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>2 腾讯AI Lab开源大规模高质量中文词向量数据</h2><p>地址：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//ai.tencent.com/ailab/nlp/embedding.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">ai.tencent.com/ailab/nl</span><span class=\"invisible\">p/embedding.html</span><span class=\"ellipsis\"></span></a></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><hr/><h2>应用一：gensim - fast2vec 简单查询功能</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>笔者自己使用的是腾讯的词向量，自己清洗过之后使用，还是很不错的。</p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\"># 初始化\n    fv = fast2vec()\n\n    # 加载模型\n    fv.load_word2vec_format(word2vec_path = &#39;Tencent_AILab_ChineseEmbedding_refine.txt&#39;)  # 加载.txt文件\n    fv.model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path,binary=True) # 加载.bin文件\n\n    # 求相似词 - 不带oov\n    fv.model.most_similar(positive=[ &#39;香味&#39;], negative=[&#39;香&#39;], topn=10)  # 带否定词,不带OOV\n    fv.model.most_similar(positive=&#39;香味&#39;, topn=10)  # 词条求相似,不带OOV\n\n    # 求相似词 - 带oov\n    fv.most_similar(&#39;苹果&#39;, topn=10,min_n = 1, max_n = 3)# 词条求相似,带OOV\n    fv.most_similar(&#39;苹果&#39;,negative = [&#39;手机&#39;,&#39;水果&#39;], topn=10,min_n = 1, max_n = 3)# 带否定词,带OOV\n\n    # 词条之间求相似\n    fv.similarity(&#39;香味&#39;,&#39;香&#39;)\n\n    # 词条拆分,用在截取词向量环节\n    fv.compute_ngrams(&#39;香&#39;)\n\n    # 得到词向量\n    fv.wordVec(&#39;香味&#39;)\n\n    # 其他word2vec用法\n    fv.model</code></pre></div><p>本模块是基于gensim-word2vec使用的，那么之前的所有功能都是可以继续使用的</p><hr/><h2>应用三：拓词</h2><p>通过一些种子词进行相近词查找。</p><div class=\"highlight\"><pre><code class=\"language-text\">vocab_dict_word2vec = fv.EmbeddingFilter(&#39;香味&#39;,topn = 100,min_n = 1, max_n = 3,GotWord2vec = False)\nvocab_dict_word2vec = fv.EmbeddingFilter([&#39;香味&#39;,&#39;香气&#39;],topn = 100,min_n = 1, max_n = 3,GotWord2vec = False)</code></pre></div><p>其中主函数中<code>TencentEmbeddingFilter</code>中参数分别代表：</p><ul><li>topn ，每个单词查找相近词的范围，一般为topn = 50;</li><li>min_n = 1，OOV功能中，拆分词，最小n-grams</li><li>max_n = 3，OOV功能中，拆分词，最大n-grams</li><li>GotWord2vec ,GotWord2vec = True为可获得拓展词的词向量，可以保存；GotWord2vec = False的时候，只能返回附近的词群 该函数可以输入单词条，可以输入词语List。</li></ul><p>其中，OOV问题如何解决的思路可参考： </p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/sinat_26917383/article/details/83041424\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">极简使用︱Gemsim-FastText 词向量训练以及OOV（out-of-word）问题有效解决</a><hr/><h2>应用二：gensim - fast2vec 抽取部分词向量</h2><p>大规模的词向量想截取其中一部分使用，一种方式就是查询之后保存。首先就需要准备一些种子词，然后通过托词，找到相关联的一批词，然后进行保存。</p><div class=\"highlight\"><pre><code class=\"language-text\">seed_words = [&#39;牛仔骨&#39;,&#39;泡椒牛蛙&#39;]\n# 查询\nvocab_dict_word2vec = fv.EmbeddingFilter(seed_words,topn = 100,min_n = 1, max_n = 3,GotWord2vec = True)\n# 保存\nfv.wordvec_save2txt(vocab_dict_word2vec,save_path = &#39;test_word2vec_1.txt&#39;,encoding = &#39;utf-8-sig&#39;)</code></pre></div><p>抽取部分词向量的前提是，提供一些要截取的这个行业的种子词，然后查找这些词附近所有的词群（most_similar），同时每个词拆分开来的词条也要记录（compute_ngrams，用于OOV）。 这些词，导出，保存在.txt之中。</p><hr/><h2>应用三：gensim - entity2vec 实体词抽取与查询（类item2vec用法）</h2><p>这个是建立在有庞大的词向量训练基础，譬如腾讯的大规模词向量，里面有非常多的词，这些词一部分也是可以用来当作item2vec的用法。</p><p>譬如，一个简单案例，我要做针对菜谱的查询，那么我这边准备好了一些菜式名称，然后截取一部分出来，供以后不断使用。</p><div class=\"highlight\"><pre><code class=\"language-text\">items = [&#39;牛仔骨&#39;,&#39;泡椒牛蛙&#39;,&#39;农家小炒肉&#39;,&#39;目鱼大烤&#39;,&#39;龙虾意面&#39;,&#39;榴莲酥&#39;,&#39;越式牛肉粒&#39;]\n\n# 拓词 +  保存\nvocab_dict_word2vec = fv.EmbeddingFilter(items,GotWord2vec = True)\nfv.wordvec_save2txt(vocab_dict_word2vec,save_path = &#39;food2vec.txt&#39;,encoding = &#39;utf-8-sig&#39;)\n\n# 加载新模型\nfv2 = fast2vec()\nfv2.load_word2vec_format(word2vec_path = &#39;food2vec.txt&#39;)\n\n# 查询\n fv2.entity_similar(&#39;牛仔骨&#39;,items,topn=500)\n\n&gt;&gt;&gt; [(&#39;牛仔骨&#39;, 1.0),\n&gt;&gt;&gt;  (&#39;农家小炒肉&#39;, 0.7015136480331421),\n&gt;&gt;&gt;  (&#39;榴莲酥&#39;, 0.6885859966278076),\n&gt;&gt;&gt;  (&#39;泡椒牛蛙&#39;, 0.6880079507827759),\n&gt;&gt;&gt;  (&#39;龙虾意面&#39;, 0.6354280710220337),\n&gt;&gt;&gt;  (&#39;越式牛肉粒&#39;, 0.6056148409843445),\n&gt;&gt;&gt;  (&#39;目鱼大烤&#39;, 0.6046081185340881)]</code></pre></div><p>其中，<code>entity_similar</code>，就是查询的时候只能显示<code>items</code>，提供的词群里面的内容，还是一个比较简单的应用。</p><p>本模块是非常有意思的一个模块，当然，虽然有OOV的功能，一些生僻菜名还是很难根据线索找到他们的词向量，如何解决这个问题，是个可以后续研究的地方。</p>", 
            "topic": [
                {
                    "tag": "词向量", 
                    "tagLink": "https://api.zhihu.com/topics/20142325"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/53883377", 
            "userName": "悟乙己", 
            "userLink": "https://www.zhihu.com/people/428693835688c688c6c9ac1d30fc0ece", 
            "upvote": 21, 
            "title": "目标检测︱自有数据YOLOv3训练，最简单的keras实现教程", 
            "content": "<p>开篇先来看看最近目标检测领域的进展：</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-90d68a7baeda91d898f033faf6a680be_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"908\" data-rawheight=\"508\" class=\"origin_image zh-lightbox-thumb\" width=\"908\" data-original=\"https://pic3.zhimg.com/v2-90d68a7baeda91d898f033faf6a680be_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;908&#39; height=&#39;508&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"908\" data-rawheight=\"508\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"908\" data-original=\"https://pic3.zhimg.com/v2-90d68a7baeda91d898f033faf6a680be_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-90d68a7baeda91d898f033faf6a680be_b.jpg\"/></figure><h2>hoya012/deep_learning_object_detection这个开源项目实时更新 + 罗列了目标检测的一些top论文。该项目集合了从 2013 年 11 月提出的 R-CNN 至在近期举办的 ECCV2018 上发表的 RFBNet 等四十多篇关于目标检测的论文，相当全面。</h2><hr/><a href=\"https://link.zhihu.com/?target=https%3A//github.com/qqwweee/keras-yolo3\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">qqwweee/keras-yolo3</a><p>是最简单的自数据训练yolov3的开源项目了。非常简单，相比其他的开源项目，太适合新手练习yolov3。 而公开的很多开源框架的都是基于VOC/COCO来写预训练，整理数据起麻烦不少。 </p><p>本来笔者看到mxnet/gluoncv有yolov3的自训练，而且Mxnet还进行一定改进把精度提升了不少，还欢欣鼓舞的去尝试，但是一旦遇到坑，基本没法解决。。社区人太少，搜不到前人的经验，报错信息稀奇古怪，定位到报错code较难，留言给社区也一时半会儿没人回，还真是从入门到放弃。。</p><p>在此之上进行一些微调，我的项目地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/mattzheng/keras-yolo3-improved\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">keras-yolo3-improved</a><p> 其中， </p><p>- selfdata_keras_yolov3.ipynb，自己训练时候的ipynb</p><p>- selfdata_yolov3_test.ipynb，自己预测时候的ipynb </p><p>- yolo_matt.py,预测时候改进输出结果。</p><hr/><h2>1 数据准备</h2><p>最简单是因为把数据整理成以下的样子就可以开始训练：</p><div class=\"highlight\"><pre><code class=\"language-text\">path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3\npath/to/img2.jpg 120,300,250,600,2\n...</code></pre></div><p>也就是：地址，xmin,ymin,xmax,ymax，类别ID然后空格下一个box，每张图一行。 例子：</p><div class=\"highlight\"><pre><code class=\"language-text\">images/images_all/86900fb6gy1fl4822o7qmj22ao328qv7.jpg 10,259,399,580,27\nimages/images_all/b95fe9cbgw1eyw88vlifjj20c70hsq46.jpg 10,353,439,640,29\nimages/images_all/005CsCZ0jw1f1n8kcj8m1j30ku0kumz6.jpg 75,141,343,321,27</code></pre></div><hr/><h2>2 训练：</h2><p>keras源码中有两段训练：</p><ul><li>第一段冻结前面的249层进行迁移学习（原有的yolov3）</li><li>第二段解冻全部层进行训练</li></ul><p>笔者自己的训练数据集是专业领域的图像，所以基本第一阶段的迁移学习阶段没啥用，因为与原有的yolov3训练集差异太大，如果你也是，请直接开始第二段或者重新根据</p><a href=\"https://link.zhihu.com/?target=https%3A//pjreddie.com/media/files/darknet53.conv.74\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">darknet53</a><p>训练。 那么这边就有三样可能需要预下载的模型：</p><ul><li>yolo_weights.h5 预训练模型（用作迁移） <code>python convert.py -w yolov3.cfg yolov3.weights model_data/yolo_weights.h5</code></li><li>darknet53.weights （用作重新训练） <code>wget https://pjreddie.com/media/files/darknet53.conv.74</code></li><li>yolo.h5 （yolov3-VOC训练模型，可以直接用来做预测 ） <code>python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5</code></li></ul><p>来看看训练时候需要的参数：</p><div class=\"highlight\"><pre><code class=\"language-text\">class yolo_args:\n        annotation_path = &#39;train.txt&#39;\n        log_dir = &#39;logs/003/&#39;\n        classes_path = &#39;model_data/class_file_en.txt&#39;\n        anchors_path = &#39;model_data/yolo_anchors.txt&#39;\n        input_shape = (416,416) # multiple of 32, hw\n        # 608*608  416*416  320*320\n        val_split = 0.1\n        batch_size = 16\n        epochs_stage_1 = 10\n        stage_1_train = False\n        epochs_finally = 100\n        finally_train = True\n        weights_path =   &#39;logs/003/ep009-loss33.297-val_loss32.851.h5&#39;# 可以使用&#39;model_data/tiny_yolo_weights.h5&#39; 也可以使用tiny_yolo的：&#39;model_data/yolo_weights.h5&#39;\n\n\n\n    # train\n    _main(yolo_args)</code></pre></div><ul><li>annotation_path就是数据集准备的txt</li><li>log_dir ，Model存放地址，譬如：<code>events.out.tfevents.1545966202</code>、<code>ep077-loss19.318-val_loss19.682.h5</code></li><li>classes_path ，分类内容</li><li>anchors_path ，yolo anchors，可自行调整，也可以使用默认的</li><li> input_shape ，一般是416<br/> </li><li><code>epochs_stage_1 = 10</code>和 <code>stage_1_train = False</code>，是同一个，也就是是否进行迁移学习（<code>stage_1_train</code> ），要学习的话，学习几个epoch（<code>epochs_stage_1</code> ）<br/> </li><li><code>epochs_finally = 100</code>和 <code>finally_train = True</code> ，是，是否进行后面开放所有层的学习（<code>finally_train</code> ），学习几个epoch（<code>epochs_finally</code>）</li><li>weights_path ，调用model的路径</li></ul><p><b>这里需要注意：</b> 如果要在之前训练基础上，追加训练，一般要把batch_size设置小一些，然后加载之前的权重。</p><hr/><h2>3 预测：</h2><p>来看一个简单的预测</p><div class=\"highlight\"><pre><code class=\"language-text\">import sys\nimport argparse\nfrom yolo import YOLO, detect_video\nfrom PIL import Image\n\nyolo_test_args = {\n    &#34;model_path&#34;: &#39;model_data/yolo.h5&#39;,\n    &#34;anchors_path&#34;: &#39;model_data/yolo_anchors.txt&#39;,\n    &#34;classes_path&#34;: &#39;model_data/coco_classes.txt&#39;,\n    &#34;score&#34; : 0.3,\n    &#34;iou&#34; : 0.45,\n    &#34;model_image_size&#34; : (416, 416),\n    &#34;gpu_num&#34; : 1,\n}\n\n\nyolo_test = YOLO(**yolo_test_args)\nimage = Image.open(&#39;images/part1/path1.jpg&#39;)\nr_image = yolo_test.detect_image(image)\nr_image.show()</code></pre></div><p>直接返回的是带框的图片，如果你要输出boxes，可以自己改一下<code>detect_image</code>函数。</p><p>此时注意以下：<code>out_boxes, out_scores, out_classes</code>中out_boxes，每个Boxes代表的是：<code>y_min, x_min, y_max, x_max</code></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-49ebd018a4c3b500fa3f16ffb4cd5b55_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"854\" data-rawheight=\"615\" class=\"origin_image zh-lightbox-thumb\" width=\"854\" data-original=\"https://pic2.zhimg.com/v2-49ebd018a4c3b500fa3f16ffb4cd5b55_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;854&#39; height=&#39;615&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"854\" data-rawheight=\"615\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"854\" data-original=\"https://pic2.zhimg.com/v2-49ebd018a4c3b500fa3f16ffb4cd5b55_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-49ebd018a4c3b500fa3f16ffb4cd5b55_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在此之上，进行预测结果优化，可参考：<code>yolo_matt.py</code>:</p><div class=\"highlight\"><pre><code class=\"language-text\">import sys\nimport argparse\nfrom yolo_matt import YOLO, detect_video\nfrom PIL import Image\n\nyolo_test_args = {\n    &#34;model_path&#34;: &#39;logs/003/ep077-loss19.318-val_loss19.682.h5&#39;,\n    &#34;anchors_path&#34;: &#39;model_data/yolo_anchors.txt&#39;,\n    &#34;classes_path&#34;: &#39;model_data/class_file_en.txt&#39;,\n    &#34;score&#34; : 0.2,# 0.2\n    &#34;iou&#34; : 0.1,# 0.45\n    &#34;model_image_size&#34; : (416, 416),\n    &#34;gpu_num&#34; : 1,\n}\n\nyolo_test = YOLO(**yolo_test_args)\n\n# 输出内容整理\ndef yolov3_output(image,out_boxes,out_scores,out_classes):\n    output = []\n    for n,box in enumerate(out_boxes):\n        y_min, x_min, y_max, x_max = box\n        y_min = max(0, np.floor(y_min + 0.5).astype(&#39;int32&#39;))\n        x_min = max(0, np.floor(x_min + 0.5).astype(&#39;int32&#39;))\n        y_max = min(image.size[1], np.floor(y_max + 0.5).astype(&#39;int32&#39;))\n        x_max = min(image.size[0], np.floor(x_max + 0.5).astype(&#39;int32&#39;))\n        score = out_scores[n]\n        yo_class = yolo_classes[out_classes[n]]\n        output.append({ &#39;y_min&#39;:y_min, &#39;x_min&#39;:x_min, &#39;y_max&#39;:y_max, &#39;x_max&#39;:x_max,\\\n                       &#39;width&#39;:image.size[0],&#39;height&#39;:image.size[1],\\\n                       &#39;score&#39;:score,&#39;yo_class&#39;:yo_class})\n    return output\n\n    image = Image.open(&#39;images/images_all/path1.jpg&#39;)\n    r_image,out_boxes, out_scores, out_classes = yolo_test.detect_image(image)\n    output = yolov3_output(r_image,out_boxes,out_scores,out_classes)</code></pre></div><p>输出结果类似：</p><div class=\"highlight\"><pre><code class=\"language-text\">{\n&#39;path1.jpg&#39;: \n[{&#39;y_min&#39;: 416,   &#39;x_min&#39;: 34,   &#39;y_max&#39;: 754,   &#39;x_max&#39;: 367,   &#39;width&#39;: 440,   &#39;height&#39;: 783,   &#39;score&#39;: 0.9224778,   &#39;yo_class&#39;: &#39;class1&#39;},\n  {&#39;y_min&#39;: 428,   &#39;x_min&#39;: 3,   &#39;y_max&#39;: 783,   &#39;x_max&#39;: 352,   &#39;width&#39;: 440,   &#39;height&#39;: 783,   &#39;score&#39;: 0.2180994,   &#39;yo_class&#39;: &#39;class2&#39;}]\n  }</code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "目标检测", 
                    "tagLink": "https://api.zhihu.com/topics/19596960"
                }, 
                {
                    "tag": "图像识别", 
                    "tagLink": "https://api.zhihu.com/topics/19588774"
                }
            ], 
            "comments": [
                {
                    "userName": "嘻哈在发芽", 
                    "userLink": "https://www.zhihu.com/people/f841e9c81639e95262f5b6661891a304", 
                    "content": "<p>请问如果自己的数据集跟原始的相差不是很大，训练时相应的参数需要修改哪些？如 batch_size、epochs_stage_1 、epochs_finally、stage_1_train、finally_train？以及 weights_path = './logs/003/ep009-loss33.297-val_loss32.851.h5'是什么？难道不是用yolo_weights.h5 预训练模型吗（用作迁移）？</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>github上面下载的代码有问题 确少get_random_data</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "TuringYang", 
                    "userLink": "https://www.zhihu.com/people/ee697f219077f5b59deda01246b7f43b", 
                    "content": "<p>请问一下，达到你这个效果需要多少数据集？为啥我的训练完后，测试结果不能看，我这边只训练了100多张照片，是数据量少的原因吗？</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50645023", 
            "userName": "悟乙己", 
            "userLink": "https://www.zhihu.com/people/428693835688c688c6c9ac1d30fc0ece", 
            "upvote": 4, 
            "title": "Jupyter notebook最简原型界面设计- ipywidgets与lineup_widget", 
            "content": "<p>﻿Tkinter的GUI设计 和 django页面设计，那么笔者只是想快速做个demo原型，以上的内容能不能结合着来，有一些简单的交互 + web可以快速访问的到，于是就看到了jupyter notebook这两个库，非常简单的玩具，来看看呗~ ipywidgets比较强调输入项的各式花样，但是其对输出内容的格式的花样非常少。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>原文见：</p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/sinat_26917383/article/details/84345407\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-18a4850041d8fb47e16e3d6edcfe3de9_180x120.jpg\" data-image-width=\"1600\" data-image-height=\"843\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Jupyter notebook最简原型界面设计 - ipywidgets与lineup_widget</a><hr/><h2>一 ipywidgets</h2><p>文档：<a href=\"https://link.zhihu.com/?target=https%3A//ipywidgets.readthedocs.io/en/stable/index.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">ipywidgets.readthedocs.io</span><span class=\"invisible\">/en/stable/index.html</span><span class=\"ellipsis\"></span></a> </p><p>github：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/jupyter-widgets/ipywidgets\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/jupyter-widg</span><span class=\"invisible\">ets/ipywidgets</span><span class=\"ellipsis\"></span></a></p><p>安装：</p><div class=\"highlight\"><pre><code class=\"language-text\"># 方式一\npip install ipywidgets\njupyter nbextension enable --py widgetsnbextension\n# 方式二\nconda install -c conda-forge ipywidgets</code></pre></div><p>效果： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-18a4850041d8fb47e16e3d6edcfe3de9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"843\" class=\"origin_image zh-lightbox-thumb\" width=\"1600\" data-original=\"https://pic2.zhimg.com/v2-18a4850041d8fb47e16e3d6edcfe3de9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1600&#39; height=&#39;843&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"843\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1600\" data-original=\"https://pic2.zhimg.com/v2-18a4850041d8fb47e16e3d6edcfe3de9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-18a4850041d8fb47e16e3d6edcfe3de9_b.jpg\"/></figure><p> （参考自：</p><a href=\"https://link.zhihu.com/?target=https%3A//towardsdatascience.com/a-very-simple-demo-of-interactive-controls-on-jupyter-notebook-4429cf46aabd\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">A very simple demo of interactive controls on Jupyter notebook</a><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a5d0ce9446914b56e60c4e2a4aa164e6_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1064\" data-rawheight=\"720\" data-thumbnail=\"https://pic3.zhimg.com/v2-a5d0ce9446914b56e60c4e2a4aa164e6_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1064\" data-original=\"https://pic3.zhimg.com/v2-a5d0ce9446914b56e60c4e2a4aa164e6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1064&#39; height=&#39;720&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1064\" data-rawheight=\"720\" data-thumbnail=\"https://pic3.zhimg.com/v2-a5d0ce9446914b56e60c4e2a4aa164e6_b.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1064\" data-original=\"https://pic3.zhimg.com/v2-a5d0ce9446914b56e60c4e2a4aa164e6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a5d0ce9446914b56e60c4e2a4aa164e6_b.gif\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ef8a54350be1a3597de1b8aee80b33f3_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"870\" data-rawheight=\"680\" data-thumbnail=\"https://pic4.zhimg.com/v2-ef8a54350be1a3597de1b8aee80b33f3_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"870\" data-original=\"https://pic4.zhimg.com/v2-ef8a54350be1a3597de1b8aee80b33f3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;870&#39; height=&#39;680&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"870\" data-rawheight=\"680\" data-thumbnail=\"https://pic4.zhimg.com/v2-ef8a54350be1a3597de1b8aee80b33f3_b.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"870\" data-original=\"https://pic4.zhimg.com/v2-ef8a54350be1a3597de1b8aee80b33f3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ef8a54350be1a3597de1b8aee80b33f3_b.gif\"/></figure><p> 参考于：</p><a href=\"https://link.zhihu.com/?target=https%3A//towardsdatascience.com/interactive-visualizations-in-jupyter-notebook-3be02ab2b8cd\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Interactive Visualizations In Jupyter Notebook</a><p class=\"ztext-empty-paragraph\"><br/></p><p>来看一些组件与模块。</p><h2>1.1 基础组件</h2><p>主要参考：<a href=\"https://link.zhihu.com/?target=https%3A//ipywidgets.readthedocs.io/en/stable/examples/Widget%2520List.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Widget List</a></p><h2>1.1.1 button 按钮</h2><div class=\"highlight\"><pre><code class=\"language-text\">widgets.Button(\n    description=&#39;Click me&#39;,\n    disabled=False,\n    button_style=&#39;success&#39;, # &#39;success&#39;, &#39;info&#39;, &#39;warning&#39;, &#39;danger&#39; or &#39;&#39;\n    tooltip=&#39;Click me&#39;,\n    icon=&#39;check&#39;\n)\n\n\n# 调整按钮\nfrom ipywidgets import Button, Layout\n\nb = Button(description=&#39;(50% width, 80px height) button&#39;,\n           layout=Layout(width=&#39;50%&#39;, height=&#39;80px&#39;),\n          button_style=&#39;success&#39;)\nb</code></pre></div><p>button是作为输入项的， </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6305a88578484035ea88cde3c780ffc2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"528\" data-rawheight=\"91\" class=\"origin_image zh-lightbox-thumb\" width=\"528\" data-original=\"https://pic3.zhimg.com/v2-6305a88578484035ea88cde3c780ffc2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;528&#39; height=&#39;91&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"528\" data-rawheight=\"91\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"528\" data-original=\"https://pic3.zhimg.com/v2-6305a88578484035ea88cde3c780ffc2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6305a88578484035ea88cde3c780ffc2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>1.1.2 IntSlider、FloatSlider</h2><div class=\"highlight\"><pre><code class=\"language-text\">widgets.FloatSlider(\n    value=7.5,\n    min=0,\n    max=10.0,\n    step=0.1,\n    description=&#39;Test:&#39;,\n    disabled=False,\n    continuous_update=False,\n    orientation=&#39;horizontal&#39;,\n    readout=True,\n    readout_format=&#39;.1f&#39;,\n)</code></pre></div><p>一个整数型滑块，一个数值型滑块。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f47912f780ab5652e91ca62cb1c17aa0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"293\" data-rawheight=\"49\" class=\"content_image\" width=\"293\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;293&#39; height=&#39;49&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"293\" data-rawheight=\"49\" class=\"content_image lazy\" width=\"293\" data-actualsrc=\"https://pic1.zhimg.com/v2-f47912f780ab5652e91ca62cb1c17aa0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>1.1.3 FloatProgress / IntProgress</h2><div class=\"highlight\"><pre><code class=\"language-text\">widgets.IntProgress(\n    value=7,\n    min=0,\n    max=10,\n    step=1,\n    description=&#39;Loading:&#39;,\n    bar_style=&#39;&#39;, # &#39;success&#39;, &#39;info&#39;, &#39;warning&#39;, &#39;danger&#39; or &#39;&#39;\n    orientation=&#39;horizontal&#39;\n)\n\nwidgets.FloatProgress(\n    value=7.5,\n    min=0,\n    max=10.0,\n    step=0.1,\n    description=&#39;Loading:&#39;,\n    bar_style=&#39;info&#39;,\n    orientation=&#39;horizontal&#39;\n)</code></pre></div><p>一个整数型进度条，一个数值型进度条。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1c8734ff314374a1fb0d31f7d34aac44_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"324\" data-rawheight=\"37\" class=\"content_image\" width=\"324\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;324&#39; height=&#39;37&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"324\" data-rawheight=\"37\" class=\"content_image lazy\" width=\"324\" data-actualsrc=\"https://pic1.zhimg.com/v2-1c8734ff314374a1fb0d31f7d34aac44_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>1.1.4 Text、Textarea</h2><div class=\"highlight\"><pre><code class=\"language-text\">widgets.Text(\n    value=&#39;Hello World&#39;,\n    placeholder=&#39;Type something&#39;,\n    description=&#39;String:&#39;,\n    disabled=False\n)\n\n\nwidgets.Textarea(\n    value=&#39;Hello World&#39;,\n    placeholder=&#39;Type something&#39;,\n    description=&#39;String:&#39;,\n    disabled=False\n)</code></pre></div><p>一般来说，textarea比text更好用，模块是可伸缩的。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fc1ff1c3016fd5eea0b41640df5b691a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"733\" data-rawheight=\"456\" class=\"origin_image zh-lightbox-thumb\" width=\"733\" data-original=\"https://pic3.zhimg.com/v2-fc1ff1c3016fd5eea0b41640df5b691a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;733&#39; height=&#39;456&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"733\" data-rawheight=\"456\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"733\" data-original=\"https://pic3.zhimg.com/v2-fc1ff1c3016fd5eea0b41640df5b691a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fc1ff1c3016fd5eea0b41640df5b691a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>1.1.5 图片Image</h2><div class=\"highlight\"><pre><code class=\"language-text\">file = open(&#34;images/WidgetArch.png&#34;, &#34;rb&#34;)\nimage = file.read()\nwidgets.Image(\n    value=image,\n    format=&#39;png&#39;,\n    width=300,\n    height=400,\n)</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9935c837f45fa2a42f562dfe6ec3327b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"326\" data-rawheight=\"421\" class=\"content_image\" width=\"326\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;326&#39; height=&#39;421&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"326\" data-rawheight=\"421\" class=\"content_image lazy\" width=\"326\" data-actualsrc=\"https://pic4.zhimg.com/v2-9935c837f45fa2a42f562dfe6ec3327b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>1.2 单控件 - interact 简单交互</h2><div class=\"highlight\"><pre><code class=\"language-text\">from __future__ import print_function\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\ndef f(segx,opt):\n    if opt:\n        return segx\n    else:\n        return 1\ninteract(f, segx=10, opt = True)</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e7d940d26f3242f625260d3dd7f9c07f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"302\" data-rawheight=\"129\" class=\"content_image\" width=\"302\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;302&#39; height=&#39;129&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"302\" data-rawheight=\"129\" class=\"content_image lazy\" width=\"302\" data-actualsrc=\"https://pic4.zhimg.com/v2-e7d940d26f3242f625260d3dd7f9c07f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><code>interact</code>代表交互，第一个<code>f</code>代表函数，<code>segx</code>与<code>opt</code>都代表f函数的参数。 - segx代表滑条 - opt = True/False代表选项框</p><p>注意<code>interact</code>，相当于给函数<code>f</code>赋值，除了第一个，之后的参数都是第一个函数的参数，名字需对齐。</p><h2>1.3 单控件 - interact_manual简单交互</h2><div class=\"highlight\"><pre><code class=\"language-text\">from ipywidgets import FloatSlider\n# 横轴进度可以拖拽\ndef slow_function(i):\n    print(int(i),list(x for x in range(int(i)) if\n                str(x)==str(x)[::-1] and\n                str(x**2)==str(x**2)[::-1]))\n    return\n\ninteract_manual(slow_function,i=FloatSlider(min=1e5, max=1e7, step=1e5));</code></pre></div><p><code>FloatSlider</code>表示拖拽滑块，<code>interact_manual</code>(函数，函数参数)，此时函数参数是由拖拽滑块<code>FloatSlider</code>来确定。 <b>与interact的区别：</b> <code>interact</code>是实时改变，<code>interact_manual</code>是人工点击RUN才能执行一次。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-00b7bba2d8cca7375517701f01ce7086_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"991\" data-rawheight=\"135\" class=\"origin_image zh-lightbox-thumb\" width=\"991\" data-original=\"https://pic3.zhimg.com/v2-00b7bba2d8cca7375517701f01ce7086_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;991&#39; height=&#39;135&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"991\" data-rawheight=\"135\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"991\" data-original=\"https://pic3.zhimg.com/v2-00b7bba2d8cca7375517701f01ce7086_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-00b7bba2d8cca7375517701f01ce7086_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>1.4 单控件 - interactive_output + HBox交互</h2><div class=\"highlight\"><pre><code class=\"language-text\">from IPython.display import display, HTML\na = widgets.IntSlider()\nb = widgets.IntSlider()\nc = widgets.IntSlider()\nui = widgets.HBox([a, b, c])\n\ndef f(a, b, c):\n    print((a, b, c))\n\nout = widgets.interactive_output(f, {&#39;a&#39;: a, &#39;b&#39;: b, &#39;c&#39;: c})\n\ndisplay(ui, out)</code></pre></div><p>a,b,c是三个滑块，通过<code>widgets.HBox</code>进行拼接成为一个Box组件。 <code>interactive_output</code>（函数，函数参数），函数参数是一个组合Box组件。 <code>display</code>是展示滑块组合以及输出项。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6351212416320042e933b0ccd475170f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"934\" data-rawheight=\"80\" class=\"origin_image zh-lightbox-thumb\" width=\"934\" data-original=\"https://pic4.zhimg.com/v2-6351212416320042e933b0ccd475170f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;934&#39; height=&#39;80&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"934\" data-rawheight=\"80\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"934\" data-original=\"https://pic4.zhimg.com/v2-6351212416320042e933b0ccd475170f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6351212416320042e933b0ccd475170f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>1.5 单控件 - 文本交互</h2><div class=\"highlight\"><pre><code class=\"language-text\">widgets.Textarea(\n    value=&#39;Hello World&#39;,          # 默认语句\n    placeholder=&#39;Type something&#39;,\n    description=&#39;String:&#39;,        # 框的名字\n    disabled=False                # 是否可修改\n)</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-22d1a688fb3786221bb5f707deba7b73_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"297\" data-rawheight=\"65\" class=\"content_image\" width=\"297\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;297&#39; height=&#39;65&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"297\" data-rawheight=\"65\" class=\"content_image lazy\" width=\"297\" data-actualsrc=\"https://pic4.zhimg.com/v2-22d1a688fb3786221bb5f707deba7b73_b.jpg\"/></figure><p> Textarea是一个比较长的文本框作为输入项。</p><h2>1.6 两个控件 - 组合交互jslink</h2><div class=\"highlight\"><pre><code class=\"language-text\"># jslink\n# 两个控件的交互\na = widgets.FloatText()\nb = widgets.FloatSlider()\ndisplay(a,b)\n\nmylink = widgets.jslink((a, &#39;value&#39;), (b, &#39;value&#39;))</code></pre></div><p>jslink把控件a,b组合起来，a是文本控件，b是数值控件。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ac79a0c84ce5540c6569a692b150e0d7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"347\" data-rawheight=\"74\" class=\"content_image\" width=\"347\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;347&#39; height=&#39;74&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"347\" data-rawheight=\"74\" class=\"content_image lazy\" width=\"347\" data-actualsrc=\"https://pic4.zhimg.com/v2-ac79a0c84ce5540c6569a692b150e0d7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>1.7 多模块 - 控件独立分屏Accordion</h2><div class=\"highlight\"><pre><code class=\"language-text\">accordion = widgets.Accordion(children=[widgets.Text(), widgets.Text()])\naccordion.set_title(0, &#39;Text1&#39;)\naccordion.set_title(1, &#39;Text2&#39;)\naccordion</code></pre></div><p>可以把两个组件独立的链接在一起，而不是如<code>jslink</code>交互影响。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7c66844899d552fece7d59b2266c9eb3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"998\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb\" width=\"998\" data-original=\"https://pic4.zhimg.com/v2-7c66844899d552fece7d59b2266c9eb3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;998&#39; height=&#39;168&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"998\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"998\" data-original=\"https://pic4.zhimg.com/v2-7c66844899d552fece7d59b2266c9eb3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7c66844899d552fece7d59b2266c9eb3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\"># 选项分屏\ntab_contents = [&#39;P0&#39;, &#39;P1&#39;, &#39;P2&#39;, &#39;P3&#39;, &#39;P4&#39;]\nchildren = [widgets.Text(description=name) for name in tab_contents]\ntab = widgets.Tab()\ntab.children = children\nfor i in range(len(children)):\n    tab.set_title(i, str(i))\ntab</code></pre></div><p>多个控件独立组合。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8e4e3195ab682033c9dfdc38fcbc17ac_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"988\" data-rawheight=\"100\" class=\"origin_image zh-lightbox-thumb\" width=\"988\" data-original=\"https://pic1.zhimg.com/v2-8e4e3195ab682033c9dfdc38fcbc17ac_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;988&#39; height=&#39;100&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"988\" data-rawheight=\"100\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"988\" data-original=\"https://pic1.zhimg.com/v2-8e4e3195ab682033c9dfdc38fcbc17ac_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8e4e3195ab682033c9dfdc38fcbc17ac_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\"># 双模块分屏 + 选项分屏\ntab_nest = widgets.Tab()\ntab_nest.children = [accordion, accordion]\ntab_nest.set_title(0, &#39;An accordion&#39;)\ntab_nest.set_title(1, &#39;Copy of the accordion&#39;)\ntab_nest</code></pre></div><p>多个控件组合独立分开。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-11b17800c4c60faf76f9fe8675ab7a88_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"991\" data-rawheight=\"214\" class=\"origin_image zh-lightbox-thumb\" width=\"991\" data-original=\"https://pic1.zhimg.com/v2-11b17800c4c60faf76f9fe8675ab7a88_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;991&#39; height=&#39;214&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"991\" data-rawheight=\"214\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"991\" data-original=\"https://pic1.zhimg.com/v2-11b17800c4c60faf76f9fe8675ab7a88_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-11b17800c4c60faf76f9fe8675ab7a88_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>1.8 一些小案例</h2><h2>1.8.1 图形 + 滑块</h2><div class=\"highlight\"><pre><code class=\"language-text\">%matplotlib inline\nfrom ipywidgets import interactive\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef f(m, b):\n    plt.figure(2)\n    x = np.linspace(-10, 10, num=1000)\n    plt.plot(x, m * x + b)\n    plt.ylim(-5, 5)\n    plt.show()\n\ninteractive_plot = interactive(f, m=(-2.0, 2.0), b=(-3, 3, 0.5))     \n# m代表范围\noutput = interactive_plot.children[-1]\noutput.layout.height = &#39;350px&#39;\ninteractive_plot</code></pre></div><p>interactive（函数，函数参数），m/b都是可变滑块。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-dba4ccfeb7b249f3f33d93df5e7c4e3d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"398\" data-rawheight=\"329\" class=\"content_image\" width=\"398\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;398&#39; height=&#39;329&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"398\" data-rawheight=\"329\" class=\"content_image lazy\" width=\"398\" data-actualsrc=\"https://pic2.zhimg.com/v2-dba4ccfeb7b249f3f33d93df5e7c4e3d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>第二个案例： 来着：</p><a href=\"https://link.zhihu.com/?target=https%3A//ipython-books.github.io/33-mastering-widgets-in-the-jupyter-notebook/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-cd6936fafcfaf1261df4c0675e19f629_180x120.jpg\" data-image-width=\"919\" data-image-height=\"190\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">3.3. Mastering widgets in the Jupyter Notebook</a><div class=\"highlight\"><pre><code class=\"language-text\">@widgets.interact_manual(\n    color=[&#39;blue&#39;, &#39;red&#39;, &#39;green&#39;], lw=(1., 10.))\ndef plot(freq=1., color=&#39;blue&#39;, lw=2, grid=True):\n    t = np.linspace(-1., +1., 1000)\n    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n    ax.plot(t, np.sin(2 * np.pi * freq * t),\n            lw=lw, color=color)\n    ax.grid(grid)</code></pre></div><p><code>interact_manual</code>是单控件函数交互，此时通过装饰器，<code>interact_manual</code>（函数，函数参数）中的函数被隐去。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1654be7ee4c3eccdf4b694ef24336e86_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"547\" data-rawheight=\"525\" class=\"origin_image zh-lightbox-thumb\" width=\"547\" data-original=\"https://pic3.zhimg.com/v2-1654be7ee4c3eccdf4b694ef24336e86_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;547&#39; height=&#39;525&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"547\" data-rawheight=\"525\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"547\" data-original=\"https://pic3.zhimg.com/v2-1654be7ee4c3eccdf4b694ef24336e86_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1654be7ee4c3eccdf4b694ef24336e86_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>1.8.2 一个可控的进度读条</h2><div class=\"highlight\"><pre><code class=\"language-text\"># 一个可控的进度条\nplay = widgets.Play(\n#     interval=10,\n    value=0,\n    min=0,\n    max=100,\n    step=1,\n    description=&#34;Press play&#34;,\n    disabled=False\n)\n#slider = widgets.IntSlider()\nslider = widgets.FloatProgress(\n    value=50,\n    min=0,\n    max=100.0,\n    step=1,\n    description=&#39;Loading:&#39;,\n    bar_style=&#39;success&#39;,\n    orientation=&#39;horizontal&#39;\n)\n\nwidgets.jslink((play, &#39;value&#39;), (slider, &#39;value&#39;))\nwidgets.HBox([play, slider])</code></pre></div><p>Play是一个控制按钮，FloatProgress是一个数值进度条。 通过jslink将两个空间链接，点击按钮就Loading就可以开始走动。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c88393afa7caa0180e50d920b1867761_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"490\" data-rawheight=\"43\" class=\"origin_image zh-lightbox-thumb\" width=\"490\" data-original=\"https://pic2.zhimg.com/v2-c88393afa7caa0180e50d920b1867761_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;490&#39; height=&#39;43&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"490\" data-rawheight=\"43\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"490\" data-original=\"https://pic2.zhimg.com/v2-c88393afa7caa0180e50d920b1867761_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c88393afa7caa0180e50d920b1867761_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>1.8.3 颜色筛选器</h2><div class=\"highlight\"><pre><code class=\"language-text\"># 颜色选择器\nwidgets.ColorPicker(\n    concise=False,\n    description=&#39;Pick a color&#39;,\n    value=&#39;blue&#39;,\n    disabled=False\n)</code></pre></div><p>点击之后就会出现颜色筛选内容，筛选出来的结果为该颜色的具体数值，<code>#800080</code> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c9e5cd252fffdb68ec45880ca8eb86e8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"324\" data-rawheight=\"36\" class=\"content_image\" width=\"324\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;324&#39; height=&#39;36&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"324\" data-rawheight=\"36\" class=\"content_image lazy\" width=\"324\" data-actualsrc=\"https://pic1.zhimg.com/v2-c9e5cd252fffdb68ec45880ca8eb86e8_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1e7c1a1b4344e2ce270ba6f0b86eb08e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"534\" data-rawheight=\"407\" class=\"origin_image zh-lightbox-thumb\" width=\"534\" data-original=\"https://pic3.zhimg.com/v2-1e7c1a1b4344e2ce270ba6f0b86eb08e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;534&#39; height=&#39;407&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"534\" data-rawheight=\"407\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"534\" data-original=\"https://pic3.zhimg.com/v2-1e7c1a1b4344e2ce270ba6f0b86eb08e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1e7c1a1b4344e2ce270ba6f0b86eb08e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>1.8.4 复杂输入框</h2><div class=\"highlight\"><pre><code class=\"language-text\"># 复合功能\nfrom ipywidgets import Layout, Button, Box, FloatText, Textarea, Dropdown, Label, IntSlider\n\nform_item_layout = Layout(\n    display=&#39;flex&#39;,\n    flex_flow=&#39;row&#39;,\n    justify_content=&#39;space-between&#39;\n)\n\nform_items = [\n    Box([Label(value=&#39;Age of the captain&#39;), IntSlider(min=40, max=60)], layout=form_item_layout),\n    Box([Label(value=&#39;Egg style&#39;),\n         Dropdown(options=[&#39;Scrambled&#39;, &#39;Sunny side up&#39;, &#39;Over easy&#39;])], layout=form_item_layout),\n    Box([Label(value=&#39;Ship size&#39;),\n         FloatText()], layout=form_item_layout),\n    Box([Label(value=&#39;Information&#39;),\n         Textarea()], layout=form_item_layout)\n]\n\nform = Box(form_items, layout=Layout(\n    display=&#39;flex&#39;,\n    flex_flow=&#39;column&#39;,\n    border=&#39;solid 2px&#39;,\n    align_items=&#39;stretch&#39;,\n    width=&#39;50%&#39;\n))\nform</code></pre></div><p><code>form_item_layout</code>统一的Box布局， Dropdown是下拉框，一个Box是一个独立组件。 form_items是多个Box的组合，Box( [Label(),Textarea()] , layout   ) =&gt; Box( [前缀名,控件函数] , 布局   ) </p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c845967f5655c755b7ff039696b116eb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"510\" data-rawheight=\"165\" class=\"origin_image zh-lightbox-thumb\" width=\"510\" data-original=\"https://pic4.zhimg.com/v2-c845967f5655c755b7ff039696b116eb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;510&#39; height=&#39;165&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"510\" data-rawheight=\"165\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"510\" data-original=\"https://pic4.zhimg.com/v2-c845967f5655c755b7ff039696b116eb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c845967f5655c755b7ff039696b116eb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>.</p><hr/><h2>二 lineup_widget</h2><p>github：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/datavisyn/lineup_widget\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/datavisyn/li</span><span class=\"invisible\">neup_widget</span><span class=\"ellipsis\"></span></a></p><p>这是一个专门为展示dataframe + ipywidgets而来的包。</p><p>参考：</p><a href=\"https://link.zhihu.com/?target=https%3A//lineup.js.org/integrations/jupyter.html\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-35af1b9d53bc9823f64e2330fbe9be94_180x120.jpg\" data-image-width=\"500\" data-image-height=\"247\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Jupyter Widget</a><h2>2.1 安装</h2><div class=\"highlight\"><pre><code class=\"language-text\">## install Jupyter Widgets\npip install ipywidgets\njupyter nbextension enable --py widgetsnbextension\n\n## install library\npip install lineup_widget\njupyter nbextension enable --py --sys-prefix lineup_widget</code></pre></div><h2>2.2 主函数</h2><blockquote> w = lineup_widget.LineUpWidget(df, options=dict(rowHeight=20))<br/> </blockquote><div class=\"highlight\"><pre><code class=\"language-text\">_data = List(trait=Dict(), default_value=[]).tag(sync=True)\n  _columns = List(trait=Dict(), default_value=[]).tag(sync=True)\n  options = Dict(traits=dict(filterGlobally=Bool(), singleSelection=Bool(), noCriteriaLimits=Bool(), animated=Bool(),\n                             sidePanel=Enum((True, False, &#39;collapsed&#39;)), summaryHeader=Bool(), overviewMode=Bool(),\n                             hierarchyIndicator=Bool(), labelRotation=Int(), ignoreUnsupportedBrowser=Bool(),\n                             rowHeight=Int(), rowPadding=Int(), groupHeight=Int(), groupPadding=Int(),\n                             expandLineOnHover=Bool(), defaultSlopeGraphMode=Enum((&#39;item&#39;, &#39;band&#39;))),\n                 default_value=dict(filterGlobally=True, singleSelection=False, noCriteriaLimits=False, animated=True,\n                                    sidePanel=&#39;collapsed&#39;, summaryHeader=True, overviewMode=False,\n                                    hierarchyIndicator=True, labelRotation=0, ignoreUnsupportedBrowser=False,\n                                    rowHeight=18, rowPadding=2, groupHeight=40, groupPadding=5,\n                                    expandLineOnHover=False, defaultSlopeGraphMode=&#39;item&#39;\n                                    )).tag(sync=True)\n  rankings = List(trait=Dict(traits=dict(columns=List(trait=Union((Unicode(), Dict()))), sort_by=List(trait=Unicode()),\n                                         group_by=List(trait=Unicode())),\n                             default_value=dict(columns=[&#39;_*&#39;, &#39;*&#39;], sort_by=[], group_by=[])), default_value=[]).tag(\n    sync=True)</code></pre></div><p>其中options之中有非常多的参数，由于文档也没具体说明，笔者这边只对几个参数有了解。 其中：<code>sidePanel=Enum((True, False, &#39;collapsed&#39;))</code>代表侧边的面板是否打开，笔者觉得很碍人，一般是<code>sidePanel = False</code></p><h2>2.3 案例</h2><h2>案例一：</h2><div class=\"highlight\"><pre><code class=\"language-text\">import lineup_widget\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list(&#39;ABCD&#39;))\n\nw = lineup_widget.LineUpWidget(df)\nw.on_selection_changed(lambda selection: print(selection))\nw</code></pre></div><p>非常简单，唯一需要整理的就是df，一个DataFrame的格式作为输入，其他不用调整任何东西，就可以使用了。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9ff4bcc673a3715f393bd8bb46f49ba6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"915\" data-rawheight=\"514\" class=\"origin_image zh-lightbox-thumb\" width=\"915\" data-original=\"https://pic3.zhimg.com/v2-9ff4bcc673a3715f393bd8bb46f49ba6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;915&#39; height=&#39;514&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"915\" data-rawheight=\"514\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"915\" data-original=\"https://pic3.zhimg.com/v2-9ff4bcc673a3715f393bd8bb46f49ba6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9ff4bcc673a3715f393bd8bb46f49ba6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>案例二：</h2><div class=\"highlight\"><pre><code class=\"language-text\">from __future__ import print_function\nfrom ipywidgets import interact, interactive, interact_manual\n\ndef selection_changed(selection):\n    return df.iloc[selection]\n\ninteract(selection_changed, selection=lineup_widget.LineUpWidget(df));</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ce15794ec3652561d6b4d137b1d3e1ba_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"908\" data-rawheight=\"567\" class=\"origin_image zh-lightbox-thumb\" width=\"908\" data-original=\"https://pic3.zhimg.com/v2-ce15794ec3652561d6b4d137b1d3e1ba_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;908&#39; height=&#39;567&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"908\" data-rawheight=\"567\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"908\" data-original=\"https://pic3.zhimg.com/v2-ce15794ec3652561d6b4d137b1d3e1ba_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ce15794ec3652561d6b4d137b1d3e1ba_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>三 相似的Jupyter画图小模块</h2><p>参考于：</p><a href=\"https://link.zhihu.com/?target=https%3A//blog.jupyter.org/authoring-custom-jupyter-widgets-2884a462e724\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Authoring Custom Jupyter Widgets</a><h2>3.1 d3-slider widget</h2><p>This custom d3-slider widget wraps a simple custom slider based on the fantastic d3.js library. You can run and try it on the Binder repo or watch it on nbviewer.</p><div class=\"highlight\"><pre><code class=\"language-text\">pip install jupyter_widget_d3_slider</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-773f4f5aa2d55310b2b0256f46382f61_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"932\" data-rawheight=\"237\" data-thumbnail=\"https://pic2.zhimg.com/v2-773f4f5aa2d55310b2b0256f46382f61_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"932\" data-original=\"https://pic2.zhimg.com/v2-773f4f5aa2d55310b2b0256f46382f61_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;932&#39; height=&#39;237&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"932\" data-rawheight=\"237\" data-thumbnail=\"https://pic2.zhimg.com/v2-773f4f5aa2d55310b2b0256f46382f61_b.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"932\" data-original=\"https://pic2.zhimg.com/v2-773f4f5aa2d55310b2b0256f46382f61_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-773f4f5aa2d55310b2b0256f46382f61_b.gif\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>3.2 drawing-pad</h2><p>This small drawing pad app, is inspired from this codepen. You can run and try it on the Binder repo or watch it on nbviewer.</p><div class=\"highlight\"><pre><code class=\"language-text\">pip install jupyter-drawing-pad</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-38b8000995e1b45da53123c1e878b387_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"723\" data-rawheight=\"427\" class=\"origin_image zh-lightbox-thumb\" width=\"723\" data-original=\"https://pic4.zhimg.com/v2-38b8000995e1b45da53123c1e878b387_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;723&#39; height=&#39;427&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"723\" data-rawheight=\"427\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"723\" data-original=\"https://pic4.zhimg.com/v2-38b8000995e1b45da53123c1e878b387_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-38b8000995e1b45da53123c1e878b387_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>3.3 ipypivot</h2><p>The ipypivot widget, wraps the convenient PivotTable.js library. You can run and try it on the binder repo or watch it on nbviewer.</p><div class=\"highlight\"><pre><code class=\"language-text\">pip install ipypivot</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-625b12489182c849a5f4104e20f3a4ff_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"731\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb\" width=\"731\" data-original=\"https://pic4.zhimg.com/v2-625b12489182c849a5f4104e20f3a4ff_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;731&#39; height=&#39;430&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"731\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"731\" data-original=\"https://pic4.zhimg.com/v2-625b12489182c849a5f4104e20f3a4ff_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-625b12489182c849a5f4104e20f3a4ff_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "GUI设计", 
                    "tagLink": "https://api.zhihu.com/topics/19703816"
                }, 
                {
                    "tag": "Jupyter Notebook", 
                    "tagLink": "https://api.zhihu.com/topics/20064574"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/49491628", 
            "userName": "悟乙己", 
            "userLink": "https://www.zhihu.com/people/428693835688c688c6c9ac1d30fc0ece", 
            "upvote": 10, 
            "title": "基于腾讯AI Lab词向量进行未知词、短语向量补齐与域内相似词搜索", 
            "content": "<p></p><figure data-size=\"small\"><noscript><img src=\"https://pic1.zhimg.com/v2-8ed20c1e8f07313d950334d0995ddd4c_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"824\" data-rawheight=\"466\" class=\"origin_image zh-lightbox-thumb\" width=\"824\" data-original=\"https://pic1.zhimg.com/v2-8ed20c1e8f07313d950334d0995ddd4c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;824&#39; height=&#39;466&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"824\" data-rawheight=\"466\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"824\" data-original=\"https://pic1.zhimg.com/v2-8ed20c1e8f07313d950334d0995ddd4c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8ed20c1e8f07313d950334d0995ddd4c_b.jpg\"/></figure><p><i>（~~~免费广告位一则~~~）</i></p><p>AI Lab开源大规模高质量中文词向量数据，800万中文词随你用，质量非常高，就是一个词向量.txt文件都有16G之多，太夸张了。。不过的确非常有特点：</p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li> ⒈ 覆盖率（Coverage）：</li></ul><p>该词向量数据包含很多现有公开的词向量数据所欠缺的短语，比如“不念僧面念佛面”、“冰火两重天”、“煮酒论英雄”、“皇帝菜”、“喀拉喀什河”等。以“喀拉喀什河”为例，利用腾讯AI Lab词向量计算出的语义相似词如下：</p><p>墨玉河、和田河、玉龙喀什河、白玉河、喀什河、叶尔羌河、克里雅河、玛纳斯河</p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>⒉ 新鲜度（Freshness）：</li></ul><p>该数据包含一些最近一两年出现的新词，如“恋与制作人”、“三生三世十里桃花”、“打call”、“十动然拒”、“供给侧改革”、“因吹斯汀”等。以“因吹斯汀”为例，利用腾讯AI Lab词向量计算出的语义相似词如下：</p><p>一颗赛艇、因吹斯听、城会玩、厉害了word哥、emmmmm、扎心了老铁、神吐槽、可以说是非常爆笑了</p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>⒊ 准确性（Accuracy）：</li></ul><p>由于采用了更大规模的训练数据和更好的训练算法，所生成的词向量能够更好地表达词之间的语义关系。</p><p>腾讯AI Lab采用自研的Directional Skip-Gram (DSG)算法作为词向量的训练算法。DSG算法基于广泛采用的词向量训练算法Skip-Gram (SG)，在文本窗口中词对共现关系的基础上，额外考虑了词对的相对位置，以提高词向量语义表示的准确性。</p><hr/><h2>1  Tencent_AILab_ChineseEmbedding读入与高效查询</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>来看一下一个比较常见的读入方式：</p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/lvyufeng/keras_text_sum/blob/77d2f8beef5739fa51f507d06e63a916f9d7de87/utils/load_embedding.py\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">lvyufeng/keras_text_sum/load_embedding.py</a><div class=\"highlight\"><pre><code class=\"language-text\">import numpy as np\n\ndef load_embedding(path):\n    embedding_index = {}\n    f = open(path,encoding=&#39;utf8&#39;)\n    for index,line in enumerate(f):\n        if index == 0:\n            continue\n        values = line.split(&#39; &#39;)\n        word = values[0]\n        coefs = np.asarray(values[1:],dtype=&#39;float32&#39;)\n        embedding_index[word] = coefs\n    f.close()\n\n    return embedding_index\n\nload_embedding(&#39;/home/lv/data_set/Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding.txt&#39;)</code></pre></div><p>这样纯粹就是以字典的方式读入，当然用于建模没有任何问题，但是笔者想在之中进行一些相似性操作，最好的就是重新载入gensim.word2vec系统之中，但是笔者发现载入半天都会报错：</p><div class=\"highlight\"><pre><code class=\"language-text\">ValueError: invalid vector on line 418987 (is this really the text format?)</code></pre></div><p>仔细一查看，发现原来一些词向量的词就是数字，譬如<code>-0.2121</code>或 <code>57851</code>，所以一直导入不进去。只能自己用txt读入后，删除掉这一部分，保存的格式参考下面。</p><div class=\"highlight\"><pre><code class=\"language-text\">5 4\n是 -0.119938 0.042054504 -0.02282253 -0.10101332\n中国人 0.080497965 0.103521846 -0.13045108 -0.01050107\n你 -0.0788643 -0.082788676 -0.14035964 0.09101376\n我 -0.14597991 0.035916027 -0.120259814 -0.06904249</code></pre></div><p>第一行是一共5个词，每个词维度为4.</p><p>然后清洗完毕之后，就可以读入了：</p><div class=\"highlight\"><pre><code class=\"language-text\">wv_from_text = gensim.models.KeyedVectors.load_word2vec_format(&#39;Tencent_AILab_ChineseEmbedding_refine.txt&#39;,binary=False)</code></pre></div><p>但是又是一个问题，占用内存太大，导致不能查询相似词，所以这里可以用一下这个神奇的函数，可以高效运行，这样就可以顺利使用<code>most_similar</code>这类函数了：</p><div class=\"highlight\"><pre><code class=\"language-text\">wv_from_text.init_sims(replace=True)  # 神奇，很省内存，可以运算most_similar</code></pre></div><p>该操作是指model已经不再继续训练了，那么就锁定起来，让Model变为只读的，这样可以预载相似度矩阵，对于后面得相似查询非常有利。</p><hr/><h2>2 未知词、短语向量补齐与域内相似词搜索</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>这边未知词语、短语的补齐手法是参考FastText的用法：</p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/sinat_26917383/article/details/83041424\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-b35da9d78dbe87fa6e7452f4b6b97140_120x160.jpg\" data-image-width=\"648\" data-image-height=\"880\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">极简使用︱Gemsim-FastText 词向量训练以及OOV（out-of-word）问题有效解决</a><p>这边笔者借鉴了fasttext之中的方式，当出现未登录词或短语的时候，会： - 先将输入词进行n-grams - 然后去词表之中查找 - 查找到的词向量进行平均</p><p>主要函数可见：</p><div class=\"highlight\"><pre><code class=\"language-text\">import numpy as np\n\ndef compute_ngrams(word, min_n, max_n):\n    #BOW, EOW = (&#39;&lt;&#39;, &#39;&gt;&#39;)  # Used by FastText to attach to all words as prefix and suffix\n    extended_word =  word\n    ngrams = []\n    for ngram_length in range(min_n, min(len(extended_word), max_n) + 1):\n        for i in range(0, len(extended_word) - ngram_length + 1):\n            ngrams.append(extended_word[i:i + ngram_length])\n    return list(set(ngrams))\n\n\ndef wordVec(word,wv_from_text,min_n = 1, max_n = 3):\n    &#39;&#39;&#39;\n    ngrams_single/ngrams_more,主要是为了当出现oov的情况下,最好先不考虑单字词向量\n    &#39;&#39;&#39;\n    # 确认词向量维度\n    word_size = wv_from_text.wv.syn0[0].shape[0]   \n    # 计算word的ngrams词组\n    ngrams = compute_ngrams(word,min_n = min_n, max_n = max_n)\n    # 如果在词典之中，直接返回词向量\n    if word in wv_from_text.wv.vocab.keys():\n        return wv_from_text[word]\n    else:  \n        # 不在词典的情况下\n        word_vec = np.zeros(word_size, dtype=np.float32)\n        ngrams_found = 0\n        ngrams_single = [ng for ng in ngrams if len(ng) == 1]\n        ngrams_more = [ng for ng in ngrams if len(ng) &gt; 1]\n        # 先只接受2个单词长度以上的词向量\n        for ngram in ngrams_more:\n            if ngram in wv_from_text.wv.vocab.keys():\n                word_vec += wv_from_text[ngram]\n                ngrams_found += 1\n                #print(ngram)\n        # 如果，没有匹配到，那么最后是考虑单个词向量\n        if ngrams_found == 0:\n            for ngram in ngrams_single:\n                word_vec += wv_from_text[ngram]\n                ngrams_found += 1\n        if word_vec.any():\n            return word_vec / max(1, ngrams_found)\n        else:\n            raise KeyError(&#39;all ngrams for word %s absent from model&#39; % word)\n\nvec = wordVec(&#39;千奇百怪的词向量&#39;,wv_from_text,min_n = 1, max_n = 3)  # 词向量获取\nwv_from_text.most_similar(positive=[vec], topn=10)    # 相似词查找</code></pre></div><p><code>compute_ngrams</code>函数是将词条N-grams找出来，譬如：</p><div class=\"highlight\"><pre><code class=\"language-text\">compute_ngrams(&#39;萌萌的哒的&#39;,min_n = 1,max_n = 3)\n&gt;&gt;&gt; [&#39;哒&#39;, &#39;的哒的&#39;, &#39;萌的&#39;, &#39;的哒&#39;, &#39;哒的&#39;, &#39;萌萌的&#39;, &#39;萌的哒&#39;, &#39;的&#39;, &#39;萌萌&#39;, &#39;萌&#39;]</code></pre></div><p>这边没有沿用fasttext之中的<code>&lt;&gt;</code>来区分词头、词尾。</p><p><code>wordVec</code>函数是计算未登录词的，其中笔者小小加了一些内容，就是：当出现oov的情况下,最好先不考虑单字词向量，如果能匹配到两个字以上的内容就优先进行平均。</p><p>在得到未登录词或短语的向量之后，就可以快速进行查找，gensim里面是支持给入向量进行相似词查找：</p><div class=\"highlight\"><pre><code class=\"language-text\">wv_from_text.most_similar(positive=[vec], topn=10)</code></pre></div><p>其实，有了这么一个小函数 + 稍微大内存的服务器，就可以开始挖金矿了，笔者在此给出一部分可供参考与使用的小案例，案例中找出来的相似肯定还是不那么干净，需要自行清洗一下：</p><h2>网络用语挖掘：</h2><div class=\"highlight\"><pre><code class=\"language-text\">vec = wordVec(&#39;天了噜&#39;,wv_from_text,min_n = 1, max_n = 3)\nwv_from_text.most_similar(positive=[vec], topn=20)\n\n[(&#39;天了噜&#39;, 1.0),\n (&#39;天啦噜&#39;, 0.910751223564148),\n (&#39;天惹&#39;, 0.8336831331253052),\n (&#39;我的天呐&#39;, 0.8315592408180237),\n (&#39;天哪噜&#39;, 0.8200887441635132),\n (&#39;也是醉了&#39;, 0.8048921823501587),\n (&#39;哦买噶&#39;, 0.7951157093048096),\n (&#39;我也是醉了&#39;, 0.7925893664360046),\n (&#39;我的天哪&#39;, 0.7903991937637329),\n (&#39;天呐&#39;, 0.7862901091575623)\n......\n]</code></pre></div><h2>评论观点</h2><div class=\"highlight\"><pre><code class=\"language-text\">vec = wordVec(&#39;真难吃&#39;,wv_from_text,min_n = 1, max_n = 3)\nwv_from_text.most_similar(positive=[vec], negative=[&#39;好吃&#39;], topn=20)\n\n[(&#39;真难&#39;, 0.8344259858131409),\n (&#39;难吃&#39;, 0.8344259262084961),\n (&#39;不好吃&#39;, 0.7413374185562134),\n (&#39;难啊&#39;, 0.7120314836502075),\n (&#39;难喝&#39;, 0.6996017694473267),\n (&#39;难以下咽&#39;, 0.6920732259750366),\n (&#39;好难&#39;, 0.6856701374053955),\n (&#39;挺好吃&#39;, 0.6801191568374634),\n (&#39;真不容易&#39;, 0.6788320541381836),\n (&#39;真的很难&#39;, 0.671592116355896),\n (&#39;真的很好吃&#39;, 0.6692471504211426),\n...</code></pre></div><p>例子2：</p><div class=\"highlight\"><pre><code class=\"language-text\">vec = wordVec(&#39;环境干净&#39;,wv_from_text,min_n = 1, max_n = 3)\nwv_from_text.most_similar(positive=[vec], topn=20)\n\n[(&#39;环境干净&#39;, 0.9999998807907104),\n (&#39;环境干净整洁&#39;, 0.8523852825164795),\n (&#39;环境舒适&#39;, 0.8281853199005127),\n (&#39;环境干净卫生&#39;, 0.8241869211196899),\n (&#39;卫生干净&#39;, 0.8118663430213928),\n (&#39;干净卫生&#39;, 0.7971832156181335),\n (&#39;干净舒适&#39;, 0.796349287033081),\n (&#39;环境清新&#39;, 0.7937666773796082),\n (&#39;卫生好&#39;, 0.7925254702568054),\n (&#39;环境整洁&#39;, 0.7919654846191406),\n (&#39;环境好&#39;, 0.7814522981643677),\n (&#39;房间干净&#39;, 0.7802159786224365),\n (&#39;环境优雅&#39;, 0.7685255408287048),</code></pre></div><h2>同义词挖掘</h2><div class=\"highlight\"><pre><code class=\"language-text\">vec = wordVec(&#39;苹果&#39;,wv_from_text,min_n = 1, max_n = 3)\nwv_from_text.most_similar(positive=[vec],negative=[&#39;水果&#39;], topn=20)\n\n[(&#39;苹果公司&#39;, 0.5877306461334229),\n (&#39;苹果开发&#39;, 0.5226757526397705),\n (&#39;高通&#39;, 0.5215991735458374),\n (&#39;谷歌&#39;, 0.5213730335235596),\n (&#39;苹果的iphone&#39;, 0.5150437355041504),\n (&#39;微软&#39;, 0.5127487778663635),\n (&#39;苹果新&#39;, 0.5012987852096558),\n (&#39;pixel手机&#39;, 0.49072039127349854),\n (&#39;苹果高管&#39;, 0.4897959530353546),\n (&#39;苹果iphone&#39;, 0.4875335991382599),\n (&#39;苹果手机iphone&#39;, 0.4791686534881592),\n (&#39;苹果芯片&#39;, 0.47766292095184326),\n (&#39;iphone&#39;, 0.4754045307636261),</code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "词向量", 
                    "tagLink": "https://api.zhihu.com/topics/20142325"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/48167933", 
            "userName": "悟乙己", 
            "userLink": "https://www.zhihu.com/people/428693835688c688c6c9ac1d30fc0ece", 
            "upvote": 54, 
            "title": "比赛必备 ︱ 省力搞定三款词向量训练 + OOV词向量问题的可性方案", 
            "content": "<p>﻿本篇为资源汇总，一些NLP的比赛在抽取文本特征的时候会使用非常多的方式。 </p><ul><li>- 传统的有：TFIDF/LDA/LSI等 </li><li>- 偏深度的有：word2vec/glove/fasttext等 </li><li>- 还有一些预训练方式：elmo / bert</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e7f0d2794939bedd6bb88caec76ea47a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"169\" data-rawheight=\"413\" class=\"content_image\" width=\"169\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;169&#39; height=&#39;413&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"169\" data-rawheight=\"413\" class=\"content_image lazy\" width=\"169\" data-actualsrc=\"https://pic3.zhimg.com/v2-e7f0d2794939bedd6bb88caec76ea47a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><hr/><h2>1 之前的几款词向量介绍与训练帖子</h2><p>glove：</p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/sinat_26917383/article/details/54847240\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-33ff6d5be0745c7f4995015954144ed2_180x120.jpg\" data-image-width=\"808\" data-image-height=\"448\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">NLP︱高级词向量表达（一）--GloVe（理论、相关测评结果、R&amp;python实现、相关应用） - 素质云笔记/Recorder... - CSDN博客</a><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/sinat_26917383/article/details/83029140\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">极简使用︱Glove-python词向量训练与使用 - 素质云笔记/Recorder... - CSDN博客</a><p class=\"ztext-empty-paragraph\"><br/></p><p>fasttext：</p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/sinat_26917383/article/details/54850933\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-237e8e9a0526bceacffd6366b6811f0e_ipico.jpg\" data-image-width=\"364\" data-image-height=\"389\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">NLP︱高级词向量表达（二）--FastText（简述、学习笔记） - 素质云笔记/Recorder... - CSDN博客</a><p class=\"ztext-empty-paragraph\"><br/></p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/sinat_26917383/article/details/78367905\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-7da8f90f5e9c3425b24c5498a165a3d9_180x120.jpg\" data-image-width=\"1014\" data-image-height=\"361\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">fastrtext︱R语言使用facebook的fasttext快速文本分类算法 - 素质云笔记/Recorder... - CSDN博客</a><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/sinat_26917383/article/details/83041424\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-ca3a0c394a580ba01cf4e8b602382f78_120x160.jpg\" data-image-width=\"648\" data-image-height=\"880\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">极简使用︱Gemsim-FastText 词向量训练以及OOV（out-of-word）问题有效解决</a><p>word2vec：</p><p class=\"ztext-empty-paragraph\"><br/></p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/sinat_26917383/article/details/69803018\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-40dd9c275998428a5ddc25c638af037d_180x120.jpg\" data-image-width=\"843\" data-image-height=\"313\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">python︱gensim训练word2vec及相关函数与功能理解 - 素质云笔记/Recorder... - CSDN博客</a><p class=\"ztext-empty-paragraph\"><br/></p><p>tfidf： </p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/sinat_26917383/article/details/71436563\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">sklearn+gensim︱jieba分词、词袋doc2bow、TfidfVectorizer - 素质云笔记/Recorder... - CSDN博客</a><hr/><h2>2  极简训练glove/word2vec/fasttext</h2><h2>2.1 word2vec的训练与简易使用</h2><p>gensim里面可以快速的进行训练word2vec。</p><div class=\"highlight\"><pre><code class=\"language-text\"># 最简单的开始\nimport gensim\nsentences = [[&#39;first&#39;, &#39;sentence&#39;], [&#39;second&#39;, &#39;sentence&#39;,&#39;is&#39;]]\n\n# 模型训练\nmodel = gensim.models.Word2Vec(sentences, min_count=1)\n    # min_count,频数阈值，大于等于1的保留\n    # size，神经网络 NN 层单元数，它也对应了训练算法的自由程度\n    # workers=4，default = 1 worker = no parallelization 只有在机器已安装 Cython 情况下才会起到作用。如没有 Cython，则只能单核运行。</code></pre></div><p><b>几个常用功能的列举：</b> - 如何获取词向量？</p><div class=\"highlight\"><pre><code class=\"language-text\">model[&#39;computer&#39;]  # raw NumPy vector of a word\narray([-0.00449447, -0.00310097,  0.02421786, ...], dtype=float32)\n\nmodel.wv[&#39;word&#39;]</code></pre></div><ul><li>如何获取词表？</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">model.wv.vocab.keys()</code></pre></div><ul><li>如何求相似？</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">y2=model.similarity(u&#34;好&#34;, u&#34;还行&#34;)     # 比较两个词的相似\nprint(y2)\n\nfor i in model.most_similar(u&#34;滋润&#34;):  # 一个词相似词有哪些\n    print i[0],i[1]</code></pre></div><hr/><h2>2.2 glove的训练与简易使用</h2><p>比较快的有一个封装比较好的库<a href=\"https://link.zhihu.com/?target=https%3A//github.com/maciejkula/glove-python\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">maciejkula/glove-python</a>:</p><div class=\"highlight\"><pre><code class=\"language-text\">pip install glove_python</code></pre></div><p>该库可以快速训练了，而且与gensim使用方式即为相似，给好评：</p><div class=\"highlight\"><pre><code class=\"language-text\">from __future__ import print_function\nimport argparse\nimport pprint\nimport gensim\nfrom glove import Glove\nfrom glove import Corpus\n\nsentense = [[&#39;你&#39;,&#39;是&#39;,&#39;谁&#39;],[&#39;我&#39;,&#39;是&#39;,&#39;中国人&#39;]]\ncorpus_model = Corpus()\ncorpus_model.fit(sentense, window=10)\n#corpus_model.save(&#39;corpus.model&#39;)\nprint(&#39;Dict size: %s&#39; % len(corpus_model.dictionary))\nprint(&#39;Collocations: %s&#39; % corpus_model.matrix.nnz)</code></pre></div><p>其中<code>corpus_model.fit(corpus, window=10, ignore_missing=False)</code>ignore_missing代表如果出现OOV的词，该如何处理。 来看一下原文描述这个ignore_missing：</p><blockquote> whether to ignore words missing from  the dictionary (if it was supplied).Context window distances will be preserved even if out-of-vocabulary words are ignored. If False, a KeyError is raised.<br/> </blockquote><p><b>几个常用功能的列举：</b> - 如何获取词向量？</p><div class=\"highlight\"><pre><code class=\"language-text\"># 全部词向量矩阵\nglove.word_vectors\n# 指定词条词向量\nglove.word_vectors[glove.dictionary[&#39;你&#39;]]</code></pre></div><ul><li>如何求相似？</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">glove.most_similar(&#39;我&#39;, number=10)\n\n&gt;&gt;&gt; [(&#39;中国人&#39;, 0.15130809810072138),\n&gt;&gt;&gt;  (&#39;你&#39;, 0.0739901044877504),\n&gt;&gt;&gt;  (&#39;谁&#39;, -0.05137569131012555),\n&gt;&gt;&gt;  (&#39;是&#39;, -0.08668606334919005)]</code></pre></div><hr/><h2>2.3 fasttext的训练与简易使用</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>fasttext的训练，facebook有自己的训练方式：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/facebookresearch/fastText\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">facebookresearch/fastText</a>，不过训练起来还挺费劲，对新手不友好。那么gensim在新版本里面已经封装了fasttext，也挺好用，已经满足了基本要求。</p><div class=\"highlight\"><pre><code class=\"language-text\">from gensim.models import FastText\nsentences = [[&#34;你&#34;, &#34;是&#34;, &#34;谁&#34;], [&#34;我&#34;, &#34;是&#34;, &#34;中国人&#34;]]\n\nmodel = FastText(sentences,  size=4, window=3, min_count=1, iter=10,min_n = 3 , max_n = 6,word_ngrams = 0)\nmodel[&#39;你&#39;]  # 词向量获得的方式\nmodel.wv[&#39;你&#39;] # 词向量获得的方式</code></pre></div><p>同时gensim里面既有py版本的，也有c++版本的。</p><div class=\"highlight\"><pre><code class=\"language-text\"># 使用c++ 版本的fasttext\nfrom gensim.models.wrappers.fasttext import FastText as FT_wrapper\n\n# Set FastText home to the path to the FastText executable\nft_home = &#39;/home/chinmaya/GSOC/Gensim/fastText/fasttext&#39;\n\n# train the model\nmodel_wrapper = FT_wrapper.train(ft_home, lee_train_file)\n\nprint(model_wrapper)</code></pre></div><p><b>几个常用功能的列举：</b> - 如何获取词向量？</p><div class=\"highlight\"><pre><code class=\"language-text\">model[&#39;你&#39;]  # 词向量获得的方式\nmodel.wv[&#39;你&#39;] # 词向量获得的方式</code></pre></div><ul><li>如何获取词表？</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">model.wv.vocab</code></pre></div><ul><li>如何求相似？</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">model.wv.similarity(&#39;你&#39;, &#39;是&#39;)  # 求相似\nmodel.n_similarity([&#39;cat&#39;, &#39;say&#39;], [&#39;dog&#39;, &#39;say&#39;])  # 多个词条求相似\nmodel.most_similar(&#34;滋润&#34;)  # 求词附近的相似词</code></pre></div><p>similarity求两个词之间的相似性；n_similarity为求多个词之间的相似性 其中还可以求词条之间的WMD距离：</p><div class=\"highlight\"><pre><code class=\"language-text\"># !pip3 install pyemd \nmodel.wmdistance([&#39;cat&#39;, &#39;say&#39;], [&#39;dog&#39;, &#39;say&#39;]) # 求词条之间的WMD距离</code></pre></div><hr/><h2>2.4 elmo 预训练模型</h2><p>在ELMo 中，每个单词被赋予一个表示，它是它们所属的整个语料库句子的函数。所述的嵌入来自于计算一个两层双向语言模型（LM）的内部状态，因此得名「ELMo」：Embeddings from Language Models。 笔者在本篇里面记叙了一下自己在之前尝试的时候看到比较好的训练开源项目： </p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/sinat_26917383/article/details/81913790\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-88660673a0375f06033dff139a0cce63_180x120.jpg\" data-image-width=\"669\" data-image-height=\"422\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">流水账︱Elmo词向量中文训练过程杂记 - 素质云笔记/Recorder... - CSDN博客</a><p> 一共有三个中文训练的源头： </p><p>（1）可参考：</p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/searobbersduck/ELMo_Chin\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">searobbersduck/ELMo_Chin</a><p>，不过好像过程中有些问题，笔者还没证实原因。 </p><p>（2）博文：</p><a href=\"https://link.zhihu.com/?target=http%3A//www.linzehui.me/2018/08/12/%25E7%25A2%258E%25E7%2589%2587%25E7%259F%25A5%25E8%25AF%2586%25E7%2582%25B9/%25E5%25A6%2582%25E4%25BD%2595%25E5%25B0%2586ELMo%25E8%25AF%258D%25E5%2590%2591%25E9%2587%258F%25E7%2594%25A8%25E4%25BA%258E%25E4%25B8%25AD%25E6%2596%2587/\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">《如何将ELMo词向量用于中文》</a><p>，该教程用glove作为初始化向量，思路如下：</p><ul><li>将预训练的词向量读入</li><li>修改bilm-tf代码</li><ul><li>option部分</li><li>添加给embedding weight赋初值</li><li>添加保存embedding weight的代码</li></ul><li>开始训练，获得checkpoint和option文件</li><li>运行脚本，获得language model的weight文件</li><li>将embedding weight保存为hdf5文件形式</li><li>运行脚本，将语料转化成ELMo embedding。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>（3）</p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/HIT-SCIR/ELMoForManyLangs\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-8c4743f9da92b3c0b5f1b1e328777ee8_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">HIT-SCIR/ELMoForManyLangs</a><p>，哈工大今年CoNLL评测的多国语ELMo，有繁体中文。</p><hr/><h2>2.5 BERT预训练模型</h2><p>BERT预训练笔者未尝试，给出几个开源项目：</p><p><b>1 </b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/brightmart/bert_language_understanding\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-b246340e0afa4b76ec0d31d6e42db6c3_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">brightmart/bert_language_understanding</a><p>该篇的一个新闻稿：</p><a href=\"https://link.zhihu.com/?target=https%3A//www.jiqizhixin.com/articles/2018-10-30-13\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-1769a94eb7a1f0b6d9ef2a7657bc8abf_180x120.jpg\" data-image-width=\"1080\" data-image-height=\"286\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">预训练BERT，官方代码发布前他们是这样用TensorFlow解决的</a><p> 用tensorflow实现的，下面一个效果图是私人测试，还是挺明显的： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-9a7cc8105f6e7c4ff8d9901c3f89c331_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"838\" data-rawheight=\"164\" class=\"origin_image zh-lightbox-thumb\" width=\"838\" data-original=\"https://pic2.zhimg.com/v2-9a7cc8105f6e7c4ff8d9901c3f89c331_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;838&#39; height=&#39;164&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"838\" data-rawheight=\"164\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"838\" data-original=\"https://pic2.zhimg.com/v2-9a7cc8105f6e7c4ff8d9901c3f89c331_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-9a7cc8105f6e7c4ff8d9901c3f89c331_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>2 </p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/codertimo/BERT-pytorch\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-ea60dd1b93e44a49b3daded645c6579f_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">codertimo/BERT-pytorch</a><a href=\"https://link.zhihu.com/?target=https%3A//www.jiqizhixin.com/articles/2018-10-18-14\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-b8f3dbea2391a73eaf9aa1a2d21adb09_180x120.jpg\" data-image-width=\"806\" data-image-height=\"299\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">最强预训练模型BERT的Pytorch实现（非官方）</a><p>3 </p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/google-research/bert\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-a773b59dd18af6ec21208560afedd39d_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">google-research/bert</a><p class=\"ztext-empty-paragraph\"><br/></p><hr/><h2>2.6 已有的中文的词向量举例</h2><h2>2.6.1 facebook Pre-trained word vectors</h2><p>facebook公开了90种语言的Pre-trained word vectors，使用默认参数，300维度，链接：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/facebookrese</span><span class=\"invisible\">arch/fastText/blob/master/pretrained-vectors.md</span><span class=\"ellipsis\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-314080eec65e7055d91d7dec06880a1e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"836\" data-rawheight=\"859\" class=\"origin_image zh-lightbox-thumb\" width=\"836\" data-original=\"https://pic3.zhimg.com/v2-314080eec65e7055d91d7dec06880a1e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;836&#39; height=&#39;859&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"836\" data-rawheight=\"859\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"836\" data-original=\"https://pic3.zhimg.com/v2-314080eec65e7055d91d7dec06880a1e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-314080eec65e7055d91d7dec06880a1e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.6.2 Embedding/Chinese-Word-Vectors 中文词向量大汇总</h2><blockquote>This project provides 100+ Chinese Word Vectors (embeddings) trained with different representations (dense and sparse), context features (word, ngram, character, and more), and corpora. One can easily obtain pre-trained vectors with different properties and use them for downstream tasks.</blockquote><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4da9b5fa0fe4655056b8778241ef76f9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"854\" data-rawheight=\"502\" class=\"origin_image zh-lightbox-thumb\" width=\"854\" data-original=\"https://pic2.zhimg.com/v2-4da9b5fa0fe4655056b8778241ef76f9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;854&#39; height=&#39;502&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"854\" data-rawheight=\"502\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"854\" data-original=\"https://pic2.zhimg.com/v2-4da9b5fa0fe4655056b8778241ef76f9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-4da9b5fa0fe4655056b8778241ef76f9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.6.3  腾讯AI Lab开源大规模高质量中文词向量数据</h2><blockquote>This corpus provides 200-dimension vector representations, a.k.a. embeddings, for over 8 million Chinese words and phrases, which are pre-trained on large-scale high-quality data. These vectors, capturing semantic meanings for Chinese words and phrases, can be widely applied in many downstream Chinese processing tasks (e.g., named entity recognition and text classification) and in further research.</blockquote><p>地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//ai.tencent.com/ailab/nlp/embedding.html\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Embedding Dataset -- NLP Center, Tencent AI Lab</a><p class=\"ztext-empty-paragraph\"><br/></p><hr/><h2>3 OOV(out of vocabulary，OOV)未登录词向量问题</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>未登录词又称为生词（unknown word），可以有两种解释：一是指已有的词表中没有收录的词；二是指已有的训练语料中未曾出现过的词。在第二种含义下，未登录词又称为集外词（out of vocabulary, OOV），即训练集以外的词。通常情况下将OOV与未登录词看作一回事。</p><p>未登录词可以粗略划分为如下几种类型： </p><ul><li>①新出现的普通词汇，如博客、房奴、给力等，尤其在网络用语中这种词汇层出不穷。</li><li> ②专有名词（proper names）。专有名词在早期主要是指人名、地名和组织机构名这三类实体名称。1996年第六届信息理解会议对此进行了扩展，首次提出了命名实体（named entity）的概念，新增了时间和数字表达（日期、时刻、时段、数量值、百分比、序数、货币数量等），并且地名被进一步细化为城市名、州（省）名和国家名称等。 </li><li>③专业名词和研究领域名称。特定领域的专业名词和新出现的研究领域名称也是造成生词的原因之一，如三聚氰胺、苏丹红、禽流感、堰塞湖等。</li><li> ④其他专用名词，如新出现的产品名，电影、书籍等文艺作品的名称，等等。</li></ul><p>该问题在kaggle的《Toxic Comment Classification Challenge》提供了一些解决办法。</p><h2>3.1 fasttext 解决OOV的词向量最佳方案</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>其中在《About my 0.9872 single model》中提及fasttext的使用：</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote>Fixed misspellings by finding word vector neighborhoods. Fasttext tool can create vectors for out-of-dictionary words which is really nice. I trained my own fasttext vectors on Wikipedia comments corpus and used them to do this. I also used those vectors as embeddings but results were not as good as with regular fasttext vectors.</blockquote><p class=\"ztext-empty-paragraph\"><br/></p><p>如果按照上面的训练方式，也能够快速解决OOV问题吗？在gensim之中训练fasttext:</p><div class=\"highlight\"><pre><code class=\"language-text\">from gensim.models import FastText\nsentences = [[&#34;你&#34;, &#34;是&#34;, &#34;谁&#34;], [&#34;我&#34;, &#34;是&#34;, &#34;中国人&#34;]]\nmodel = FastText(sentences,  size=4, window=3, min_count=1, iter=10,min_n = 3 , max_n = 6,word_ngrams = 0)\nmodel .most_similar(&#34;滋润&#34;)</code></pre></div><p>在gensim之中，直接使用<code>most_similar</code>就可以实现，笔者来举几个例子： 案例一：</p><div class=\"highlight\"><pre><code class=\"language-text\">model.most_similar(&#34;萌萌的的的哒&#34;)\n\n&gt;&gt;&gt; [(&#39;胸上&#39;, 0.5790194272994995),\n (&#39;萌萌哒&#39;, 0.5606832504272461),\n (&#39;西子湖畔&#39;, 0.5515443086624146),\n (&#39;门用&#39;, 0.5502334833145142),\n (&#39;憨态可掬&#39;, 0.5367637872695923),\n (&#39;古筝曲&#39;, 0.5365685224533081),\n (&#39;济济一堂&#39;, 0.5358039140701294),\n (&#39;雕塑感&#39;, 0.5351789593696594),\n (&#39;ハ&#39;, 0.5332954525947571),\n (&#39;桃江&#39;, 0.5323261022567749)]</code></pre></div><p>案例二：</p><div class=\"highlight\"><pre><code class=\"language-text\">model.most_similar(&#34;这是一个未登录词&#34;)\n\n&gt;&gt;&gt;[(&#39;马忠真&#39;, 0.6866541504859924),\n (&#39;闻讯&#39;, 0.6599311828613281),\n (&#39;手板&#39;, 0.6508469581604004),\n (&#39;米仁&#39;, 0.6501662731170654),\n (&#39;弹指神功&#39;, 0.6470856666564941),\n (&#39;表交&#39;, 0.6461164951324463),\n (&#39;和级&#39;, 0.645912766456604),\n (&#39;膏厚&#39;, 0.6436258554458618),\n (&#39;带弹&#39;, 0.643358588218689),\n (&#39;宴請&#39;, 0.6427510976791382)]</code></pre></div><p>虽然第二个有点无厘头，但是一些比较合理地未登录词会找到比较合理地答案。 使用这个工具可以很快地利用未登录词中的字词片段来找到最相似的词是哪些，然后可以赋值。</p><h2>3.2 两个词向量空间对齐</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>上面提到的fasttext是解决单个OOV，笔者看到比赛中也有尝试两个词向量集合对齐的方案，比较简单易懂，而且使用的该方案能排在比赛的33rd，应该属于比较合理地方案，来看看</p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/mattmotoki/toxic-comment-classification\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-a8c4ec74215fa292fda249fcb58a0cd4_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">mattmotoki/toxic-comment-classification</a><p>：</p><p>we get a vector for every word in our vocabulary. We can now find the most similar vector in the intersection of the local vocabulary (from this competition) with the external vocabulary (from pretrained embeddings). </p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">local = {local_words: local_vectors}\nexternal = {external_words: external_vectors}\nshared_words = intersect(local_words, external_words)\nmissing_words = setdiff(local_words, external_words)\nreference_matrix = array(local[w] for w in shared_words).T\n\nfor w in missing_words:\n     similarity = local[w] * reference_matrix\n     most_similar_word = shared_words[argmax(similarity)]\n     external[w] = external_vectors[most_similar_word]\n\nreturn {w: external[w] for w in local_words}</code></pre></div><blockquote>With this technique, GloVe performed just as well if not better than the fastText with OOV prediction; LexVec performed slightly worse but added valuable diversity to ensembles. </blockquote><p class=\"ztext-empty-paragraph\"><br/></p><p>笔者理解的大致意思就是，A词向量集合---&gt;B词向量集合： </p><ul><li>- 先找出A &amp; B 词向量集合都拥有的词shared_words ；</li><li> - 找出 A - B，A中B没有的词missing_words ；</li><li> - A词向量集合中，共同拥有的词shared_words 的词向量矩阵reference_matrix （标准化）；</li><li> - 在missing_words 词中，譬如a词，一一找出与shared_words 词集合最相近的词b；</li><li> - 在B词向量集合中，B（a） = B（b），B词向量集合中就有a词的向量了。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>笔者说的比较绕口，可以直接看</p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/mattmotoki/toxic-comment-classification/blob/master/code/embeddings/analyze_wordvector_similarity.ipynb\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-a8c4ec74215fa292fda249fcb58a0cd4_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">mattmotoki/toxic-comment-classification</a><p>，该作者写了：</p><ul><li> - 一个一个循环查找；</li><li> - 整个missing_words空间一起查找；</li><li> - 用torch GPU加速查找</li></ul><p>比较适合拿来用，供观众参考。</p><hr/><p>笔者微信公众号：<i>素质云笔记</i></p><p>原文连载于：</p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/sinat_26917383/article/details/83584728\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-e7f0d2794939bedd6bb88caec76ea47a_120x160.jpg\" data-image-width=\"169\" data-image-height=\"413\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">比赛必备 ︱ 省力搞定三款词向量训练 + OOV词向量问题的可性方案</a><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "词向量", 
                    "tagLink": "https://api.zhihu.com/topics/20142325"
                }, 
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }
            ], 
            "comments": [
                {
                    "userName": "智仁", 
                    "userLink": "https://www.zhihu.com/people/5d61a59815bde68db1409c2c65fd6d57", 
                    "content": "文章不错，值得参考，谢谢！", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44890664", 
            "userName": "悟乙己", 
            "userLink": "https://www.zhihu.com/people/428693835688c688c6c9ac1d30fc0ece", 
            "upvote": 16, 
            "title": "ltp︱基于ltp的无监督信息抽取模块", 
            "content": "<p>﻿无监督信息抽取较多都是使用哈工大的ltp作为底层框架。那么基于ltp其实有了非常多的小伙伴进行了尝试，笔者私自将其归纳为：</p><ul><li>事件抽取（三元组）</li><li>观点抽取</li></ul><p>“语言云” 以哈工大社会计算与信息检索研究中心研发的 “语言技术平台（LTP）” 为基础，为用户提供高效精准的中文自然语言处理云服务。  pyltp 是 LTP 的 Python 封装，提供了分词，词性标注，命名实体识别，依存句法分析，语义角色标注的功能。</p><ul><li>技术文档：<a href=\"https://link.zhihu.com/?target=http%3A//pyltp.readthedocs.io/zh_CN/latest/api.html%23id15\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">pyltp.readthedocs.io/zh</span><span class=\"invisible\">_CN/latest/api.html#id15</span><span class=\"ellipsis\"></span></a> </li><li>介绍文档：<a href=\"https://link.zhihu.com/?target=https%3A//www.ltp-cloud.com/intro/%23introduction\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">ltp-cloud.com/intro/#</span><span class=\"invisible\">introduction</span><span class=\"ellipsis\"></span></a> </li><li>介绍文档：<a href=\"https://link.zhihu.com/?target=http%3A//ltp.readthedocs.io/zh_CN/latest/appendix.html%23id5\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">ltp.readthedocs.io/zh_C</span><span class=\"invisible\">N/latest/appendix.html#id5</span><span class=\"ellipsis\"></span></a></li></ul><p>需要先载入他们训练好的模型，<a href=\"https://link.zhihu.com/?target=https%3A//pan.baidu.com/share/link%3Fshareid%3D1988562907%26uk%3D2738088569%23list/path%3D/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">下载地址</a></p><p>初始化pyltp的时候一定要留意内存问题，初始化任何子模块（<code>Postagger()</code> /<code>NamedEntityRecognizer()</code>等等）都是需要占用内存，如果不及时释放会爆内存。 之前比较好的尝试是由该小伙伴已经做的小项目：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/liuhuanyong/EventTriplesExtraction\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">liuhuanyong/EventTriplesExtraction</a>，是做三元组抽取的一个实验，该同学另外一个<a href=\"https://link.zhihu.com/?target=https%3A//github.com/liuhuanyong/CausalityEventExtraction\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">liuhuanyong/CausalityEventExtraction</a>因果事件抽取的项目也很不错，辛苦写了一大堆规则，之后会对因果推理进行简单描述。</p><blockquote> 笔者也自己写了一个抽取模块，不过只是简单评论观点抽取模块。 留心的小伙伴可以基于此继续做很多拓展：搭配用语挖掘，同义词挖掘，新词挖掘 code可见：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/mattzheng/LtpExtraction\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">mattzheng/LtpExtraction</a></blockquote><hr/><h2>1 信息抽取 - 搭配抽取</h2><blockquote> code可见：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/mattzheng/LtpExtraction\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">mattzheng/LtpExtraction</a><br/> </blockquote><h2>1.1 逻辑整理</h2><p>整个逻辑主要根据依存句法分析，笔者主要利用了以下的关系类型： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b9115b5ac7defce0ef6fd743d02c714d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"507\" data-rawheight=\"143\" class=\"origin_image zh-lightbox-thumb\" width=\"507\" data-original=\"https://pic2.zhimg.com/v2-b9115b5ac7defce0ef6fd743d02c714d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;507&#39; height=&#39;143&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"507\" data-rawheight=\"143\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"507\" data-original=\"https://pic2.zhimg.com/v2-b9115b5ac7defce0ef6fd743d02c714d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b9115b5ac7defce0ef6fd743d02c714d_b.jpg\"/></figure><p>那么笔者理解 + 整理后得到四类抽取类型：</p><ul><li>搭配用语查找（SVB,ATT,ADV）</li><li>并列词查找（COO）</li><li>核心观点抽取（HED+主谓宾逻辑）</li><li>实体名词搭配（词性n ）</li></ul><p>其中笔者还加入了停词,可以对结果进行一些筛选。</p><h2>1.2 code粗解读</h2><p>这边细节会在github上公开，提一下code主要分的内容：<code>ltp启动模块</code> / <code>依存句法解读</code> / <code>结果筛选</code>。</p><ul><li>ltp模块，一定要注意释放模型，不要反复 <code>Postagger() / Segmentor() / NamedEntityRecognizer() /SementicRoleLabeller()</code>，会持续Load进内存，然后boom...</li><li>依存句法模块，笔者主要是整理结果，将其整理为一个dataframe，便于后续结构化理解与抽取内容，可见：</li><li>结果筛选模块，根据上述的几个关系进行拼接。</li></ul><blockquote> 案例句：艇仔粥料很足，香葱自己添加，很贴心。<br/> </blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d318d1088a5afd6b18382489f8d75ceb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"525\" data-rawheight=\"352\" class=\"origin_image zh-lightbox-thumb\" width=\"525\" data-original=\"https://pic4.zhimg.com/v2-d318d1088a5afd6b18382489f8d75ceb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;525&#39; height=&#39;352&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"525\" data-rawheight=\"352\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"525\" data-original=\"https://pic4.zhimg.com/v2-d318d1088a5afd6b18382489f8d75ceb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d318d1088a5afd6b18382489f8d75ceb_b.jpg\"/></figure><p> 表的解读，其中： - word列，就是这句话主要分词结果 - relation列/pos列，代表该词的词性与关系 - match_word列/match_word_n列，根据关系匹配到的词条 - tuples_words列，就是两者贴一起</p><p>同时若觉得需要去掉一些无效词搭配，也可以额外添加无效词进来，还是比较弹性的。</p><h2>1.3 结果展示</h2><p>句子一: </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-afdff72b600fb9d2624ee7ebbb17ff82_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"977\" data-rawheight=\"358\" class=\"origin_image zh-lightbox-thumb\" width=\"977\" data-original=\"https://pic3.zhimg.com/v2-afdff72b600fb9d2624ee7ebbb17ff82_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;977&#39; height=&#39;358&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"977\" data-rawheight=\"358\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"977\" data-original=\"https://pic3.zhimg.com/v2-afdff72b600fb9d2624ee7ebbb17ff82_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-afdff72b600fb9d2624ee7ebbb17ff82_b.jpg\"/></figure><p>句子二： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f86970bf933a032528f0c501af2e7086_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"561\" data-rawheight=\"340\" class=\"origin_image zh-lightbox-thumb\" width=\"561\" data-original=\"https://pic3.zhimg.com/v2-f86970bf933a032528f0c501af2e7086_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;561&#39; height=&#39;340&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"561\" data-rawheight=\"340\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"561\" data-original=\"https://pic3.zhimg.com/v2-f86970bf933a032528f0c501af2e7086_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f86970bf933a032528f0c501af2e7086_b.jpg\"/></figure><p>句子三： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0803448e7f74eefc09601980677366c7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"782\" data-rawheight=\"339\" class=\"origin_image zh-lightbox-thumb\" width=\"782\" data-original=\"https://pic4.zhimg.com/v2-0803448e7f74eefc09601980677366c7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;782&#39; height=&#39;339&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"782\" data-rawheight=\"339\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"782\" data-original=\"https://pic4.zhimg.com/v2-0803448e7f74eefc09601980677366c7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0803448e7f74eefc09601980677366c7_b.jpg\"/></figure><hr/><h2>2 三元组事件抽取 + 因果事件抽取</h2><p>帮这位小伙伴打波广告~</p><h2>2.1 三元组事件抽取</h2><blockquote> 该模块主要利用了语义角色srl，先定位关键谓语，然后进行结构化解析，核心的语义角色为 A0-5 六种，A0 通常表示动作的施事，A1通常表示动作的影响等，A2-5 根据谓语动词不同会有不同的语义含义。其余的15个语义角色为附加语义角色，如LOC， 表示地点，TMP，表示时间等（一些符号可见笔者另一篇博客：<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/sinat_26917383/article/details/77067515\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">python︱六款中文分词模块尝试:jieba、THULAC、SnowNLP、pynlpir、CoreNLP、pyLTP</a>）。<br/> </blockquote><p>基于依存句法与语义角色标注的事件三元组抽取 文本表示一直是个重要问题，如何以清晰，简介的方式对一个文本信息进行有效表示是个长远的任务.我尝试过使用关键词，实体之间的关联关系，并使用textgrapher的方式进行展示，但以词作为文本信息单元表示这种效果不是特别好，所以，本项目想尝试从事件三元组的方式出发，对文本进行表示． 项目地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/liuhuanyong/EventTriplesExtraction\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/liuhuanyong/</span><span class=\"invisible\">EventTriplesExtraction</span><span class=\"ellipsis\"></span></a></p><p>使用之后的效果： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f73b488abb1cc6c85450b9d3154e3e49_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"459\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb\" width=\"459\" data-original=\"https://pic2.zhimg.com/v2-f73b488abb1cc6c85450b9d3154e3e49_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;459&#39; height=&#39;132&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"459\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"459\" data-original=\"https://pic2.zhimg.com/v2-f73b488abb1cc6c85450b9d3154e3e49_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f73b488abb1cc6c85450b9d3154e3e49_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c8c76f79bc72ff9c8236a3cc5ccbc224_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1015\" data-rawheight=\"214\" class=\"origin_image zh-lightbox-thumb\" width=\"1015\" data-original=\"https://pic1.zhimg.com/v2-c8c76f79bc72ff9c8236a3cc5ccbc224_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1015&#39; height=&#39;214&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1015\" data-rawheight=\"214\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1015\" data-original=\"https://pic1.zhimg.com/v2-c8c76f79bc72ff9c8236a3cc5ccbc224_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c8c76f79bc72ff9c8236a3cc5ccbc224_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这边笔者觉得在结果之上，进行一些清洗的话，效果还是可以的，特别是事件性较强的，有效实体比较多的句子效果会比较好。当然，把这个用在评论中简直...</p><h2>2.2 因果事件抽取</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a30a70279bef1f547c15c016ee3c58be_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"598\" data-rawheight=\"456\" class=\"origin_image zh-lightbox-thumb\" width=\"598\" data-original=\"https://pic3.zhimg.com/v2-a30a70279bef1f547c15c016ee3c58be_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;598&#39; height=&#39;456&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"598\" data-rawheight=\"456\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"598\" data-original=\"https://pic3.zhimg.com/v2-a30a70279bef1f547c15c016ee3c58be_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a30a70279bef1f547c15c016ee3c58be_b.jpg\"/></figure><p> 主要包括以下几个步骤：</p><ul><li>1、因果知识库的构建。因果知识库的构建包括因果连词库，结果词库、因果模式库等。</li><li>2、文本预处理。这个包括对文本进行噪声移除，非关键信息去除等。 </li><li>3、因果事件抽取。这个包括基于因果模式库的因果对抽取。</li><li>4、事件表示。这是整个因果图谱构建的核心问题，因为事件图谱本质上是联通的，如何选择一种恰当（短语、短句、句子主干）等方式很重要。</li><li>5、事件融合。事件融合跟知识图谱中的实体对齐任务很像</li><li>6、事件存储。事件存储是最后步骤，基于业务需求，可以用相应的数据库进行存储，比如图数据库等。</li></ul><p><b>以下是运行结果：</b> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7dacf5d4405c98626af8b7d307e2e427_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"399\" data-rawheight=\"691\" class=\"content_image\" width=\"399\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;399&#39; height=&#39;691&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"399\" data-rawheight=\"691\" class=\"content_image lazy\" width=\"399\" data-actualsrc=\"https://pic4.zhimg.com/v2-7dacf5d4405c98626af8b7d307e2e427_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>整理之后的结果：</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-86b8a6f181a1c6aabc9c93a8b1b83026_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"909\" data-rawheight=\"570\" class=\"origin_image zh-lightbox-thumb\" width=\"909\" data-original=\"https://pic3.zhimg.com/v2-86b8a6f181a1c6aabc9c93a8b1b83026_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;909&#39; height=&#39;570&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"909\" data-rawheight=\"570\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"909\" data-original=\"https://pic3.zhimg.com/v2-86b8a6f181a1c6aabc9c93a8b1b83026_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-86b8a6f181a1c6aabc9c93a8b1b83026_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "无监督学习", 
                    "tagLink": "https://api.zhihu.com/topics/19590194"
                }, 
                {
                    "tag": "特征抽取", 
                    "tagLink": "https://api.zhihu.com/topics/20004048"
                }
            ], 
            "comments": [
                {
                    "userName": "天天向上", 
                    "userLink": "https://www.zhihu.com/people/ec9620cb2d88fc255f5d4648381a9f3e", 
                    "content": "<p>厉害了</p><p><br></p><p>拿来主义  多谢卤煮解决了燃眉之急</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "上善", 
                    "userLink": "https://www.zhihu.com/people/f89024b904dfb4a83d7a51c8a056759a", 
                    "content": "<p>不错</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/38969209", 
            "userName": "悟乙己", 
            "userLink": "https://www.zhihu.com/people/428693835688c688c6c9ac1d30fc0ece", 
            "upvote": 3, 
            "title": "练习题 | 14款机器学习加权平均模型融合的火花", 
            "content": "<p>﻿&gt; 模型融合的方法很多，Voting、Averaging、Bagging 、Boosting、 Stacking，那么一些kaggle比赛中选手会选用各种方法进行融合，其中岭回归就是一类轻巧且非常有效的方法，当然现在还有很多更有逼格的方法。本文是受快照集成的启发，把<a href=\"https://link.zhihu.com/?target=https%3A//github.com/titu1994/Snapshot-Ensembles\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">titu1994/Snapshot-Ensembles</a>项目中，比较有意思的加权平均集成的内容抽取出来，单独应用。</p><p>code更新于：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/mattzheng/WA-ModelEnsemble\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">mattzheng/WA-ModelEnsemble</a></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>1、 快照集成</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>因为受其启发，所以在这提一下，快照集成是一种无需额外训练代价的多神经网络集成方法。 通过使单个神经网络沿它的优化路径进行多个局部最小化，保存模型参数。 利用多重学习速率退火循环实现了重复的快速收敛。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-64a22b7db20d06ed662bcf1eda889821_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"914\" data-rawheight=\"909\" class=\"origin_image zh-lightbox-thumb\" width=\"914\" data-original=\"https://pic2.zhimg.com/v2-64a22b7db20d06ed662bcf1eda889821_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;914&#39; height=&#39;909&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"914\" data-rawheight=\"909\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"914\" data-original=\"https://pic2.zhimg.com/v2-64a22b7db20d06ed662bcf1eda889821_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-64a22b7db20d06ed662bcf1eda889821_b.jpg\"/></figure><h2>1.1 比较有意思的做法</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>作者在训练相同网络时使用权重快照，在训练结束后用这些结构相同但权重不同的模型创建一个集成模型。这种方法使测试集效果提升，而且这也是一种非常简单的方法，因为你只需要训练一次模型，将每一时刻的权重保存下来就可以了。</p><p>也就是，同一款模型，在学习率稍微调高，<b>训练中得到的不同阶段的模型文件都保存并拿来做最后的模型融合</b>。</p><p><b>长学习率循环的思想</b> 在于能够在权重空间找到足够多不同的模型。如果模型相似度太高，集合中各网络的预测就会太接近，而体现不出集成带来的好处。</p><h2>1.2 权重的解决方案</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>对于一个给定的网络结构，每一种不同的权重组合将得到不同的模型。因为所有模型结构都有无限多种权重组合，<b>所以将有无限多种组合方法。</b></p><p>训练神经网络的目标是找到一个特别的解决方案（权重空间中的点），从而使训练集和测试集上的损失函数的值达到很小。</p><h2>1.3 相关实现：cifar100 图像分类任务</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>可参考项目：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/titu1994/Snapshot-Ensembles\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">titu1994/Snapshot-Ensembles</a><br/>该项目用keras1.1 做了cifar_10、cifar_100两套练习，使用的是比较有意思的图像框架： Wide Residual Net (16-4)。作者已经预先给定了5款训练快照，拿着5套模型的预测结果做模型集成，使使训练集和测试集上的损失函数的值达到很小。</p><hr/><p>2、 14款常规的机器学习模型</p><p class=\"ztext-empty-paragraph\"><br/></p><p>sklearn官方案例中就有非常多的机器学习算法示例，本着实验的精神笔者借鉴了其中几个。本案例中使用到的算法主要分为两套：</p><ul><li>第一套，8款比较常见的机器学习算法，<code>&#34;Nearest Neighbors&#34;, &#34;Linear SVM&#34;, &#34;RBF SVM&#34;,</code><br/><code>            &#34;Decision Tree&#34;, &#34;Neural Net&#34;, &#34;AdaBoost&#34;,         &#34;Naive Bayes&#34;, &#34;QDA‘’</code>（参考：<a href=\"https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html%23sphx-glr-auto-examples-classification-plot-classifier-comparison-py\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Classifier</a><br/><a href=\"https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html%23sphx-glr-auto-examples-classification-plot-classifier-comparison-py\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">   comparison</a>）</li><li>第二套，偏向组合方案，<code>RandomTreesEmbedding, RandomForestClassifier,</code><br/><code>   GradientBoostingClassifier、LogisticRegression</code>（参考：<a href=\"https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html%23sphx-glr-auto-examples-ensemble-plot-feature-transformation-py\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Feature</a><br/><a href=\"https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html%23sphx-glr-auto-examples-ensemble-plot-feature-transformation-py\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">   transformations with ensembles of</a><br/><a href=\"https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html%23sphx-glr-auto-examples-ensemble-plot-feature-transformation-py\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">   trees</a>）</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0f4295ed44d70cebbf4c28fbc0c6e6c9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"889\" data-rawheight=\"331\" class=\"origin_image zh-lightbox-thumb\" width=\"889\" data-original=\"https://pic2.zhimg.com/v2-0f4295ed44d70cebbf4c28fbc0c6e6c9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;889&#39; height=&#39;331&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"889\" data-rawheight=\"331\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"889\" data-original=\"https://pic2.zhimg.com/v2-0f4295ed44d70cebbf4c28fbc0c6e6c9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0f4295ed44d70cebbf4c28fbc0c6e6c9_b.jpg\"/></figure><p>机器学习模型除了预测还有重要的特征筛选的功能，不同的模型也有不同的重要性输出：</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.1 特征选择</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>在本次10+机器学习案例之中，可以看到，可以输出重要性的模型有： </p><ul><li>随机森林<code>rf.feature_importances_</code> </li><li>GBT<code>grd.feature_importances_</code> </li><li>Decision <code>Tree decision.feature_importances_</code> </li><li>AdaBoost <code>AdaBoost.feature_importances_</code> </li></ul><p>可以计算系数的有：线性模型，<code>lm.coef_</code> 、 SVM <code>svm.coef_</code><br/>Naive Bayes得到的是：NaiveBayes.sigma_<code>解释为：variance of each feature per class</code> </p><h2>2.2 机器学习算法输出</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>算法输出主要有：重要指标（本案例中提到的是<code>acc/recall</code>）、ROC值的计算与plot、校准曲线（Calibration curves）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cd06068725e7a38a84061d9fbe5a4d0c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"792\" data-rawheight=\"785\" class=\"origin_image zh-lightbox-thumb\" width=\"792\" data-original=\"https://pic1.zhimg.com/v2-cd06068725e7a38a84061d9fbe5a4d0c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;792&#39; height=&#39;785&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"792\" data-rawheight=\"785\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"792\" data-original=\"https://pic1.zhimg.com/v2-cd06068725e7a38a84061d9fbe5a4d0c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-cd06068725e7a38a84061d9fbe5a4d0c_b.jpg\"/></figure><p><br/>该图为校准曲线（Calibration curves），Calibration curves may also be referred to as reliability diagrams. 是一种算法可靠性检验的方式。<br/>.</p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h2>3、optimize 权重空间优化</h2><p>主要是从<a href=\"https://link.zhihu.com/?target=https%3A//github.com/titu1994/Snapshot-Ensembles\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">titu1994/Snapshot-Ensembles</a>抽取出来的。简单看看逻辑:</p><h2>3.1 简述权重空间优化逻辑</h2><h2>3.1.1 先定义loss函数：</h2><div class=\"highlight\"><pre><code class=\"language-text\"># Create the loss metric \ndef log_loss_func(weights):\n    &#39;&#39;&#39; scipy minimize will pass the weights as a numpy array &#39;&#39;&#39;\n    final_prediction = np.zeros((sample_N, nb_classes), dtype=&#39;float32&#39;)\n\n    for weight, prediction in zip(weights, preds):\n        final_prediction += weight * prediction\n\n    return log_loss(testY_cat, final_prediction)\n</code></pre></div><p>testY_cat为正确预测标签， final_prediction为多款模型预测概率组合。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>3.1.2 迭代策略</h2><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">minimize(log_loss_func, prediction_weights, method=&#39;SLSQP&#39;, bounds=bounds, constraints=constraints)\n</code></pre></div><p>SciPy的optimize模块提供了许多数值优化算法，minimize就是其中一种。<br/>其中：</p><ul><li>log_loss_func,loss函数</li><li>prediction_weights,array,(6,)</li><li>method,很多：SLSQP、Nelder-Mead、Powell、CG、BFGS等</li><li>bounds,代表x的每个维度对应的界限，如果只有一维，也要写成bounds= ((0, None), )</li><li>constraints,常数项,Constraints definition (only for COBYLA, SLSQP and<br/>   trust-constr)</li></ul><h2>3.2 实践</h2><p>具体code笔者会上传至笔者的github之上了。步骤为：</p><ul><li>1、随机准备数据<code>make_classification</code> </li><li>2、两套模型的训练与基本信息准备</li><li>3、观察14套模型的准确率与召回率</li><li>4、刻画14套模型的calibration plots校准曲线</li><li>5、14套模型的重要性输出</li><li>6、14套模型的ROC值计算与plot</li><li>7、加权模型融合数据准备</li><li>8、基准优化策略：14套模型融合——平均</li><li>9、加权平均优化策略：14套模型融合——加权平均优化</li></ul><p>一些细节了解：</p><h2>3.2.7  加权模型融合数据准备</h2><div class=\"highlight\"><pre><code class=\"language-text\"># 集成数据准备\npreds_dict = {}\nfor pred_tmp,name in [[predictEight[n][&#39;prob_pos&#39;],n] for n in names] + [(y_pred_lm,&#39;LM&#39;),\n                       (y_pred_rt,&#39;RT + LM&#39;),\n                       (y_pred_rf_lm,&#39;RF + LM&#39;),\n                       (y_pred_grd_lm,&#39;GBT + LM&#39;),\n                       (y_pred_grd,&#39;GBT&#39;),\n                       (y_pred_rf,&#39;RF&#39;)]:\n    preds_dict[name] = np.array([1 - pred_tmp , pred_tmp]).T\n\n# 参数准备\npreds = list(preds_dict.values())\nmodels_filenames = list(preds_dict.keys())\nsample_N,nb_classes = preds[0].shape\ntestY = y_test.reshape((len(y_test),1))  # 真实Label (2000,1)\ntestY_cat = np.array([1 - y_test ,y_test]).T # (2000,2)   \n</code></pre></div><p>models_filenames 代表模型的名字；sample_N样本个数；nb_classes 分类个数（此时为2分类）；testY 真实label；testY_cat 基于真实Label简单处理。</p><h2>3.2.8  基准优化策略：14套模型融合——平均</h2><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">def calculate_weighted_accuracy(prediction_weights):\n    weighted_predictions = np.zeros((sample_N, nb_classes), dtype=&#39;float32&#39;)\n    for weight, prediction in zip(prediction_weights, preds):\n        weighted_predictions += weight * prediction\n    yPred = np.argmax(weighted_predictions, axis=1)\n    yTrue = testY\n    accuracy = metrics.accuracy_score(yTrue, yPred) * 100\n    recall = recall_score(yTrue, yPred)\n    print(&#34;Accuracy : &#34;, accuracy)\n    print(&#34;Recall : &#34;, recall)\n\n# 模型集成：无权重\n    # 无权重则代表权重为平均值\nprediction_weights = [1. / len(models_filenames)] * len(models_filenames)\ncalculate_weighted_accuracy(prediction_weights)\n&gt;&gt;&gt; Accuracy :  79.7\n&gt;&gt;&gt; Recall :  0.7043390514631686\n</code></pre></div><p>对14套模型，平均权重并进行加权。可以看到结论非常差。</p><h2>3.2.9  加权平均优化策略：14套模型融合——加权平均优化</h2><div class=\"highlight\"><pre><code class=\"language-text\">def MinimiseOptimize(preds,models_filenames,nb_classes,sample_N,testY,NUM_TESTS = 20):\n    best_acc = 0.0\n    best_weights = None\n    # Parameters for optimization\n    constraints = ({&#39;type&#39;: &#39;eq&#39;, &#39;fun&#39;:lambda w: 1 - sum(w)})\n    bounds = [(0, 1)] * len(preds)\n\n    # Check for NUM_TESTS times\n    for iteration in range(NUM_TESTS):  # NUM_TESTS,迭代次数为25\n        # Random initialization of weights\n        prediction_weights = np.random.random(len(models_filenames))\n\n        # Minimise the loss \n        result = minimize(log_loss_func, prediction_weights, method=&#39;SLSQP&#39;, bounds=bounds, constraints=constraints)\n\n        weights = result[&#39;x&#39;]\n        weighted_predictions = np.zeros((sample_N, nb_classes), dtype=&#39;float32&#39;)\n\n        # Calculate weighted predictions\n        for weight, prediction in zip(weights, preds):\n            weighted_predictions += weight * prediction\n\n        yPred = np.argmax(weighted_predictions, axis=1)\n        yTrue = testY\n\n        # Calculate weight prediction accuracy\n        accuracy = metrics.accuracy_score(yTrue, yPred) * 100\n        recall = recall_score(yTrue, yPred)\n\n        print(&#39;\\n ------- Iteration : %d  - acc: %s  - rec:%s -------  &#39;%((iteration + 1),accuracy,recall))\n        print(&#39;    Best Ensemble Weights: \\n&#39;,result[&#39;x&#39;])\n\n        # Save current best weights \n        if accuracy &gt; best_acc:\n            best_acc = accuracy\n            best_weights = weights\n    return best_acc,best_weights\n\n# 模型集成：附权重\nbest_acc,best_weights = MinimiseOptimize(preds,models_filenames,nb_classes,sample_N,testY,NUM_TESTS = 20)\n\n&gt;&gt;&gt; Best Accuracy :  90.4\n&gt;&gt;&gt; Best Weights :  [1.57919854e-02 2.25437178e-02 1.60078948e-01 1.37993631e-01\n     1.60363024e-03 1.91105368e-01 2.34578651e-02 1.24696769e-02\n     3.18793907e-03 1.29753377e-02 1.12151337e-01 7.62845967e-04\n     3.05643629e-01 2.34089531e-04]\n&gt;&gt;&gt; Accuracy :  90.4\n&gt;&gt;&gt; Recall :  0.9112008072653885\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>在迭代了20次之后，通过加权求得的综合预测水平，要高于平均水平很多。不过，跟一些比较出众的机器学习模型差异不大。</p>", 
            "topic": [
                {
                    "tag": "数据挖掘", 
                    "tagLink": "https://api.zhihu.com/topics/19553534"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/38487913", 
            "userName": "悟乙己", 
            "userLink": "https://www.zhihu.com/people/428693835688c688c6c9ac1d30fc0ece", 
            "upvote": 4, 
            "title": "MOne︱基于词包的无监督多主题得分 练习题", 
            "content": "<p>﻿&gt; 代码可见：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/mattzheng/TopicClassifier\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">mattzheng/TopicClassifier</a></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>1 开源的今日头条数据</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>某机构又开源了一个整理的今日头条数据，可见：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/fateleak/toutiao-multilevel-text-classfication-dataset\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">今日头条中文新闻文本(多层)分类数据集</a></p><p>本数据集有1000+分类，2914000条数据，虽然没有放开正文，但是也是非常好的词包收集源，于是笔者花了很久整理一版本。今日头条的数据样式为：</p><p>以|,|分割的各字段，从前往后分别是 新闻ID，分类代码，新闻字符串（仅含标题），新闻关键词，新闻label</p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">1000866069|,|tip,news|,|【互联网资讯】PPT设计宝典!十招教你做出拿得出手的PPT|,|互联网,美国,ppt,powerpoint,幻灯片,演示文稿,微软,字体列表|,|\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>由开源的内容就可以构建一套新闻类的词包。但是发现，分类代码太过详细，1000+类别，项目太多，而且准确率有待考察，还不如直接归类到大类，粗线条一些的。下面做了一些数据清洗：</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>1、分类约束，只考察大类，把譬如： <code>digital/appliances/small_home_appliance</code><br/>   或<code>digital/appliances/television</code>折算成<code>digital</code> </li><li>2、有一些素材不是那么优质，主题点很多，反正数据2000W+，只筛选主题分类代码只有一种类别的素材</li><li>3、约束字数，不能有1个字以及字数超过8</li></ul><p><code>&#39;新闻关键词&#39;</code>与<code>&#39;新闻label&#39;</code>字段，同质化挺多，经过筛选之中，两个语料质量都挺高的。</p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h2>2 准备主题词包素材</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>主题词包素材，包括四样内容：</p><ul><li>每个词出现在哪些主题之中,<code>topic</code>；</li><li>每个词出现在哪些主题之中，同时统计每个词主题频次,<code>topic_detail</code>；</li><li>每个词TF,<code>tf</code>；</li><li>每个词IDF,<code>idf</code>。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>每个单词的格式样式为：</p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">{\n    &#39;word&#39;:{\n            &#39;idf&#39;:1,\n            &#39;tf&#39;:1,\n            &#39;topic&#39;: [&#39;new&#39;,&#39;sports&#39;]\n            &#39;topic_detail&#39;:{\n                &#39;digital&#39;: 16,\n                &#39;emotion&#39;: 4,\n                &#39;general_positive&#39;: 0\n    }\n  }\n}\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>其中<code>&#39;topic&#39;</code> 和 <code>&#39;topic_detail&#39;</code>是对应的。同时自己写了一个计算TFIDF的过程，但是计算IDF必须要不断遍历全文件，花了好几天时间终于计算出结果。</p><p>最终整理得到了如下为<code>&#39;网易云音乐&#39;</code>的主题内容：</p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">{&#39;idf&#39;: 9.0134188194616147,\n &#39;tf&#39;: 88,\n &#39;topic&#39;: [&#39;digital&#39;,\n  &#39;news_tech&#39;,\n  &#39;news_entertainment&#39;,\n  &#39;news_baby&#39;,\n  &#39;news_game&#39;,\n  &#39;news_car&#39;,\n  &#39;news_house&#39;,\n  &#39;emotion&#39;,\n  &#39;news_finance&#39;],\n &#39;topic_detail&#39;: {&#39;digital&#39;: 16,\n  &#39;emotion&#39;: 4,\n  &#39;general_positive&#39;: 0,\n  &#39;news&#39;: 0,\n  &#39;news_agriculture&#39;: 0,\n  &#39;news_astrology&#39;: 0,\n  &#39;news_baby&#39;: 2,\n  &#39;news_car&#39;: 2,\n  &#39;news_collect&#39;: 0,\n  &#39;news_comic&#39;: 0,\n  &#39;news_culture&#39;: 0,\n  &#39;news_design&#39;: 0,\n  &#39;news_edu&#39;: 0,\n  &#39;news_entertainment&#39;: 35,\n  &#39;news_fashion&#39;: 0,\n  &#39;news_finance&#39;: 1,\n  &#39;news_food&#39;: 0,\n  &#39;news_game&#39;: 4,\n  &#39;news_health&#39;: 0,\n  &#39;news_history&#39;: 0,\n  &#39;news_home&#39;: 0,\n  &#39;news_house&#39;: 1,\n  &#39;news_military&#39;: 0,\n  &#39;news_novel&#39;: 0,\n  &#39;news_pet&#39;: 0,\n  &#39;news_politics&#39;: 0,\n  &#39;news_society&#39;: 0,\n  &#39;news_sports&#39;: 0,\n  &#39;news_tech&#39;: 28,\n  &#39;news_travel&#39;: 0,\n  &#39;news_world&#39;: 0,\n  &#39;science_all&#39;: 0,\n  &#39;technique&#39;: 0,\n  &#39;video_ent&#39;: 0}}\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>记录每个词的词频、IDF，以及分属主题以及每个主题出现频次。</p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h2>3 MOneTopic 无监督主题得分流程</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>有些主题判定分出关键内容之后就打上一个标签，但是每个词语的属性很多样，那么句子的属性也有可能有很多属性。<br/>基于前面整理的词包素材内容，包含词粒度的四样内容：每个词分属主题、分属主题频数、词TF/IDF信息。<br/>那么接下来的流程就是：</p><h2>3.1 一句话输入：</h2><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">网易云音乐是一款专注于发现与分享的音乐产品,依托专业音乐人、DJ、好友推荐及社交功能,为用户打造全新的音乐生活。\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2>3.2 用户词典载入</h2><p><code>jieba.add_word</code>并规定词性，然后分词时候根据自定义的词性就可以比较好的筛选出这些关键词。当然这些关键词把顺序打乱了。</p><h2>3.3 计算主题得分</h2><p>这边主题得分可以有两种类型：</p><ul><li>粗粒度:每个词分属主题topic、</li><li>细粒度：分属主题频数topic_detail<br/>{&#39;perTopic&#39;: {&#39;digital&#39;: {&#39;degree&#39;: 5.1740782122905022, &#39;num&#39;: 1362},<br/>  &#39;news_baby&#39;: {&#39;degree&#39;: 1.3448044692737429, &#39;num&#39;: 354},<br/>  &#39;news_culture&#39;: {&#39;degree&#39;: 1.4739664804469272, &#39;num&#39;: 388},<br/>  &#39;news_entertainment&#39;: {&#39;degree&#39;: 19.64022346368715, &#39;num&#39;: 5170}}<br/></li></ul><p>那么这个就是细粒度版本的，可以看到每个主题的计数都很夸张，这个把每个单词的属性；</p><div class=\"highlight\"><pre><code class=\"language-text\">{&#39;totalTopic&#39;: {&#39;digital&#39;: {&#39;degree&#39;: 2.6153846153846154, &#39;num&#39;: 6},\n  &#39;news_entertainment&#39;: {&#39;degree&#39;: 2.6153846153846154, &#39;num&#39;: 6},\n  &#39;news_finance&#39;: {&#39;degree&#39;: 2.6153846153846154, &#39;num&#39;: 6},\n  &#39;news_tech&#39;: {&#39;degree&#39;: 3.4871794871794872, &#39;num&#39;: 8}\n</code></pre></div><p>粗粒度版本中的主题数量就没那么夸张。</p><p>这边的筛选规则是，超过这些主题内容的数值的90%分位数的进行保留。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>3.4 贴上关键词的TF/IDF</h2><p>从词包之中拿出来，贴出：</p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">{&#39;DJ&#39;: {&#39;idf&#39;: 10.794004988091546,\n   &#39;tf&#39;: 14,\n   &#39;tfidf&#39;: 151.11606983328164},\n  &#39;产品&#39;: {&#39;idf&#39;: 12.403442900525645, &#39;tf&#39;: 2, &#39;tfidf&#39;: 24.80688580105129},\n  &#39;分享&#39;: {&#39;idf&#39;: 12.808908008633811, &#39;tf&#39;: 1, &#39;tfidf&#39;: 12.808908008633811},\n  &#39;发现&#39;: {&#39;idf&#39;: 12.403442900525645, &#39;tf&#39;: 2, &#39;tfidf&#39;: 24.80688580105129},\n  &#39;好友&#39;: {&#39;idf&#39;: 10.729466466953975, &#39;tf&#39;: 15, &#39;tfidf&#39;: 160.94199700430963},\n  &#39;打造&#39;: {&#39;idf&#39;: 10.457532751470332, &#39;tf&#39;: 20, &#39;tfidf&#39;: 209.15065502940664},\n  &#39;推荐&#39;: {&#39;idf&#39;: 11.7102957199657, &#39;tf&#39;: 5, &#39;tfidf&#39;: 58.551478599828499},\n  &#39;生活&#39;: {&#39;idf&#39;: 10.668841845137539, &#39;tf&#39;: 16, &#39;tfidf&#39;: 170.70146952220063},\n  &#39;用户&#39;: {&#39;idf&#39;: 12.403442900525645, &#39;tf&#39;: 2, &#39;tfidf&#39;: 24.80688580105129},\n  &#39;社交功能&#39;: {&#39;idf&#39;: 12.403442900525645, &#39;tf&#39;: 2, &#39;tfidf&#39;: 24.80688580105129},\n  &#39;网易云音乐&#39;: {&#39;idf&#39;: 9.0134188194616147, &#39;tf&#39;: 88, &#39;tfidf&#39;: 793.18085611262211},\n  &#39;音乐&#39;: {&#39;idf&#39;: 4.4418402757952107, &#39;tf&#39;: 8605, &#39;tfidf&#39;: 38222.035573217785},\n  &#39;音乐人&#39;: {&#39;idf&#39;: 8.4458093838454467, &#39;tf&#39;: 156, &#39;tfidf&#39;: 1317.5462638798897}}\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h2>4 MOneTopic 函数记录</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>在本篇使用的函数中，笔者为了方便自己记忆，有如下的函数：</p><ul><li>getPseg:按照词性进行分词，可以有效将定义的关键词暴露出来；</li><li>totalTopic，粗粒度主题得分，每个词基本属性，不带词频，[&#39;体育&#39;,&#39;新闻&#39;]</li><li>perTopic，细粒度主题打分，每个基本属性+带词频，[&#39;体育&#39;:10,&#39;新闻&#39;:1]</li><li>ShowTfidf，根据词典把每个词的IDF、TF灌入；</li><li>TopN，主要对totalTopic/perTopic进行排序处理，原则是大于90%分位数的保留，该函数可以调节。</li><li>TopicClassifier，主函数，如下解释。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">TopicClassifier(sentense,TopicDict,topic_class,percs = 90,allowPOSs = [&#39;topic&#39;])\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>其中，</p><ul><li>sentense为输入单个句子；</li><li>TopicDict为今日头条整理的词典；</li><li>topic_class，如附件；</li><li>percs，代表粗粒度、细粒度主题得分的时候，筛选前90%分位数；</li><li>allowPOSs ，代表分词的时候，给入的词性。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h2>MOneTopic 无监督主题标记设想</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>由于整理出来的质量高的分类都是新闻类的，所以笔者自己整理的数据集比较适合鉴别新闻类文本的主题。<br/>主题标记的粗粒度以及细粒度版本都各有自己优缺点。</p><p>那么模块如何使用，就留由看客自己鉴定了。</p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><p>案例一：</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote>&#39;网易云音乐是一款专注于发现与分享的音乐产品,依托专业音乐人、DJ、好友推荐及社交功能,为用户打造全新的音乐生活。&#39;<br/><br/>关键词：音乐、音乐人、网易云音乐、打造、生活<br/><br/>细粒度主题：娱乐、数码 <br/><br/>粗粒度主题：科技、娱乐、数码、金融</blockquote><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><p>案例二：</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote>&#39;世界杯小组赛进入最后一轮，前2轮表现极其出色的C罗赢得了全世界的称赞，就连葡萄牙总统马塞洛-雷贝洛-德索萨也在同俄罗斯总统普京会面时，也不禁自夸：我们葡萄牙可是有C罗这种顶级巨星的。&#39;<br/><br/>关键词：C罗、马塞洛、葡萄牙、表现、总统、俄罗斯总统普京<br/>细粒度主题：体育<br/>粗粒度主题：体育、国际、健康</blockquote><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><p>案例三：</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote>&#39;《创造101》终于收官了——经过昨晚（6月23日）的一夜鏖战，十一名女团人选最终确定：孟美岐、吴宣仪、杨超越、段奥娟、yamy、赖美云、紫宁、Sunnee（杨芸晴）、李紫婷、傅菁、徐梦洁。&#39;<br/><br/>关键词：创造101、女团、杨超越<br/>细粒度主题：娱乐<br/>粗粒度主题：娱乐、时尚、体育</blockquote>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/37813922", 
            "userName": "悟乙己", 
            "userLink": "https://www.zhihu.com/people/428693835688c688c6c9ac1d30fc0ece", 
            "upvote": 3, 
            "title": "练习题︱基于今日头条开源数据（二）——两款Apriori算法实践", 
            "content": "<p>﻿&gt; Apriori算法是通过限制候选产生发现频繁项集。总的来说，Apriori算法其实效率并不高，大规模数据计算的时候，需要考虑性能问题。</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote>code + data可见：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/mattzheng/AprioriDemo\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">mattzheng/AprioriDemo</a></blockquote><p class=\"ztext-empty-paragraph\"><br/></p><p>盗图盗图：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6f3254de98206492b85868fe3f935d4d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"670\" data-rawheight=\"505\" class=\"origin_image zh-lightbox-thumb\" width=\"670\" data-original=\"https://pic2.zhimg.com/v2-6f3254de98206492b85868fe3f935d4d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;670&#39; height=&#39;505&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"670\" data-rawheight=\"505\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"670\" data-original=\"https://pic2.zhimg.com/v2-6f3254de98206492b85868fe3f935d4d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6f3254de98206492b85868fe3f935d4d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>在R语言里面有非常好的package，可见我之前的博客：</b><br/><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/sinat_26917383/article/details/50662709\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">R语言实现关联规则与推荐算法(学习笔记)</a><br/>该packages能够实现以下一些可视化：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-141fecc6d9e91a087a9e6df5a5c32dfa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1061\" data-rawheight=\"837\" class=\"origin_image zh-lightbox-thumb\" width=\"1061\" data-original=\"https://pic3.zhimg.com/v2-141fecc6d9e91a087a9e6df5a5c32dfa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1061&#39; height=&#39;837&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1061\" data-rawheight=\"837\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1061\" data-original=\"https://pic3.zhimg.com/v2-141fecc6d9e91a087a9e6df5a5c32dfa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-141fecc6d9e91a087a9e6df5a5c32dfa_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b1b9a4cc0859b5c61f67e6c9e041d44f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1022\" data-rawheight=\"837\" class=\"origin_image zh-lightbox-thumb\" width=\"1022\" data-original=\"https://pic4.zhimg.com/v2-b1b9a4cc0859b5c61f67e6c9e041d44f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1022&#39; height=&#39;837&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1022\" data-rawheight=\"837\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1022\" data-original=\"https://pic4.zhimg.com/v2-b1b9a4cc0859b5c61f67e6c9e041d44f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b1b9a4cc0859b5c61f67e6c9e041d44f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>但是好像Python里面没有这样封装比较好的库...搜刮了一下，发现很多人写了，但是没有可视化模块，不过先拿着用呗。<br/>笔者参考这两位大神的作品：</p><ul><li><a href=\"https://link.zhihu.com/?target=https%3A//spaces.ac.cn/archives/3380\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">用Pandas实现高效的Apriori算法</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//github.com/asaini/Apriori\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">asaini/Apriori</a></li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>当然也会结合今日头条数据来做，之前做过一个练习，可见我之前博客：<br/> <a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/sinat_26917383/article/details/80454736\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">练习题︱基于今日头条开源数据的词共现、新热词发现、短语发现</a></p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h2>一、Apriori关联算法一：asaini/Apriori</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>因为该大神写的时候用得py2，我现在习惯py3；同时对一些细节进行一些调整。主要以介绍案例为主。<br/>整体来看，效率还是很不错的，要比第二款效率高。</p><h2>1.1 主函数介绍</h2><div class=\"highlight\"><pre><code class=\"language-text\">runApriori(inFile, minSupport, minConfidence)</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>输入的内容有三样：</p><ul><li>inFile：数据集输入，迭代器</li><li>minSupport：最小支持度阈值，作者推荐：0.1-0.2之间</li><li>minConfidence：最小置信度阈值，作者推荐：0.5-0.7之间</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>输出内容两样：</p><ul><li>items ，支持度表，形式为：(tuple, support)，一个词的支持度、一对词的支持度【无指向】</li><li>rules ，置信度表，形式为((pretuple, posttuple), confidence)，（起点词，终点词），置信度【有指向】</li></ul><h2>1.2 改编两函数：dataFromFile、transferDataFrame</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>为了更便于使用，同时笔者改编了一个函数 dataFromFile + 新写了一个函数 transferDataFrame。</p><div class=\"highlight\"><pre><code class=\"language-text\">dataFromFile(fname,extra = False)</code></pre></div><p>作者函数中只能从外部读入，如果笔者要对数据集做点操作，就可以使用extra  = True,当然只适用dataframe，可见下面的今日头条数据例子。</p><div class=\"highlight\"><pre><code class=\"language-text\">transferDataFrame(items, rules,removal = True)</code></pre></div><p>items、rules计算出来之后，作者只是print出来，并没有形成正规的格式输出，这里写了一个变成dataframe的格式。可见下面例子的格式。<br/>同时，这边的removal  =True，是因为会出现：‘A-&gt;B’，‘B-&gt;A’的情况，这边True就是只保留一个。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>1.3 作者提供的数据实践</h2><p>作者的数据为，而且可以支持<b>不对齐、不等长</b>：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d37b00653a01ee9990735a1d6bd8ec07_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"354\" data-rawheight=\"594\" class=\"content_image\" width=\"354\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;354&#39; height=&#39;594&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"354\" data-rawheight=\"594\" class=\"content_image lazy\" width=\"354\" data-actualsrc=\"https://pic4.zhimg.com/v2-d37b00653a01ee9990735a1d6bd8ec07_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\">inFile = dataFromFile(&#39;INTEGRATED-DATASET.csv&#39;,extra = False)\nminSupport = 0.15\nminConfidence = 0.6\nitems, rules = runApriori(inFile, minSupport, minConfidence)\n\n# ------------ print函数 ------------\nprintResults(items, rules)\n\n# ------------ dataframe------------ \nitems_data,rules_data = transferDataFrame(items, rules)\n\n</code></pre></div><p>这里的支持度、置信度都还挺高的，得出的结果：<br/>items_data的支持度的表格，其中Len，代表词表中的词个数。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-564c21ed9ac9dadf38d595aa205b43c4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"416\" data-rawheight=\"481\" class=\"content_image\" width=\"416\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;416&#39; height=&#39;481&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"416\" data-rawheight=\"481\" class=\"content_image lazy\" width=\"416\" data-actualsrc=\"https://pic1.zhimg.com/v2-564c21ed9ac9dadf38d595aa205b43c4_b.jpg\"/></figure><p><br/>rules_data 的置信度表格，指向为<code>word_x-&gt;word_y</code><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a0d254b038de015f71b190a70d4e5bc7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"410\" data-rawheight=\"188\" class=\"content_image\" width=\"410\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;410&#39; height=&#39;188&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"410\" data-rawheight=\"188\" class=\"content_image lazy\" width=\"410\" data-actualsrc=\"https://pic4.zhimg.com/v2-a0d254b038de015f71b190a70d4e5bc7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>1.4 今日头条二元组词条</h2><p>今日头条的数据处理，主要参考上一篇练习题。然后把二元组的内容，截取前800个，放在此处。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-df1d2e90e9b488d3dee1d7cba262778c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"303\" data-rawheight=\"439\" class=\"content_image\" width=\"303\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;303&#39; height=&#39;439&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"303\" data-rawheight=\"439\" class=\"content_image lazy\" width=\"303\" data-actualsrc=\"https://pic1.zhimg.com/v2-df1d2e90e9b488d3dee1d7cba262778c_b.jpg\"/></figure><p><br/>其中第一列为共现频数，其他为共现词，在这里面不用第一列共现频数。</p><div class=\"highlight\"><pre><code class=\"language-text\">data = pd.read_csv(&#39;CoOccurrence_data_800.csv&#39;,header = None)\ninFile = dataFromFile(data[[1,2]],extra = True)\ndata_iter = dataFromFile(data[[1,2]],extra = True)\n#list(inFile)\nminSupport = 0.0\nminConfidence = 0.0\n\nitems, rules = runApriori(inFile, minSupport, minConfidence)\nprint(&#39;--------items number is: %s , rules number is : %s--------&#39;%(len(items),len(rules)))\n\n# ------------ print函数 ------------\nprintResults(items, rules)\n\n# ------------ dataframe------------ \nitems_data,rules_data = transferDataFrame(items, rules)\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>此时，因为词语与词语之间的关系很稀疏，支持度与置信度都不会高的，所以练习题中要把两个比例都设置为0比较好。<br/>items_data的支持度的表格，其中Len，代表词表中的词个数。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3d74fab8b8e1c0a4e602197abcddb4a6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"523\" data-rawheight=\"453\" class=\"origin_image zh-lightbox-thumb\" width=\"523\" data-original=\"https://pic3.zhimg.com/v2-3d74fab8b8e1c0a4e602197abcddb4a6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;523&#39; height=&#39;453&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"523\" data-rawheight=\"453\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"523\" data-original=\"https://pic3.zhimg.com/v2-3d74fab8b8e1c0a4e602197abcddb4a6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-3d74fab8b8e1c0a4e602197abcddb4a6_b.jpg\"/></figure><p><br/>rules_data 的置信度表格，指向为<code>word_x-&gt;word_y</code><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c91cdc67fc652b9cc62332707b266e51_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"512\" data-rawheight=\"451\" class=\"origin_image zh-lightbox-thumb\" width=\"512\" data-original=\"https://pic2.zhimg.com/v2-c91cdc67fc652b9cc62332707b266e51_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;512&#39; height=&#39;451&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"512\" data-rawheight=\"451\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"512\" data-original=\"https://pic2.zhimg.com/v2-c91cdc67fc652b9cc62332707b266e51_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c91cdc67fc652b9cc62332707b266e51_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>.</p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><h2>二、Apriori关联算法二：Pandas实现高效的Apriori算法</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>用Pandas写的，效率在生成频繁集的时候会爆炸，所以合理选择支持度很重要。<br/>大神写的很服从中文环境，所以不用改啥，给赞！</p><h2>2.1 官方案例</h2><p>所使用的数据比较规则：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5d90833fa4ba88deac456a2cdecd91cf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"808\" data-rawheight=\"445\" class=\"origin_image zh-lightbox-thumb\" width=\"808\" data-original=\"https://pic4.zhimg.com/v2-5d90833fa4ba88deac456a2cdecd91cf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;808&#39; height=&#39;445&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"808\" data-rawheight=\"445\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"808\" data-original=\"https://pic4.zhimg.com/v2-5d90833fa4ba88deac456a2cdecd91cf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5d90833fa4ba88deac456a2cdecd91cf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\"># ------------ 官方 ------------\n d = pd.read_csv(&#39;apriori.txt&#39;, header=None, dtype = object)\n d = ToD(d)\n support = 0.06 #最小支持度\n confidence = 0.75 #最小置信度\n output = find_rule(d, support, confidence)\n output.to_excel(&#39;rules.xls&#39;)</code></pre></div><p>大神已经整理好结果，可见：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1ca26dbda7959ca7ad943c1f9e953680_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"394\" data-rawheight=\"186\" class=\"content_image\" width=\"394\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;394&#39; height=&#39;186&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"394\" data-rawheight=\"186\" class=\"content_image lazy\" width=\"394\" data-actualsrc=\"https://pic1.zhimg.com/v2-1ca26dbda7959ca7ad943c1f9e953680_b.jpg\"/></figure><h2>2.2 今日头条数据</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>今日头条的数据处理，主要参考上一篇练习题。然后把二元组的内容，截取前800个，放在此处。</p><p>其中第一列为共现频数，其他为共现词，在这里面不用第一列共现频数。</p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\"># ------------自己 ------------\ndata = pd.read_csv(&#39;CoOccurrence_data_800.csv&#39;,header = None)\nsupport = 0.002 #最小支持度\nconfidence = 0.0 #最小置信度\nd = ToD(data[[1,2]])\noutput = find_rule(d, support, confidence)\n</code></pre></div><p>因为词条之间非常稀疏，支持度与置信度需要设置非常小，如果support设置为0的话，又会超级慢，笔者实验的数据，支持度比较合适在0.002。<br/>最终输出的结果如下：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d379e7a039f62befa55771a334329051_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"392\" data-rawheight=\"444\" class=\"content_image\" width=\"392\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;392&#39; height=&#39;444&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"392\" data-rawheight=\"444\" class=\"content_image lazy\" width=\"392\" data-actualsrc=\"https://pic2.zhimg.com/v2-d379e7a039f62befa55771a334329051_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "大数据", 
                    "tagLink": "https://api.zhihu.com/topics/19740929"
                }, 
                {
                    "tag": "数据挖掘", 
                    "tagLink": "https://api.zhihu.com/topics/19553534"
                }
            ], 
            "comments": [
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>想请问您一下，memoryerroy怎么解决，数据量太大了</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/37347979", 
            "userName": "悟乙己", 
            "userLink": "https://www.zhihu.com/people/428693835688c688c6c9ac1d30fc0ece", 
            "upvote": 4, 
            "title": "练习题︱基于今日头条开源数据的文本挖掘", 
            "content": "<p>﻿最近笔者在做文本挖掘项目时候，写了一些小算法，不过写的比较重，没有进行效率优化，针对大数据集不是特别好用，不过在小数据集、不在意性能的情况下还是可以用用的。</p><p><b>本次练习题中可以实现的功能大致有三个：</b></p><ul><li>短语发现</li><li>新词发现</li><li>词共现</li></ul><p><b>短语发现、新词发现跟词共现有些许区别：</b><br/>[‘举’，&#39;个&#39;，‘例子’，‘来说’]</p><ul><li>短语发现、新词发现，是词-词连续共现的频率，窗口范围为1，也就是：‘举’，‘例子’；&#39;个&#39;，‘例子’；‘例子’，‘来说’，探究挨得很近的词之间的关系</li><li>词共现是词-词离散出现，词共现包括了上面的内容，探究：‘举’，‘来说’，不用挨着的词出现的次数</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><blockquote>code可见我的github：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/mattzheng/LangueOne\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">mattzheng/LangueOne</a></blockquote><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h2>一、数据集介绍</h2><p>练习数据来源：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/fateleak/toutiao-text-classfication-dataset\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">今日头条中文新闻（文本）分类数据集</a><br/>今日头条是最近开源的数据集，38w，其中的数据格式为：</p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">6552391948794069256_!_106_!_news_house_!_新手买房，去售楼部该如何咨询？_!_\n6552263884172952072_!_106_!_news_house_!_南京90后这么有钱吗？南京百分之四五十都是小杆子买了_!_公积金,江宁,麒麟镇,南京90后,大数据\n6552313685874835726_!_106_!_news_house_!_涨价之前买房的人，现在是什么心情？_!_\n6552447172724392456_!_106_!_news_house_!_这种凸阳台房子万万不要买，若不是售楼闺蜜说，我家就吃大亏_!_凸阳台,售楼,买房\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>每行为一条数据，以_!_分割的个字段，从前往后分别是 新闻ID，分类code（见下文），分类名称（见下文），新闻字符串（仅含标题），新闻关键词</p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h2>二、短语发现、新词发现算法介绍</h2><p>2.1  理论介绍</p><p class=\"ztext-empty-paragraph\"><br/></p><p>短语发现、新词发现，内容与算法基础源于该博客：<a href=\"https://link.zhihu.com/?target=http%3A//zhanghonglun.cn/blog/project/%25E5%259F%25BA%25E4%25BA%258E%25E5%2587%259D%25E8%2581%259A%25E5%25BA%25A6%25E5%2592%258C%25E8%2587%25AA%25E7%2594%25B1%25E5%25BA%25A6%25E7%259A%2584%25E9%259D%259E%25E7%259B%2591%25E7%259D%25A3%25E8%25AF%258D%25E5%25BA%2593%25E7%2594%259F%25E6%2588%2590/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">基于凝聚度和自由度的非监督词库生成</a></p><p>评估词之间的几个指标，出了频率还有：</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>凝聚度：</li></ul><blockquote>该词语为S，首先计算该词语出现的概率P(S)，然后尝试S的所有可能的二切分，即分为左半部分sl和右半部分sr并计算P(sl)和P(sr)，<br/>    例如双汉字词语存在一种二切分、三汉字词语存在两种二切分。接下来计算所有二切分方案中，P(S)/(P(sl)×P(sr))的最小值，取对数之后即可作为聚合度的衡量。<br/>    以双汉字词语为例，可以想象到，如果该词语的聚合度很低，说明其第一个字和第二个字的相关性很弱，甚至是不相关的，那么P(S)和P(sl)×P(sr)将处于同一个数量级。<br/>    相反，如果该词语的聚合度很高，“齐天”、“大圣”和“齐天大圣”三者的概率都很接近，因此P(S)/(P(sl)×P(sr))将是一个远大于1的数值。</blockquote><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>自由度：</li></ul><blockquote>用熵来衡量一个词语的自由度。假设一个词语一共出现了N次，其左边共出现过n个汉字，每个汉字依次出现N1，N2，……，Nn次，则满足N = N1 + N2 + …… + Nn，因此可以计算该词语左边各个汉字出现的概率，<br/>并根据熵公式计算左邻熵。熵越小则自由度越低，例如“天大圣”的左邻熵接近于0，因为“齐”字的概率几乎为1；熵越大则自由度越高，表示用词搭配越混乱、越自由、越多样。<br/>因为“天大圣”的左邻熵很小，而右邻熵则相对较大，因此我们将一个词语左邻熵和右邻熵中较小者作为最终的自由度。</blockquote><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>IDF:</li></ul><blockquote>逆文档词频</blockquote><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.2 主函数参数</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>算法的参数描述：</p><div class=\"highlight\"><pre><code class=\"language-text\">class termsRecognition(object):\n    def __init__(self, content=&#39;&#39;,  topK=-1, tfreq=10, tDOA=0, tDOF=0, is_jieba= False,mode = [1]):\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>其中的参数：：</p><ul><li>content: 待成词的文本</li><li>maxlen: 词的最大长度</li><li>topK: 返回的词数量</li><li>tfreq: 频数阈值</li><li>tDOA: 聚合度阈值</li><li>tDOF: 自由度阈值</li><li>mode：词语生成模式，一共四种模式，其中第二种模式比较好,一定要写成[1]</li><li>diction:字典，第一批Jieba分词之后的内容</li><li>idf_diction:在第一批字典之后，又生成一批tuple words 的idf，计算方式是，两个词语的平均</li><li>punct:标点符号，Jieba分词之后删除</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>算法步骤：</p><ul><li>jieba_tuples_generator， <br/>   利用Jieba分词，并去除标点符号，去除清除&#39;&#39;(写入self.jieba_content)，利用wordsGenerator函数生成词语对（四种模式）(写入self.tuple_content)</li><li>word_get_frequency_idf，计算freq<br/>   以及贴idf,同时生成&#39;left&#39;/right框，把词语对（self.tuple_content)）写入result(★ 主要写入部分)</li><li>get_doa：只输入result,计算数据的doa，直接更新result中的[&#39;doa&#39;]</li><li>word_get_dof:只输入result,计算数据的dof，直接更新result中的[&#39;dof&#39;],左熵的文字,右熵的文字</li><li>get_score,只输入result,更新result中的[&#39;scores&#39;]</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>可用的函数:</p><ul><li>get_idf,文档的IDF计算</li><li>wordsGenerator,生成词语对</li><li>get_entropy计算左、右熵值，填充result</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><hr/><h2>三、词共现算法介绍</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>就是计算词语共同出现的概率，一般用在构建词条网络的时候用得到，之前看到这边博客提到他们自己的算法：《<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/alanconstantinelau/article/details/69258443\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">python构建关键词共现矩阵</a>》看着好麻烦，于是乎自己简单写了一个，还是那个问题....效率比较低...<br/>之前一般的做法是先生成一个基于词-词矩阵，然后去累计词-词之间的出现次数。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-25b7da74c3bd6c3d20412dca1dc249ca_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"371\" data-rawheight=\"230\" class=\"content_image\" width=\"371\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;371&#39; height=&#39;230&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"371\" data-rawheight=\"230\" class=\"content_image lazy\" width=\"371\" data-actualsrc=\"https://pic3.zhimg.com/v2-25b7da74c3bd6c3d20412dca1dc249ca_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我这边只是简单利用笛卡尔积:</p><ul><li>permutations 排列</li><li>combinations 组合,没有重复</li><li>combinations_with_replacement 组合,有重复</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">&gt;&gt;&gt; import itertools  \n&gt;&gt;&gt; for i in itertools.product(&#39;ABCD&#39;, repeat = 2):  \n...     print i,  \n...   \n(&#39;A&#39;, &#39;A&#39;) (&#39;A&#39;, &#39;B&#39;) (&#39;A&#39;, &#39;C&#39;) (&#39;A&#39;, &#39;D&#39;) (&#39;B&#39;, &#39;A&#39;) (&#39;B&#39;, &#39;B&#39;) (&#39;B&#39;, &#39;C&#39;) (&#39;B&#39;, &#39;D&#39;) (&#39;C&#39;, &#39;A&#39;) (&#39;C&#39;, &#39;B&#39;) (&#39;C&#39;, &#39;C&#39;) (&#39;C&#39;, &#39;D&#39;) (&#39;D&#39;, &#39;A&#39;) (&#39;D&#39;, &#39;B&#39;) (&#39;D&#39;, &#39;C&#39;) (&#39;D&#39;, &#39;D&#39;)  \n&gt;&gt;&gt; for i in itertools.permutations(&#39;ABCD&#39;, 2):  \n...     print i,  \n...   \n(&#39;A&#39;, &#39;B&#39;) (&#39;A&#39;, &#39;C&#39;) (&#39;A&#39;, &#39;D&#39;) (&#39;B&#39;, &#39;A&#39;) (&#39;B&#39;, &#39;C&#39;) (&#39;B&#39;, &#39;D&#39;) (&#39;C&#39;, &#39;A&#39;) (&#39;C&#39;, &#39;B&#39;) (&#39;C&#39;, &#39;D&#39;) (&#39;D&#39;, &#39;A&#39;) (&#39;D&#39;, &#39;B&#39;) (&#39;D&#39;, &#39;C&#39;)  \n&gt;&gt;&gt; for i in itertools.combinations(&#39;ABCD&#39;, 2):  \n...     print i,  \n...   \n(&#39;A&#39;, &#39;B&#39;) (&#39;A&#39;, &#39;C&#39;) (&#39;A&#39;, &#39;D&#39;) (&#39;B&#39;, &#39;C&#39;) (&#39;B&#39;, &#39;D&#39;) (&#39;C&#39;, &#39;D&#39;)  \n&gt;&gt;&gt; for i in itertools.combinations_with_replacement(&#39;ABCD&#39;, 2):  \n...     print i,  \n...   \n(&#39;A&#39;, &#39;A&#39;) (&#39;A&#39;, &#39;B&#39;) (&#39;A&#39;, &#39;C&#39;) (&#39;A&#39;, &#39;D&#39;) (&#39;B&#39;, &#39;B&#39;) (&#39;B&#39;, &#39;C&#39;) (&#39;B&#39;, &#39;D&#39;) (&#39;C&#39;, &#39;C&#39;) (&#39;C&#39;, &#39;D&#39;) (&#39;D&#39;, &#39;D&#39;) \n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h2>四、练习题</h2><p class=\"ztext-empty-paragraph\"><br/></p><p><b>文件夹介绍：</b></p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>短语发现、新词发现算法：termsRecognition.py</li><li>今日头条数据38w：toutiao_data.csv</li><li>二元组算法：tuplewords.py</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p><b>先来看看数据长啥样：</b><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4ceb097e02a712307174104f3f517447_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1236\" data-rawheight=\"490\" class=\"origin_image zh-lightbox-thumb\" width=\"1236\" data-original=\"https://pic4.zhimg.com/v2-4ceb097e02a712307174104f3f517447_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1236&#39; height=&#39;490&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1236\" data-rawheight=\"490\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1236\" data-original=\"https://pic4.zhimg.com/v2-4ceb097e02a712307174104f3f517447_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4ceb097e02a712307174104f3f517447_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>废话不多说，直接使用一下：</b></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>4.1 短语发现、新词发现模块</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>该模块可以允许两种内容输入，探究的是词-词之间连续共现，一种数据格式是没有经过分词的、第二种是经过分词的。<br/>其中，算法会提到全部发现以及部分发现两种模式，这两种模式的区别主要在于考察指标的多少。</p><ul><li>全部发现会考察：凝聚度、自由度、IDF、词频</li><li>部分发现会考察：IDF、词频</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2>4.1.1 没有经过分词的原始语料</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>在今日头条数据之中就是标题数据了，一般用来新词发现，这边整体运行很慢，就截取前10000个。</p><div class=\"highlight\"><pre><code class=\"language-text\">data = pd.read_csv(&#39;toutiao_data.csv&#39;,encoding = &#39;utf-8&#39;)\n\ngenerator = termsRecognition(content = data[&#39;new_title&#39;][:10000] ,is_jieba=False, topK = 20 , mode = [1])   # 文字版\n# 全部发现\nresult_dict = generator.generate_word()\nresult_dataframe = generator.get_result()\n# 部分发现\nresult_dataframe = generator.part_found()\n\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>得到的结论，如图：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7c576bd84265e3d8aa2b73c0dda3f32b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"708\" data-rawheight=\"451\" class=\"origin_image zh-lightbox-thumb\" width=\"708\" data-original=\"https://pic4.zhimg.com/v2-7c576bd84265e3d8aa2b73c0dda3f32b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;708&#39; height=&#39;451&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"708\" data-rawheight=\"451\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"708\" data-original=\"https://pic4.zhimg.com/v2-7c576bd84265e3d8aa2b73c0dda3f32b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7c576bd84265e3d8aa2b73c0dda3f32b_b.jpg\"/></figure><p><br/>这边其实可以在Jieba分词的时候，预先载入一些停用词。这边来看，发现的有：对下联、王者荣耀<br/>新词发现的能力还不够好。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>4.1.2 经过分词的原始语料</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>比较适合用在已经分完词的语料比较适合：[[&#39;经过&#39;,&#39;分词&#39;],[&#39;的&#39;,&#39;原始&#39;],[&#39;原始&#39;,&#39;语料&#39;],...]<br/>当然，探究的是词-词之间的连续共现的情况。此时，我用今日头条的关键词其实不是特别合适，因为关键词之间没有前后逻辑关系在其中。<br/>在此只是简单给观众看一下功能点。</p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">data = pd.read_csv(&#39;toutiao_data.csv&#39;,encoding = &#39;utf-8&#39;)\ndef not_nan(obj):\n    return obj == obj\n\nkeywords = []\nfor word in tqdm(data.new_keyword):\n    if not_nan(word):\n        keywords.append(word.split(&#39;,&#39;))\n\ngenerator = termsRecognition(content = keywords[:1000] , is_jieba=True , topK = 20,mode = [1]) #图像版\n# 部分发现\nresult_dataframe = generator.part_found()\n</code></pre></div><p>得到的结论：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b8f14a7449e2eed4d49b87c204f7df05_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"618\" data-rawheight=\"444\" class=\"origin_image zh-lightbox-thumb\" width=\"618\" data-original=\"https://pic2.zhimg.com/v2-b8f14a7449e2eed4d49b87c204f7df05_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;618&#39; height=&#39;444&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"618\" data-rawheight=\"444\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"618\" data-original=\"https://pic2.zhimg.com/v2-b8f14a7449e2eed4d49b87c204f7df05_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b8f14a7449e2eed4d49b87c204f7df05_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其中发现了的规律都没啥用，大家看看就行。。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>4.2 词共现模块</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>二元组模块跟4.1中，分完词之后的应用有点像，但是这边是离散的，之前的那个考察词-词之间的排列需要有逻辑关系，这边词共现会更加普遍。<br/>该模块较多会应用在基于关键词的SNA社交网络发现之中，给张好看的图：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-bb342a3fffd4fbe0661acf71402c79aa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"673\" data-rawheight=\"440\" class=\"origin_image zh-lightbox-thumb\" width=\"673\" data-original=\"https://pic3.zhimg.com/v2-bb342a3fffd4fbe0661acf71402c79aa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;673&#39; height=&#39;440&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"673\" data-rawheight=\"440\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"673\" data-original=\"https://pic3.zhimg.com/v2-bb342a3fffd4fbe0661acf71402c79aa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-bb342a3fffd4fbe0661acf71402c79aa_b.jpg\"/></figure><p><br/><b>其中，在该模块写入了两种：</b></p><ul><li>热词统计</li><li>词共现统计</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">data = pd.read_csv(&#39;toutiao_data.csv&#39;,encoding = &#39;utf-8&#39;)\n\n\ndef not_nan(obj):\n    return obj == obj\n\nkeywords = []\nfor word in tqdm(data.new_keyword):\n    if not_nan(word):\n        keywords.append(word.split(&#39;,&#39;))\n\n# 设置停用词\nstop_word = [&#39;方法&#39;,&#39;结论&#39;]\ntw = TupleWords(stop_word)\n\n# 得到结果\nid_pools,tuple_pools = tw.CoOccurrence(keywords[:1000])\n\n# 内容变成dataframe\nCoOccurrence_data = tw.tansferDataFrame(tuple_pools)\nCoOccurrence_data\n\n# 热词统计模块\nhotwords_dataframe = tw.Hotwords(keywords[:1000])\nhotwords_dataframe\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>该模块输入的是keywords，List形：</p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">[[&#39;保利集团&#39;, &#39;马未都&#39;, &#39;中国科学技术馆&#39;, &#39;博物馆&#39;, &#39;新中国&#39;],\n [&#39;林风眠&#39;,\n  &#39;黄海归来步步云&#39;,\n  &#39;秋山图&#39;,\n  &#39;计白当黑&#39;,\n  &#39;山水画&#39;,\n  &#39;江山万里图&#39;,\n  &#39;张大千&#39;,\n  &#39;巫峡清秋图&#39;,\n  &#39;活眼&#39;,\n  &#39;山雨欲来图&#39;],\n [&#39;牡丹&#39;, &#39;收藏价值&#39;],\n [&#39;叶浅予&#39;, &#39;田世光&#39;, &#39;李苦禅&#39;, &#39;花鸟画&#39;, &#39;中央美术学院&#39;]\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>tw.CoOccurrence就是对上面的内容进行解析，得到了：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d85dd0441315f19ebda916c465e57109_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"570\" data-rawheight=\"627\" class=\"origin_image zh-lightbox-thumb\" width=\"570\" data-original=\"https://pic2.zhimg.com/v2-d85dd0441315f19ebda916c465e57109_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;570&#39; height=&#39;627&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"570\" data-rawheight=\"627\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"570\" data-original=\"https://pic2.zhimg.com/v2-d85dd0441315f19ebda916c465e57109_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d85dd0441315f19ebda916c465e57109_b.jpg\"/></figure><p><br/>发现，快乐大本营-谢娜的组合比较多，詹姆斯-猛龙嘛，看客们懂的，詹皇血克猛龙，哈哈~</p><p>热词发现这个很常规：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-dae339bb04a70a7d599cd8a9866dc3de_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"213\" data-rawheight=\"429\" class=\"content_image\" width=\"213\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;213&#39; height=&#39;429&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"213\" data-rawheight=\"429\" class=\"content_image lazy\" width=\"213\" data-actualsrc=\"https://pic3.zhimg.com/v2-dae339bb04a70a7d599cd8a9866dc3de_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p>后续拓展——SNA社交网络发现网络图：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>得到了CoOccurrence_data 的表格，有了词共现，就可以画社交网络图啦，有很多好的博客都有这样的介绍，推荐几篇：<br/><a href=\"https://link.zhihu.com/?target=https%3A//www.jianshu.com/p/2c8a81112ad4\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">基于共现发现人物关系的python实现</a><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ea7f563139e4eb7207537b9d70cd1e9d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"384\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pic2.zhimg.com/v2-ea7f563139e4eb7207537b9d70cd1e9d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;690&#39; height=&#39;384&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"384\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"690\" data-original=\"https://pic2.zhimg.com/v2-ea7f563139e4eb7207537b9d70cd1e9d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ea7f563139e4eb7207537b9d70cd1e9d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.jianshu.com/p/db7e3e4f728d\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">python简单实战项目：《冰与火之歌1-5》角色关系图谱构建——人物关系可视化</a><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f22ffd3bf408fcd08cf2a1c557ad967a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"700\" data-rawheight=\"700\" class=\"origin_image zh-lightbox-thumb\" width=\"700\" data-original=\"https://pic3.zhimg.com/v2-f22ffd3bf408fcd08cf2a1c557ad967a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;700&#39; height=&#39;700&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"700\" data-rawheight=\"700\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"700\" data-original=\"https://pic3.zhimg.com/v2-f22ffd3bf408fcd08cf2a1c557ad967a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f22ffd3bf408fcd08cf2a1c557ad967a_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "数据挖掘", 
                    "tagLink": "https://api.zhihu.com/topics/19553534"
                }, 
                {
                    "tag": "文本挖掘", 
                    "tagLink": "https://api.zhihu.com/topics/19565718"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/36251601", 
            "userName": "悟乙己", 
            "userLink": "https://www.zhihu.com/people/428693835688c688c6c9ac1d30fc0ece", 
            "upvote": 85, 
            "title": "练习题︱图像分割与识别——UNet网络练习案例（两则）", 
            "content": "<p>﻿&gt; 代码见Github：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/mattzheng/U-Net-Demo\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">mattzheng/U-Net-Demo</a></p><p class=\"ztext-empty-paragraph\"><br/></p><p><i>CSDN越来越不好使，给差评！！！</i></p><p class=\"ztext-empty-paragraph\"><br/></p><p>U-Net是Kaggle比赛非常青睐的模型，简单、高效、易懂，容易定制，可以从相对较小的训练集中学习。来看几个变形：</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>（1）<a href=\"https://link.zhihu.com/?target=https%3A//supervise.ly/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Supervise.ly</a> 公司。<br/>在用  Faster-RCNN（基于   NasNet）定位 + UNet-like 架构的分割，来做他们数据众包图像分割方向的主动学习，当时没有使用   Mask-RCNN，因为靠近物体边缘的分割质量很低（<a href=\"https://link.zhihu.com/?target=https%3A//www.leiphone.com/news/201804/h2LP6OeEwgmGghER.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">终于！Supervise.ly 发布人像分割数据集啦（免费开源）</a>）；</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>（2）Kaggle-卫星图像分割与识别。<br/>需要分割出：房屋和楼房；混杂的人工建筑；道路；铁路；树木；农作物；河流；积水区；大型车辆；小轿车。在U-Net基础上微调了一下。   而且针对不同的图像类型，微调的地方不一样，就会有不同的分割模型，最后融合。（<a href=\"https://zhuanlan.zhihu.com/p/26377387\" class=\"internal\">Kaggle优胜者详解：如何用深度学习实现卫星图像分割与识别</a>）</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>（3）广东政务数据创新大赛—智能算法赛 。<br/>国土监察业务中须监管地上建筑物的建、拆、改、扩，高分辨率图像和智能算法以自动化完成工作。并且：八通道U-Net：直接输出房屋变化，可应对高层建筑倾斜问题；数据增强：增加模型泛化性，简单有效；加权损失函数：增强对新增建筑的检测能力；模型融合：取长补短，结果更全。（参考：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/LiuDongjing/BuildingChangeDetector\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">LiuDongjing/BuildingChangeDetector</a>）<br/></li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6db5700e6a9fbc9bcf4642dba9bbad76_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"916\" data-rawheight=\"837\" class=\"origin_image zh-lightbox-thumb\" width=\"916\" data-original=\"https://pic3.zhimg.com/v2-6db5700e6a9fbc9bcf4642dba9bbad76_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;916&#39; height=&#39;837&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"916\" data-rawheight=\"837\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"916\" data-original=\"https://pic3.zhimg.com/v2-6db5700e6a9fbc9bcf4642dba9bbad76_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6db5700e6a9fbc9bcf4642dba9bbad76_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>（4）Kaggle车辆边界识别——TernausNet。<br/>由VGG初始化权重 + U-Net网络，Kaggle Carvana Image Masking Challenge 第一名，使用的预训练权重改进U-Net，提升图像分割的效果。开源的代码在<a href=\"https://link.zhihu.com/?target=https%3A//github.com/ternaus/TernausNet\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ternaus/TernausNet</a><br/><br/><br/>当然现在还有很多流行、好用的分割网络：谷歌的DeepLabv3+（<a href=\"https://link.zhihu.com/?target=https%3A//github.com/tensorflow/models/tree/master/research/deeplab\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">DeepLab: Deep Labelling for</a><br/><a href=\"https://link.zhihu.com/?target=https%3A//github.com/tensorflow/models/tree/master/research/deeplab\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Semantic Image  Segmentation</a>）、<a href=\"https://link.zhihu.com/?target=https%3A//github.com/matterport/Mask_RCNN/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Mask  R-CNN</a>、COCO-16  图像分割冠军的实例分割FCIS（<a href=\"https://link.zhihu.com/?target=https%3A//github.com/msracver/FCIS\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">msracver/FCIS</a>） 等。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>跟目标检测需要准备的数据集不一样，因为图像分割是图像中实体的整个轮廓，所以标注的内容就是物体的掩膜。有两种标记方式：一种是提供单个物体的掩膜、一种是提供物体轮廓的标点。</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>一、U-Net网络练习题一： Kaggle - 2018 Data Science Bowl</h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>因为Kaggle有该比赛，而且code写的很简单易懂，于是乎拿来玩一下。<a href=\"https://link.zhihu.com/?target=https%3A//www.kaggle.com/keegil/keras-u-net-starter-lb-0-277%3FscriptVersionId%3D2164855/notebook\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Keras U-Net starter - LB 0.277</a><br/>与U-Net相关的开源项目与code很多，各种框架的版本都有：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/jakeret/tf_unet\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Tensorflow Unet</a>、<a href=\"https://link.zhihu.com/?target=https%3A//www.kaggle.com/drn01z3/end-to-end-baseline-with-u-net-keras\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">End-to-end baseline with U-net (keras)</a>等等。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>1.1 训练集的构造</h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>因为使用的是比赛数据，赛方已经很好地帮我们做好了前期数据整理的工作，所以目前来说可能很方便的制作训练集、测试集然后跑模型。这里下载得到的数据为提供图像中单个物体的掩膜。其中，笔者认为最麻烦的就是标注集的构造（掩膜）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>原图：</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a5c0ec38eb8397327466904a393cff77_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"320\" data-rawheight=\"256\" class=\"content_image\" width=\"320\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;320&#39; height=&#39;256&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"320\" data-rawheight=\"256\" class=\"content_image lazy\" width=\"320\" data-actualsrc=\"https://pic4.zhimg.com/v2-a5c0ec38eb8397327466904a393cff77_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>掩膜图：</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6f25d224b2388d46e65a4970dc17000a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"810\" data-rawheight=\"485\" class=\"origin_image zh-lightbox-thumb\" width=\"810\" data-original=\"https://pic3.zhimg.com/v2-6f25d224b2388d46e65a4970dc17000a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;810&#39; height=&#39;485&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"810\" data-rawheight=\"485\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"810\" data-original=\"https://pic3.zhimg.com/v2-6f25d224b2388d46e65a4970dc17000a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6f25d224b2388d46e65a4970dc17000a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>从掩膜列表可以到，比赛中是把每个细胞的掩膜都分开来了。来看一下这个掩膜标注内容如何：</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\nY_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\nfor mask_file in next(os.walk(path + &#39;/masks/&#39;))[2]:\n    mask_ = imread(path + &#39;/masks/&#39; + mask_file)\n    mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode=&#39;constant&#39;, \n                                  preserve_range=True), axis=-1)\n    mask = np.maximum(mask, mask_)\nY_train[n] = mask\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>读入（imread）掩膜图，图像的格式为：(m,n)；</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>resize，掩膜的尺寸缩放在128*128</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>np.expand_dims步骤改变图像维度为(m,n,1)；</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>np.maximum，当出现很多掩膜的时候，有些掩膜会重叠，那么就需要留下共有的部分；</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>Y_train的数据格式已经定义为bool型，那么最后存储得到的数据即为(x,m,n,1)，且数据格式为True/False：</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">array([[[[False],\n         [False],\n         [False],\n         ..., \n         [False],\n         [False],\n         [False]],\n\n        [[False],\n         [False],\n         [False],\n         ..., \n         [False],\n...\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>其他X_train训练数据集，就会被存储成：(x,m,n,3)，同时需要resize成128*128</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>1.2 预测</h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>预测就可以用<code>model.predict(X_test, verbose=1)</code>，即可以得到结果。那么得到的结果是(128,128,1)的，那么就是一个图层，也就是说U-Net出来的结果是单标签的，如果是多标签那么可以多套模型，可参考：Kaggle-卫星图像分割与识别。<br/>预测出来的结果为单图层，可以重新回到原尺寸：</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">resize(np.squeeze(preds_test[i]),\n     (sizes_test[i][0], sizes_test[i][1]), mode=&#39;constant&#39;, preserve_range=True)\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>1.3 结果提交</h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>图像分割在提交结果的时候，主要就是掩膜了。那么掩膜的提交需要编码压缩：<br/>Run-Length Encoding（RLE）行程长度的原理是将一扫描行中的颜色值相同的相邻像素用一个计数值和那些像素的颜色值来代替。例如:aaabccccccddeee，则可用3a1b6c2d3e来代替。对于拥有大面积，相同颜色区域的图像，用RLE压缩方法非常有效。由RLE原理派生出许多具体行程压缩方法。<br/>那么图像压缩出来的结果即为：</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">&#39;137795 3 138292 25 138802 29 139312 32 139823 34 140334 36 140845 38 141356 40 141867 42 142371 51 142881 54 143391 57 143902 59 144414 59 144925 61 145436 62 145948 63 146459 65 146970 66 147482 66 147994 66 148506 66 149017 67 149529 67 150041 67 150553 67 151065 67 151577 66 152089 66 152602 65 153114 64 153626 64 154138 63 154650 63 155162 63 155674 63 156187 62 156699 62 157212 60 157724 60 158236 60 158749 59 159261 59 159773 58 160285 58 160798 56 161310 56 161823 55 162335 54 162848 53 163361 52 163874 50 164387 49 164899 48 165412 47 165925 45 166439 42 166953 40 167466 38 167980 35 168495 31 169009 28 169522 26 170036 23 170549 21 171062 18 171577 12 172093 4&#39;\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>那么下图就是出来的结果了，第一张为原图，第二张为标注的掩膜图，第三张为预测图。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-365f9ddbbc697d0384f9d69faeb3cff4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"255\" data-rawheight=\"720\" class=\"content_image\" width=\"255\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;255&#39; height=&#39;720&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"255\" data-rawheight=\"720\" class=\"content_image lazy\" width=\"255\" data-actualsrc=\"https://pic1.zhimg.com/v2-365f9ddbbc697d0384f9d69faeb3cff4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>二、U-Net网络练习题二：气球识别</h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>在《如何使用Mask RCNN模型进行图像实体分割？》一文中提到了用Mask-RCNN来做气球分割，官网之中也有对应的代码，本着练习的态度，那么笔者就拿来这个数据集继续练手，最麻烦的仍然是如何得到标注数据。MaskRCNN的开源code为<a href=\"https://link.zhihu.com/?target=https%3A//github.com/matterport/Mask_RCNN/blob/v2.1/samples/balloon/inspect_balloon_data.ipynb\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Mask R-CNN - Inspect Balloon Training Data</a></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>由于很多内容是从Mask R-CNN之中挖过来的，笔者也没细究，能用就行，所以会显得很笨拙...</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.1 训练集的准备</h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>数据下载页面：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">balloon_dataset.zip</a><br/>该案例更为通用，因为比赛的训练集是比赛方写好的，一般实际训练的时候，掩膜都是没有给出的，而只是给出标记点，如：</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1c5d6f18f76cb46c9832358d9dd17963_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"854\" data-rawheight=\"525\" class=\"origin_image zh-lightbox-thumb\" width=\"854\" data-original=\"https://pic4.zhimg.com/v2-1c5d6f18f76cb46c9832358d9dd17963_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;854&#39; height=&#39;525&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"854\" data-rawheight=\"525\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"854\" data-original=\"https://pic4.zhimg.com/v2-1c5d6f18f76cb46c9832358d9dd17963_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1c5d6f18f76cb46c9832358d9dd17963_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5ce4435fcc5cc77af995a5ee5533a260_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1159\" data-rawheight=\"527\" class=\"origin_image zh-lightbox-thumb\" width=\"1159\" data-original=\"https://pic1.zhimg.com/v2-5ce4435fcc5cc77af995a5ee5533a260_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1159&#39; height=&#39;527&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1159\" data-rawheight=\"527\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1159\" data-original=\"https://pic1.zhimg.com/v2-5ce4435fcc5cc77af995a5ee5533a260_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-5ce4435fcc5cc77af995a5ee5533a260_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>此时的标注数据都放在json之中，譬如：</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">{&#39;10464445726_6f1e3bbe6a_k.jpg712154&#39;: {&#39;base64_img_data&#39;: &#39;&#39;,\n  &#39;file_attributes&#39;: {},\n  &#39;filename&#39;: &#39;10464445726_6f1e3bbe6a_k.jpg&#39;,\n  &#39;fileref&#39;: &#39;&#39;,\n  &#39;regions&#39;: {&#39;0&#39;: {&#39;region_attributes&#39;: {},\n    &#39;shape_attributes&#39;: {&#39;all_points_x&#39;: [1757,\n      1772,\n      1787,\n      1780,\n      1764],\n     &#39;all_points_y&#39;: [867,\n      913,\n      986,\n      1104,\n      1170],\n     &#39;name&#39;: &#39;polygon&#39;}},\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><code>all_points_x</code>以及<code>all_points_y</code>都是掩膜标记的（x，y）点坐标，每一个物体都是由很多个box构造而成：</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">def get_mask(a,dataset_dir):\n    image_path = os.path.join(dataset_dir, a[&#39;filename&#39;])\n    image = io.imread(image_path)\n    height, width = image.shape[:2]\n    polygons = [r[&#39;shape_attributes&#39;] for r in a[&#39;regions&#39;].values()]\n    mask = np.zeros([height, width, len(polygons)],dtype=np.uint8) \n\n    # 掩膜mask\n    for i, p in enumerate(polygons):\n        # Get indexes of pixels inside the polygon and set them to 1\n        rr, cc = skimage.draw.polygon(p[&#39;all_points_y&#39;], p[&#39;all_points_x&#39;])\n        mask[rr, cc, i] = 1\n    # 此时mask为(685, 1024, 1)\n\n    # mask二值化\n    mask, class_ids = mask.astype(np.bool), np.ones([mask.shape[-1]], dtype=np.int32)\n\n    # 提取每个掩膜的坐标\n    boxes = extract_bboxes(resize(mask, (128, 128), mode=&#39;constant&#39;,preserve_range=True))\n\n    unique_class_ids = np.unique(class_ids)\n    mask_area = [np.sum(mask[:, :, np.where(class_ids == i)[0]])\n                     for i in unique_class_ids]\n    top_ids = [v[0] for v in sorted(zip(unique_class_ids, mask_area),\n                                    key=lambda r: r[1], reverse=True) if v[1] &gt; 0]\n\n    class_id = top_ids[0]\n    # Pull masks of instances belonging to the same class.\n    m = mask[:, :, np.where(class_ids == class_id)[0]]\n    m = np.sum(m * np.arange(1, m.shape[-1] + 1), -1)\n\n    return m,image,height,width,class_ids,boxes\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>polygon之中记录的是一个掩膜的(x，y)点坐标，然后通过<code>skimage.draw.polygon</code>连成圈；</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li><code>mask[rr, cc, i] = 1</code>这句中，mask变成了一个0/1的(m,n,x)的矩阵,x代表可能有x个物体；</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>mask.astype(np.bool)将上述的0/1矩阵，变为T/F矩阵；</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>extract_bboxes()函数，要着重说，因为他是根据掩膜的位置，找出整体掩膜的坐标点，给入5个物体，他就会返回5个物体的坐标<code>（xmax,ymax,xmin,ymin）</code></li></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>np.sum()是降维的过程，把(m,n,1)到(m,n)</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>那么，最终 Y_train的数据格式如案例一，一样的：</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">array([[[[False],\n         [False],\n         [False],\n         ..., \n         [False],\n         [False],\n         [False]],\n\n        [[False],\n         [False],\n         [False],\n         ..., \n         [False],\n...\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.2 模型预测</h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">model = load_model(model_name, custom_objects={&#39;mean_iou&#39;: mean_iou})\npreds_train = model.predict(X_train[:int(X_train.shape[0]*0.9)], verbose=1) \npreds_val = model.predict(X_train[int(X_train.shape[0]*0.9):],verbose=1)   \npreds_test = model.predict(X_test,verbose=1)  \n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>这边的操作是把trainset按照9:1，分为训练集、验证集，还有一部分是测试集</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>输入维度:</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">X_train (670, 128, 128, 3)\n    Y_train (670, 128, 128, 1)\n    X_test  (65, 128, 128, 3)\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>输出维度:<br/>每个像素点的概率[0,1]</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">preds_train  (603, 128, 128, 1)\n    preds_val    (67, 128, 128, 1)\n    preds_test   (65, 128, 128, 1)\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.3 画图函数</h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>该部分是从MaskRCNN中搬过来的，</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">def display_instances(image, boxes, masks, class_names,\n                      scores=None, title=&#34;&#34;,\n                      figsize=(16, 16), ax=None,\n                      show_mask=True, show_bbox=True,\n                      colors=None, captions=None):\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>需要图像矩阵image，boxes代表每个实例的boxes，masks是图像的掩膜，class_names，是每张图标签的名称。下图是128*128像素的，很模糊，将就着看吧...</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-31670609de5d5e265efd610cb86e6fcb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"599\" data-rawheight=\"628\" class=\"origin_image zh-lightbox-thumb\" width=\"599\" data-original=\"https://pic4.zhimg.com/v2-31670609de5d5e265efd610cb86e6fcb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;599&#39; height=&#39;628&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"599\" data-rawheight=\"628\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"599\" data-original=\"https://pic4.zhimg.com/v2-31670609de5d5e265efd610cb86e6fcb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-31670609de5d5e265efd610cb86e6fcb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>随机颜色生成函数random_colors</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">def random_colors(N, bright=True):\n    &#34;&#34;&#34;\n    Generate random colors.\n    To get visually distinct colors, generate them in HSV space then\n    convert to RGB.\n    &#34;&#34;&#34;\n    brightness = 1.0 if bright else 0.7\n    hsv = [(i / N, 1, brightness) for i in range(N)]\n    colors = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))\n    random.shuffle(colors)\n    return colors\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>还有就是一般来说，掩膜如果是(m,n)，或者让是(m,n,1)都是可以画出来的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">imshow(mask)\nplt.show()</code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }, 
                {
                    "tag": "图像分割", 
                    "tagLink": "https://api.zhihu.com/topics/20137632"
                }, 
                {
                    "tag": "图像识别", 
                    "tagLink": "https://api.zhihu.com/topics/19588774"
                }
            ], 
            "comments": [
                {
                    "userName": "马良", 
                    "userLink": "https://www.zhihu.com/people/972b92a07a4e50ef0bf5f32c2695cda5", 
                    "content": "<p>老哥，请问关于Unet训练医疗影像的二分类问题，预测出来的图片全黑或者全白是什么原因呢？</p>", 
                    "likes": 1, 
                    "childComments": [
                        {
                            "userName": "Loing", 
                            "userLink": "https://www.zhihu.com/people/632895f9939671383a20fd8527492b9e", 
                            "content": "我也遇到过，换参数重新训练就好了，貌似不resize训练效果会好点，但是会慢点。", 
                            "likes": 0, 
                            "replyToAuthor": "马良"
                        }
                    ]
                }, 
                {
                    "userName": "jeremy", 
                    "userLink": "https://www.zhihu.com/people/2ca6bab935156c45851e98a367584f86", 
                    "content": "老哥unet做多分类可以指导下吗", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "qq957606954", 
                    "userLink": "https://www.zhihu.com/people/f6999cd02b0a362622a3a69d46901c5c", 
                    "content": "我在对医学图像分割，可以指导下吗<br>，有点迷茫", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "Kevin", 
                    "userLink": "https://www.zhihu.com/people/7c11821486daae5a67e54b5e2a95ba4b", 
                    "content": "<p>楼主请问您的测试集是带有GT label的吧。如果使用没有label的测试集结果会怎么样？我的分割结果是有label的比没有的要好，不知道是不是对结果产生了影响</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_179237046"
}
