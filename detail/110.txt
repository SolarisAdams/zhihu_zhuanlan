{
    "title": "广告算法", 
    "description": "分享工业界机器学习的实践经验。\n想要内推一线互联网的可以直接私信。阿里腾讯百度头条滴滴拼多多等等都行，可以帮忙直接简历投送给小组组长。", 
    "followers": [
        "https://www.zhihu.com/people/yinghuacao", 
        "https://www.zhihu.com/people/xiao-xu-33-83", 
        "https://www.zhihu.com/people/wang-xiao-xiao-11-45", 
        "https://www.zhihu.com/people/zhao-wei-255", 
        "https://www.zhihu.com/people/jayden-li", 
        "https://www.zhihu.com/people/yu-yang-62-85", 
        "https://www.zhihu.com/people/lin-lin-87-38", 
        "https://www.zhihu.com/people/starletwan", 
        "https://www.zhihu.com/people/timefeeling", 
        "https://www.zhihu.com/people/feng-feng-feng-tu", 
        "https://www.zhihu.com/people/tonk-83", 
        "https://www.zhihu.com/people/tangkh", 
        "https://www.zhihu.com/people/yy-cc-63-54", 
        "https://www.zhihu.com/people/worhol", 
        "https://www.zhihu.com/people/wengou422", 
        "https://www.zhihu.com/people/she-liang", 
        "https://www.zhihu.com/people/ni-jeremy", 
        "https://www.zhihu.com/people/wdw110", 
        "https://www.zhihu.com/people/youngsterxyf", 
        "https://www.zhihu.com/people/li-wei-qiang", 
        "https://www.zhihu.com/people/cao-ji-49-42", 
        "https://www.zhihu.com/people/kyle-chen-15", 
        "https://www.zhihu.com/people/yan-1", 
        "https://www.zhihu.com/people/da-xiang-yu-zhu", 
        "https://www.zhihu.com/people/zhang-xiao-hui-66-53", 
        "https://www.zhihu.com/people/yi-qi-8-5-8", 
        "https://www.zhihu.com/people/good_ad_rd", 
        "https://www.zhihu.com/people/ningman", 
        "https://www.zhihu.com/people/TzeSing", 
        "https://www.zhihu.com/people/hao-qi-xin-82", 
        "https://www.zhihu.com/people/shan-shan-sunshine-51", 
        "https://www.zhihu.com/people/zhang-wen-ling-27", 
        "https://www.zhihu.com/people/555-19-86", 
        "https://www.zhihu.com/people/wu-ye-nan-30", 
        "https://www.zhihu.com/people/xu-qing-xin-44", 
        "https://www.zhihu.com/people/zheng-zhong-yi", 
        "https://www.zhihu.com/people/lu-zi-ye-14-58", 
        "https://www.zhihu.com/people/chen-xi-gua-41", 
        "https://www.zhihu.com/people/ykp-41", 
        "https://www.zhihu.com/people/pan-xiang-9-14", 
        "https://www.zhihu.com/people/xiaowei-ling", 
        "https://www.zhihu.com/people/arvin-carl", 
        "https://www.zhihu.com/people/zhu-forrest", 
        "https://www.zhihu.com/people/zhang-tong-24-20", 
        "https://www.zhihu.com/people/zhi-mu-8", 
        "https://www.zhihu.com/people/wang-xiong-wei", 
        "https://www.zhihu.com/people/kim-74-51", 
        "https://www.zhihu.com/people/biubiu-91-64-54", 
        "https://www.zhihu.com/people/greg-51-42", 
        "https://www.zhihu.com/people/cheng-gong-98-7", 
        "https://www.zhihu.com/people/screamlol", 
        "https://www.zhihu.com/people/xu-zhang-74", 
        "https://www.zhihu.com/people/dong-mei-47", 
        "https://www.zhihu.com/people/wangcheny91", 
        "https://www.zhihu.com/people/eathen-71", 
        "https://www.zhihu.com/people/zip-er", 
        "https://www.zhihu.com/people/zhang-mou-61-11", 
        "https://www.zhihu.com/people/alex-zhai-19", 
        "https://www.zhihu.com/people/jun-yue-19-99", 
        "https://www.zhihu.com/people/ye-lang-26-53", 
        "https://www.zhihu.com/people/lenna-hammer", 
        "https://www.zhihu.com/people/shi-shui-57-20", 
        "https://www.zhihu.com/people/zhang-miao-miao-92", 
        "https://www.zhihu.com/people/fuckingperfect", 
        "https://www.zhihu.com/people/niceworld-49-6", 
        "https://www.zhihu.com/people/sally-young-sh", 
        "https://www.zhihu.com/people/li-yi-83-91", 
        "https://www.zhihu.com/people/sheng-zhu-77", 
        "https://www.zhihu.com/people/rongkun.xing", 
        "https://www.zhihu.com/people/zhu-jian-23-14", 
        "https://www.zhihu.com/people/wang-lang-96-64", 
        "https://www.zhihu.com/people/williamwhe", 
        "https://www.zhihu.com/people/damon-data", 
        "https://www.zhihu.com/people/jack-jack-92-79", 
        "https://www.zhihu.com/people/void-25", 
        "https://www.zhihu.com/people/da-da-18-6-40", 
        "https://www.zhihu.com/people/liesel-lainey", 
        "https://www.zhihu.com/people/xin-wang-20-79", 
        "https://www.zhihu.com/people/jixinzhang", 
        "https://www.zhihu.com/people/long-hong-xing", 
        "https://www.zhihu.com/people/yang-jing-lin-16", 
        "https://www.zhihu.com/people/wang-he-yu-1", 
        "https://www.zhihu.com/people/wtzhang95", 
        "https://www.zhihu.com/people/chen-lei-69-28", 
        "https://www.zhihu.com/people/li-xiao-bo-33-74", 
        "https://www.zhihu.com/people/peonioner-40", 
        "https://www.zhihu.com/people/monkeydeking", 
        "https://www.zhihu.com/people/su-xiao-run", 
        "https://www.zhihu.com/people/cheng-cheng-27-88", 
        "https://www.zhihu.com/people/lbnnewdm", 
        "https://www.zhihu.com/people/rice-lyn", 
        "https://www.zhihu.com/people/xiong-mao-89-5", 
        "https://www.zhihu.com/people/zhong-er-cheng-xu-yuan", 
        "https://www.zhihu.com/people/zhua-zhua-72-30", 
        "https://www.zhihu.com/people/wai-bo-zi-25", 
        "https://www.zhihu.com/people/luan-xue-dong", 
        "https://www.zhihu.com/people/goalkeeper22", 
        "https://www.zhihu.com/people/liu-wen-yi-81", 
        "https://www.zhihu.com/people/dhlsc", 
        "https://www.zhihu.com/people/zhang-ming-feng-91", 
        "https://www.zhihu.com/people/hp-wang-8-10", 
        "https://www.zhihu.com/people/xu-yan-gen-99", 
        "https://www.zhihu.com/people/wei-xiao-jie-44-9", 
        "https://www.zhihu.com/people/diegozhang", 
        "https://www.zhihu.com/people/wang-meng-89-3", 
        "https://www.zhihu.com/people/ANSWER_SHI", 
        "https://www.zhihu.com/people/zheng-hai-bin-79-52", 
        "https://www.zhihu.com/people/ju-shang-38", 
        "https://www.zhihu.com/people/hongyu-liu", 
        "https://www.zhihu.com/people/wu-ye-45-74", 
        "https://www.zhihu.com/people/jifei", 
        "https://www.zhihu.com/people/yi-meng-ai-yi-cheng-ge", 
        "https://www.zhihu.com/people/little-ming-38", 
        "https://www.zhihu.com/people/lukai-16", 
        "https://www.zhihu.com/people/huan-xiang-yun-dian", 
        "https://www.zhihu.com/people/raymond-40-48", 
        "https://www.zhihu.com/people/feng-yue-17", 
        "https://www.zhihu.com/people/fang-albert", 
        "https://www.zhihu.com/people/yu-ji-shi-17", 
        "https://www.zhihu.com/people/li-ming-yang-46-98", 
        "https://www.zhihu.com/people/hachikoopenres", 
        "https://www.zhihu.com/people/monkeyeric", 
        "https://www.zhihu.com/people/huo-chen-87", 
        "https://www.zhihu.com/people/liu-peng-27-1", 
        "https://www.zhihu.com/people/mr-lin-82-68", 
        "https://www.zhihu.com/people/scripter", 
        "https://www.zhihu.com/people/typhoonbxq", 
        "https://www.zhihu.com/people/wang-crown-94", 
        "https://www.zhihu.com/people/xiyao-lin", 
        "https://www.zhihu.com/people/wang-zhe-hu-38", 
        "https://www.zhihu.com/people/arvin-liu-52-75", 
        "https://www.zhihu.com/people/wht-buaa", 
        "https://www.zhihu.com/people/ruan-zhi-qiang-39", 
        "https://www.zhihu.com/people/pray-90-47", 
        "https://www.zhihu.com/people/andrew-wang-67-85", 
        "https://www.zhihu.com/people/liu-yi-25-9", 
        "https://www.zhihu.com/people/yao-yuan-17-92", 
        "https://www.zhihu.com/people/Christ-XD", 
        "https://www.zhihu.com/people/xu-kai-71-15", 
        "https://www.zhihu.com/people/qinkang-69", 
        "https://www.zhihu.com/people/liu-fei-94-95", 
        "https://www.zhihu.com/people/joker-78-24-86", 
        "https://www.zhihu.com/people/redmond-ye", 
        "https://www.zhihu.com/people/wu-xiao-han-24", 
        "https://www.zhihu.com/people/yan-ya-chen"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/87288876", 
            "userName": "定西", 
            "userLink": "https://www.zhihu.com/people/22490c77f08111a9214a5d20fe979667", 
            "upvote": 2, 
            "title": "Multi-Task专题之SNR", 
            "content": "<p>SNR: Sub-Network Routing for Flexible Parameter Sharing in Multi-task Learning </p><h2>背景</h2><ol><li>当 task 之间相关性比较强的时候，MTL 可以学习到多个任务之间的关系，但是当 task 之间相关性比较差的时候，在预估准确度上会比较差，主要是 task 之间的干扰造成的（shared-bottom 部分难以学好）。</li><li>MTL 需要人工去tuning 模型的机构，以前的 soft-parameter sharing models 在灵活性上和计算复杂度上不能兼得。</li></ol><h2>主要贡献</h2><ol><li>提出了一种 SNR （sub-network routing）的 MTL 模型结构，该模型对任务之间的相关性强弱不敏感，借助简单的 NAS（Neural Architecture Search），可以对 sub-network 进行组合，学习一个好的模型结构。</li><li>在大规模数据集 YouTube8M 进行试验，和典型的 MTL 模型对比（SB、MMoE、ML-MMoE），验证了模型的有效性。</li><li>探索了模型不同部分之间关系，探索在线上 serving 的时候 性能和效果之间如何权衡。</li></ol><h2>结构</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7ca5d39a0eff7890325d2c2ae67654d3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"444\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-7ca5d39a0eff7890325d2c2ae67654d3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;444&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"444\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-7ca5d39a0eff7890325d2c2ae67654d3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7ca5d39a0eff7890325d2c2ae67654d3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>Why</p><ol><li>SB model is known to suffer from <b>significant degeneration</b> in accuracy when tasks are unrelated to each other (Ma et al. 2018). 当 task 之间 相关性差的时候，会损害模型各个task 预估的准确度。</li><ol><li>multi-task -&gt; single task</li><li>manually tune the network architecture</li></ol><li>如果用 single task，模型的参数会很多。</li><li>判断task 之间相关性是否强，是一个比较难的事情。                                  </li></ol><h2>Sub-Network Modularization and Routing </h2><p>目的：make more related tasks share more model parameters and less related tasks share fewer model parameters. </p><p>SNR-Trans</p><p>transform function:</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e7e0ef5028a37118054103024b646a22_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"176\" class=\"origin_image zh-lightbox-thumb\" width=\"800\" data-original=\"https://pic3.zhimg.com/v2-e7e0ef5028a37118054103024b646a22_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;800&#39; height=&#39;176&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"176\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"800\" data-original=\"https://pic3.zhimg.com/v2-e7e0ef5028a37118054103024b646a22_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e7e0ef5028a37118054103024b646a22_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>W_ij：transformation matrix from the jth lower- level sub-network to the ith higher-level sub-network </p><p>z: coding variables，a group of binary variables controlling the connection.</p><p>SNR-Aver</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d6799630b337844e9ce0ee4e0d6933dc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"724\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb\" width=\"724\" data-original=\"https://pic1.zhimg.com/v2-d6799630b337844e9ce0ee4e0d6933dc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;724&#39; height=&#39;168&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"724\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"724\" data-original=\"https://pic1.zhimg.com/v2-d6799630b337844e9ce0ee4e0d6933dc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d6799630b337844e9ce0ee4e0d6933dc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>I_ij：is an identity matrix for all i, j </p><p>z: coding variables，a group of binary variables controlling the connection.                           </p><p>example     </p><ul><li>If we set all elements of z as 1, then the corresponding model degenerates to the classic <b>shared-bottom model</b>.</li><li>If we set z11 = z22 = 1 and all other elements of z as 0, then the model degenerates to two small <b>single-task models</b>.                                                    </li></ul><h2>Learning the Architecture with Latent Variables </h2><p>目标函数</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-876355f7b185c49d455e54b3524daa5a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"728\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb\" width=\"728\" data-original=\"https://pic3.zhimg.com/v2-876355f7b185c49d455e54b3524daa5a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;728&#39; height=&#39;160&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"728\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"728\" data-original=\"https://pic3.zhimg.com/v2-876355f7b185c49d455e54b3524daa5a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-876355f7b185c49d455e54b3524daa5a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>W 和 z 是我们要学习的参数。假设 z 服从 Bernoulli 分布。</p><p>z 是不可微的，把 z 做一个变换，变换成一个平滑的函数（s 是一个连续的随机变量）：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-35b4d274111cd6c8a754e24b1eedede0_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"612\" data-rawheight=\"64\" class=\"origin_image zh-lightbox-thumb\" width=\"612\" data-original=\"https://pic1.zhimg.com/v2-35b4d274111cd6c8a754e24b1eedede0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;612&#39; height=&#39;64&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"612\" data-rawheight=\"64\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"612\" data-original=\"https://pic1.zhimg.com/v2-35b4d274111cd6c8a754e24b1eedede0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-35b4d274111cd6c8a754e24b1eedede0_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>目标变成：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-480e4f5c298bca09c6a2c1aa8059909a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"772\" data-rawheight=\"162\" class=\"origin_image zh-lightbox-thumb\" width=\"772\" data-original=\"https://pic3.zhimg.com/v2-480e4f5c298bca09c6a2c1aa8059909a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;772&#39; height=&#39;162&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"772\" data-rawheight=\"162\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"772\" data-original=\"https://pic3.zhimg.com/v2-480e4f5c298bca09c6a2c1aa8059909a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-480e4f5c298bca09c6a2c1aa8059909a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>使用 重采样技术，和 hard concrete distribution，可以得到：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-59880e10fc0eb614ebd4ad08870cfe12_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1046\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb\" width=\"1046\" data-original=\"https://pic3.zhimg.com/v2-59880e10fc0eb614ebd4ad08870cfe12_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1046&#39; height=&#39;152&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1046\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1046\" data-original=\"https://pic3.zhimg.com/v2-59880e10fc0eb614ebd4ad08870cfe12_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-59880e10fc0eb614ebd4ad08870cfe12_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ol><li>如果 log( alpha ) = 0，s 就是 线性函数（y=x）</li><li>log( alpha ) / beta = -0.5</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-274232c794ea412ad5bfdb7f1bfde0da_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"818\" data-rawheight=\"330\" class=\"origin_image zh-lightbox-thumb\" width=\"818\" data-original=\"https://pic3.zhimg.com/v2-274232c794ea412ad5bfdb7f1bfde0da_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;818&#39; height=&#39;330&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"818\" data-rawheight=\"330\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"818\" data-original=\"https://pic3.zhimg.com/v2-274232c794ea412ad5bfdb7f1bfde0da_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-274232c794ea412ad5bfdb7f1bfde0da_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>Applying L0 Regularization on Latent Variables </h2><p>正则函数</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1a55bc043cb48a22dcf572d78f62636c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"672\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb\" width=\"672\" data-original=\"https://pic1.zhimg.com/v2-1a55bc043cb48a22dcf572d78f62636c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;672&#39; height=&#39;148&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"672\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"672\" data-original=\"https://pic1.zhimg.com/v2-1a55bc043cb48a22dcf572d78f62636c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1a55bc043cb48a22dcf572d78f62636c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>容易得到 z =1 的概率，Q 是 s 的累计概率分布函数：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-dd9834cbf793b432e514e83d8f8541cc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"632\" data-rawheight=\"106\" class=\"origin_image zh-lightbox-thumb\" width=\"632\" data-original=\"https://pic1.zhimg.com/v2-dd9834cbf793b432e514e83d8f8541cc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;632&#39; height=&#39;106&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"632\" data-rawheight=\"106\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"632\" data-original=\"https://pic1.zhimg.com/v2-dd9834cbf793b432e514e83d8f8541cc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-dd9834cbf793b432e514e83d8f8541cc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>则最后的优化目标：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-523554306d123ca6309f041c809932fa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"838\" data-rawheight=\"328\" class=\"origin_image zh-lightbox-thumb\" width=\"838\" data-original=\"https://pic3.zhimg.com/v2-523554306d123ca6309f041c809932fa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;838&#39; height=&#39;328&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"838\" data-rawheight=\"328\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"838\" data-original=\"https://pic3.zhimg.com/v2-523554306d123ca6309f041c809932fa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-523554306d123ca6309f041c809932fa_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>Additional Details of Model Training and Serving </h2><p>模型要学的参数就是 W 和 alpha</p><p>how to learn model</p><ol><li>first sample a group of uniform random <b>variables u</b></li><li><b>calculate z</b> to obtain the network architecture</li><li>and finally feed the input data into the model to compute the loss. The gradients W and log(α) are calculated by back propagation.</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6b2cf8d69618c26710f937b30fb4b731_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1054\" data-rawheight=\"200\" class=\"origin_image zh-lightbox-thumb\" width=\"1054\" data-original=\"https://pic2.zhimg.com/v2-6b2cf8d69618c26710f937b30fb4b731_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1054&#39; height=&#39;200&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1054\" data-rawheight=\"200\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1054\" data-original=\"https://pic2.zhimg.com/v2-6b2cf8d69618c26710f937b30fb4b731_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6b2cf8d69618c26710f937b30fb4b731_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>Experiment </h2><p>Dataset</p><ol><li>This dataset consists of 6.1 million of YouTube videos</li><li>each with (multiple) labels from a vocabulary of more than 3, 000 topical entities;</li><li>the topical entities can be further grouped into 24 top-level topic categories.</li></ol><p>Accuracy of Best-Tuned Models </p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-aa5836c166d90ccee420d088097d1104_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"972\" data-rawheight=\"1186\" class=\"origin_image zh-lightbox-thumb\" width=\"972\" data-original=\"https://pic1.zhimg.com/v2-aa5836c166d90ccee420d088097d1104_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;972&#39; height=&#39;1186&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"972\" data-rawheight=\"1186\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"972\" data-original=\"https://pic1.zhimg.com/v2-aa5836c166d90ccee420d088097d1104_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-aa5836c166d90ccee420d088097d1104_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>ML-MMoE</b> : </p><ol><li>This model extends MMoE by adding multiple layers of <b>sub-networks</b>.</li><li>The connections from the lower-level sub-networks to the higher-level sub-networks are also controlled by some gating networks.                               </li></ol><p>The ML-MMoE model performs surprisingly worse than all other models. </p><p><b>why?: </b>This is possibly due to the input of gating networks in the second layer of sub-networks is still the whole model input, which does not give enough information about how to route in higher-level layers. </p><p>Accuracy vs Model Size</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a95ed8eea6a56783e4ba96f2e1e65bb7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"976\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb\" width=\"976\" data-original=\"https://pic4.zhimg.com/v2-a95ed8eea6a56783e4ba96f2e1e65bb7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;976&#39; height=&#39;610&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"976\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"976\" data-original=\"https://pic4.zhimg.com/v2-a95ed8eea6a56783e4ba96f2e1e65bb7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a95ed8eea6a56783e4ba96f2e1e65bb7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ol><li>This result shows that the SNR method can train larger models better than baseline methods.</li><li>This phenomenon aligns well with our speculation that modularization in multi-task learning could improve model trainability.</li></ol><p>Accuracy of Sparse Models</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e170da9d6601a0f794ad78e26c39e5d3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"982\" data-rawheight=\"626\" class=\"origin_image zh-lightbox-thumb\" width=\"982\" data-original=\"https://pic4.zhimg.com/v2-e170da9d6601a0f794ad78e26c39e5d3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;982&#39; height=&#39;626&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"982\" data-rawheight=\"626\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"982\" data-original=\"https://pic4.zhimg.com/v2-e170da9d6601a0f794ad78e26c39e5d3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e170da9d6601a0f794ad78e26c39e5d3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ol><li>SNR-Trans-Dense indicates SNR-Trans models with L0 regularization parameter 0.00001</li><li>SNR-Trans-Sparse indicates SNR-Trans models with L0 regularization parameter 0.0001.    </li></ol><p><b>Lambda 作用：  </b> </p><ol><li>When λ is set to 0.00001, the learned coding variables z are almost all 1s, which means the learned architecture is densely connected. </li><li>When λ is set to 0.0001, the learned architecture is sparse and the sparse model has significantly smaller serving model size than the dense model.       </li></ol><p>Analysis of Sub-Network Utilization         </p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2162d293a6c8d3055e384f98f840b25a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1030\" data-rawheight=\"850\" class=\"origin_image zh-lightbox-thumb\" width=\"1030\" data-original=\"https://pic3.zhimg.com/v2-2162d293a6c8d3055e384f98f840b25a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1030&#39; height=&#39;850&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1030\" data-rawheight=\"850\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1030\" data-original=\"https://pic3.zhimg.com/v2-2162d293a6c8d3055e384f98f840b25a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2162d293a6c8d3055e384f98f840b25a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ol><li>The figure shows that the utilization of sub-networks is positively related to the sample size of the tasks.</li><li>This implicates that when we add a stronger L0 regularization on the model, the model will learn to allocate more capacity to the tasks with more data. </li></ol>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "推荐算法", 
                    "tagLink": "https://api.zhihu.com/topics/19580544"
                }, 
                {
                    "tag": "互联网广告", 
                    "tagLink": "https://api.zhihu.com/topics/19554609"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/87288170", 
            "userName": "定西", 
            "userLink": "https://www.zhihu.com/people/22490c77f08111a9214a5d20fe979667", 
            "upvote": 5, 
            "title": "Multi-Task专题之MMOE", 
            "content": "<h2>背景</h2><p>在 Google 的推荐系统场景中，有 MTL 的必要，比如我们需要同时优化用户的多个目标：</p><ol><li>engagement related objectives：such as click through rate and engagement time</li><li>satisfaction related objectives：such as like rate</li></ol><p>如果对每个 task 都搞一个模型，总的 model 的 size 会很大。</p><p>目前 [2018 以前] 在 Google 的场景中，已经是 Shared-Bottom 的 MTL 结构。</p><h2>主要贡献</h2><ol><li>提出了一种 MMOE 的 MTL 结构，相对于其它的 MTL 模型（share bottom，L2-Constrained，Cross-Stitch）能更好的学习到各个 task 的分布，并且在计算量上也有优势（模型参数的减少）</li><li>通过人工生成数据来测试 task 的相关性对 MTL 的学习能力的影响，同时验证了 MMOE 相对于 Shared-Bottom 结构具有明显的优势。</li><li>在 Google 的大规模推荐系统上实践了一把，Live Experiment 验证了 MMOE 的有效性。</li></ol><h2>结构</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-aebfde1b4350a8f8d1c797c7dbe03bc9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"530\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-aebfde1b4350a8f8d1c797c7dbe03bc9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;530&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"530\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-aebfde1b4350a8f8d1c797c7dbe03bc9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-aebfde1b4350a8f8d1c797c7dbe03bc9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>Shared-Bottom</p><p><b>one</b> shared bottom network + <b>k</b> tower networks</p><p>formulated as：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7b74358aae68acae91ca3825440b0a0f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"318\" data-rawheight=\"86\" class=\"content_image\" width=\"318\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;318&#39; height=&#39;86&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"318\" data-rawheight=\"86\" class=\"content_image lazy\" width=\"318\" data-actualsrc=\"https://pic4.zhimg.com/v2-7b74358aae68acae91ca3825440b0a0f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>K: task number</li><li>f: shared-bottom network, represented as function f</li><li>h: tower networks</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>Moe（Mixture-of-Experts）</p><p><b>n</b> shared bottom network + <b>k</b> tower networks</p><p>formulated as：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2be808c0a348f3b4f0f5ce72c224f27f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"376\" data-rawheight=\"144\" class=\"content_image\" width=\"376\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;376&#39; height=&#39;144&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"376\" data-rawheight=\"144\" class=\"content_image lazy\" width=\"376\" data-actualsrc=\"https://pic4.zhimg.com/v2-2be808c0a348f3b4f0f5ce72c224f27f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>f_i：<b>expert network</b></li><li>g_i：represents a <b>gating network</b> that ensembles the results from all experts</li></ul><p>Multi-gate Moe</p><p><b>n</b> shared bottom network + <b>k</b> tower networks + <b>k</b> gate layer</p><p>formulated as：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-edbdf6e73645bd300cba986d6f8c8080_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"226\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-edbdf6e73645bd300cba986d6f8c8080_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;226&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"226\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-edbdf6e73645bd300cba986d6f8c8080_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-edbdf6e73645bd300cba986d6f8c8080_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>gate layer: </p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-442829b9d8605eb91a79376b33d0bd08_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"464\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb\" width=\"464\" data-original=\"https://pic1.zhimg.com/v2-442829b9d8605eb91a79376b33d0bd08_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;464&#39; height=&#39;108&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"464\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"464\" data-original=\"https://pic1.zhimg.com/v2-442829b9d8605eb91a79376b33d0bd08_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-442829b9d8605eb91a79376b33d0bd08_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>PRELIMINARY</h2><p>Synthetic Data Generation</p><p>label 相关性指标：Pearson correlation，用来度量 task 之间的相关性。</p><p>steps:</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0330905e575fe940af5b9d47c8f86416_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1130\" data-rawheight=\"1144\" class=\"origin_image zh-lightbox-thumb\" width=\"1130\" data-original=\"https://pic3.zhimg.com/v2-0330905e575fe940af5b9d47c8f86416_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1130&#39; height=&#39;1144&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1130\" data-rawheight=\"1144\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1130\" data-original=\"https://pic3.zhimg.com/v2-0330905e575fe940af5b9d47c8f86416_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0330905e575fe940af5b9d47c8f86416_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Linear or Non-linear</b></p><ol><li>对于 w 之间是线性关系，容易得到 label y 也是线性关系：</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0098c2eeae0072cc92a2425aece66b10_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"350\" data-rawheight=\"156\" class=\"content_image\" width=\"350\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;350&#39; height=&#39;156&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"350\" data-rawheight=\"156\" class=\"content_image lazy\" width=\"350\" data-actualsrc=\"https://pic1.zhimg.com/v2-0098c2eeae0072cc92a2425aece66b10_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ol><li>对于 w 之间是非线性关系，w 和 label 之间的关系如下图：</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-644a2b7bdbf2c022bd911866c0ede654_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"964\" data-rawheight=\"592\" class=\"origin_image zh-lightbox-thumb\" width=\"964\" data-original=\"https://pic1.zhimg.com/v2-644a2b7bdbf2c022bd911866c0ede654_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;964&#39; height=&#39;592&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"964\" data-rawheight=\"592\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"964\" data-original=\"https://pic1.zhimg.com/v2-644a2b7bdbf2c022bd911866c0ede654_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-644a2b7bdbf2c022bd911866c0ede654_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>Impact of Task Relatedness</p><ul><li>the performance of the model trends down as the task correlation decreases</li><li>This phenomenon validates our hypothesis that the traditional multi-task model is sensitive to the task relationships.</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-66ae83289cc3273fe40aea97a507d2ae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"934\" data-rawheight=\"586\" class=\"origin_image zh-lightbox-thumb\" width=\"934\" data-original=\"https://pic3.zhimg.com/v2-66ae83289cc3273fe40aea97a507d2ae_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;934&#39; height=&#39;586&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"934\" data-rawheight=\"586\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"934\" data-original=\"https://pic3.zhimg.com/v2-66ae83289cc3273fe40aea97a507d2ae_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-66ae83289cc3273fe40aea97a507d2ae_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>MMOE ON SYNTHETIC DATA</h2><p>MMOE 相对于 Shared-Bottom 有明显的提升。</p><ol><li>对于 MTL，task 相关性越高，其 performance 更好</li><li>OMOE 相对于 MMOE 在 task 的相关性大的时候，其性能没有差多少，相关性小的时候，其性能差了很多。在多任务中，multi gate 是很重要的。</li><li>OMOE 和 MMOE 性能比 Shared-bottom 好很多，MoE structure 很重要。</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-21d730e22974dbf8fdbb709274640f21_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"383\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-21d730e22974dbf8fdbb709274640f21_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;383&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"383\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-21d730e22974dbf8fdbb709274640f21_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-21d730e22974dbf8fdbb709274640f21_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>Trainability</p><ol><li>less performance variances（Shared-Bottom models in general have much more poor quality local minima than the MoE）</li><li>the performance variance of OMoE models is <b>similarly</b> robust as that of MMoE models when task <b>correlation is 1</b>, the robustness of the OMoE has an obvious <b>drop</b> when the task correlation <b>decreases to 0.5</b>.</li></ol><p>This validates the usefulness of the multi-gate structure in resolving bad local minima caused by the conflict from task difference. Finally</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0b216b9c7c21e59184144cfdfd858ad1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"941\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-0b216b9c7c21e59184144cfdfd858ad1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;941&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"941\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-0b216b9c7c21e59184144cfdfd858ad1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0b216b9c7c21e59184144cfdfd858ad1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>REAL DATA EXPERIMENTS </h2><ol><li>对每个对比的模型都进行了超参调优</li><li>每个模型的 NN size 都一样</li></ol><p>Census-income Data</p><p><b>Dataset Description</b>：The UCI census-income dataset [2] is extracted from the 1994 census database. It contains 299,285 instances of demographic information of American adults. There are 40 features in total.</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Experiments</p><p>In both groups, we treat the marital status task as the auxiliary task</p><p><b>Group 1</b></p><ul><li>Task 1: Predict whether the income exceeds $50K;</li><li>Task 2: Predict whether this person’s marital status is never married.</li><li>Absolute Pearson correlation: 0.1768.</li></ul><p><b>Group 2</b></p><ul><li>Task1:Predict whether the education level is at least college;</li><li>Task 2: Predict whether this person’s marital status is never married. </li><li>Absolute Pearson correlation: 0.2373.</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6974510a87c9e2077cc2ebe76607f8dc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1162\" data-rawheight=\"734\" class=\"origin_image zh-lightbox-thumb\" width=\"1162\" data-original=\"https://pic1.zhimg.com/v2-6974510a87c9e2077cc2ebe76607f8dc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1162&#39; height=&#39;734&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1162\" data-rawheight=\"734\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1162\" data-original=\"https://pic1.zhimg.com/v2-6974510a87c9e2077cc2ebe76607f8dc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6974510a87c9e2077cc2ebe76607f8dc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-20adde874900c8e717c8f5758a30c6e2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1170\" data-rawheight=\"736\" class=\"origin_image zh-lightbox-thumb\" width=\"1170\" data-original=\"https://pic3.zhimg.com/v2-20adde874900c8e717c8f5758a30c6e2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1170&#39; height=&#39;736&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1170\" data-rawheight=\"736\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1170\" data-original=\"https://pic3.zhimg.com/v2-20adde874900c8e717c8f5758a30c6e2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-20adde874900c8e717c8f5758a30c6e2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>MMOE On Google Recommendation</h2><p>优化目标：</p><ol><li>engagement related objectives：such as click through rate and engagement time</li><li>satisfaction related objectives：such as like rate</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><p>Offline Evaluation Results</p><p>the satisfaction subtask is much <b>sparser </b>than the engagement subtask, the offline results have very high noise levels. We only show the AUC scores and R-Squared scores on the <b>engagement subtask</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6578b9dfe27404544d19a198bf531d6a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"361\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-6578b9dfe27404544d19a198bf531d6a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;361&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"361\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-6578b9dfe27404544d19a198bf531d6a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6578b9dfe27404544d19a198bf531d6a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>gates work</b></p><p>satisfaction subtask’s labels are <b>sparser</b> than the engagement sub- task’s, the gate for satisfaction subtask is more <b>focused on a single expert</b>.</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3f61cf3b2269a2cb03af269f016ddc79_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"852\" data-rawheight=\"508\" class=\"origin_image zh-lightbox-thumb\" width=\"852\" data-original=\"https://pic2.zhimg.com/v2-3f61cf3b2269a2cb03af269f016ddc79_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;852&#39; height=&#39;508&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"852\" data-rawheight=\"508\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"852\" data-original=\"https://pic2.zhimg.com/v2-3f61cf3b2269a2cb03af269f016ddc79_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3f61cf3b2269a2cb03af269f016ddc79_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>Live Experiment Results</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e4fbfd8f5c0c9faedd5d1b33511c20b6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1196\" data-rawheight=\"542\" class=\"origin_image zh-lightbox-thumb\" width=\"1196\" data-original=\"https://pic3.zhimg.com/v2-e4fbfd8f5c0c9faedd5d1b33511c20b6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1196&#39; height=&#39;542&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1196\" data-rawheight=\"542\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1196\" data-original=\"https://pic3.zhimg.com/v2-e4fbfd8f5c0c9faedd5d1b33511c20b6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e4fbfd8f5c0c9faedd5d1b33511c20b6_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "推荐算法", 
                    "tagLink": "https://api.zhihu.com/topics/19580544"
                }, 
                {
                    "tag": "互联网广告", 
                    "tagLink": "https://api.zhihu.com/topics/19554609"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/84341534", 
            "userName": "定西", 
            "userLink": "https://www.zhihu.com/people/22490c77f08111a9214a5d20fe979667", 
            "upvote": 7, 
            "title": "[论文分享]阿里ESMM模型解决cvr样本空间不一致问题", 
            "content": "<p>Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate</p><p>比较老也比较出名的模型了，最近工作中遇到类似的问题，实际上更复杂，因此重温了下文章。</p><p>最近阿里也出了一篇新的该方向的文章，下一篇文章打算分享下。</p><p>论文地址</p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1804.07931.pdf\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1804.0793</span><span class=\"invisible\">1.pdf</span><span class=\"ellipsis\"></span></a><h2>一段话总结</h2><p>文章解决了工业界cvr模型基本都存在的两个问题：</p><ol><li>cvr模型的训练样本空间是点击样本，但是在线上servering的时候，是在impression空间。（链路是：impression-&gt;click-&gt;convert）</li><li>cvr模型样本比ctr要少很多</li></ol><p>解决方式就是采用一个类似于Multi-Task Learning的方法，模型同时预估pctr和pcvr，两者乘积pctcvr表示从impression到convert的概率。训练样本是所有的impression样本，同时模型的ctr模块和cvr模块共享Embedding层。通过这个方法同时解决了上述两个问题。</p><p>同时，文章在最后也提到了，这个问题更本质的链路是：</p><p>request -&gt; impression -&gt; click -&gt; conversion。也是后续需要解决的终极问题。</p><h2>背景</h2><p>广告中，一个广告我们需要预估ctr（impression-&gt;click）和cvr(click-&gt;convert)。其中cvr的训练样本是click样本。这样就会有两个问题：</p><p><b>1）sample selection bias(SSB) problem</b></p><p>cvr模型的训练样本是在点击样本的空间，但是在线上预估的时候，其实我们是对所有样本进行预估（文章提到是在impression样本空间，这样的假设是说，只对那些impression的样本进行预估，这样ctr和cvr模型就变成了串行。但是在我们公司，ctr和cvr是并行的，也就是不仅仅对于impression样本进行预估。这样的好处是线上两个模型预估是并行的，可以减少线上的延时。当然，这个问题通过文章的方法也能进一步解决。）</p><p>下图就是一个SSB问题的解释：conversion的train space是inference space的一个子集。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-bf0cf743fac101f79d8196e91ab8f09f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1628\" data-rawheight=\"908\" class=\"origin_image zh-lightbox-thumb\" width=\"1628\" data-original=\"https://pic4.zhimg.com/v2-bf0cf743fac101f79d8196e91ab8f09f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1628&#39; height=&#39;908&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1628\" data-rawheight=\"908\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1628\" data-original=\"https://pic4.zhimg.com/v2-bf0cf743fac101f79d8196e91ab8f09f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-bf0cf743fac101f79d8196e91ab8f09f_b.jpg\"/></figure><p><b>2) data sparsity(DS) problem</b></p><p>cvr基于点击样本进行训练，点击样本数比impression样本数少很多。下表可见，click样本只是impression样本的4%。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ee4989361ad63f26c0571e357ce0a1a4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1148\" data-rawheight=\"344\" class=\"origin_image zh-lightbox-thumb\" width=\"1148\" data-original=\"https://pic1.zhimg.com/v2-ee4989361ad63f26c0571e357ce0a1a4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1148&#39; height=&#39;344&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1148\" data-rawheight=\"344\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1148\" data-original=\"https://pic1.zhimg.com/v2-ee4989361ad63f26c0571e357ce0a1a4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ee4989361ad63f26c0571e357ce0a1a4_b.jpg\"/></figure><p>文中还提到cvr模型另一个挑战就是样本回传延时问题。在另一篇文章有提到（<a href=\"https://link.zhihu.com/?target=http%3A//olivier.chapelle.cc/pub/delayedConv.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">olivier.chapelle.cc/pub</span><span class=\"invisible\">/delayedConv.pdf</span><span class=\"ellipsis\"></span></a>），其中这个问题也比较有趣，公司内部也调研了很多种方法。下次可以分享下这篇文章以及公司内部的一些思路。</p><h2>方案</h2><p><b>整体框架</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-fbd4375479951d20664cafc6dfb4e02d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"131\" class=\"origin_image zh-lightbox-thumb\" width=\"720\" data-original=\"https://pic2.zhimg.com/v2-fbd4375479951d20664cafc6dfb4e02d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;720&#39; height=&#39;131&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"131\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"720\" data-original=\"https://pic2.zhimg.com/v2-fbd4375479951d20664cafc6dfb4e02d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-fbd4375479951d20664cafc6dfb4e02d_b.jpg\"/></figure><p>pCTCVR = pCTR * pCVR。</p><p>pCTR就是预估的ctr，pCVR就是预估的cvr，pCTCVR就是预估从impression到convert的概率。其中，pCVR可以认为是条件概率（clikc后convert的概率）。</p><p><b>Loss Function</b></p><p>Loss Function由两部分组成：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6a98664bb4f39ca68ec3ee3b4b7ddacf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"208\" class=\"origin_image zh-lightbox-thumb\" width=\"720\" data-original=\"https://pic4.zhimg.com/v2-6a98664bb4f39ca68ec3ee3b4b7ddacf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;720&#39; height=&#39;208&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"208\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"720\" data-original=\"https://pic4.zhimg.com/v2-6a98664bb4f39ca68ec3ee3b4b7ddacf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6a98664bb4f39ca68ec3ee3b4b7ddacf_b.jpg\"/></figure><p>pCTR的预估偏差和pCTCVR的预估偏差。直观理解就是，只要我ctr预估准确了，并且pCTCVR也预估准确了，因为pCTCVR = pCTR * pCVR，所以pCVR的预估也一定是准确的。</p><div class=\"highlight\"><pre><code class=\"language-text\">有必要多说两句，一开始看的时候也有些疑惑，因为我们最终的目标是为了学习pCVR，但是我们看Loss Function的时候可以发现，\n其中是通过约束pCTR和pCTCVR的预估准确性来间接约束pCVR的预估准确性。为什么不直接约束pCVR的准确性呢？因为我们是不知道”pCVR的后验“的，\n因为我们的样本空间是impression，对于有impression并且有click的样本，我们是可以知道这些样本是否被convert，对于有impression但是没有\nclick的样本，我们是无法知道这些样本”被click之后“，是否会convert。（粗浅的理解）</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-abe0a49a609254759620f497b5b4bae4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1778\" data-rawheight=\"1318\" class=\"origin_image zh-lightbox-thumb\" width=\"1778\" data-original=\"https://pic1.zhimg.com/v2-abe0a49a609254759620f497b5b4bae4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1778&#39; height=&#39;1318&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1778\" data-rawheight=\"1318\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1778\" data-original=\"https://pic1.zhimg.com/v2-abe0a49a609254759620f497b5b4bae4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-abe0a49a609254759620f497b5b4bae4_b.jpg\"/></figure><p>整体框架是借鉴Multi-Task Learning的思想（<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1706.05098\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/abs/1706.0509</span><span class=\"invisible\">8</span><span class=\"ellipsis\"></span></a>）。</p><p>有两点需要注意的：</p><p><b>1）Modeling over entire space</b></p><p>模型的训练样本是impression样本，解决sample selection bias的问题。</p><p><b>2）Feature representation transfer</b></p><p>Embedding layer左右两边模型结构是共享的，解决data sparsity问题。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>实验</h2><p>实验蛮有意思，可以详细看下。</p><p>数据集是在淘宝推荐数据集上。</p><p><b>Competitors</b></p><p>对比的其他方法：</p><ol><li>Base：阿里自己的深度兴趣网络（DIN）模型 </li><li>AMAN： 负采样方法，大概是从ctr样本里面随机采样部分样本当做cvr的负样本</li><li>OVERSAMPLING： 正样本过采样，整个训练空间是ctr样本，然后从cvr里面随机采样部分正样本</li><li>UNBIAS：样本分布映射方法。cvr样本空间跟ctr不一致，那么就将cvr样本空间映射到真实的样本空间。采用pctr作为映射概率。</li><li>DIVISION：分别训练pctr和pctcvr，pcvr通过将pctcvr/pctr获得。</li><li>ESMM-NS：ESMM不共享Embedding Layer的版本</li></ol><p>基本上把所有可能的方法都罗列了，很严谨了~</p><p><b>Metric</b></p><p>评估方法：</p><p>评估1：click样本的pcvr预估准确性  </p><p>就是传统的cvr模型评估</p><p>评估2：impression样本的pctcvr的预估准确性</p><p>是在整个impression上样本空间的评估。其中pctcvr都是通过pctr*pcvr获取。pctr每个评估模型都是统一一个模型，pcvr每个评估模型各自获取。</p><p>评估结果如下：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8b845b239d706cdd2a7605c3ec1210b7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1522\" data-rawheight=\"614\" class=\"origin_image zh-lightbox-thumb\" width=\"1522\" data-original=\"https://pic4.zhimg.com/v2-8b845b239d706cdd2a7605c3ec1210b7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1522&#39; height=&#39;614&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1522\" data-rawheight=\"614\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1522\" data-original=\"https://pic4.zhimg.com/v2-8b845b239d706cdd2a7605c3ec1210b7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8b845b239d706cdd2a7605c3ec1210b7_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3c594fd631f620f2dc45f789e7853542_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1724\" data-rawheight=\"954\" class=\"origin_image zh-lightbox-thumb\" width=\"1724\" data-original=\"https://pic3.zhimg.com/v2-3c594fd631f620f2dc45f789e7853542_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1724&#39; height=&#39;954&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1724\" data-rawheight=\"954\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1724\" data-original=\"https://pic3.zhimg.com/v2-3c594fd631f620f2dc45f789e7853542_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-3c594fd631f620f2dc45f789e7853542_b.jpg\"/></figure><p></p><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "论文", 
                    "tagLink": "https://api.zhihu.com/topics/19572442"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/83575678", 
            "userName": "定西", 
            "userLink": "https://www.zhihu.com/people/22490c77f08111a9214a5d20fe979667", 
            "upvote": 18, 
            "title": "[论文分享]FPENN：解决FFM模型过拟合的问题", 
            "content": "<h2>论文地址：</h2><a href=\"https://link.zhihu.com/?target=https%3A//dl.acm.org/citation.cfm%3Fid%3D3240396\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">dl.acm.org/citation.cfm?</span><span class=\"invisible\">id=3240396</span><span class=\"ellipsis\"></span></a><p>香港科技大学&amp;华为&amp;腾讯 2018文章，解决FFM过拟合的问题</p><h2>一段话总结文章：</h2><p>FFM容易过拟合，因此文章提出去学习特征Embedding的一个分布而不是FFM里面一个确定的Embedding。学习EMbedding的分布即是学习Embedding的均值和方差。在有了均值和方差后，生成的Embedding的时候，文章采用了采样的方法。两种采样方法：Thompson Sampling（TS）和Upper Confidence Bound（UCB）这两种方法来生成Embedding（即在生成Embedding的时候考虑了Embedding的方差，方差越小则该Embedding越置信）。</p><h2>背景</h2><p>FFM容易存在过拟合的问题，参数空间n*k*f</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3b11b55ab1667346de18472b2da39a9a_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1152\" data-rawheight=\"140\" class=\"origin_image zh-lightbox-thumb\" width=\"1152\" data-original=\"https://pic3.zhimg.com/v2-3b11b55ab1667346de18472b2da39a9a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1152&#39; height=&#39;140&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1152\" data-rawheight=\"140\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1152\" data-original=\"https://pic3.zhimg.com/v2-3b11b55ab1667346de18472b2da39a9a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-3b11b55ab1667346de18472b2da39a9a_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>​</p><a href=\"https://link.zhihu.com/?target=https%3A//bytedance.feishu.cn/space/doc/doccnsUPuWno7dMVloMWbH\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Factorization Machines 因子分解机</a><p>​ </p><h2>解决方案</h2><p>生成embedding的分布：均值 + 标准差，而不是一个固定的embedding。标准差可以反映该embedding的置信度，生成最终embedding的时候，考虑该方差可以解决overfitting。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b8d3eaa234ff391140b4cc45c7318c85_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"884\" data-rawheight=\"949\" class=\"origin_image zh-lightbox-thumb\" width=\"884\" data-original=\"https://pic2.zhimg.com/v2-b8d3eaa234ff391140b4cc45c7318c85_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;884&#39; height=&#39;949&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"884\" data-rawheight=\"949\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"884\" data-original=\"https://pic2.zhimg.com/v2-b8d3eaa234ff391140b4cc45c7318c85_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b8d3eaa234ff391140b4cc45c7318c85_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>还是由三个模块组成：线性模块、二次项模块、Deep模块</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-960832aa6f3ad330d59334b8e70e3658_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"275\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic1.zhimg.com/v2-960832aa6f3ad330d59334b8e70e3658_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;275&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"275\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic1.zhimg.com/v2-960832aa6f3ad330d59334b8e70e3658_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-960832aa6f3ad330d59334b8e70e3658_b.jpg\"/></figure><h2>2.1 FPE Component        </h2><p>The Field-aware Probabilistic Embedding (FPE) component </p><p>生成特征的embedding（一个d*(f-1)*k的tensor），生成embedding时会引入随机变量来提升模型的鲁棒性。</p><p><b>Training：</b></p><p>不再是学一个确定的embedding，而是学embedding的一个分布（均值+标准差）。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8adbb7c48c12ce3a73744f54d2df80ab_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"643\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-8adbb7c48c12ce3a73744f54d2df80ab_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;643&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"643\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-8adbb7c48c12ce3a73744f54d2df80ab_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8adbb7c48c12ce3a73744f54d2df80ab_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Testing：</b></p><p>采用Thompson Sampling（TS）和Upper Confidence Bound（UCB）两种策略来生成embedding。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e41aca74e3f5037d4ebdfe1e4c3faa7e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"581\" data-rawheight=\"439\" class=\"origin_image zh-lightbox-thumb\" width=\"581\" data-original=\"https://pic3.zhimg.com/v2-e41aca74e3f5037d4ebdfe1e4c3faa7e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;581&#39; height=&#39;439&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"581\" data-rawheight=\"439\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"581\" data-original=\"https://pic3.zhimg.com/v2-e41aca74e3f5037d4ebdfe1e4c3faa7e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e41aca74e3f5037d4ebdfe1e4c3faa7e_b.jpg\"/></figure><h2>2.2 Linear Component</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0c9d387aed52fbc91689cf597e60319b_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"192\" data-rawheight=\"37\" class=\"content_image\" width=\"192\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;192&#39; height=&#39;37&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"192\" data-rawheight=\"37\" class=\"content_image lazy\" width=\"192\" data-actualsrc=\"https://pic4.zhimg.com/v2-0c9d387aed52fbc91689cf597e60319b_b.png\"/></figure><h2>2.3 Quadratic Component</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0b37cca4f8ac0937ce896f1541c266e2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"525\" data-rawheight=\"117\" class=\"origin_image zh-lightbox-thumb\" width=\"525\" data-original=\"https://pic3.zhimg.com/v2-0b37cca4f8ac0937ce896f1541c266e2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;525&#39; height=&#39;117&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"525\" data-rawheight=\"117\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"525\" data-original=\"https://pic3.zhimg.com/v2-0b37cca4f8ac0937ce896f1541c266e2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0b37cca4f8ac0937ce896f1541c266e2_b.jpg\"/></figure><h2>2.4 Deep Component</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-79a58b05188e34f31a094b78cc7ed7a6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"251\" data-rawheight=\"50\" class=\"content_image\" width=\"251\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;251&#39; height=&#39;50&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"251\" data-rawheight=\"50\" class=\"content_image lazy\" width=\"251\" data-actualsrc=\"https://pic3.zhimg.com/v2-79a58b05188e34f31a094b78cc7ed7a6_b.jpg\"/></figure><h2>实验</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-dd63d702a698c48aaf1689c084b38c14_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1155\" data-rawheight=\"626\" class=\"origin_image zh-lightbox-thumb\" width=\"1155\" data-original=\"https://pic1.zhimg.com/v2-dd63d702a698c48aaf1689c084b38c14_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1155&#39; height=&#39;626&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1155\" data-rawheight=\"626\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1155\" data-original=\"https://pic1.zhimg.com/v2-dd63d702a698c48aaf1689c084b38c14_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-dd63d702a698c48aaf1689c084b38c14_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1b8c5b1fe7d1d72e12a09e6d21f1fc72_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"921\" data-rawheight=\"496\" class=\"origin_image zh-lightbox-thumb\" width=\"921\" data-original=\"https://pic3.zhimg.com/v2-1b8c5b1fe7d1d72e12a09e6d21f1fc72_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;921&#39; height=&#39;496&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"921\" data-rawheight=\"496\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"921\" data-original=\"https://pic3.zhimg.com/v2-1b8c5b1fe7d1d72e12a09e6d21f1fc72_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1b8c5b1fe7d1d72e12a09e6d21f1fc72_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>REFERRENCE</h2><ul><ul><li>FFM：<a href=\"https://link.zhihu.com/?target=https%3A//www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">csie.ntu.edu.tw/~cjlin/</span><span class=\"invisible\">papers/ffm.pdf</span><span class=\"ellipsis\"></span></a></li><li>Field-weighted FM： <a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1806.03514.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1806.0351</span><span class=\"invisible\">4.pdf</span><span class=\"ellipsis\"></span></a> （解决FFM参数过多的问题，只需要4%的参数）</li></ul></ul>", 
            "topic": [
                {
                    "tag": "论文", 
                    "tagLink": "https://api.zhihu.com/topics/19572442"
                }, 
                {
                    "tag": "推荐算法", 
                    "tagLink": "https://api.zhihu.com/topics/19580544"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/83572664", 
            "userName": "定西", 
            "userLink": "https://www.zhihu.com/people/22490c77f08111a9214a5d20fe979667", 
            "upvote": 5, 
            "title": "[论文分享]AutoCross：第四范式自动机器学习框架", 
            "content": "<h2>论文地址：</h2><p>AutoCross: Automatic Feature Crossing for Tabular Data in Real-World Applications</p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1904.12857\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">AutoCross: Automatic Feature Crossing for Tabular Data in Real-World Applications</a><h2>一段话总结</h2><p>文章属于automl方向。第四范式自动学习特征交叉的文章，主要解决Tabular Data（表数据）。一个是提出了一种数结构来表达特征集合。同时利用Beam Search来高效查找最优的特征组合列表（穷举特征集合空间非常大）。同时在训练模型的时候，</p><p>a) 引入Field-wise Logistic Regression来加速增加特征时模型的学习速度（只更新新增的特征的参数权重）</p><p>b) 引入Successive Mini-batch Gradient Descent方法，二分淘汰候选特征结合（每次淘汰一般的候选特征集合，将机器资源分配给更优的特征候选集合）。</p><h2>背景</h2><p>现有特征交叉的模型：</p><p>FM、FFM、DeepFM、xDeepFM</p><p>问题：</p><p>要么没有考虑高阶交叉要么高阶交叉是通过隐式的、或者显式的但是是通过网络结构来表达。</p><p>（deep-learning-based feature generation approaches, where interactions among features are implicitly or explicitly represented by specific networks ）</p><p>比如：xDeepFM只是特征交叉的一种特殊形式（在论文附录有证明）。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-66cf4b7f1e1fc6558b2fcaace5d1c559_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"676\" data-rawheight=\"183\" class=\"origin_image zh-lightbox-thumb\" width=\"676\" data-original=\"https://pic2.zhimg.com/v2-66cf4b7f1e1fc6558b2fcaace5d1c559_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;676&#39; height=&#39;183&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"676\" data-rawheight=\"183\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"676\" data-original=\"https://pic2.zhimg.com/v2-66cf4b7f1e1fc6558b2fcaace5d1c559_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-66cf4b7f1e1fc6558b2fcaace5d1c559_b.jpg\"/></figure><p>该文章主要解决特征的自动交叉。同时考虑以下几点：</p><ol><li>简单易用（不会依赖大量的超参数、网络结构等）</li><li>分布式计算</li><li>线上inference速度</li></ol><h2>方案</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-9a5056fb377a6e6843651f9569a62ac1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"675\" data-rawheight=\"512\" class=\"origin_image zh-lightbox-thumb\" width=\"675\" data-original=\"https://pic2.zhimg.com/v2-9a5056fb377a6e6843651f9569a62ac1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;675&#39; height=&#39;512&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"675\" data-rawheight=\"512\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"675\" data-original=\"https://pic2.zhimg.com/v2-9a5056fb377a6e6843651f9569a62ac1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-9a5056fb377a6e6843651f9569a62ac1_b.jpg\"/></figure><h2><br/>2.1 preprocess</h2><ul><li>hyper-paramegers determined</li><li>missing values filled</li><li>numerical features discretized</li></ul><h2>2.2 feature set generation </h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8cdb618e3fd7525b195d8d14044abcc4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"920\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic1.zhimg.com/v2-8cdb618e3fd7525b195d8d14044abcc4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;920&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"920\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic1.zhimg.com/v2-8cdb618e3fd7525b195d8d14044abcc4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8cdb618e3fd7525b195d8d14044abcc4_b.jpg\"/></figure><p>这个图一开始不是很好理解，每一个节点的子节点就是该节点内所有样本两两交叉，生成一个新的特征。比如原始特征是ABCD，第二层就是第一层两两交叉，对于第一个节点，就表示在原来特征集合的基础上（ABCD），再加入一个交叉特征AB。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>检索特征结合等价于从上图中找到一条从根节点开始的路径。检索空间是O((d^2/2)^k)。</p><p>d为原始特征数量，k为路径的最大长度。</p><p>通过<b>beam search</b>方法来加速检索特征集合。每次只检索最优的那个节点的子节点，复杂度为O(k*d^2)。</p><h2>2.3 feature set evaluation</h2><p>对于候选特征集合，需要进行评估。如果对于每一个候选特征集合都训练一个模型进行评估显然是无法接受的。因此这边提出了两种优化方法。</p><p><b>2.3.1 Field-wise Logistic Regression</b></p><p>该方法很简单，就是每次要评估一个新增进来的特征，就只更新该特征对应的权重（因为用的是LR模型）。</p><p>1）用最简单的LR模型来训练、评估，近似替代之后可能真正用的更复杂的模型；</p><p>2）在模型训练的时候，只更新新加进来的特征的权重，其他特征的权重保持不变；</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e26a884c4d971912a7ef06d8558fbe17_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"1176\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-e26a884c4d971912a7ef06d8558fbe17_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;1176&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"1176\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-e26a884c4d971912a7ef06d8558fbe17_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e26a884c4d971912a7ef06d8558fbe17_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2.3.2 Successive Mini-batch Gradient Descent</b></p><p>Successive mini-batch gradient descent allocates more resources to more promising candidates.</p><p>这个方法大概思想就是：如果要在当前n个特征集合里面选一个最优的特征，那么就采用了二分的方法来加速选取过程。将样本分成N份，每次淘汰掉最次的1/2之一特征，相当于将机器资源分配给了更优的那些特征。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d3e5ba4271593520b30614aadb869c73_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"717\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-d3e5ba4271593520b30614aadb869c73_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;717&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"717\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-d3e5ba4271593520b30614aadb869c73_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d3e5ba4271593520b30614aadb869c73_b.jpg\"/></figure><p>整个过程如下：</p><p>就是生成特征集合，评估特征集合一直loop指导满足一定的条件。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-354b8b230af8f4b55fd49f818f74c5d7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"707\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-354b8b230af8f4b55fd49f818f74c5d7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;707&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"707\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-354b8b230af8f4b55fd49f818f74c5d7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-354b8b230af8f4b55fd49f818f74c5d7_b.jpg\"/></figure><h2>实验</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8d94bb22648e05a3566d8ff9ba514bc9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"886\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-8d94bb22648e05a3566d8ff9ba514bc9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;886&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"886\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-8d94bb22648e05a3566d8ff9ba514bc9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8d94bb22648e05a3566d8ff9ba514bc9_b.jpg\"/></figure><h2>REFERRENCE</h2><ul><ul><li>successive halving algorithm：Non-stochastic best arm identification and hyperparameter optimization</li></ul></ul>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "论文", 
                    "tagLink": "https://api.zhihu.com/topics/19572442"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/83571732", 
            "userName": "定西", 
            "userLink": "https://www.zhihu.com/people/22490c77f08111a9214a5d20fe979667", 
            "upvote": 68, 
            "title": "[论文分享]华为PAL论文：解决推荐、广告中的position-bias问题", 
            "content": "<h2>论文地址：</h2><a href=\"https://link.zhihu.com/?target=https%3A//www.researchgate.net/publication/335771749_PAL_a_position-bias_aware_learning_framework_for_CTR_prediction_in_live_recommender_systems\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">PAL: A Position-bias Aware Learning Framework for CTR Prediction in Live Recommender Systems</a><p>华为2019文章，解决推荐、广告中的position-bias问题。</p><h2>一段话总结</h2><p>解决推荐、广告中的position-bias问题（训练的时候知道位置信息，预估的时候不知道）。文章将广告被点击的概率分为两个因素：广告被用户看到的概率和用户看到广告后点击广告的概率。进一步假设用户看到广告后点击广告的概率与广告的位置无关。因此建模成两个模块：</p><p>广告被看到的概率预估模型和用户看到广告后，点击的概率预估模型。同时在线上servering的时候，只需要预估第二个模型（不需要位置信息）。</p><p>个人觉得主要问题在于假设太强：</p><p>第一个假设，广告是否被用户看到只跟广告位置有关，这个假设在广告场景是不成立的。跟广告、以及用户的属性都有关系（广告大图、小图等）。不过可以对第一个模型更精细建模解决（看论文中这个模型只用了position信息）。</p><p>第二个假设，用户看到广告后，点击广告的位置，其实跟广告是否被点击很可能还是有关的。比如都在一个页面，用户同时看到了位置1的广告和位置3的广告，但用户点击位置1的广告的概率更大。</p><h2>背景</h2><p>用户是否点击广告有两个影响因素：</p><ol><li>广告的位置</li><li>用户的兴趣<br/></li></ol><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0bf137def7386236c7aeaa5408976cf7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-0bf137def7386236c7aeaa5408976cf7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;604&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-0bf137def7386236c7aeaa5408976cf7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0bf137def7386236c7aeaa5408976cf7_b.jpg\"/></figure><p>推荐广告都会存在position-bias的问题：</p><p>训练的样本里面有广告位置信息，但是inference的时候无法知道广告位置（预估之后才决定广告的位置，因此预估时是无法知道的）。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-acb67f6e9b770fd0a7194de044bb455d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"316\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-acb67f6e9b770fd0a7194de044bb455d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;316&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"316\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-acb67f6e9b770fd0a7194de044bb455d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-acb67f6e9b770fd0a7194de044bb455d_b.jpg\"/></figure><p>工业界常见的解决方法：</p><p>训练的时候，把广告位置信息当做特征进行训练，inference的时候，用一个默认值来预估。不同的默认值会影响到最终线上的效果，因此一般只能获取到一个次优解。</p><h2>解决方案</h2><p>用户点击广告的概率由两部分组成：</p><ol><li>广告被用户看到的概率</li><li>用户看到广告后，点击广告的概率</li></ol><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e55fa98dd5466ba32285a0ef2872d98a_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"111\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-e55fa98dd5466ba32285a0ef2872d98a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;111&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"111\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-e55fa98dd5466ba32285a0ef2872d98a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e55fa98dd5466ba32285a0ef2872d98a_b.png\"/></figure><p>进一步假设：</p><ol><li>用户是否看到广告只跟广告的位置有关系</li><li>用户看到广告后，是否点击广告与广告的位置无关</li></ol><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e870314e0bd91b1cb9788a438158a631_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1134\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb\" width=\"1134\" data-original=\"https://pic2.zhimg.com/v2-e870314e0bd91b1cb9788a438158a631_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1134&#39; height=&#39;114&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1134\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1134\" data-original=\"https://pic2.zhimg.com/v2-e870314e0bd91b1cb9788a438158a631_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e870314e0bd91b1cb9788a438158a631_b.png\"/></figure><p>基于该假设，就可以分开建模：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3cc983acf31547238ee020c13e3dc00b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"294\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-3cc983acf31547238ee020c13e3dc00b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;294&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"294\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-3cc983acf31547238ee020c13e3dc00b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3cc983acf31547238ee020c13e3dc00b_b.jpg\"/></figure><p><b>模型框架</b>：</p><p>基于该假设就可以分开建模（左边是分开建模，右边是工业界的常用方法，也是该文章实验的baseline）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3cc983acf31547238ee020c13e3dc00b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"294\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-3cc983acf31547238ee020c13e3dc00b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;294&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"294\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-3cc983acf31547238ee020c13e3dc00b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3cc983acf31547238ee020c13e3dc00b_b.jpg\"/></figure><p>有两个模块组成：</p><ol><li>ProbSeen：预估广告被用户看到的概率</li><li>pCTR：用户看到广告后，点击广告的概率</li></ol><p><b>训练方法</b>：</p><p>同时训练两个模型（单独训练会得到次优解）</p><p>Loss Function：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1b5634f71ed83c7e2b2e21115fb4fe01_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"119\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-1b5634f71ed83c7e2b2e21115fb4fe01_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;119&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"119\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-1b5634f71ed83c7e2b2e21115fb4fe01_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1b5634f71ed83c7e2b2e21115fb4fe01_b.png\"/></figure><p>参数更新：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f546203a18fba4a566e679b14610fc26_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"257\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-f546203a18fba4a566e679b14610fc26_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;257&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"257\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-f546203a18fba4a566e679b14610fc26_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f546203a18fba4a566e679b14610fc26_b.jpg\"/></figure><p><b>线上servering</b>的时候，直接预估pCTR，不需要知道位置信息（这个点需要注意下，线上servering的时候，就预估假设这个广告被曝光后，用户点击的概率，只需要预估第二个模型）。</p><h2>实验</h2><p>不同位置默认值对于离线AUC、LogLoss的影响</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a12afd83cea8f57d53d663a5306e12f6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1190\" data-rawheight=\"656\" class=\"origin_image zh-lightbox-thumb\" width=\"1190\" data-original=\"https://pic3.zhimg.com/v2-a12afd83cea8f57d53d663a5306e12f6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1190&#39; height=&#39;656&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1190\" data-rawheight=\"656\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1190\" data-original=\"https://pic3.zhimg.com/v2-a12afd83cea8f57d53d663a5306e12f6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a12afd83cea8f57d53d663a5306e12f6_b.jpg\"/></figure><p>线上效果</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d83cfc69fd932335e4dc4c800b358277_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"245\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-d83cfc69fd932335e4dc4c800b358277_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;245&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"245\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-d83cfc69fd932335e4dc4c800b358277_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d83cfc69fd932335e4dc4c800b358277_b.jpg\"/></figure><h2>REFERENCES</h2><p>其他两篇也是关于position-bias的文章，google和微软的，也很值得看一遍。</p><ul><ul><li>Google：<a href=\"https://link.zhihu.com/?target=https%3A//static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45286.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Learning to Rank with Selection Bias in Personal Search</a></li><li>Bing：<a href=\"https://link.zhihu.com/?target=https%3A//www.microsoft.com/en-us/research/wp-content/uploads/2017/04/main-1.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Model Ensemble for Click Prediction in Bing Search Ads</a></li></ul></ul>", 
            "topic": [
                {
                    "tag": "论文", 
                    "tagLink": "https://api.zhihu.com/topics/19572442"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "推荐算法", 
                    "tagLink": "https://api.zhihu.com/topics/19580544"
                }
            ], 
            "comments": [
                {
                    "userName": "buaawht", 
                    "userLink": "https://www.zhihu.com/people/bbec1d82e05e75dfbb2fc827ed07d569", 
                    "content": "<p>论文中广告被用户看到的概率不就是广告被曝光的概率吗？这样的话ctr模型训练样本都是曝光样本的话，相应的被看到概率不就是1了么，没太明白论文的含义</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "定西", 
                            "userLink": "https://www.zhihu.com/people/22490c77f08111a9214a5d20fe979667", 
                            "content": "<p>训练样本包括曝光和未曝光的。第一个模型预估被曝光的概率，第二个模型预估假设被曝光后被点击的概率。线上预估的时候，就是预估假设这个广告被曝光之后被用户点击的概率。</p>", 
                            "likes": 0, 
                            "replyToAuthor": "buaawht"
                        }, 
                        {
                            "userName": "buaawht", 
                            "userLink": "https://www.zhihu.com/people/bbec1d82e05e75dfbb2fc827ed07d569", 
                            "content": "该评论已删除", 
                            "likes": 0, 
                            "replyToAuthor": "定西"
                        }
                    ]
                }, 
                {
                    "userName": "全全", 
                    "userLink": "https://www.zhihu.com/people/54953b088c3a146f2114ee0e3330ef9a", 
                    "content": "YouTube 的paper \"Recommending What Video to Watch Next\"<br>中也有个类似解决 position bias的思想", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "定西", 
                            "userLink": "https://www.zhihu.com/people/22490c77f08111a9214a5d20fe979667", 
                            "content": "有空看下！", 
                            "likes": 0, 
                            "replyToAuthor": "全全"
                        }, 
                        {
                            "userName": "知乎用户", 
                            "userLink": "https://www.zhihu.com/people/0", 
                            "content": "<p>前段时间也看过这篇paper，跟本文比思路确实蛮像，都是train的时候加个模块用于建模 bias，然后 serving的时候remove掉。只不过youtube那个是通过multi-task 实现的，train的时候多个auxiliary的task建模bias</p>", 
                            "likes": 0, 
                            "replyToAuthor": "全全"
                        }
                    ]
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>如楼上所说，YouTube 今年一篇kdd也是讲 position bias，还有 multi-task</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/83027199", 
            "userName": "定西", 
            "userLink": "https://www.zhihu.com/people/22490c77f08111a9214a5d20fe979667", 
            "upvote": 0, 
            "title": "[论文分享]Google NIS论文详解", 
            "content": "<p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1907.04471\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Neural Input Search for Large Scale Recommendation Models</a></p><p>Google发表的解决DNN模型中，embedding向量的维度问题，通过增强学习自动学习每个特征最优的语料库长度和embedding维度</p><h2>背景</h2><p>与图像、语音等领域不同，在推荐、广告中，embedding占据了模型的大部分参数。而现有的模型中，某个特征的vocabulary size和embedding dimensions都是基于人工经验拍的。</p><p>以YouTube推荐模型为例，video ID的vocabulary size是1million，每个ID的embedding dimension是256，那么该特征就包含了256 million的参数。</p><p>因此，文章提出如何自动为每个特征寻找一个最优的vocabulary size，并且为该特征的每一个值都寻找一个最优的embedding dimensions（在一定的空间约束下）。</p><h2>方案</h2><h2>2.1 问题描述</h2><p>特征的embedding维度现有方法一般是同一个特征的所有value都分配一样的size（SE），并不是最优的。</p><p><b>Single-size Embedding（SE）</b></p><p>一个特征的不同value都拥有一样的embedding维度。</p><p>要解决的问题就变成：为每个特征寻找一个最优的vocabulary size和embedding size。</p><ul><li>vocabulary size更大，能提高覆盖面（可以包含那些稀疏的item）</li><li>embedding size更大，对于头部item可以学习的更充分。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1eb8c7d324ff8a7cf51cd92940eba87a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1120\" data-rawheight=\"297\" class=\"origin_image zh-lightbox-thumb\" width=\"1120\" data-original=\"https://pic3.zhimg.com/v2-1eb8c7d324ff8a7cf51cd92940eba87a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1120&#39; height=&#39;297&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1120\" data-rawheight=\"297\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1120\" data-original=\"https://pic3.zhimg.com/v2-1eb8c7d324ff8a7cf51cd92940eba87a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1eb8c7d324ff8a7cf51cd92940eba87a_b.jpg\"/></figure><p><b>Multi-size Embedding（ME）</b></p><p>同一个特征，不同的item拥有不同的embedding size。对于头部的item，分配更大的embedding size，尾部的item分配更小的embedding size。同时为了解决不同item embedding size不一样 sum-pooling的问题，为每一个item学习一个mapping。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-29c3a48b991f39b03ce89f3ce85e5735_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1151\" data-rawheight=\"425\" class=\"origin_image zh-lightbox-thumb\" width=\"1151\" data-original=\"https://pic2.zhimg.com/v2-29c3a48b991f39b03ce89f3ce85e5735_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1151&#39; height=&#39;425&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1151\" data-rawheight=\"425\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1151\" data-original=\"https://pic2.zhimg.com/v2-29c3a48b991f39b03ce89f3ce85e5735_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-29c3a48b991f39b03ce89f3ce85e5735_b.jpg\"/></figure><p>要解决的问题就变成：为每个特征寻找一个最优的vocabulary size，并且为每一个item寻找一个最优的embedding size。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fd82e262ee362f30ac80e916e3146156_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1106\" data-rawheight=\"551\" class=\"origin_image zh-lightbox-thumb\" width=\"1106\" data-original=\"https://pic3.zhimg.com/v2-fd82e262ee362f30ac80e916e3146156_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1106&#39; height=&#39;551&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1106\" data-rawheight=\"551\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1106\" data-original=\"https://pic3.zhimg.com/v2-fd82e262ee362f30ac80e916e3146156_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fd82e262ee362f30ac80e916e3146156_b.jpg\"/></figure><h2>2.2 Neural Input Search Approach</h2><p><b>Search Space</b></p><ul><li><b>Embedding Blocks</b></li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4f522091aae266b74c001784115419f9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1104\" data-rawheight=\"517\" class=\"origin_image zh-lightbox-thumb\" width=\"1104\" data-original=\"https://pic2.zhimg.com/v2-4f522091aae266b74c001784115419f9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1104&#39; height=&#39;517&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1104\" data-rawheight=\"517\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1104\" data-original=\"https://pic2.zhimg.com/v2-4f522091aae266b74c001784115419f9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-4f522091aae266b74c001784115419f9_b.jpg\"/></figure><p>a）items的个数为v，最大的Embedding维度为d，则将v*d的大矩阵划分成S*T个小的矩阵块。目标就是从中挑选出最优的矩阵块来作为Embedding。</p><p>b）对于SE类型：sample出了S=4，T=3，则对于该特征，生成的Embedding为（7M * 192），内存空间为7M*192个float。</p><p>c）对于ME类型：对于每个T，都sample一个S值。比如上图，sample的S数组为[2,5,0,2]，则sample出的样本为：[(3M, 192), (7M, 64)]，内存空间为3M*192+7M*64个float。</p><ul><li><b>Controller Choices</b></li></ul><p>基于增强学习算法，来sample SE和ME的Embedding blocks。论文采用A3C算法。</p><p><b>Reward</b></p><p>reward考虑两部分：模型的精度和Embedding的内存空间大小。更高的模型精读&amp;更小的内存空间。</p><p><b>Objective</b>：</p><p>对于召回和排序设定不同的评估指标。</p><p>召回：Recall@1，Sampled Recall@1；排序：ROC-AUC</p><p>Cost Loss：</p><p>内存空间：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e34124f00e1b7d015e32b6c9e211b2c8_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"184\" data-rawheight=\"39\" class=\"content_image\" width=\"184\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;184&#39; height=&#39;39&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"184\" data-rawheight=\"39\" class=\"content_image lazy\" width=\"184\" data-actualsrc=\"https://pic1.zhimg.com/v2-e34124f00e1b7d015e32b6c9e211b2c8_b.png\"/></figure><p>cost-loss：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a4aa31b0de09553068cb57c410cb6881_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"253\" data-rawheight=\"42\" class=\"content_image\" width=\"253\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;253&#39; height=&#39;42&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"253\" data-rawheight=\"42\" class=\"content_image lazy\" width=\"253\" data-actualsrc=\"https://pic2.zhimg.com/v2-a4aa31b0de09553068cb57c410cb6881_b.png\"/></figure><p><b>Training</b></p><p>Warm-up阶段：</p><p>一开始所有的Embedding Blocks都会被选中并训练（controller的参数随机初始化，并且固定不更新来实现）</p><p>warm-up之后，main model和controller交替训练。</p><h2>实验</h2><p>Embedding Blocks划分：</p><p>vocabulary size划分：[0.1v, 0.2v, 0.2v, 0.2v, 0.3v]</p><p>embedding size划分：[0.25d, 0.25d, 0.25d, 0.25d]</p><p>d = 32 * [v^0.35 / 32]</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7d36ae3fd38ad61449b3caabc345baff_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"449\" class=\"origin_image zh-lightbox-thumb\" width=\"990\" data-original=\"https://pic4.zhimg.com/v2-7d36ae3fd38ad61449b3caabc345baff_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;990&#39; height=&#39;449&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"449\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"990\" data-original=\"https://pic4.zhimg.com/v2-7d36ae3fd38ad61449b3caabc345baff_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7d36ae3fd38ad61449b3caabc345baff_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "论文", 
                    "tagLink": "https://api.zhihu.com/topics/19572442"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/82820539", 
            "userName": "定西", 
            "userLink": "https://www.zhihu.com/people/22490c77f08111a9214a5d20fe979667", 
            "upvote": 4, 
            "title": "值得反复阅读的推荐、广告方向的经典论文", 
            "content": "<p>记录一些经典的推荐、广告方向的论文，基本以工业界一线公司发的为主：</p><p>Google、FaceBook、BAT等，会保持持续更新。</p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li><b>Ad Click Prediction: a View from the Trenches</b></li><ul><li>Google，FTRL</li><li>LR的在线学习算法，即使现在大公司的模型都是DNN框架，但是其中的bias模块也都是基于FTRL的LR算法</li><li>文章同时有很多工业界的实践经验</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li><b>Deep Neural Networks for YouTube Recommendations</b> <a href=\"https://link.zhihu.com/?target=https%3A//static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45530.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">地址</a></li><ul><li>Google的YouTube团队在推荐系统上DNN方面的尝试</li><li>详细介绍了YouTube的推荐算法和架构细节，还介绍了很多实际的应用实践经验，非常值得反复阅读</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li><b>Neural Input Search for Large Scale Recommendation Models </b></li><ul><li>同样也是Google的文章，2019年的</li><li>现在工业界里面的ctr、cvr模型都是大量的embedding框架，其中embedding占据了模型的大部分参数空间，因此，如何设计每个特征的embedding维度就非常关键。</li><li>目前主流的方法都是基于业务经验来，这样往往是一个次优解。文章采用增强学习的方法自动搜寻最优embedding维度、以及语料库大小。</li><li>之前专栏分析过这篇文章：</li></ul></ul><a href=\"https://zhuanlan.zhihu.com/p/83027199\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-edce63633defe05ee96ffb11f9747682_180x120.jpg\" data-image-width=\"1494\" data-image-height=\"702\" class=\"internal\">定西：[论文分享]Google NIS论文详解</a><p><br/> <br/> <br/> <br/> </p>", 
            "topic": [
                {
                    "tag": "广告", 
                    "tagLink": "https://api.zhihu.com/topics/19553032"
                }, 
                {
                    "tag": "论文", 
                    "tagLink": "https://api.zhihu.com/topics/19572442"
                }, 
                {
                    "tag": "推荐", 
                    "tagLink": "https://api.zhihu.com/topics/19551728"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/81464000", 
            "userName": "定西", 
            "userLink": "https://www.zhihu.com/people/22490c77f08111a9214a5d20fe979667", 
            "upvote": 2, 
            "title": "BERT前世今生以及BERT在推荐中的应用", 
            "content": "<h2>背景</h2><p><b>NLP关注的四个问题</b></p><ol><li>数据源</li><li>网络结构</li><ol><li>RNN/CNN/LSTM</li><li>transformer</li></ol><li>语言模型</li><ol><li>word2vec</li><li>ELMO</li><li>GPT</li><li>BERT</li></ol><li>应用模式</li><ol><li>QA问答</li><li>文本相似度</li><li>实体识别</li><li>推理</li></ol></ol><p><b>Transformer</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-49c1f86391af516766662e18fc5e28eb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"694\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-49c1f86391af516766662e18fc5e28eb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;694&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"694\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-49c1f86391af516766662e18fc5e28eb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-49c1f86391af516766662e18fc5e28eb_b.jpg\"/></figure><p>Transformer解决RNN中的效率问题和距离问题：</p><ol><li>可以并行处理句子中的所有词</li><li>任意两个词之间的操作距离都是1</li></ol><h2>语言模型</h2><p>整体框架是，先pre-train一个模型，而后基于该模型来执行下游特定任务。</p><p>当前主要有两种方式：</p><ol><li>feature-based</li><ol><li>在大语料 A 上训练得到 Language Model </li><li>在特定的 Task 中(使用语料 B)，不调整 Language Model 的参数，使用 Language Model  得到的 embedding 作为 feature 输入</li></ol><li>fine-tuning</li><ol><li>在大语料 A 上训练得到 Language Model</li><li>在 A 的 Language Model 基础上加入特定的一些参数，在语料 B 上重新训练整个模型</li></ol></ol><h2>ELMO（feature-based）</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e677aa7ea68fc03dde5464ae82b49a66_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-e677aa7ea68fc03dde5464ae82b49a66_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;610&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-e677aa7ea68fc03dde5464ae82b49a66_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e677aa7ea68fc03dde5464ae82b49a66_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>双层bi-lstm网络结构</li><li>没有直接存embedding，而是存 LM</li><li>给予特定 Task 自己去学习获取由 ELMo 得到的 vector 在 Task 中的权重</li><li>能够学习到词汇用法的复杂性，比如语法、语义</li><li>ELMo能够学习不同上下文情况下的词汇多义性</li></ul><h2>GPT（fine-tuning）</h2><p><b>框架</b></p><ol><li>Unsupervised pre-training（多层Transformer decoder）</li></ol><ul><li>单向transformer结构</li><li>训练过程：</li><ul><li>将句子n个词的词向量(第一个为&lt;SOS&gt;)加上Positional Encoding后输入到前面提到的Transfromer中，n个输出分别预测该位置的下一个词(&lt;SOS&gt;预测句子中的第一个词，最后一个词的预测结果不用于语言模型的训练)</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-12c5f895f2e42589c7f1ea0a7110426b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"424\" data-rawheight=\"386\" class=\"origin_image zh-lightbox-thumb\" width=\"424\" data-original=\"https://pic4.zhimg.com/v2-12c5f895f2e42589c7f1ea0a7110426b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;424&#39; height=&#39;386&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"424\" data-rawheight=\"386\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"424\" data-original=\"https://pic4.zhimg.com/v2-12c5f895f2e42589c7f1ea0a7110426b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-12c5f895f2e42589c7f1ea0a7110426b_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5db37906602e86d5193d5f754e5a3a0a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"527\" data-rawheight=\"121\" class=\"origin_image zh-lightbox-thumb\" width=\"527\" data-original=\"https://pic3.zhimg.com/v2-5db37906602e86d5193d5f754e5a3a0a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;527&#39; height=&#39;121&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"527\" data-rawheight=\"121\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"527\" data-original=\"https://pic3.zhimg.com/v2-5db37906602e86d5193d5f754e5a3a0a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5db37906602e86d5193d5f754e5a3a0a_b.jpg\"/></figure><ol><li>Supervised fine-turning</li><ol><ol><li>不同的下游任务，只需要调整数据输入格式</li><li>将最后一个transformer的activation输出（hl）经过一个线性分类器来预估最终结果</li><li>为了避免Fine-Tuning陷入过拟合，采用了辅助训练目标的方法：有监督loss + 无监督loss </li></ol></ol></ol><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d188a5ae664c77e9db01f3b9d9a1d033_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"322\" data-rawheight=\"43\" class=\"content_image\" width=\"322\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;322&#39; height=&#39;43&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"322\" data-rawheight=\"43\" class=\"content_image lazy\" width=\"322\" data-actualsrc=\"https://pic4.zhimg.com/v2-d188a5ae664c77e9db01f3b9d9a1d033_b.png\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5eeed4a5e9977db81d09b658ec85ed6d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"599\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-5eeed4a5e9977db81d09b658ec85ed6d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;599&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"599\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-5eeed4a5e9977db81d09b658ec85ed6d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5eeed4a5e9977db81d09b658ec85ed6d_b.jpg\"/></figure><h2>BERT</h2><p>与GPT的异同点：</p><ol><li>都是pre-train + fine-tuning的两阶段方式</li><li>都是使用了transformer结构，但是gpt是单项transformer，bert是双向transformer</li><li>由于使用了双向transformer，所以pre-train的方法也完全不一样</li></ol><p><b>整体结构</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5a3d9a750ec804c8551bc48ebe06c7b2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"640\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-5a3d9a750ec804c8551bc48ebe06c7b2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;640&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"640\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-5a3d9a750ec804c8551bc48ebe06c7b2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5a3d9a750ec804c8551bc48ebe06c7b2_b.jpg\"/></figure><p><b>pre-training</b></p><ol><li>双向transformer（encoder模块）</li></ol><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-da6913b6b59b401cebdfd40a466fd60c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"287\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic1.zhimg.com/v2-da6913b6b59b401cebdfd40a466fd60c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;287&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"287\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic1.zhimg.com/v2-da6913b6b59b401cebdfd40a466fd60c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-da6913b6b59b401cebdfd40a466fd60c_b.jpg\"/></figure><ol><li>input representation</li></ol><ul><li>word embedding：WordPiece embeddings，30000个字典</li><li>segment embedding：区分双句（为了适应句子关系类任务）</li><li>position embedding：同transformer</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9a5c634f43f921631e72511b224c89b4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"389\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic1.zhimg.com/v2-9a5c634f43f921631e72511b224c89b4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;389&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"389\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic1.zhimg.com/v2-9a5c634f43f921631e72511b224c89b4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-9a5c634f43f921631e72511b224c89b4_b.jpg\"/></figure><ol><li>预测目标：</li></ol><ul><li>Masked LM（MLM）</li><ul><li>解决双向transformer信息泄露的问题：预测E2的时候，由于是双向的，所以自身信息已经被隐式编码进来了</li></ul></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6edea99675b55931cbe34df6102a6c9f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"535\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-6edea99675b55931cbe34df6102a6c9f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;535&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"535\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-6edea99675b55931cbe34df6102a6c9f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6edea99675b55931cbe34df6102a6c9f_b.jpg\"/></figure><ul><ul><ul><li>一个句子中随机遮蔽15%tokens，预测被遮蔽的tokens</li><li>随机遮蔽的15%tokens中：10%用正确的tokens，10%用随机的tokens（学习所有的输入embedding，而不是masked token）</li></ul></ul></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5377a6accad2dd81ada27fac8ef4e296_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"480\" data-rawheight=\"381\" class=\"origin_image zh-lightbox-thumb\" width=\"480\" data-original=\"https://pic3.zhimg.com/v2-5377a6accad2dd81ada27fac8ef4e296_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;480&#39; height=&#39;381&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"480\" data-rawheight=\"381\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"480\" data-original=\"https://pic3.zhimg.com/v2-5377a6accad2dd81ada27fac8ef4e296_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5377a6accad2dd81ada27fac8ef4e296_b.jpg\"/></figure><ul><ul><li>Next Sentence Prediction（NSP）</li><ul><li>学习句子之间的关系，使用QA类任务</li><ul><li>50% IsNext：   [CLS] 我  要  喝  水  [SEP]  我 去 给 你 倒 [SEP]</li><li>50% NotNext：  [CLS] 我 要  喝  水  [SEP]  今 天 天 气 不 错  [SEP]</li></ul></ul></ul></ul><p><b>Fine-tuning </b></p><p>不同任务采用不用的结构</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c00b975c17314d9aeb261ef7e84f5e4e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"872\" data-rawheight=\"848\" class=\"origin_image zh-lightbox-thumb\" width=\"872\" data-original=\"https://pic3.zhimg.com/v2-c00b975c17314d9aeb261ef7e84f5e4e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;872&#39; height=&#39;848&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"872\" data-rawheight=\"848\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"872\" data-original=\"https://pic3.zhimg.com/v2-c00b975c17314d9aeb261ef7e84f5e4e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c00b975c17314d9aeb261ef7e84f5e4e_b.jpg\"/></figure><h2>总结</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2002de5af23f44591adb2e133bdddf86_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1248\" data-rawheight=\"392\" class=\"origin_image zh-lightbox-thumb\" width=\"1248\" data-original=\"https://pic3.zhimg.com/v2-2002de5af23f44591adb2e133bdddf86_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1248&#39; height=&#39;392&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1248\" data-rawheight=\"392\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1248\" data-original=\"https://pic3.zhimg.com/v2-2002de5af23f44591adb2e133bdddf86_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2002de5af23f44591adb2e133bdddf86_b.jpg\"/></figure><h2>资料</h2><ol><li>ELMO：<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1802.05365\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/abs/1802.0536</span><span class=\"invisible\">5</span><span class=\"ellipsis\"></span></a></li><li>GPT：<a href=\"https://link.zhihu.com/?target=https%3A//s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">s3-us-west-2.amazonaws.com</span><span class=\"invisible\">/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf</span><span class=\"ellipsis\"></span></a></li><li>BERT：<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1810.04805\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/abs/1810.0480</span><span class=\"invisible\">5</span><span class=\"ellipsis\"></span></a></li></ol><h2>BERT4Rec：bert在推荐领域的应用</h2><p>阿里2019，比较简单，基本就是把bert结构迁移到推荐中，bert中的一个token就是推荐中用户对于商品的一次操作（比如点击）。</p><p>动机</p><ol><li>RNN/LSTM：单向模型对于hidden representation的学习比双向模型要弱</li><li>RNN/LSTM：强假设用户历史行为满足严格时序关系，但是现实中并不严格满足，因为用户的兴趣行为往往是跳跃的</li></ol><p>整体框架<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-33858671f294471f613a6bdf40821e4a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"553\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-33858671f294471f613a6bdf40821e4a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;553&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"553\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-33858671f294471f613a6bdf40821e4a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-33858671f294471f613a6bdf40821e4a_b.jpg\"/></figure><p>训练细节</p><ol><li>训练中随机mask，但是预测的时候没有mask，所以就在预测的时候，在末尾人工加上一个mask节点</li><li>训练样本中，构造一定比例的以mask结尾的样本</li></ol><p></p><p></p>", 
            "topic": [
                {
                    "tag": "推荐算法", 
                    "tagLink": "https://api.zhihu.com/topics/19580544"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/79493572", 
            "userName": "定西", 
            "userLink": "https://www.zhihu.com/people/22490c77f08111a9214a5d20fe979667", 
            "upvote": 5, 
            "title": "深度学习中如何自动学习特征交叉综述", 
            "content": "<p>Learning feature interactions in DNN without manually crafted features</p><h2>一、Abstract</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-89c197c067e36f6261332223a9e35f1c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"1196\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic1.zhimg.com/v2-89c197c067e36f6261332223a9e35f1c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;1196&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"1196\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic1.zhimg.com/v2-89c197c067e36f6261332223a9e35f1c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-89c197c067e36f6261332223a9e35f1c_b.jpg\"/></figure><p>（图片引用网络，侵权联系本人删除）。<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p> 特征工程的重要性。nontrivial &amp; manual feature engineering or exhaustive searching。</p><ol><li>获取高质量交叉特征代价很大。</li><li>在大规模系统中，无法获取所有有效的交叉特征。</li><li>人工交叉特征不具备泛化性。</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>ResNet 2015 CVPR  微软</li><li>Deep Crossing 2016 KDD </li><li>wide &amp; deep  2016 KDD</li><li>deepFM  2017 IJCAI  华为</li><li>Deep &amp; Cross （DCN） 2017</li><li>xDeepFM    2018 KDD</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2>二、Learning Feature Interactions</h2><p>2.1 Deep Crossing</p><ol><li>ResNet </li></ol><ul><li>理想情况，网络越深效果越好</li></ul><ol><li>能够提取到不同level的特征越丰富。</li><li>越深的网络提取的特征越抽象，越具有语义信息</li></ol><ul><li>简单增加网络深度的问题 </li></ul><ol><li>梯度消失、梯度爆炸 </li><ol><li>可以通过Batch Normalization解决</li></ol><li>退化问题（degradation problem）</li><ol><li>not caused by overfitting</li><li>adding more layer =&gt; higher train error</li></ol></ol><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8b8629c94d1dcfc9c53f4dfbd63e8138_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"279\" class=\"origin_image zh-lightbox-thumb\" width=\"500\" data-original=\"https://pic1.zhimg.com/v2-8b8629c94d1dcfc9c53f4dfbd63e8138_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;500&#39; height=&#39;279&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"279\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"500\" data-original=\"https://pic1.zhimg.com/v2-8b8629c94d1dcfc9c53f4dfbd63e8138_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8b8629c94d1dcfc9c53f4dfbd63e8138_b.jpg\"/></figure><p><br/> </p><figure data-size=\"small\"><noscript><img src=\"https://pic1.zhimg.com/v2-cc3077df4441bd2e285515113108f9e4_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"932\" data-rawheight=\"1782\" class=\"origin_image zh-lightbox-thumb\" width=\"932\" data-original=\"https://pic1.zhimg.com/v2-cc3077df4441bd2e285515113108f9e4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;932&#39; height=&#39;1782&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"932\" data-rawheight=\"1782\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"932\" data-original=\"https://pic1.zhimg.com/v2-cc3077df4441bd2e285515113108f9e4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-cc3077df4441bd2e285515113108f9e4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ul><ul><li>为什么拟合残差更加容易？ </li><li><b>引入残差后的映射对输出的变化更敏感</b>。 </li><ul><li>F是求和前网络映射，H是从输入到求和后的网络映射。比如把5映射到5.1，那么引入残差前是F’(5)=5.1，引入残差后是H(5)=5.1, H(5)=F(5)+5, F(5)=0.1。这里的F’和F都表示网络参数映射。</li><li>如果输出从5.1变到5.2，映射F’的输出增加了1/51=2%，而对于残差结构输出从5.1到5.2，映射F是从0.1到0.2，增加了100%。</li><li>明显后者输出变化对权重的调整作用更大，所以效果更好。</li></ul><li><b>残差的思想都是去掉相同的主体部分，从而突出微小的变化，类似差分放大器。</b></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ol><li>MODEL ARCHITECTURE </li></ol><figure data-size=\"small\"><noscript><img src=\"https://pic3.zhimg.com/v2-7d1df6f70ed6d813b5a4be94e88da36e_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1200\" data-rawheight=\"965\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-7d1df6f70ed6d813b5a4be94e88da36e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;965&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1200\" data-rawheight=\"965\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-7d1df6f70ed6d813b5a4be94e88da36e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7d1df6f70ed6d813b5a4be94e88da36e_b.jpg\"/></figure><ol><ol><li><b>the Embedding Layer</b></li></ol></ol><figure data-size=\"small\"><noscript><img src=\"https://pic1.zhimg.com/v2-76265c1194edd8d3c3deadd3a08de9f4_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"926\" data-rawheight=\"178\" class=\"origin_image zh-lightbox-thumb\" width=\"926\" data-original=\"https://pic1.zhimg.com/v2-76265c1194edd8d3c3deadd3a08de9f4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;926&#39; height=&#39;178&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"926\" data-rawheight=\"178\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"926\" data-original=\"https://pic1.zhimg.com/v2-76265c1194edd8d3c3deadd3a08de9f4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-76265c1194edd8d3c3deadd3a08de9f4_b.jpg\"/></figure><ol><ol><li><b>the Stacking Layer</b></li></ol></ol><figure data-size=\"small\"><noscript><img src=\"https://pic1.zhimg.com/v2-397a238c0996f6bee97f4ba8d15ce16c_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"868\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb\" width=\"868\" data-original=\"https://pic1.zhimg.com/v2-397a238c0996f6bee97f4ba8d15ce16c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;868&#39; height=&#39;150&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"868\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"868\" data-original=\"https://pic1.zhimg.com/v2-397a238c0996f6bee97f4ba8d15ce16c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-397a238c0996f6bee97f4ba8d15ce16c_b.jpg\"/></figure><ol><ol><li><b>the Residual Unit</b><br/> </li></ol></ol><figure data-size=\"small\"><noscript><img src=\"https://pic3.zhimg.com/v2-f4663db25c85e4f1f1532a21ad5656be_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1070\" data-rawheight=\"828\" class=\"origin_image zh-lightbox-thumb\" width=\"1070\" data-original=\"https://pic3.zhimg.com/v2-f4663db25c85e4f1f1532a21ad5656be_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1070&#39; height=&#39;828&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1070\" data-rawheight=\"828\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1070\" data-original=\"https://pic3.zhimg.com/v2-f4663db25c85e4f1f1532a21ad5656be_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f4663db25c85e4f1f1532a21ad5656be_b.jpg\"/></figure><figure data-size=\"small\"><noscript><img src=\"https://pic2.zhimg.com/v2-e0c0dea57c1b9067b592ef18ba4ba3c1_b.png\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1200\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-e0c0dea57c1b9067b592ef18ba4ba3c1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;124&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1200\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-e0c0dea57c1b9067b592ef18ba4ba3c1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e0c0dea57c1b9067b592ef18ba4ba3c1_b.png\"/></figure><ol><ol><li><b>the Scoring Layer</b></li></ol><li>Deep crossing vs DSSM</li></ol><ul><li>Deep crossing的两个优化：</li></ul><ol><li>引入残差网络结构</li><li>特征交叉前置</li></ol><ul><li>Early Crossing vs. Late Crossing <br/> </li></ul><figure data-size=\"small\"><noscript><img src=\"https://pic3.zhimg.com/v2-ad1e0384be7fdf5f419ed0141e8f700a_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1104\" data-rawheight=\"1098\" class=\"origin_image zh-lightbox-thumb\" width=\"1104\" data-original=\"https://pic3.zhimg.com/v2-ad1e0384be7fdf5f419ed0141e8f700a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1104&#39; height=&#39;1098&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1104\" data-rawheight=\"1098\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1104\" data-original=\"https://pic3.zhimg.com/v2-ad1e0384be7fdf5f419ed0141e8f700a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ad1e0384be7fdf5f419ed0141e8f700a_b.jpg\"/></figure><p>2.2 Deep &amp; Cross</p><ol><li>MODEL ARCHITECTURE</li></ol><figure data-size=\"small\"><noscript><img src=\"https://pic3.zhimg.com/v2-94e2dac9278198b0f0f0879ea8619792_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1200\" data-rawheight=\"1298\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-94e2dac9278198b0f0f0879ea8619792_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;1298&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1200\" data-rawheight=\"1298\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-94e2dac9278198b0f0f0879ea8619792_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-94e2dac9278198b0f0f0879ea8619792_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ol><ol><li>Embedding and Stacking Layer </li></ol></ol><figure data-size=\"small\"><noscript><img src=\"https://pic1.zhimg.com/v2-fa15709ccba004746781f815b93dc644_b.png\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"640\" data-rawheight=\"90\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-fa15709ccba004746781f815b93dc644_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;90&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"640\" data-rawheight=\"90\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-fa15709ccba004746781f815b93dc644_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-fa15709ccba004746781f815b93dc644_b.png\"/></figure><figure data-size=\"small\"><noscript><img src=\"https://pic1.zhimg.com/v2-ce7761d70617bad6c26bfed91293a880_b.png\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"958\" data-rawheight=\"138\" class=\"origin_image zh-lightbox-thumb\" width=\"958\" data-original=\"https://pic1.zhimg.com/v2-ce7761d70617bad6c26bfed91293a880_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;958&#39; height=&#39;138&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"958\" data-rawheight=\"138\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"958\" data-original=\"https://pic1.zhimg.com/v2-ce7761d70617bad6c26bfed91293a880_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ce7761d70617bad6c26bfed91293a880_b.png\"/></figure><ol><ol><li>Cross Network</li></ol></ol><figure data-size=\"small\"><noscript><img src=\"https://pic3.zhimg.com/v2-8c7750763d3ecb19172e70c7d090f35a_b.png\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1200\" data-rawheight=\"125\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-8c7750763d3ecb19172e70c7d090f35a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;125&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1200\" data-rawheight=\"125\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-8c7750763d3ecb19172e70c7d090f35a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8c7750763d3ecb19172e70c7d090f35a_b.png\"/></figure><figure data-size=\"small\"><noscript><img src=\"https://pic3.zhimg.com/v2-fb3df74b80747077bf937bd4a456216a_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1200\" data-rawheight=\"615\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-fb3df74b80747077bf937bd4a456216a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;615&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1200\" data-rawheight=\"615\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-fb3df74b80747077bf937bd4a456216a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fb3df74b80747077bf937bd4a456216a_b.jpg\"/></figure><ol><ol><li>Deep Network</li></ol></ol><figure data-size=\"small\"><noscript><img src=\"https://pic3.zhimg.com/v2-52a453427b7b3904f70664e1d573c97a_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"672\" data-rawheight=\"116\" class=\"origin_image zh-lightbox-thumb\" width=\"672\" data-original=\"https://pic3.zhimg.com/v2-52a453427b7b3904f70664e1d573c97a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;672&#39; height=&#39;116&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"672\" data-rawheight=\"116\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"672\" data-original=\"https://pic3.zhimg.com/v2-52a453427b7b3904f70664e1d573c97a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-52a453427b7b3904f70664e1d573c97a_b.jpg\"/></figure><ol><ol><li>Combination Layer</li></ol></ol><figure data-size=\"small\"><noscript><img src=\"https://pic3.zhimg.com/v2-20f0187151884c3b04703276902c9b8a_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"844\" data-rawheight=\"202\" class=\"origin_image zh-lightbox-thumb\" width=\"844\" data-original=\"https://pic3.zhimg.com/v2-20f0187151884c3b04703276902c9b8a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;844&#39; height=&#39;202&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"844\" data-rawheight=\"202\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"844\" data-original=\"https://pic3.zhimg.com/v2-20f0187151884c3b04703276902c9b8a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-20f0187151884c3b04703276902c9b8a_b.jpg\"/></figure><ol><li>more on Cross Network</li><ol><li>每层的神经元个数都相同，都等于输入 x0的维度d,也即每层的输入输出维度都是相等的；</li><li>受残差网络（Residual Network）结构启发，每层的函数f拟合的是xl1 - xl 的残差，残差网络有很多优点，其中一点是处理梯度消失的问题，使网络可以“更深”。</li><li>Cross Network中的降维</li></ol></ol><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d59fb67fd96f3c00b9d931aebf956348_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"488\" data-rawheight=\"143\" class=\"origin_image zh-lightbox-thumb\" width=\"488\" data-original=\"https://pic1.zhimg.com/v2-d59fb67fd96f3c00b9d931aebf956348_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;488&#39; height=&#39;143&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"488\" data-rawheight=\"143\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"488\" data-original=\"https://pic1.zhimg.com/v2-d59fb67fd96f3c00b9d931aebf956348_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d59fb67fd96f3c00b9d931aebf956348_b.jpg\"/></figure><p>2.3 xDeepFM</p><p>2.3.1 Back to DCN</p><ul><li>DCN learns a special type of high-order feature interactions</li><ul><li>the output of CrossNet is limited <b>in a special form</b></li><li>interactions come in a <b>bit-wise fashion</b></li></ul></ul><p><br/> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ab8ef1068510fcb49fe2052bca066b2f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"972\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-ab8ef1068510fcb49fe2052bca066b2f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;972&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"972\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-ab8ef1068510fcb49fe2052bca066b2f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ab8ef1068510fcb49fe2052bca066b2f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>2.3.2 key point to xDeepFM</p><ol><li>jointly learns explicit and implicit high-order feature interactions </li><li>feature interact at the vector-wise level rather than the bit-wise level</li><ol><li>bit-wise: even the elements within the same field embedding vector will influence each other</li><li>vector-wise: embedding vector is regarded as a unit for vector-wise interactions</li><ol><li>FM (vector-wise):</li></ol></ol></ol><figure data-size=\"small\"><noscript><img src=\"https://pic4.zhimg.com/v2-6f350341240efb277451959433a2962f_b.png\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"998\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb\" width=\"998\" data-original=\"https://pic4.zhimg.com/v2-6f350341240efb277451959433a2962f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;998&#39; height=&#39;150&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"998\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"998\" data-original=\"https://pic4.zhimg.com/v2-6f350341240efb277451959433a2962f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6f350341240efb277451959433a2962f_b.png\"/></figure><ol><ol><ol><li>DCN(bit-wise): </li></ol></ol></ol><figure data-size=\"small\"><noscript><img src=\"https://pic2.zhimg.com/v2-aa61e687c71f9484c3227cdf914ee8a5_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1200\" data-rawheight=\"1200\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-aa61e687c71f9484c3227cdf914ee8a5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;1200&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1200\" data-rawheight=\"1200\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-aa61e687c71f9484c3227cdf914ee8a5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-aa61e687c71f9484c3227cdf914ee8a5_b.jpg\"/></figure><p>2.3.3 <b>Compressed Interaction Network （CIN）</b></p><p><br/> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0e7ba4712f0c51c959865294be602495_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"487\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-0e7ba4712f0c51c959865294be602495_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;487&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"487\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-0e7ba4712f0c51c959865294be602495_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0e7ba4712f0c51c959865294be602495_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-50b0f86a9f278b69657ac171c86b2a19_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"316\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-50b0f86a9f278b69657ac171c86b2a19_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;316&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"316\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-50b0f86a9f278b69657ac171c86b2a19_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-50b0f86a9f278b69657ac171c86b2a19_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Space Complexity</b></p><p><b>Time Complexity</b></p><p><b>Polynomial Approximation</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-061a74f73eef2554fd04e9e6fe3eed28_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"436\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb\" width=\"436\" data-original=\"https://pic1.zhimg.com/v2-061a74f73eef2554fd04e9e6fe3eed28_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;436&#39; height=&#39;168&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"436\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"436\" data-original=\"https://pic1.zhimg.com/v2-061a74f73eef2554fd04e9e6fe3eed28_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-061a74f73eef2554fd04e9e6fe3eed28_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"small\"><noscript><img src=\"https://pic1.zhimg.com/v2-1497bfc28c137339b4c6306e83d3fd04_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"732\" data-rawheight=\"326\" class=\"origin_image zh-lightbox-thumb\" width=\"732\" data-original=\"https://pic1.zhimg.com/v2-1497bfc28c137339b4c6306e83d3fd04_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;732&#39; height=&#39;326&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"732\" data-rawheight=\"326\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"732\" data-original=\"https://pic1.zhimg.com/v2-1497bfc28c137339b4c6306e83d3fd04_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1497bfc28c137339b4c6306e83d3fd04_b.jpg\"/></figure><p><br/> </p><figure data-size=\"small\"><noscript><img src=\"https://pic1.zhimg.com/v2-f243a3ad762d16346411273c19fe80dc_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"940\" data-rawheight=\"328\" class=\"origin_image zh-lightbox-thumb\" width=\"940\" data-original=\"https://pic1.zhimg.com/v2-f243a3ad762d16346411273c19fe80dc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;940&#39; height=&#39;328&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"940\" data-rawheight=\"328\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"940\" data-original=\"https://pic1.zhimg.com/v2-f243a3ad762d16346411273c19fe80dc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f243a3ad762d16346411273c19fe80dc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>Combination with Implicit Networks</p><p><br/> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-61c886e7e06e0dbff07417e57c6b897d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"938\" data-rawheight=\"626\" class=\"origin_image zh-lightbox-thumb\" width=\"938\" data-original=\"https://pic2.zhimg.com/v2-61c886e7e06e0dbff07417e57c6b897d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;938&#39; height=&#39;626&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"938\" data-rawheight=\"626\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"938\" data-original=\"https://pic2.zhimg.com/v2-61c886e7e06e0dbff07417e57c6b897d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-61c886e7e06e0dbff07417e57c6b897d_b.jpg\"/></figure><figure data-size=\"small\"><noscript><img src=\"https://pic3.zhimg.com/v2-592a0a52276bfb6e3a9da803df3df7d2_b.png\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1200\" data-rawheight=\"147\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-592a0a52276bfb6e3a9da803df3df7d2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;147&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1200\" data-rawheight=\"147\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-592a0a52276bfb6e3a9da803df3df7d2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-592a0a52276bfb6e3a9da803df3df7d2_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>三、Summary</h2><figure data-size=\"small\"><noscript><img src=\"https://pic4.zhimg.com/v2-a6cc777df9efa29235807cba7f1dff7f_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1100\" data-rawheight=\"1066\" class=\"origin_image zh-lightbox-thumb\" width=\"1100\" data-original=\"https://pic4.zhimg.com/v2-a6cc777df9efa29235807cba7f1dff7f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1100&#39; height=&#39;1066&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1100\" data-rawheight=\"1066\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1100\" data-original=\"https://pic4.zhimg.com/v2-a6cc777df9efa29235807cba7f1dff7f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a6cc777df9efa29235807cba7f1dff7f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>DNN DCN xDeepFM &gt; FM =&gt; 高阶交叉特征对于稀疏数据集确实是有必要的</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e7155cd1b3d40700b6cd73dfb71cef78_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"424\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic1.zhimg.com/v2-e7155cd1b3d40700b6cd73dfb71cef78_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;424&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"424\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic1.zhimg.com/v2-e7155cd1b3d40700b6cd73dfb71cef78_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e7155cd1b3d40700b6cd73dfb71cef78_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>LR最差: factorization-based models 的必要性</li><li>Wide &amp; Deep, DCN, DeepFM, xDeepFM &gt; DNN：incorporating hybrid components 的必要性</li><li>xDeepFM: 显式&amp;隐式学习高阶特征交叉的必要性</li><li>所有neural-based model 深度最多3、4层</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b28b159b69da923f226ec40b4d6d87b3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"803\" data-rawheight=\"453\" class=\"origin_image zh-lightbox-thumb\" width=\"803\" data-original=\"https://pic4.zhimg.com/v2-b28b159b69da923f226ec40b4d6d87b3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;803&#39; height=&#39;453&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"803\" data-rawheight=\"453\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"803\" data-original=\"https://pic4.zhimg.com/v2-b28b159b69da923f226ec40b4d6d87b3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b28b159b69da923f226ec40b4d6d87b3_b.jpg\"/></figure><h2>四、References</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>ResNet （2015）</p><p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1603.05027.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1603.0502</span><span class=\"invisible\">7.pdf</span><span class=\"ellipsis\"></span></a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//share.leanote.cn/s/5b0eaf5a7968705f10000002\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">share.leanote.cn/s/5b0e</span><span class=\"invisible\">af5a7968705f10000002</span><span class=\"ellipsis\"></span></a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/5gFpyZBUzUz0Y_culZGTFQ\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">mp.weixin.qq.com/s/5gFp</span><span class=\"invisible\">yZBUzUz0Y_culZGTFQ</span><span class=\"ellipsis\"></span></a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.jianshu.com/p/11f1a979b384\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">jianshu.com/p/11f1a979b</span><span class=\"invisible\">384</span><span class=\"ellipsis\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>Deep Crossing （KDD 2016 微软 bing）</p><ol><li><a href=\"https://link.zhihu.com/?target=https%3A//www.kdd.org/kdd2016/papers/files/adf0975-shanA.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">kdd.org/kdd2016/papers/</span><span class=\"invisible\">files/adf0975-shanA.pdf</span><span class=\"ellipsis\"></span></a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//nirvanada.github.io/2017/12/14/DCN/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">nirvanada.github.io/201</span><span class=\"invisible\">7/12/14/DCN/</span><span class=\"ellipsis\"></span></a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//fuhailin.github.io/Deep-and-Cross-Network/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">fuhailin.github.io/Deep</span><span class=\"invisible\">-and-Cross-Network/</span><span class=\"ellipsis\"></span></a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/Dby_freedom/article/details/86502623\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/Dby_freed</span><span class=\"invisible\">om/article/details/86502623</span><span class=\"ellipsis\"></span></a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//www.jianshu.com/p/e58437f39f65\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">jianshu.com/p/e58437f39</span><span class=\"invisible\">f65</span><span class=\"ellipsis\"></span></a></li></ol><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><p>wide &amp; deep (KDD 2016)</p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.kdd.org/kdd2016/papers/files/adf0975-shanA.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">kdd.org/kdd2016/papers/</span><span class=\"invisible\">files/adf0975-shanA.pdf</span><span class=\"ellipsis\"></span></a></p><p>deepFM 2017</p><p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1703.04247\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/abs/1703.0424</span><span class=\"invisible\">7</span><span class=\"ellipsis\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><p>DCN （2017 google &amp; stanford）</p><p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1708.05123\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/abs/1708.0512</span><span class=\"invisible\">3</span><span class=\"ellipsis\"></span></a></p><p><a href=\"https://zhuanlan.zhihu.com/p/55234968\" class=\"internal\"><span class=\"invisible\">https://</span><span class=\"visible\">zhuanlan.zhihu.com/p/55</span><span class=\"invisible\">234968</span><span class=\"ellipsis\"></span></a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//videolectures.net/kdd2017_wang_click_predictions/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">videolectures.net/kdd20</span><span class=\"invisible\">17_wang_click_predictions/</span><span class=\"ellipsis\"></span></a></p><p>DCN talk：<a href=\"https://link.zhihu.com/?target=http%3A//videolectures.net/kdd2017_wang_click_predictions/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">videolectures.net/kdd20</span><span class=\"invisible\">17_wang_click_predictions/</span><span class=\"ellipsis\"></span></a> </p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><p>xDeepFM （KDD 2018）</p><p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1803.05170\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/abs/1803.0517</span><span class=\"invisible\">0</span><span class=\"ellipsis\"></span></a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.jianshu.com/p/b4128bc79df0\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">jianshu.com/p/b4128bc79</span><span class=\"invisible\">df0</span><span class=\"ellipsis\"></span></a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//xudongyang.coding.me/xdeepfm/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">xudongyang.coding.me/xd</span><span class=\"invisible\">eepfm/</span><span class=\"ellipsis\"></span></a></p><ol><ol><li>FM模型中的隐向量也可以理解为embedding向量。Embedding向量中的元素用术语bit表示，可以看出plain-DNN的高阶特征交互建模是元素级的（bit-wise），也就是说同一个域对应的embedding向量中的元素也会相互影响</li><li>FM类方法是以向量级（vector-wise）的方式来构建高阶交叉关系。经验上，vector-wise的方式构建的特征交叉关系比bit-wise的方式更容易学习。</li><li>DeepFM模型融合了FM和WDL模型，其FM部分实现了低阶特征和vector-wise的二阶交叉特征建模，其Deep部分使模型具有了bit-wise的高阶交叉特征建模的能力</li></ol></ol>", 
            "topic": [
                {
                    "tag": "推荐算法", 
                    "tagLink": "https://api.zhihu.com/topics/19580544"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/79492868", 
            "userName": "定西", 
            "userLink": "https://www.zhihu.com/people/22490c77f08111a9214a5d20fe979667", 
            "upvote": 1, 
            "title": "阿里深度兴趣网络详解（DIN）", 
            "content": "<p><b>Deep Interest Network for Click-Through Rate Prediction（KDD2018）</b></p><h2>Motivation</h2><p>wide&amp;deep、deepFM、deep&amp;cross、YouTube Recommend CTR model等模型结构，套路基本一样：</p><p><b>Sparse features -&gt; Embedding Vector -&gt; MLPs -&gt; Sigmoid -&gt; Output</b></p><p>将大规模稀疏特征映射到固定长度的低维度embedding向量，再通过pooling（group-wise manner）映射到固定长度的特征长度，concat起来后通过全连接层模块，最后通过sigmod/softmax层输出。</p><p>用户行为特性：</p><ul><ul><li><b>Diversity：</b>用户对于广告的兴趣是多样的</li><li><b>Local activation：</b>用户是否点击某个广告，往往只基于他过去部分的行为兴趣，而不是全部行为兴趣。</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ba6ba91590bef0281998091f8e66506f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1000\" data-rawheight=\"220\" class=\"origin_image zh-lightbox-thumb\" width=\"1000\" data-original=\"https://pic4.zhimg.com/v2-ba6ba91590bef0281998091f8e66506f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1000&#39; height=&#39;220&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1000\" data-rawheight=\"220\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1000\" data-original=\"https://pic4.zhimg.com/v2-ba6ba91590bef0281998091f8e66506f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ba6ba91590bef0281998091f8e66506f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如果把用户的历史行为映射到固定长度的低维向量，会丢失这部分信息（对于要预估的不同的广告，embedding都一样）。</p><p>Deep Interest Network (DIN) 采用来类似attention机制实现用户兴趣的Diversity和Local Activation。直观理解，Attention机制就是对不同特征赋予不同weight，这样某些weight高的特征便会主导这一次的预测，就好像模型对这些特征pay attention。<b>DIN针对当前候选广告局部地激活用户的历史兴趣，赋予和候选广告相关的历史兴趣更高的weight，从而实现Local Activation，而weight的多样性同时也实现了用户兴趣的多样性表达</b>。</p><h2>Main Contributions</h2><ol><ol><li>提出DIN结构，解决了以往采用固定长度向量表达用户兴趣多样性的问题。</li><li>提出两种DNN训练技巧：</li><ol><li>mini-batch aware regularizer 自适应正则</li><li>data adaptive activation function  新的激活函数</li></ol></ol></ol><h2>Deep Interest Network</h2><ul><li>3.1 <b>Feature Representation</b></li><ul><li>one-hot encoding &amp; multi-hot encoding</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-145760c1bc645fac3c062f3e91343013_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"789\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-145760c1bc645fac3c062f3e91343013_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;789&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"789\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-145760c1bc645fac3c062f3e91343013_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-145760c1bc645fac3c062f3e91343013_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4484f43b60c528f0770e8a910627dfa4_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic1.zhimg.com/v2-4484f43b60c528f0770e8a910627dfa4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;160&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic1.zhimg.com/v2-4484f43b60c528f0770e8a910627dfa4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-4484f43b60c528f0770e8a910627dfa4_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>3.2 <b>Base Model</b></li><ul><li>embedding layer</li><li>pooling layer and concat layer</li><ul><li>sum pooling and average pooling</li></ul><li>MLP</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fc5ae1496a23eeb539ad28fe76a045e6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"803\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-fc5ae1496a23eeb539ad28fe76a045e6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;803&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"803\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-fc5ae1496a23eeb539ad28fe76a045e6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fc5ae1496a23eeb539ad28fe76a045e6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>3.3 <b>The structure of Deep Interest Network</b></li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e16258fc999618d8fd1028e3e47b092f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"685\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-e16258fc999618d8fd1028e3e47b092f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;685&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"685\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-e16258fc999618d8fd1028e3e47b092f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e16258fc999618d8fd1028e3e47b092f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>用户向量</b>：weighted sum pooling</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-31df1f40e34b2f8e144aa28dbc833023_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"147\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-31df1f40e34b2f8e144aa28dbc833023_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;147&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"147\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-31df1f40e34b2f8e144aa28dbc833023_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-31df1f40e34b2f8e144aa28dbc833023_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Local activation unit</b>：</p><p>1）DIN的关键点在于 AU 的设计，DIN会计算候选广告与用户最近N个历史行为商品的相关性权重weight，将其作为加权系数来对这N个行为商品的embeddings做sum pooling，用户兴趣正是由这个加权求和后的embedding来体现。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e0dd9a0e107105ab81bef1ebc802bedb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1156\" data-rawheight=\"1142\" class=\"origin_image zh-lightbox-thumb\" width=\"1156\" data-original=\"https://pic4.zhimg.com/v2-e0dd9a0e107105ab81bef1ebc802bedb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1156&#39; height=&#39;1142&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1156\" data-rawheight=\"1142\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1156\" data-original=\"https://pic4.zhimg.com/v2-e0dd9a0e107105ab81bef1ebc802bedb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e0dd9a0e107105ab81bef1ebc802bedb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>2）与传统Attention的差异：</p><p>传统attention的约束：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d29548511cd1647a87fd44e28ca2f4bb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"326\" data-rawheight=\"116\" class=\"content_image\" width=\"326\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;326&#39; height=&#39;116&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"326\" data-rawheight=\"116\" class=\"content_image lazy\" width=\"326\" data-actualsrc=\"https://pic4.zhimg.com/v2-d29548511cd1647a87fd44e28ca2f4bb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>Local activation unit并没有这个约束：reserve the intensity of user interests。</p><p>举例：如果一个用户历史90%是衣服，10%是电子产品。对于T-shirt和手机这两个候选广告，T-shirt能够激活大部分历史行为，因此与手机相对，能够得到更大的VU（表示了用户的兴趣强度）。</p><p>3）文章也采用了LSTM来建模用户的历史行为并没有效果。与传统NLP中文本中上下文具有比较强的语法约束不同，用户的历史兴趣行为可能同时包含多个兴趣。用户的历史行为可能在这些兴趣中随意切换，导致用户的行为序列产生噪音。</p><h2>Training Techniques</h2><p>4.1 mini-batch aware Regularization （自适应正则）</p><p>在大规模稀疏网络中，由于特征具有“长尾效应”，因此模型很容易过拟合。通过的加入正则化的话（L1、L2），会映入大量的参数更新。</p><p>作为对比，如果采用SGD，没有正则的话，大部分特征为0，只有少部分特征非0，特征为0的那部分参数就不需要计算梯度，但是引入正则化以后，不管特征是不是0，都需要计算梯度，对大规模的稀疏特征，参数规模也非常庞大，增加的计算量就非常大。</p><p><b>自适应正则方法</b>：</p><p>1）对于一个mini-batch，只需要在该batch上feature非0对应的参数上计算L2正则</p><p>2）正则的强度与特征的频率有关，频率越高，正则越小，频率越低，正则越大。</p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li><br/></li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-63a71eb541d5bc5bb7898dd59bf4b739_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"234\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-63a71eb541d5bc5bb7898dd59bf4b739_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;234&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"234\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-63a71eb541d5bc5bb7898dd59bf4b739_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-63a71eb541d5bc5bb7898dd59bf4b739_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>Bm表示batch大小，nj表示特征在所有样本中出现的次数，mj=1表示当前batch中特征j至少有一个样本非0。</p><p>4.2 data adaptive activation function（Dice）</p><p><b>PReLU</b>的问题：固定以0为切割点（hard rectified point），与输入的分布无关。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-96ae07f14531851650bb879507fdc3a1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"215\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-96ae07f14531851650bb879507fdc3a1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;215&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"215\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-96ae07f14531851650bb879507fdc3a1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-96ae07f14531851650bb879507fdc3a1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Dice</b>：随输入数据的分布，动态调整hard rectified point（期望）</p><p>训练阶段：E(s) 、Var[s]为当前batch的期望和方差</p><p>测试阶段：滑动来计算E(s)、Var[s]</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-2f5b446384bfb4b29d88c21555b2e9f1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"200\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-2f5b446384bfb4b29d88c21555b2e9f1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;200&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"200\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-2f5b446384bfb4b29d88c21555b2e9f1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-2f5b446384bfb4b29d88c21555b2e9f1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-979078848e73ba2f9e32f3f87dbf8be5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"569\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-979078848e73ba2f9e32f3f87dbf8be5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;569&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"569\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-979078848e73ba2f9e32f3f87dbf8be5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-979078848e73ba2f9e32f3f87dbf8be5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>Experiments</h2><p>5.1 datasets</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-85226851fbf20c011150f8aa3b0240ba_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"424\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-85226851fbf20c011150f8aa3b0240ba_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;424&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"424\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-85226851fbf20c011150f8aa3b0240ba_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-85226851fbf20c011150f8aa3b0240ba_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>5.2 competitors</p><p>LR、BaseModel、wide&amp;deep、PNN、DeepFM</p><p>5.3 Metrics</p><p><b>user weighted AUC</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-05f77e09b32493825161118bf01bee72_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"238\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-05f77e09b32493825161118bf01bee72_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;238&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"238\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-05f77e09b32493825161118bf01bee72_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-05f77e09b32493825161118bf01bee72_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>RelaIMpr</b>：measure relative improvement over models</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-eab880c4cdc6290b351007bcc2cfae03_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"188\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-eab880c4cdc6290b351007bcc2cfae03_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;188&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"188\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-eab880c4cdc6290b351007bcc2cfae03_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-eab880c4cdc6290b351007bcc2cfae03_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>5.4 模型结构对比</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3e7d2ea6845fae34ac9f250a6e2cd8cf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"810\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-3e7d2ea6845fae34ac9f250a6e2cd8cf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;810&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"810\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-3e7d2ea6845fae34ac9f250a6e2cd8cf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3e7d2ea6845fae34ac9f250a6e2cd8cf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-fd957a147b71401d27d4dc4d4d66fd71_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"701\" data-rawheight=\"614\" class=\"origin_image zh-lightbox-thumb\" width=\"701\" data-original=\"https://pic2.zhimg.com/v2-fd957a147b71401d27d4dc4d4d66fd71_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;701&#39; height=&#39;614&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"701\" data-rawheight=\"614\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"701\" data-original=\"https://pic2.zhimg.com/v2-fd957a147b71401d27d4dc4d4d66fd71_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-fd957a147b71401d27d4dc4d4d66fd71_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>5.5 正则化对比</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-94262cb998be08da1f49dbdebf3269a9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"440\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-94262cb998be08da1f49dbdebf3269a9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;440&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"440\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-94262cb998be08da1f49dbdebf3269a9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-94262cb998be08da1f49dbdebf3269a9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-de9fcf70b7bd3a15e750e88e8d8bac29_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"835\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb\" width=\"835\" data-original=\"https://pic2.zhimg.com/v2-de9fcf70b7bd3a15e750e88e8d8bac29_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;835&#39; height=&#39;430&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"835\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"835\" data-original=\"https://pic2.zhimg.com/v2-de9fcf70b7bd3a15e750e88e8d8bac29_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-de9fcf70b7bd3a15e750e88e8d8bac29_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3513a3bb9b956501b6a332e104e74d02_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"689\" data-rawheight=\"313\" class=\"origin_image zh-lightbox-thumb\" width=\"689\" data-original=\"https://pic3.zhimg.com/v2-3513a3bb9b956501b6a332e104e74d02_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;689&#39; height=&#39;313&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"689\" data-rawheight=\"313\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"689\" data-original=\"https://pic3.zhimg.com/v2-3513a3bb9b956501b6a332e104e74d02_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-3513a3bb9b956501b6a332e104e74d02_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>5.6 DIN可视化</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8f2237c1df8b154aa9290f953f0cd4db_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"765\" data-rawheight=\"958\" class=\"origin_image zh-lightbox-thumb\" width=\"765\" data-original=\"https://pic4.zhimg.com/v2-8f2237c1df8b154aa9290f953f0cd4db_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;765&#39; height=&#39;958&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"765\" data-rawheight=\"958\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"765\" data-original=\"https://pic4.zhimg.com/v2-8f2237c1df8b154aa9290f953f0cd4db_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8f2237c1df8b154aa9290f953f0cd4db_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ol><li>引用</li><li><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1706.06978.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1706.0697</span><span class=\"invisible\">8.pdf</span><span class=\"ellipsis\"></span></a></li></ol>", 
            "topic": [
                {
                    "tag": "推荐算法", 
                    "tagLink": "https://api.zhihu.com/topics/19580544"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_1134785076926672896"
}
